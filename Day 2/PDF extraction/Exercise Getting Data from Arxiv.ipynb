{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accesing *ArXiv*\n",
    "\n",
    "## Elena Fernández Fernández"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use **again** the ArXiv API to do a new query: https://pypi.org/project/arxiv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArXiv API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the default API client.\n",
    "client = arxiv.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's extract the 50 most recent Twitter ArXiv articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the 50 most recent articles matching the keyword \"facebook\"\n",
    "search = arxiv.Search(\n",
    "  query = 'facebook', #if we want to do a query that contains two terms we need to add double quotation marks. For exampke: '\"climate change\"'\n",
    "  max_results = 50,\n",
    "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = client.results(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<itertools.islice object at 0x000001486FE02390>\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now let's first have a look at the titles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automated Demand Forecasting in small to medium-sized enterprises\n",
      "Integrating Zero-Shot Classification to Advance Long COVID Literature: A Systematic Social Media-Centered Review\n",
      "Pirates of Charity: Exploring Donation-based Abuses in Social Media Platforms\n",
      "ScamChatBot: An End-to-End Analysis of Fake Account Recovery on Social Media via Chatbots\n",
      "Exploration of the Dynamics of Buy and Sale of Social Media Accounts\n",
      "ConvMesh: Reimagining Mesh Quality Through Convex Optimization\n",
      "Use of diverse data sources to control which topics emerge in a science map\n",
      "Depression detection from Social Media Bangla Text Using Recurrent Neural Networks\n",
      "CTRAPS: CTAP Client Impersonation and API Confusion on FIDO2\n",
      "Characterizing the Fragmentation of the Social Media Ecosystem\n",
      "A Graph Neural Architecture Search Approach for Identifying Bots in Social Media\n",
      "Detecting Visual Triggers in Cannabis Imagery: A CLIP-Based Multi-Labeling Framework with Local-Global Aggregation\n",
      "Optimal Transcoding Preset Selection for Live Video Streaming\n",
      "Epinet for Content Cold Start\n",
      "Probe-Me-Not: Protecting Pre-trained Encoders from Malicious Probing\n",
      "SpiderDAN: Matching Augmentation in Demand-Aware Networks\n",
      "HateGPT: Unleashing GPT-3.5 Turbo to Combat Hate Speech on X\n",
      "Characteristics of Political Misinformation Over the Past Decade\n",
      "RetrieveGPT: Merging Prompts and Mathematical Models for Enhanced Code-Mixed Information Retrieval\n",
      "Automatic Identification of Political Hate Articles from Social Media using Recurrent Neural Networks\n",
      "Multi-language Video Subtitle Dataset for Image-based Text Recognition\n",
      "Exposing Cross-Platform Coordinated Inauthentic Activity in the Run-Up to the 2024 U.S. Election\n",
      "A Systematic Review of Machine Learning Approaches for Detecting Deceptive Activities on Social Media: Methods, Challenges, and Biases\n",
      "Measuring Network Dynamics of Opioid Overdose Deaths in the United States\n",
      "IPL: Leveraging Multimodal Large Language Models for Intelligent Product Listing\n",
      "On the Use of Proxies in Political Ad Targeting\n",
      "News-Driven Stock Price Forecasting in Indian Markets: A Comparative Study of Advanced Deep Learning Models\n",
      "Categorizing Social Media Screenshots for Identifying Author Misattribution\n",
      "Post-hoc Study of Climate Microtargeting on Social Media Ads with LLMs: Thematic Insights and Fairness Evaluation\n",
      "Examining the Role of Relationship Alignment in Large Language Models\n",
      "Social Media Bot Policies: Evaluating Passive and Active Enforcement\n",
      "Social media algorithms can curb misinformation, but do they?\n",
      "English offensive text detection using CNN based Bi-GRU model\n",
      "Engagement, Content Quality and Ideology over Time on the Facebook URL Dataset\n",
      "Consistent Strong Triadic Closure in Multilayer Networks\n",
      "Teen Talk: The Good, the Bad, and the Neutral of Adolescent Social Media Use\n",
      "Phantom: Untargeted Poisoning Attacks on Semi-Supervised Learning (Full Version)\n",
      "Contrastive Representation Learning for Dynamic Link Prediction in Temporal Networks\n",
      "The Matrix Reloaded: A Mechanized Formal Analysis of the Matrix Cryptographic Suite\n",
      "Looking AT the Blue Skies of Bluesky\n",
      "State surveillance in the digital age: Factors associated with citizens' attitudes towards trust registers\n",
      "SMART-TBI: Design and Evaluation of the Social Media Accessibility and Rehabilitation Toolkit for Users with Traumatic Brain Injury\n",
      "A modified Ricci flow on arbitrary weighted graph\n",
      "BnSentMix: A Diverse Bengali-English Code-Mixed Dataset for Sentiment Analysis\n",
      "PQV-Mobile: A Combined Pruning and Quantization Toolkit to Optimize Vision Transformers for Mobile Applications\n",
      "Decoding Memes: A Comparative Study of Machine Learning Models for Template Identification\n",
      "Encoding and Decoding Algorithms of ANS Variants and Evaluation of Their Average Code Lengths\n",
      "Constructing the CORD-19 Vaccine Dataset\n",
      "Political Elites in the Attention Economy: Visibility Over Civility and Credibility?\n",
      "Evaluating the effect of viral news on social media engagement\n"
     ]
    }
   ],
   "source": [
    "for r in client.results(search):\n",
    "    print(r.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, if we compare that with the actual ArXiv website, it looks correct: https://arxiv.org/search/cs?query=twitter&searchtype=all&abstracts=show&order=-announced_date_first&size=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's begin saving the first pdf in that list in our laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./2412.18779v1.Integrating_Zero_Shot_Classification_to_Advance_Long_COVID_Literature__A_Systematic_Social_Media_Centered_Review.pdf'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper = next(arxiv.Client().results(arxiv.Search(id_list = [\"2412.18779\"])))\n",
    "# Download the PDF to the PWD with a default filename.\n",
    "paper.download_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./2412.18779.pdf'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the PDF to the PWD with a custom filename.\n",
    "paper.download_pdf(filename = \"2412.18779.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\usuario\\\\ELENA\\\\it-training uzh\\\\it-training uzh\\\\Python for Digital Humanities\\\\Day 2\\\\PDF extraction'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we need to create a folder in our directory. Remember that we can also do that using bash commands here in Jupyter Notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ya existe el subdirectorio o el archivo files_arxiv.\n"
     ]
    }
   ],
   "source": [
    "mkdir files_arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\usuario\\\\ELENA\\\\it-training uzh\\\\it-training uzh\\\\Python for Digital Humanities\\\\Day 2\\\\PDF extraction\\\\files_arxiv\\\\2412.18779.pdf'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the PDF to a specified directory with a custom filename.\n",
    "paper.download_pdf(dirpath = \"C:\\\\Users\\\\usuario\\\\ELENA\\\\it-training uzh\\\\it-training uzh\\\\Python for Digital Humanities\\\\Day 2\\\\PDF extraction\\\\files_arxiv\",\n",
    "                    filename = \"2412.18779.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far I have been using just the code provided by the Arxiv API to do all those things (https://pypi.org/project/arxiv/). Now let's go to the next level and let's extract a bunch of articles. Looking at the API it looks like we need to extract the IDs of the papers. Let's do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "\n",
    "for result in client.results(search):\n",
    "    ids.append(result.entry_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://arxiv.org/abs/2412.20420v1',\n",
       " 'http://arxiv.org/abs/2412.18779v1',\n",
       " 'http://arxiv.org/abs/2412.15621v1',\n",
       " 'http://arxiv.org/abs/2412.15072v1',\n",
       " 'http://arxiv.org/abs/2412.14985v1',\n",
       " 'http://arxiv.org/abs/2412.08484v1',\n",
       " 'http://arxiv.org/abs/2412.07550v1',\n",
       " 'http://arxiv.org/abs/2412.05861v1',\n",
       " 'http://arxiv.org/abs/2412.02349v1',\n",
       " 'http://arxiv.org/abs/2411.16826v1',\n",
       " 'http://arxiv.org/abs/2411.16285v1',\n",
       " 'http://arxiv.org/abs/2412.08648v1',\n",
       " 'http://arxiv.org/abs/2411.14613v1',\n",
       " 'http://arxiv.org/abs/2412.04484v1',\n",
       " 'http://arxiv.org/abs/2411.12508v1',\n",
       " 'http://arxiv.org/abs/2411.11426v1',\n",
       " 'http://arxiv.org/abs/2411.09214v1',\n",
       " 'http://arxiv.org/abs/2411.06122v1',\n",
       " 'http://arxiv.org/abs/2411.04752v1',\n",
       " 'http://arxiv.org/abs/2411.04542v1',\n",
       " 'http://arxiv.org/abs/2411.05043v1',\n",
       " 'http://arxiv.org/abs/2410.22716v1',\n",
       " 'http://arxiv.org/abs/2410.20293v2',\n",
       " 'http://arxiv.org/abs/2410.17496v1',\n",
       " 'http://arxiv.org/abs/2410.16977v1',\n",
       " 'http://arxiv.org/abs/2410.14617v1',\n",
       " 'http://arxiv.org/abs/2411.05788v1',\n",
       " 'http://arxiv.org/abs/2410.06443v1',\n",
       " 'http://arxiv.org/abs/2410.05401v1',\n",
       " 'http://arxiv.org/abs/2410.01708v1',\n",
       " 'http://arxiv.org/abs/2409.18931v1',\n",
       " 'http://arxiv.org/abs/2409.18393v1',\n",
       " 'http://arxiv.org/abs/2409.15652v3',\n",
       " 'http://arxiv.org/abs/2409.13461v1',\n",
       " 'http://arxiv.org/abs/2409.08405v1',\n",
       " 'http://arxiv.org/abs/2409.02358v1',\n",
       " 'http://arxiv.org/abs/2409.01470v1',\n",
       " 'http://arxiv.org/abs/2408.12753v1',\n",
       " 'http://arxiv.org/abs/2408.12743v2',\n",
       " 'http://arxiv.org/abs/2408.12449v2',\n",
       " 'http://arxiv.org/abs/2408.09725v1',\n",
       " 'http://arxiv.org/abs/2408.09683v1',\n",
       " 'http://arxiv.org/abs/2408.09435v1',\n",
       " 'http://arxiv.org/abs/2408.08964v3',\n",
       " 'http://arxiv.org/abs/2408.08437v1',\n",
       " 'http://arxiv.org/abs/2408.08126v1',\n",
       " 'http://arxiv.org/abs/2408.07322v1',\n",
       " 'http://arxiv.org/abs/2407.18471v1',\n",
       " 'http://arxiv.org/abs/2407.16014v1',\n",
       " 'http://arxiv.org/abs/2407.13549v1']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://arxiv.org/abs/2412.20420v1'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need is just the ID number of the paper. Let's select that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2412.20420v1'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids[0].split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_2 = []\n",
    "\n",
    "for i in ids:\n",
    "    ids_2.append(i.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2412.20420v1',\n",
       " '2412.18779v1',\n",
       " '2412.15621v1',\n",
       " '2412.15072v1',\n",
       " '2412.14985v1',\n",
       " '2412.08484v1',\n",
       " '2412.07550v1',\n",
       " '2412.05861v1',\n",
       " '2412.02349v1',\n",
       " '2411.16826v1',\n",
       " '2411.16285v1',\n",
       " '2412.08648v1',\n",
       " '2411.14613v1',\n",
       " '2412.04484v1',\n",
       " '2411.12508v1',\n",
       " '2411.11426v1',\n",
       " '2411.09214v1',\n",
       " '2411.06122v1',\n",
       " '2411.04752v1',\n",
       " '2411.04542v1',\n",
       " '2411.05043v1',\n",
       " '2410.22716v1',\n",
       " '2410.20293v2',\n",
       " '2410.17496v1',\n",
       " '2410.16977v1',\n",
       " '2410.14617v1',\n",
       " '2411.05788v1',\n",
       " '2410.06443v1',\n",
       " '2410.05401v1',\n",
       " '2410.01708v1',\n",
       " '2409.18931v1',\n",
       " '2409.18393v1',\n",
       " '2409.15652v3',\n",
       " '2409.13461v1',\n",
       " '2409.08405v1',\n",
       " '2409.02358v1',\n",
       " '2409.01470v1',\n",
       " '2408.12753v1',\n",
       " '2408.12743v2',\n",
       " '2408.12449v2',\n",
       " '2408.09725v1',\n",
       " '2408.09683v1',\n",
       " '2408.09435v1',\n",
       " '2408.08964v3',\n",
       " '2408.08437v1',\n",
       " '2408.08126v1',\n",
       " '2408.07322v1',\n",
       " '2407.18471v1',\n",
       " '2407.16014v1',\n",
       " '2407.13549v1']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's loop around that to get all the PDFs into our laptop. First let's create a new folder called \"arxiv_pdfs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ya existe el subdirectorio o el archivo arxiv_pdfs_facebook.\n"
     ]
    }
   ],
   "source": [
    "mkdir arxiv_pdfs_facebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REMEMBER to create a new folder for the new query that you are going to do to create your own PDF database. Remember to change it in the filename variable too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mkdir (remove the #) and now add the name that you would like to have for a new folder (I suggest arxiv_pdfs_whatever (your query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's get all the pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in ids_2:\n",
    "    # Search for the article with the given ID\n",
    "    search = arxiv.Search(id_list=[id])\n",
    "    paper = next(client.results(search))\n",
    "\n",
    "    # Download the PDF to the current working directory with a default filename\n",
    "    filename = os.path.join(\"arxiv_pdfs_facebook\", urllib.parse.quote(id))\n",
    "    paper.download_pdf(filename=f\"{filename}.pdf\")\n",
    "    \n",
    "    time.sleep(3)  # 3 seconds (this is the indication of the ArXiv API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And voila! According to the arxiv API (https://pypi.org/project/arxiv/1.4.8/) the daily limit is 300.000 results: that is a lot!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
