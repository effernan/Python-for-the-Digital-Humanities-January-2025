{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fed1e21",
   "metadata": {},
   "source": [
    "Script source: many queries to Perplexity AI (https://www.perplexity.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29ba659",
   "metadata": {},
   "source": [
    "# 1. We import the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e865e4",
   "metadata": {},
   "source": [
    "We install the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "489ec001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8ae705",
   "metadata": {},
   "source": [
    "And then we import it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5cfd0db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b90ddd",
   "metadata": {},
   "source": [
    "We also import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "849dd6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5dd476",
   "metadata": {},
   "source": [
    "And the Operative System Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f86e3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651f164b",
   "metadata": {},
   "source": [
    "# 2. We extract the text from the PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c47e65",
   "metadata": {},
   "source": [
    "Let's select the PDF 2412.18779 that we have in our directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c83edce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create empty list\n",
    "text = []\n",
    "\n",
    "# Open the PDF file\n",
    "with open('2412.18779.pdf', 'rb') as file:\n",
    "    # Create a PDF reader object\n",
    "    pdf_reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "    # Get the number of pages in the PDF\n",
    "    num_pages = len(pdf_reader.pages)\n",
    "\n",
    "    # Initialize an empty string to store the extracted text\n",
    "    extracted_text = ''\n",
    "\n",
    "    # Loop through each page and extract the text\n",
    "    for page_num in range(num_pages):\n",
    "        page = pdf_reader.pages[page_num]\n",
    "        extracted_text += page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33f6993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the extracted text\n",
    "text.append(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c30a526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Integrating Zero -Shot Classification to Advance Long \\nCOVID Literature: A Systematic Social Media ‚ÄìCentered \\nReview  \\nNirmalya Tha kur \\nDepartment of Electrical Engineering and Computer Science , South Dakota School of Mines \\nand Technology , Rapid City, SD 57701, USA  \\nnirmalya.thakur@sdsmt.edu  \\nAbstract. Long COVID continues to challenge public health by affecting a signifi-\\ncant segment of individuals who have recovered from acute SARS -CoV -2 infection yet \\nendure prolonged and often debilitating symptoms. Social media has emerged as a vital \\nresource  for those seeking real -time information, peer support, and validating  their \\nhealth concerns  related to Long COVID . This paper examines recen t works  focusing  \\non mining, analyzing, and interpreting  user-generated content on social media plat-\\nforms such as X (formerly Twitter ), Reddit, Facebook, and YouTube to capture the \\nbroader discourse on persistent post -COVID conditions.  A novel transformer -based \\nzero-shot learning approach serves as the foundation for classifying research papers  in \\nthis area  into four primary categories: Clinical or Symptom Characterization, Advanced \\nNLP or Computational Methods, Policy, Advocacy, or Public Health Communication, \\nand Online Communities and Social Support . This method ology showcases the adapt-\\nability of advanced  language models in categorizing research  papers  without predefined \\ntraining labels, thus enabling a more rapid and scalable assessment of existing literature. \\nThis review  highlight s the multifaceted nature of Long COVID research, where com-\\nputational techniques applied to so cial medi a data reveal  insights  into narratives of in-\\ndividuals sufferin g from Long COVID. This review  also demonstrates the capacity of \\nsocial media analytics to inform clinical practice  and contribute to policy making  re-\\nlated  to Long COVID .  \\nKeywords:  Long COVID, COVID -19, Zer o-Shot Learning, social media , Twitter, Reddit, Face-\\nbook, and YouTube  \\n1 Introduction   \\nIn December 2019, an outbreak of coronavirus disease 2019 (COVID -19), caused \\nby the severe acute respiratory syndrome coronavirus 2 (SARS -CoV -2), began  in \\nChina.  [1,2]. Even though SARS -CoV -2 is similar in origin to SARS -CoV and MERS -\\nCoV, it has affected public health globally at a much greater scale than any prior  coro-\\nnavirus outbreaks [ 3]. Early containment efforts, including measures by the Chinese \\ngovernment, did not prevent the disease from rapidly crossing regional and interna-\\ntional boundaries  [4], leading the World Health Organization (WHO) to declare \\nCOVID -19 a global pandemic on March 11, 2021 [ 5]. According to the WHO, con-\\nfirmed cases were  776,841,264 worldwide, with 7,075,468 reported deaths  as of 10 \\nNovember 2024 [6].  2 \\nAlthough many individuals recover from the acute infection  caused by SARS -CoV -\\n2, a significant subset experiences symptoms that remain or appear after what might \\nhave been presumed clinical recovery. This phenomenon, known as Long COVID, has \\nbeen described since the earliest days of the pandemic to include persistent or emerging \\nphys ical and psychological challenges  [7-9]. As per [10], ‚ÄúLong COVID is defined as \\na chronic condition that occurs after SARS -CoV-2 infection and is present for at least \\n3 months. Long COVID includes a wide range of symptoms or conditions that may \\nimprove, worsen, or be ongoing‚Äù.  \\nIndividuals who experience Long COVID commonly face a broad set of symptoms \\nthat may disrupt daily routines and overall well -being. Frequently reported symptoms \\nof Long COVID  include shortness of breath, cough, persistent fatigue, post -exertional \\nmalaise, difficulty concentrating, memory changes, recurring headache, lightheaded-\\nness, fast heart rate, sleep disturbance, problems with taste or smell, bloating, constipa-\\ntion, and diarrhea  [11,12 ]. In more complex scenarios, patients are diagnosed with in-\\nterstiti al lung disease and hypoxemia, cardiovascular disease and arrhythmias, cogni-\\ntive impairment, mood disorders, anxiety, migraine, stroke, blood clots, chronic kidney \\ndisease, postural orthostatic tachycardia syndrome (POTS) and other forms of dysau-\\ntonomia, m yalgic encephalomyelitis/chronic fatigue syndrome (ME/CFS), mast cell ac-\\ntivation syndrome (MCAS), bromyalgia , connective tissue diseases, hyperlipidemia, \\ndiabetes, and autoimmune disorders such as lupus, rheumatoid arthritis, and Sjogren‚Äôs \\nsyndrome  [11,13 -15].  \\nSuch symptoms may persist for three months beyond the initial SARS -CoV -2 infec-\\ntion or even exceed a year [ 16]. Although some individuals gradually improve, others \\nexperience lingering or fluctuating complications that may profoundly affect their phys-\\nical, psychological, and social health [ 17,18 ]. Most treatment approaches revolve \\naround consistent monitoring and symptom -specific care. Clinicians commonly refer-\\nence established guidelines when managing symptoms of Long COVID and any ac-\\ncompanying conditions, such as diabetes, high blood pressure, or POTS, to reduce fu-\\nture complications and e nhance the patient's  quality of life  [19]. Many patients benefit \\nfrom a combination of therapies  - ranging from medications targeting pain or sleep \\nchallenges to physical or occupational rehabilitation  - along with psychological support \\nto manage both the physical and emotional aspects  of Long COVID [ 20,21 ]. In addition \\nto this, even though certain medications such as paracetamol or NSAIDs  appear to help \\nwith specific  Long COVID symptoms like fever, there is still no standardized treatment \\nto address the entire spectrum of Long COVID symptoms  [22-25]. \\nSocial media has been  a critical venue for public discussion of COVID -19 since  its \\ninitial cases in December 2019 , evolving into a resource for people seeking real -time \\ninformation and community support  [26-31]. As this pandemic advanced, platforms \\nsuch as X (formerly Twitter ) [32,33] , TikTok  [34,35] , Instagram  [36,37] , Facebook  \\n[38,39] , YouTube  [40,41] , Reddit  [42,43] , LinkedIn  [44,45] , Clubhouse  [46,47] , Dis-\\ncord [48,49] , and Snapchat  [50,51] , became pivotal for gathering firsthand insights into \\nongoing patient experiences. Tr aditional methods like surveys and interviews can be \\nconstrained by time and location, whereas social media allows continuous, unfiltered \\naccounts of Long COVID manifestations and daily struggles. Individuals suffering \\nfrom Long COVID can  document their symptoms, exchange practical advice, and dis-\\ncuss personal setbacks or milestones  on social media, leading to the generation of Big 3 \\nData  that researchers from different disciplines and healthcare professionals may ana-\\nlyze to identify evolving patterns.  \\nAs Long COVID presents multifaceted medical, social, and emotional issues,  there \\nhas been growing interest in leveraging online platforms to study it from multiple an-\\ngles. Social media channels facilitate global conversations that can reveal differences \\nin experiences related to healthcare access, post -infection complications, or  even public \\nawareness of the severity  of a health -related  conditio n [52-54]. Over time, these virtual \\nspaces have also fostered advocacy and grassroots efforts. Hashtags like #LongCOVID \\n[55] have given patients and advocates an active role in discussing everything from \\nspecialized clinics to  mental health support  [56]. Observations of this activity under-\\nscore how large -scale social media data can shape public health discourse [57] and even \\ninfluence policies [58] addressing health -related  conditio ns that are often misunder-\\nstood or underdiagnosed.  \\nA review paper that categorizes the existing work in this domain is expected to play \\na crucial role  in advancing knowledge. Studies on Long COVID and social media vary \\nwidely, incorporating sentiment analysis, network analysis, qualitative content studies, \\nand more. Combining or comparing such research can clarify  where the field has gath-\\nered robust evidence, where it lacks conclusive data, and which areas still need system-\\natic exploration. Recent works  have drawn on patient narratives to refine clinical defi-\\nnitions or inform the development of Long COVID  care frameworks, yet there is a \\ncrucial  need to consolidate these findings. Examining studies in this area under  broader \\nthematic groupings is expected  to highlight the progress made so far  and pinpoint  un-\\nresolved questions, methodological gaps, and ethical considerations surrounding pa-\\ntient data.  Addressing this research gap serves as the main motivator for this review.  \\nThis review focuses on four broad  areas  - Clinical or Symptom Characterization, \\nAdvanced NLP or Computational Methods, Policy, Advocacy, or Public Health Com-\\nmunication, Online Communities and Social Support , in the context of Long COVID -\\nrelated research works that specifically focus on mining, analyzing, and interpreting \\nsocial media data . It is relevant to mention that this review not only investigates the \\nbroad spectrum of research on Long COVID across various social media platforms but \\nalso integrates a novel zero-shot classification pipeline that organizes recent works in \\nthis field  into distinct  categories. Th is classification process was performed  without \\nexplicit task -specific training using  a transformer -based model configured for zero -shot \\nlearning. This dual perspective highlights both a comprehensive  review of the literature \\nand a demonstration of how an advanced  language model  can streamline the analysis \\nof research papers  on Long COVID. This review also discusses current research limi-\\ntations and proposes future work directions  that are expected to benefit both scientific \\ncommunities and those living with Long COVID .  \\nAlthough these studies appear  classified  into distinct areas in this review paper , \\nmany  works  address multiple facets of Long COVID research, rendering any classifi-\\ncation flexible rather than absolute. For instance, a paper listed under ‚Äú Online Commu-\\nnities and Social Support ‚Äù may also perform a detailed sentiment analysis that aligns \\nwith ‚Äú Advanced NLP or Computational Methods ‚Äù. Such overlaps arise naturally in in-\\nterdisciplinary research, especially when varied computational methods  - like sentiment \\nanalysis, topic modeling, and network analysis  - are applied to the extensive social me-\\ndia discussions surrounding patient experiences, advocacy efforts, and policy 4 \\nimplications. The four broad areas presented  here are an organizational guide, high-\\nlighting a primary thematic focus without dismissing other significant aspects of each \\nstudy.  \\nThe rest of this paper is organized as follows. First, the methodology is presented, \\ndescribing the search strategy, inclusion criteria, and the steps taken to apply a zero -\\nshot learning model. Then, the results from this automated classification process a re \\ndiscussed, followed by a detailed examination of each study identified in the review. \\nResearch gaps and directions for future work  are then explored, emphasizing  how in-\\nterdisciplinary approaches and advanced computational tools might enrich the current \\nunderstanding of Long COVID. Finally, the paper summarizes key findings and high-\\nlights  the potential impact of integrating social media analytics into ongoing research \\non persistent COVID -19 symptoms.  \\n2 Methodology  \\nA broad literature search was carried out across multiple scholarly databases, includ-\\ning PubMed, Scopus, Web of Science, and Google Scholar, to identify studies focused \\non mining, analyzing, and interpreting  the public discourse about  Long COVID on so-\\ncial media. This search aimed  to capture Long COVID -related research across diverse \\nfields, such as computer science, health sciences , and social sciences. No papers pub-\\nlished before 2020 were included, as the COVID -19 outbreak began in December 2019.  \\nThe search terms used included - ‚ÄúLong COVID,‚Äù ‚Äúpost -COVID,‚Äù ‚Äúchronic COVID‚Äù  - \\nas well as keywords indicative of social media use, such as ‚ÄúTwitter ‚Äù, ‚ÄúTikTok ‚Äù, ‚ÄúIn-\\nstagram ‚Äù, ‚ÄúFacebook ‚Äù, ‚ÄúYouTube ‚Äù, ‚ÄúReddit ‚Äù, ‚ÄúLinkedIn ‚Äù, ‚ÄúClubhouse ‚Äù, ‚ÄúDiscord ‚Äù, \\nand ‚ÄúSnapchat ‚Äù. In addition, terms like ‚Äúsentiment analysis,‚Äù ‚Äútopic modeling,‚Äù and \\n‚Äúnetwork analysis‚Äù were included to ensure the retrieval of studies that used computa-\\ntional or statistical methods to examine the public discourse  on these platforms. By \\nblending health -related terminology with references to digital platforms and relevant \\nanalytica l techniques, the search strategy was designed to capture the full breadth of \\nscholarly work investigating individuals‚Äô ongoing experiences with Long COVID . \\nStudies were selected for inclusion if they used social media data to investigate any \\naspect of Long COVID. The main inclusion criteria were that the articles utilized  a \\nrecognized research methodology  - whether qualitative, quantitative, or mixed methods  \\nand analyze d data gathered primarily from social media platforms. The selection pro-\\ncess also considered whether the authors had sufficiently detailed the nature of their \\nquantitative  or qualitative approach. Moreover, ethical practices regarding user data, \\nsuch as anonymization or compliance with platform terms of service, were taken into \\naccount to ensure that privacy concerns were handled responsibly.  Studies that just \\nmentioned  Long COVID  were excluded. Research works  such as editorials, letters to \\nthe editor, or general news articles, which usually lack methodological details, were \\nalso excluded . If the essential aspects of a paper  were missing  - for example, neglecting \\nto report how data were collected  - those were also removed from consideration. This \\napproach aimed to retain a set of methodologically sound articles  that offered substan-\\ntive insights into the public discourse about  Long COVID  on social media platforms . \\nUpon applying these inclusion and exclusion criteria, a total of 4 0 studies were se-\\nlected for this review . These works represented a range of methods, including sentiment 5 \\nanalysis, qualitative content analysis, topic modeling, and network analysis, and they \\naddressed multiple social media platforms, such as X (formerly Twitter ), Reddit, and \\nFacebook. Thereafter, a transformer -based zero -shot classification model  was devel-\\noped  to classify these papers into one out of the four thematic categories described \\nbelow . Although a few papers  fit under multiple themes , each was placed wherever its \\nprimary emphasis appeared strongest.  \\n(i) Clinical or Symptom Characterization (‚ÄúSymptom Characterization‚Äù) : Re-\\nsearch that primarily aims to identify, list, or quantify the variety of Long \\nCOVID symptoms, usually from social media data. The studies may in-\\nclude  statistic al analysis  but do not necessarily perform  extensive sentiment \\nor topic modeling. Their main motivation is to gather clinical or epidemio-\\nlogical insights from user posts.  \\n(ii) Advanced NLP or Computational Methods (‚ÄúNLP and Modeling‚Äù) : Studies \\nthat specifically emphasize methods like deep transformer networks, topic \\nmodeling, sentiment analysis , and other elaborate computational ap-\\nproaches. This goes beyond a simple symptom count; it highlights a meth-\\nods-heavy lens on analyzing data.  \\n(iii) Policy, Advocacy, or Public Health Communication (‚ÄúPolicy and Advo-\\ncacy‚Äù) : Papers exploring how organizations, governments, or communities \\ndevelop health communications, handle policy issues, and communicate  \\nguidelines .  \\n(iv) Online Communities & Social Support (‚ÄúCommunity and Support‚Äù) : Stud-\\nies focusing on how individuals find emotional or experiential support on \\nsocial media, the way they exchange personal stories, or how group dynam-\\nics form around shared experiences. The main emphasis is on the psycho-\\nsocial aspect , and the support social media platforms  provide.  \\nThereafter, a  transformer -based zero -shot classification model  was developed , \\nwhich was set up to assign the 40 papers  to these  categories . This model did not require \\nany training using any  labeled dataset. The process by which this model worked is de-\\nscribed below:  \\nFormally, let ‚Äútext‚Äù be a study‚Äôs abstract, and suppose we have candidate labels { ùëê1, \\nùëê2, ‚Ä¶, ùëêùëÅ}. The model use d a scoring function to determine the  alignment between \\n‚Äútext‚Äù and a label ck. Equation (1) shows  how the best -matching category, denoted  as \\nƒâ(text) was determined . \\n \\n (1) \\nIn Equation (1), p k(text) represents  the probability that text belongs to category  ck. \\nIn practical terms, the system prompt ed the model with textual descriptions of each \\ncategory and the study‚Äôs abstract. It then compute d a scalar score ùë†(text, ùëò) (as shown \\nin Equation (2)) that measure d how well text matche d the meaning or intent of label ck. \\nA softmax function then normalize d these raw scores, producing probabilities for all \\ncategories  as shown in Equation (3).  \\n \\n (2) \\n6 \\n (3) \\n \\nIn essence, whichever category attain ed the largest probability was selected as the \\nlabel by the model  for that document. This method ology  is called zero -shot learning \\n[59-61] because the model does not require an example corpus manually labeled under \\nthese same categories. Instead, the model draws from its vast, pre -trained language rep-\\nresentations to infer whether a given textual data  aligns more with, for instance, a \\n‚ÄúSymptom Characterization ‚Äù theme or a ‚Äú Community and Support ‚Äù theme.  A program \\nwas written in Python 3.10 to develop and implement this transformer -based zero -shot \\nclassification model . The results of the same are presented and discussed in Section 3.  \\n3 Results of Zero -Shot Classification  \\nThis section presents the results of the transformer -based zero -shot classification \\nmodel  applied to the 4 0 research papers  [62-102] that met the inclusion criteria  of this \\nreview . The underlying premise of zero -shot learning is that when prompted with suit-\\nable descriptors, a well -trained language model can identify the most relevant label for \\ntextual data, even if the model  has never seen concrete examples corresponding to that \\nlabel during training. This eliminate s the need for time -intensive and resource -heavy \\ndata labeling processes, which is especially advantageous in emerging research areas \\nsuch as COVID -19-related research where existing taxonomies may be incomplete or \\nstill evolving  [103-106]. In addition to the inherent benefit of not requiring pre -labeled \\ndata, this methodology also provide d a structured and transparent way to allocate these \\n40 research papers  into distinct categories. By integrating zero -shot learning with care-\\nfully curated dictionary -based keyword matching, it became possible to identify and  \\nhighlight the thematic focus of each study  and classify it into one of the four categories \\n- Clinical or Symptom Characterization, Advanced NLP or Computational Methods, \\nPolicy, Advocacy, or Public Health Communication, Online Communities and Social \\nSupport . The dictionary -based scores helped complement the probabilistic outputs from \\nthe zero-shot classification  model, thereby refining the final assessments of each \\nstudy‚Äôs thematic focus. This synergy proved particularly useful for works where tech-\\nnical and clinical terminologies might intersect, making it difficult to rely solely on \\neither semantic features or explici t keyword usage. The result was a more robust and \\ninterpretable categorization pipeline that could be applied to other domains as well with \\nminimal customization effort.  Table 1 shows the results where the author list, title, and \\nclassification label of each paper are shown.  \\n \\nTable 1 : Results of applying zero -shot lea rning to classify the 40 papers that met the \\ninclusion criteria  \\nFull Author List  Title  Classifica-\\ntion Label  \\nYu-Bo Fu  [62] Investigating public perceptions regarding the \\nLong COVID on Twitter using sentiment analy-\\nsis and topic modeling  NLP and \\nModeling  \\n7 \\nAlex Rushforth, Emma \\nLadds, Sietse Wieringa, \\nSharon Taylor, Laiba Hu-\\nsain and Trisha Greenhalgh  \\n[63] Long Covid ‚Äì The illness narratives  Policy and \\nAdvocacy  \\nDavid Russell, Naomi J. \\nSpence, Jo -Ana D. Chase, \\nTatum Schwartz, Christa M. \\nTumminello and Erin \\nBouldin  [64] Support amid uncertainty: Long COVID illness \\nexperiences and the role of online communities  Commu-\\nnity and \\nSupport  \\nFrancesco Meledandri  [65] The Impact of Polarised Social Media Network-\\ning Communications in the #Longcovid Debate \\nbetween Ideologies and Scientific Facts  Commu-\\nnity and \\nSupport  \\nShubh Mohan Singh and \\nChaitanya Reddy  [66] An Analysis of Self -reported Longcovid Symp-\\ntoms on Twitter  Symptom \\nCharacter-\\nization  \\nNida Ziauddeen, Deepti \\nGurdasani, Margaret E \\nO‚ÄôHara, Claire Hastie, Paul \\nRoderick, Guiqing Yao and \\nNisreen A Alwan  [67] Characteristics of Long Covid: findings from a \\nsocial media survey  Symptom \\nCharacter-\\nization  \\nAbeed Sarker and Yao Ge  \\n[68] Long COVID symptoms from Reddit: Charac-\\nterizing post -COVID syndrome from patient re-\\nports  Symptom \\nCharacter-\\nization  \\nJuan M. Banda, Nicola Ad-\\nderley, Waheed -Ul-Rahman \\nAhmed, Heba AlGhoul, \\nOsaid Alser, Muath Alser, \\nCarlos Areia, Mikail Co-\\ngenur, Krisitina Fi≈°ter, \\nSaurabh Gombar, Vojtech \\nHuser, Jitendra Jon-\\nnagaddala, Lana YH Lai, \\nAngela Leis, Lourdes Ma-\\nteu, Miguel Angel Maye r, \\nEvan Minty, Daniel Mo-\\nrales, Karthik Natarajan, \\nRoger Paredes, Vyjeyanthi \\nS. Periyakoil, Albert Prats -\\nUribe, Elsie G. Ross, \\nGurdas Singh, Vignesh Sub-\\nbian, Arani Vivekanantham \\nand Daniel Prieto -Alhambra  \\n[69] Characterization of long -term patient -reported \\nsymptoms of COVID -19: an analysis of social \\nmedia data  Symptom \\nCharacter-\\nization  \\nDaisy Massey, Diana Ber-\\nrent and Harlan Krumholz  \\n[70] Breakthrough Symptomatic COVID -19 Infec-\\ntions Leading to Long Covid: Report from Long \\nCovid Facebook Group Poll  Symptom \\nCharacter-\\nization  \\nSam Martin, Macarena \\nChepo, No√©mie D√©om, Ah-\\nmad Firas Khalid and Ce-\\ncilia Vindrola -Padros  [71] ‚Äú#LongCOVID affects children too‚Äù: A Twitter \\nanalysis of healthcare workers‚Äô sentiment and \\ndiscourse about Long COVID in children and \\nyoung people in the UK  Symptom \\nCharacter-\\nization  8 \\nElham Dolatabadi, Diana \\nMoyano, Michael Bales, \\nSofija Spasojevic, Rohan \\nBhambhoria, Junaid Bhatti, \\nShyamolima Debnath, \\nNicholas Hoell, Xin Li, \\nCeline Leng, Sasha Nanda, \\nJad Saab, Esmat Sahak, \\nFanny Sie, Sara Uppal, \\nNirma Khatri Vadlamudi, \\nAntoaneta Vladimi rova, Ar-\\ntur Yakimovich, Xiaoxue \\nYang, Sedef Akinli Kocak \\nand Angela M. Cheung  [72] Using Social Media to Help Understand Long \\nCOVID Patient Reported Health Outcomes: A \\nNatural Language Processing Approach  Symptom \\nCharacter-\\nization  \\nLin Miao, Mark Last and \\nMarina Litvak  [73] An Interactive Analysis of User -reported Long \\nCOVID Symptoms using Twitter Data  Symptom \\nCharacter-\\nization  \\nGuocheng Feng, Huaiyu Cai \\nand Wei Quan  [74] Exploring the Emotional and Mental Well -Being \\nof Individuals with Long COVID Through Twit-\\nter Analysis  Symptom \\nCharacter-\\nization  \\nAlexis Jordan and Albert \\nPark [75] Understanding the Long Haulers of COVID -19: \\nMixed Methods Analysis of YouTube Content  NLP and \\nModeling  \\nIkhwan Yuda Kusuma and \\nSuherman Suherman  [76] The Pulse of Long COVID on Twitter: A Social \\nNetwork Analysis  NLP and \\nModeling  \\nNirmalya Thakur  [77] Investigating and Analyzing Self -Reporting of \\nLong COVID on Twitter: Findings from Senti-\\nment Analysis  NLP and \\nModeling  \\nToluwalase Awoyemi, \\nUjunwa Ebili, Abiola \\nOlusanya, Kayode E. Ogun-\\nniyi and Adedolapo V. \\nAdejumo  [78] Twitter Sentiment Analysis of Long COVID \\nSyndrome  Symptom \\nCharacter-\\nization  \\nSam Rhodehamel  [79] Digital Long Hauler Lifelines: Understanding \\nHow People with Long Covid Build Community \\non Reddit  Commu-\\nnity and \\nSupport  \\nArinjita Bhattacharyya, \\nAnand Seth and Shesh Rai  \\n[80] The Effects of Long COVID -19, Its Severity, and \\nthe Need for Immediate Attention: Analysis of \\nClinical Trials and Twitter Data  Policy and \\nAdvocacy  \\nSurani Matharaarachchi, \\nMike Domaratzki, Alan \\nKatz and Saman Muthuku-\\nmarana  [81] Discovering Long COVID Symptom Patterns: \\nAssociation Rule Mining and Sentiment Analy-\\nsis in Social Media Tweets  Symptom \\nCharacter-\\nization  \\nJonathan Koss and Sabine \\nBohnet -Joschko  [82] Social Media Mining of Long -COVID Self -\\nMedication Reported by Reddit Users: Feasibil-\\nity Study to Support Drug Repurposing  Symptom \\nCharacter-\\nization  \\nHanin Ayadi, Charline \\nBour, Aur√©lie Fischer, Mo-\\nhammad Ghoniem and Guy \\nFagherazzi  [83] The Long COVID Experience from a Patient's \\nPerspective: A Clustering Analysis of 27,216 \\nReddit Posts  Symptom \\nCharacter-\\nization  \\nCamryn Garrett, Atefeh \\nAghaei, Abhishek Ag-\\ngarwal and Shan Qiao  [84] The Role of Social Media in the Experiences of \\nCOVID -19 Among Long -Hauler Women: Qual-\\nitative Study  Commu-\\nnity and \\nSupport  9 \\nLinnea I. Laestadius, \\nJeanine P. D. Guidry, An-\\ndrea Bishop and Celeste \\nCampos -Castillo  [85] State Health Department Communication about \\nLong COVID in the United States on Facebook: \\nRisks, Prevention, and Support  Policy and \\nAdvocacy  \\nJuan S. Izquierdo -Condoy, \\nRaul Fernandez -Naranjo, \\nEduardo Vasconez -Gonz√°-\\nlez, Simone Cordovez, An-\\ndrea Tello -De-la-Torre, \\nClara Paz, Karen Delgado -\\nMoreira, Sarah Carrington, \\nGin√©s Viscor and Esteban \\nOrtiz -Prado  [86] Long COVID at Different Altitudes: A Country-\\nwide Epidemiological Analysis  Symptom \\nCharacter-\\nization  \\nSara Santarossa, Ashley \\nRapp, Saily Sardinas, Janine \\nHussein, Alex Ramirez, An-\\ndrea E Cassidy -Bushrow, \\nPhilip Cheng and Eunice Yu  \\n[87] Understanding the #longCOVID and #longhaul-\\ners Conversation on Twitter: Multimethod Study  Commu-\\nnity and \\nSupport  \\nAm√©lia D√©guilhem, Joelle \\nMalaab, Manissa Talmat-\\nkadi, Simon Renner, Pierre \\nFoulqui√©, Guy Fagherazzi, \\nPaul Loussikian, Tom \\nMarty, Adel Mebarki, \\nNathalie Texier and \\nStephane Schuck  [88] Identifying Profiles and Symptoms of Patients \\nWith Long COVID in France: Data Mining Info-\\ndemiology Study Based on Social Media  Symptom \\nCharacter-\\nization  \\nElham Dolatabadi, Diana \\nMoyano, Michael Bales, \\nSofija Spasojevic, Rohan \\nBhambhoria, Junaid Bhatti, \\nShyamolima Debnath, \\nNicholas Hoell, Xin Li, \\nCeline Leng, Sasha Nanda, \\nJad Saab, Esmat Sahak, \\nFanny Sie, Sara Uppal, \\nNirma Khatri Vadlamudi, \\nAntoaneta Vladimi rova, Ar-\\ntur Yakimovich, Xiaoxue \\nYang, Sedef Akinli Kocak \\nand Angela M. Cheung  [89] Using Social Media to Help Understand Patient -\\nReported Health Outcomes of Post ‚ÄìCOVID -19 \\nCondition: Natural Language Processing Ap-\\nproach  Symptom \\nCharacter-\\nization  \\nNida Ziauddeen, Deepti \\nGurdasani, Margaret E. \\nO‚ÄôHara, Claire Hastie, Paul \\nRoderick, Guiqing Yao and \\nNisreen A. Alwan  [90] Characteristics and Impact of Long Covid: Find-\\nings from an Online Survey  Symptom \\nCharacter-\\nization  \\nLudovica Segneri, Nandor \\nBabina, Teresa \\nHammerschmidt, Andrea \\nFronzetti Colladon and \\nPeter A. Gloor  [91] Too Much Focus on Your Health Might Be Bad \\nfor Your Health: Reddit User‚Äôs Communication \\nStyle Predicts Their Long COVID Likelihood  Symptom \\nCharacter-\\nization  10 \\nSai C. Reddy, Sanjana \\nKathiravan and Shubh M. \\nSingh  [92] An Analysis of Self -reported Long COVID -19 \\nSymptoms on Twitter  Symptom \\nCharacter-\\nization  \\nAbeed Sarker  [93] Mining Long -COVID Symptoms from Reddit: \\nWhat We Know So Far  Symptom \\nCharacter-\\nization  \\nEsperanza Miyake and Sam \\nMartin  [94] Long COVID: Online Patient Narratives, Public \\nHealth Communication, and Vaccine Hesitancy  Commu-\\nnity and \\nSupport  \\nAbeed Sarker and Yao Ge  \\n[95] Mining Long -COVID Symptoms from Reddit: \\nCharacterizing Post -COVID Syndrome from Pa-\\ntient Reports  Symptom \\nCharacter-\\nization  \\nAlexis Jordan and Albert \\nPark [96] Understanding the Plight of COVID -19 Long \\nHaulers Through Computational Analysis of \\nYouTube Content  NLP and \\nModeling  \\nBrigitte Juanals and Jean -\\nLuc Minel  [97] Using topic modeling and NLP  tools for analyz-\\ning long Covid coverage by French press and \\nTwitter  Commu-\\nnity and \\nSupport  \\nErkan Ozduran and Sibel \\nB√ºy√ºk√ßoban  [98] A Content Analysis of the Reliability and Qual-\\nity of YouTube Videos as a Source of Infor-\\nmation on Health -Related Post -COVID Pain  Commu-\\nnity and \\nSupport  \\nNo√©mie D√©om, Ahmad \\nFiras Khalid, Sam Martin, \\nMacarena Chepo, and Ce-\\ncilia Vindrola -Padros  [99] Unlocking the Mysteries of Long COVID in \\nChildren and Young People: Insights from a Pol-\\nicy Review and Social Media Analysis in the UK  Policy and \\nAdvocacy  \\nErin T. Jacques, Corey H. \\nBasch, Eunsun Park, Betty \\nKollia and Emma Barry  \\n[100]  Long Haul COVID -19 Videos on YouTube: Im-\\nplications for Health Communication  Symptom \\nCharacter-\\nization  \\nWilliam David Strain, \\nOndine Sherwood, Amitava \\nBanerjee, Vicky Van der \\nTogt, Lyth Hishmeh and \\nJeremy Rossman  [101]  The Impact of COVID Vaccination on Symp-\\ntoms of Long COVID: An International Survey \\nof People with Lived Experience of Long \\nCOVID  Symptom \\nCharacter-\\nization  \\nKrittiya Wongtavavimarn  \\n[102]  Social Support and Narrative Sensemaking \\nOnline: A Content Analysis of Facebook Posts \\nby COVID -19 Long Haulers  Commu-\\nnity and \\nSupport  \\n \\nIn emerging interdisciplinary research  areas such as Long COVID, the capacity to \\ncategorize studies without manually curated labels represents a novel contribution . \\nFields involving public health, computational linguistics, and social sciences often con-\\nverge on complex research questions, making any single classification system insuffi-\\ncient on its own. The zero -shot framework addressed this issue by enabling rapid, yet \\nreliable, placement of studies within relevant categories, facilitating a coherent view o f \\nhow different facets  - like symptom trajectories, policy guidance, and community en-\\ngagements  - interact in the evolving literature. Such a methodology also offers a blue-\\nprint for future works  that require the integration of heterogeneous sources of \\nknowledge, allowing researchers to devote more time to interpreting outcomes rather \\nthan refining labeling procedures.  In Section 4, a review of all these studies is presented.  11 \\n4 Review of Papers  \\nIn this section , each study that met the inclusion c riteria of this work has been re-\\nviewed  under  one of four broad areas , according to assignments generated by the zero -\\nshot learning model (discussed in Section  3). This automated classification process dis-\\ntinguished primary thematic emphases among the papers, placing them into ‚ÄúNLP and \\nModeling,‚Äù ‚ÄúPolicy and Advocacy,‚Äù ‚ÄúCommunity and Support,‚Äù or ‚ÄúSymptom Charac-\\nterization.‚Äù The subsequent sections explore each area, discussing how individual stud-\\nies addressed multimodal forms of social media -based inquiries into Long COVID.  \\n \\n4.1 NLP and Modeling  \\nFu [42] studied  concerns  about  Long COVID  as expressed on s ocial media . They \\nanalyzed  117,789 tweets  from March 2022 to April 2022 and utilized sentiment analy-\\nsis and topic modeling . Their objectives included identifying emergent themes from \\nusers‚Äô experiences, such as the social and economic burdens tied to Long COVID . They  \\nobserved that negative attitudes toward Long COVID  were especially widespread and \\nnoted that such sentiments raise d important considerations for clinicians and policy-\\nmakers . Jordan et al. [75]  conducted an investigation that combined a mixed approach \\nwith topic modeling. They gathered online data from medical sources, news outlets, \\nand self -identified ‚Äúlong haulers ‚Äù, highlighting how personal distress connected with \\ndissatisfaction regarding the healthcare system  in the context of L ong C OVID. They  \\nfound  multiple themes, most of which showe d concerns related to Long COVID that \\nhad either been disregarded or insufficiently recognized.  \\nKusuma et al. [76]  studied  social media data to isolate the most frequently discussed \\ntopics and to identify influential users engaging with the concept of extended recovery  \\nin the context of Lon g COVID . They used social network analysis and sentiment anal-\\nysis. They anal yzed  119,185 tweets from 94 ,325 users  to demonstrate how certain pub-\\nlic figures or health professionals influenced these discussions. The  findings of senti-\\nment analysis showed that most of these tweets we re negative. Thakur [77] studied \\n1,244,051 tweets  about Long COVID with a specific f ocus on using  VADER for sen-\\ntiment analysis . The findings showed that the percentages of tweets with positive, neg-\\native, and neutral sentiments were 43.1%, 42.7%, and 14.2%, respectively . The findings \\nof this study also showed that most tweets with a positive sentiment and most tweets  \\nwith a negative sentiment  were not highly polarized .  \\nThe study by Koss et al. [82] explored  the feasibility of social media mining methods \\nto extract insights shared by  Long COVID patients . They focused  on extracting insights \\nfrom Reddit  (‚Äú/r/covidlonghaulers ‚Äù), where participants described supplements and \\nmedications that they tested for symptom relief. Using named -entity recognition, they \\nmapped out networks to illustrate how certain substances  - such as magnesium, vita-\\nmins, and steroids  - appeared frequently and often in connection with each other  on \\nReddit . Jordan [96] conducted  text-mining of Long COVID -related content on  \\nYouTube. They collected transcripts and comments to learn how self -identified ‚Äúlong \\nhaulers‚Äù perceived their illnesses  and how broader audiences reacted. Jordan identified \\nrecurring issues that spanned uncertainty regarding medical systems, misinformation, \\nand the need for  coping strategies by applying topic modeling . The work of  Awoyemi \\net al. [78] involved  another exploration of tweets  about Long COVID. Their work's 12 \\ninitial data mining process resulted  in 62,232 tweets , which  were reduced to 10,670 \\ntweets after removing the duplicates. The ir study showed that  the majority of the tweets \\nabout Long COVID originated from the United States of America (38%), United King-\\ndom (30%), and Canada (13%), with the most common hashtags being #longcovid \\n(36%) and #covid (6.36%), and the most frequently used word being people (1.05%).  \\nThey also perfor med sentiment analys is, which showed that  the top three emotions de-\\ntected in these tweets  were trust (11.68%), fear (11.26%), and sadness (9.76%) .  \\n4.2 Policy and Advocacy  \\nRushforth et al. [63] used narrative inquiry  and analyze d a dataset of narrative inter-\\nviews and focus groups with 114 people with Long Covid from the United Kingdom, \\ndrawing on socio -narratology , therapeutic emplotment , and polyphonia . Their study \\nshowed how these personal stories served as catalysts for policy efforts and structural \\nreforms, emphasizing how influential firsthand accounts can be  helpful  in compelling \\ndecision -makers  to address an emerging public health challenge  such as Long COVID.  \\nBhattacharyya et al. [80] studied tweets about Long COVID to understand  the need \\nfor more resources to investigate the extended trajectory of COVID -19. They  used the \\nNational Research Council (NRC) Emotion Lexicon method for sentiment analysis  and \\nidentified an association between  retweets  and favorite counts on Twitter  and particular \\nemotional reactions, such as sadness, joy, or trust. Laestadius et al. [85] examined how \\nUS state health departments used Facebook  for public messaging about COVID -19, \\nwith a particular focus on mentions of Long COVID. Their study identified 49,310  \\npandemic -related posts, with fewer than 200 explicitly discussing  Long COVID . Using \\nquantitative content analysis methods, they coded these posts about Long COVI D. The \\nresults showed that  75.18% included language about susceptibility, 64.96% severity, \\nand 64.23% benefits of prevention. In addition to this, c ues to preventive action ap-\\npeared in 54.01% of posts  and 19.71% of posts provided guidance for those with Long \\nCOVID .  D√©om  et al. [99] used a mixed -methods approach  to analyze policy documen-\\ntation and social media discourse about children and teenagers suffering from Long \\nCOVID in the United Kingdom . The  authors used the LISTEN framework to demon-\\nstrate inconsistency in how guidelines reached the public and to emphasize the demand \\nfor mental health services for children, young people, and healthcare workers suffering \\nfrom  Long COVID. In their work, they also presented se veral policy recommendations , \\nsuch as enhancing accountability through regular audits, promoting inclusiveness by \\nincorporating perspectives  of children and young people , ensuring transparency via reg-\\nular updates, and maintaining equity in policy impact .  \\n \\n4.3 Community and Support  \\nRussell et al. [64] investigated how online communities offered solace and informa-\\ntional resources to individuals with Long COVID  symptoms. Through qualitative in-\\nterviews, they found that people experiencing these symptoms went through significant \\nambiguity, which was often made worse by invalidation or denial in clinical settings. \\nThe findings of their work showed that online communities  filled th is gap by offering \\nmutual support and reassurance, thus illustrating the essential psychological role that \\nsocial  networks can serve.  Meledandri [65] perfo rmed  a quantitative and qualitative \\nevaluation of approxi mately 600,000  twee ts about Long COVID . Their study showed 13 \\nthat s ome of these tweets  reflect ed conspiracy  theories involving vaccination, fake \\nnews , and post -truths , clashing with scientific evidence , and the remaining tweets re-\\nflected supportive stances . Rhodehamel [79] studied the public discourse about L ong \\nCOVID on Reddit  (r/covidlonghaulers ).  Their study showed  that community in the \\ncontext of Long COVID was built on Reddit  through three main themes . First, hope \\nthrough validation, knowledge sharing, and helpfulness. Second, k inship through com-\\nmiseration and shared experiences of suffering. Finally, the discourse surrounding \\nharm , including  ableism, grifting, exploitation, infighting, and tensions between people \\nsuffer ing from Long COVID  and society .  \\nGarrett et al. [84] investigated the experiences of women with Long COVID symp-\\ntoms  with a specific focus  on how social media played a dual role in either nurturing or \\nundermining well -being. The study sh owed that t he main roles of social media included \\nfacilitating support group participation, experience sharing, interpersonal connections, \\nand media consumption. The study also showed  that participants rel ied on social media \\nto fulfill their emotional support, social engagement, spirituality, health planning, in-\\nformation gathering, professional support, and recreational relaxation needs . The work \\ndone by Santarossa et al. [87] aimed to investigate the #longCOVID and #longhaulers \\nconversations on Twitter using  topic modeling  and social network analysis . The find-\\nings of their work  showed that a mong the 2010 tweets about long COVID -19 and 490 \\ntweets by COVID -19 long haulers, 30,923 and 7817 unique words were found, respec-\\ntively. Their work also showed that f or both conversation types, ‚Äú#longcovid‚Äù and \\n‚Äúcovid‚Äù were the most frequently mentioned words , and words relevant to having Long \\nCOVID were more frequ ently found  in tweets posted by individuals  suffering  from \\nLong COVID .  \\nMiyake et al. [94]  studied  social media data  collected at different points of the pan-\\ndemic to explore how patients felt when official communications diverged from their \\nlived experiences. They used a mixed methods approach involving quantitative and \\nqualitative analyses  and studied 1.38 million posts about Long COVID from Twitter, \\nFacebook, blogs, and forums . The r esults indicate d that the negative impacts arise \\nmostly from conflicting definitions of C OVID -19 and fears around the COVID -19 vac-\\ncine for individ uals suffering from Long COVID . Their study also identified th at key \\nareas of concern  in the context of Long COVID included time or duration , symptoms  \\nor testing , emotional impact , lack of support , and resources.  Juanal s et al. [97]  studied \\nLong Covid coverage by the French  press and Twitter . More specifically , the objectives \\nof th eir study were  to analyz e the modalities of construction and progressive visibility \\nof Long Covid  in the public media  and on Twitter  and to propose a methodology based \\non topic modeling and related conce pts in NLP  to conduct a comparative analysis be-\\ntween newspapers and Twitter coverage  of Long COVID . \\nOzduran et al. [98]  classified YouTube videos about Long COVID according to \\nvideo parameters and content analysis. They also determine d the q uality, reliability, \\nand accuracy of these videos using  the Global Quality Score (GQS), the Journal of \\nAmerican Medical Association (JAMA) Benchmark Criteria, and the Modified \\nDISCERN Questionnaire . The findings showed that out of 180 vid eos about L ong \\nCOVID, 74 were of low quality, 14 were of moderate quality, and 12 were of high \\nquality; 21% contained insufficient data, 73% contained partially sufficient data, and \\n6% contained completely sufficient data. Their work also showed that v ideos uploaded \\nby academic sources (66.7%) and physicians (12.5%) made up most  of the high -quality 14 \\ngroup. The authors also found a  statistically significant correlation between the source \\nof upload and the number of views (p = 0.014), likes (p = 0.030), comments (p = 0.007), \\nand video duration (p = 0.004).  \\n \\n4.4 Symptom Characterization  \\nSingh et al.  [66] focus ed on  identifying  sympto ms on  Twitter where users self-re-\\nported  Long COVID. They stud ied the tweets published by 89 Twitter users , and the \\nfindings of their study showed that most  users described multiple symptoms, out of \\nwhich the most common were fatigue, shortness of breath, pain,  and brain  fog or con-\\ncentration difficulties. Ziauddeen et al. [67] conducted an online survey with 2,550 par-\\nticipants, to highlight  the range  of symptoms of Long COVID and infer how such sy mp-\\ntoms affected  daily functioning. Their study showed that most participants described \\nfluctuating (57.7%) or relapsing symptoms (17.6%) , with physical activity, stress, and \\nsleep disturbance being the commonly triggered symptoms. Their study also found that \\none-third of participants reported being unable to live alone without assistance  six \\nweeks from the start of the illness , and 16.9% reported being unable to work alone  due \\nto COVID -19 illness.  The goal of the work done by Sarker et al. [68] was to infer  Long \\nCOVID  symptoms self -reported by users, compare symptom distributions across stud-\\nies, and create a symptom lexicon  by studying Long COVID -related posts on Reddit. \\nThey studied  42,995 posts by 4249 Reddit users , and the res ults showed that 1744 users \\nexpressed at least one symptom . The  results of their work also showed that the most \\nfrequently reported long -COVID symptoms were mental health -related symptoms \\n(55.2%), fatigue (51.2%), general ache  or pain (48.4%), brain fog  or confusion (32.8%) \\nand dyspnea (28.9%) . Banda et al. [69] used a combination of machine learning, natural \\nlanguage processing techniques, and clinician reviews  and mined 296,154 tweets  about \\nLong COVID. The objective of their study was  to characterize the course of Long \\nCOVID , creat e detailed timelines of symptoms and conditions, and analyz e their symp-\\ntomatology during a period of over 150 days .  \\nMassey et al. [70] posted a poll to a Faceboo k group of  169,900 members that asked \\nabout breakthrough COVID -19 cases, Long Covid, and hospitalizations . The fin dings \\nshowed that out o f the 1,949 participants who responded to the poll, 44 reported a \\nsymptomatic breakthrough case , and 24 reported that COVID -19 led to symptoms of \\nLong C OVID . Their study also found that 1 out of these 24 cases was hospitaliz ed. The \\ngoal of the research by Martin et al. [71] was to explore healthcare workers' perceptions \\nconcerning Long COVID in children and you ng people  in the UK between January \\n2021 and January 2022  by studying relevant posts on Twitter . This research showed \\nthat healthcare workers  were responsive to announcements issued by authorities regard-\\ning the management of COVID -19 in the UK , and the most frequent emotion expressed \\non Twitter in this regard  was negative. Th is research  also identified th e main themes  of \\nconversat ion, which included uncertainty about the future, policies and regulations, \\nmanaging and addressing COVID -19 and Long COVID in children and you ng people , \\nvaccination, using Twitter to share scientific literature and management strategies, and \\nclinical and personal experiences.  \\nDolatabadi et al. [72] aimed to determine the validity and effectiveness of advanced \\nNLP approaches  to derive insight into Long COVID -related patient -reported health \\noutcomes from social media platforms . They used Transformer -based BERT models to 15 \\nextract and normalize long COVID symptoms and conditions from English posts on \\nTwitter and Reddit . The  results indicated that the top three most commonly occurring \\nLong COVID symptoms were  systemic (such as ‚Äúfatigue‚Äù), neuropsychiatric (such as \\n‚Äúanxiety ‚Äù and ‚Äúbrain fog‚Äù), and respiratory (such as ‚Äúshortness of breath‚Äù) .  \\nMiao et al. [73] used an interactive information extraction tool  and analyzed tweets \\nabout Long COVID. The a uthors extracted key information from the relevant tweets \\nand analyzed the user -reported Long COVID symptoms  concerning  their demographic \\nand geographical characteristics.  Feng et a l. [74] also stud ied tweets about Long \\nCOVID. They classif ied Long COVID -related  tweets into four categories based on the \\ncontent, detected the presence of six basic emotions, and extracted  prevalent topics. \\nTheir  analyses reveal ed that negative emotions dominated throughout the study period . \\nMatharaarachchi et al. [81] implemented association rule mining  to understand the \\npatterns and behavior of long COVID symptoms reported by patients on Twitter . They \\nfound t hat in the 30,327 tweets included in their study, the most frequent symptoms \\nwere brain fog , fatigue , breathing  or lung issues , heart issues , flu symptoms , depression , \\nand general pain ; loss of smell and taste,  cold, cough, chest pain, fever, headache, and \\narm pain were noted  in 1.6% to 5.3% of patients with long COVID.  Ayadi et al. [83] \\ncollecte d 27,216 Reddit posts about Long COVID and performed a comprehensive data \\nanalysis. They found that o ver 78% of the analyzed posts referenced at least one symp-\\ntom of Long COVID. The most reported  symptoms were fatigue (29.4%), pain (22%), \\nbrain fog (19.1%), anxiety (17.7%), and headaches (15.6%). These symptoms fre-\\nquently co -occurred with others, such as fever and nasal congestion. The symptoms \\nwere categorized into general (45.5%), neurological  (42.9%), mental health  or psycho-\\nlogical  or behavioral (35.2%), body pain  or mobility (35.1%), and cardiorespiratory \\n(31.2%).  \\nIzquierdo -Condoy et al. [86 ] conducted a cross -sectional analysis of 2,103 partici-\\npants in Ecuador between April and July 2022, using an online  self-reporting question-\\nnaire to investigate Long COVID symptoms. Among the respondents, 52.3% (1,100) \\nreported Long COVID symptoms, with the majority being women (64%) and individ-\\nuals aged 21 -40 years (68.5%). Notably, 71.7% of the Long COVID cases occurred \\namong residents at high altitudes (>2500m), compared to 29.3% at lower altitudes. \\nCommon symptoms included fatigue (8.4%), hair loss (5.1%), and difficulty concen-\\ntrating (5.0%). The study id entified a greater prevalence of Long COVID symptoms \\namong women, individuals with severe initial infections, and those with comorbidities, \\nemphasizing the influence of altitude on Long COVID . D√©guilhem  et al. [88] conducted \\na comprehensive analysis of 15,364 messages from 6,494 individuals with Long \\nCOVID or their caregivers in France  from January 1, 2020, to August 10, 2021. The \\nstudy identified three primary symptom co -occurrences: asthenia -dyspnea (35.3%), as-\\nthenia -anxiety (22.5%), and asthenia -headaches (17.3%). Key difficulties reported by \\npatients included managing symptoms (35.4%  of messages), dealing with psychologi-\\ncal impacts such as anxiety and uncertainty (15.1%), enduring pain (12.0%), and coping \\nwith disruptions (9.4%) and professional life (8.0%). The analysis also categorized pa-\\ntients into three distinct profiles. Profile A consisted of 406 patients who exclusively \\nreported asthenia. Profile B included 129 patients who predominantly experienced anx-\\niety (100%), a long with asthenia (21.7%), dyspnea (11.6%), and ageusia (2.3%). Profile \\nC, with 141 patients, was characterized by dyspnea (100%) and asthenia (31.9%). 16 \\nAdditionally, the findings revealed that 49.1% of users expressed symptoms beyond \\nthree months post -infection, and 20.5% continued to report symptoms even after one \\nyear.  \\nSegneri et al. [91] analyzed the communication style and network structure of 6,107 \\nReddit users to identify social traits associated with Long COVID. The study catego-\\nrized users into three groups: No COVID (2,529 users), COVID (592 users), and Long \\nCOVID (2,986 users). They anal yzed pre-pandemic posts, totaling 984,625 , with 45% \\nfrom the No COVID group, 32% from the Long COVID group, and 23% from the \\nCOVID group. Key findings from their work indicated that Long COVID users exhib-\\nited lower social media activity and fewer connections than other groups. Furthermore, \\ntheir communication style included more health -related topics and frequent use of first -\\nperson singular pronouns but fewer anger -related words. The ir study also found that \\nLong COVID users were more likely to use interrogative language and verbose posts, \\nreflecting their focus on health concerns. Sarker [93] studied  self-reported Long \\nCOVID symptoms on the subreddit /r/covidlonghaulers . Using natural language pro-\\ncessing, they identified the most common symptoms, including anxiety  or stress, fa-\\ntigue, body pain, and brain fog. Their study showed that m ost users reported 1 -5 symp-\\ntoms, with a median of 4 symptoms per user. Jacques et al. [100] analyzed the 100 \\nmost -viewed YouTube videos discussing Long COVID symptoms, uploaded between \\nJuly 2020 and December 2021, which amassed 15,319,997 views. The y found that  the \\nmajority of these videos originated from television or internet -based news sources \\n(56%), followed by consumer -generated content (32%), health professionals (9%), and \\nentertainment TV (3%). Their study inferred  that p hysical symptoms such as fatigue \\n(73%), difficulty breathing (56%), and joint or muscle pain (49%) were most frequently \\ndiscussed, alongside cognitive issues like brain fog (69%). Other frequently reported \\nchallenges included worsening symptoms post -activi ty (37%) and psychological ef-\\nfects like anxiety or depression (17%). Their work also showed that  videos from enter-\\ntainment TV received significantly more likes than other categories .  \\nStrain  et al. [101]  surveyed  812 individuals with Long COVID to evaluate the impact \\nof COVID -19 vaccination on their symptoms. The participants, primarily younger fe-\\nmales (80.6%), reported symptoms persisting for over nine months in 71.6% of cases. \\nFollowing the first vaccination dos e, 57.9% of participants reported overall symptom \\nimprovement, while 17.9% experienced deterioration  and the rest reported no change. \\nImprovements were more pronounced with mRNA vaccines, such as Moderna (31% \\nimprovement) and Pfizer (24.4%), compared to the AstraZeneca adenoviral vector vac-\\ncine (22.6%). The most improved symptoms included fatigue (p = 0. 009), brain fog (p \\n= 0.01), and myalgia (p = 0.006). Their work found that s ymptom severity reductions \\nwere proportional to baseline scores  and f or half of the participants, symptom improve-\\nments were temporary, lasting 14 -21 days, while post -vaccination deterioration re-\\nsolved within 3 -7 days.  \\nThe breadth of research surveyed in these four categories underscores how multifac-\\neted Long COVID can be, spanning elements of advanced computational analyses, pol-\\nicy formation, community engage ment , and clinical symptom documentation. Whether \\ninvestigators used sentiment analysis to map public anxieties, examined official health \\ncommunications to reveal messaging gaps, or monitored online forums where individ-\\nuals assembled in search of guidance, each study contributed a piece to the broader \\npuzzle of persistent post-COVID complications. In a collective manner , these works 17 \\nindicate  that fully understanding Long COVID requires both interdisciplinary collabo-\\nration and innovative methodologies, particularly as social media continues to serve as \\na large -scale repository of patient -driven experiences. While some researchers focused \\non ca pturing emergent narratives via topic modeling, others underscored the imperative \\nof supporting those living with Long COVID . By merging perspectives from computa-\\ntional sciences, public health, and real -world patient accounts, this  body of literature \\nhighlights the need for sustained inquiry into Long COVID  and for responsive frame-\\nworks that can adapt to new findings as they emerge.  \\n5 Research Gaps and Future Directions  \\nAlthough numerous investigations have emerged around Long COVID and its social \\nmedia narratives, critical gaps  warrant further consideration. One prominent challenge \\nlies in harmonizing definitions and frameworks for the condition itself. Several studies \\nused different inclusion criteria, with some focusing on self -reported experiences and \\nothers requiring clinical diagnoses. This lack of uniform standards hampers efforts to \\ncompare outcomes across different populations and time periods. Equally important is \\nintegrating  finer details on patients‚Äô backgrounds, disease histories, and underlying con-\\nditions into study designs. Greater clarity in such baseline data could highlight  whether \\ncertain cohorts, including older adults, are more prone to persistent complications [ 107-\\n110]. In doing so, investigators may infer  how aging -related factors, such as immunose-\\nnescence or latent comorbidities, interact with ongoing COVID -19 symptoms \\n[111,112 ]. \\nA second research gap concerns the depth and breadth of symptom documentation. \\nWhile investigators have cataloged a wide array of complaints  - ranging from cognitive \\ndifficulties to cardiorespiratory issues  - there is still a shortage of longitudinal data that \\ndetail how and why certain symptoms linger or transform over time. More robust pro-\\nspective studies could help pinpoint potential transition points or flare -ups that individ-\\nuals frequently mention in onlin e communities. This endeavor would be particularly \\nmeaningful for older demographics, given that parallel research on aging has demon-\\nstrated the value of tracking gradual physiological changes across extended intervals \\n[113-116]. Building on this approach, future work might measure how chronic inflam-\\nmation or age -associated immune variations could shape Long COVID trajectories \\n[117-119]. Such insights would not only clarify the causes  of persistent symptoms but \\nmight also inform targeted interventions tailored to different life stages . \\nIn addition to these gaps in symptom tracking, a more systematic exploration of so-\\ncial determinants of health is necessary. Many of the reviewed studies used social media \\nposts without consistently capturing users‚Äô socioeconomic status, geographic context,  \\nor access to healthcare resources. Understanding how stressors  - like insufficient med-\\nical support or economic hardship  - can amplify Long COVID  complications would be \\ninvaluable for refining health policies. It would also broaden our understanding of how \\naging adults, who may already be dealing with multiple conditions, manage additional \\nburdens imposed by COVID -19 [120-123]. In this sense, bridging findings from ger-\\nontological research  - where socioeconomic disparities often exacerbate the severity of \\nage-related disorders  - could enrich the framework for studying Long COVID‚Äôs psy-\\nchosocial dimensions . 18 \\nAnother significant avenue for future work  involves methodological innovations, \\nparticularly those that incorporate advanced analytics beyond sentiment analysis or \\nbasic topic modeling. For example, multi -modal data analysis  - integrating text, voice, \\nand video  - could provide a richer characterization of individuals‚Äô lived experiences  \\nwith Long COVI D. Such approaches might benefit from cutting -edge computational \\ntools used in aging research, where sensors and wearables have been employed to track \\nphysiological and behavioral indicators of decline [ 124-126]. Translating these meth-\\nods to the COVID -19 context could enable real -time monitoring of symptom fluctua-\\ntions or early warning signs. Coordinated collaborations between specialists in geron-\\ntology and emerging fields like machine learning could foster the exchange of tech-\\nniques that have proven effective in monitoring complex, chronic conditions  [127-129]. \\nFurthermore , many researchers have drawn attention to the lack of formal clinical \\ntrials or intervention studies aimed at mitigating long -term symptoms  despite ample \\nanecdotal evidence shared on social media. Addressing this shortfall requires not just \\nlarger sample sizes but also ethically designed studies that compare different manage-\\nment strategies  - pharmacological, rehabilitative, or psychosocial. Such work would be \\nespecially beneficial if it includes older adults, given that gerontology has a long -stand-\\ning tradition of rigorously testing interventions aimed at prolonging functional auton-\\nomy [ 130-133]. By synthesizing expertise from both Long COVID and aging -focused \\ninvestigations, scholars could devise clinical protocols that emphasize the realities of \\nmulti -morbidity, polypharmacy, and overall resilience [ 134-136]. In effect, the field \\ncould progress toward interventions that systematically address both the biological un-\\nderpinnings of prolonged COVID -19 symptoms and the socio -emotional hurdles en-\\ncountered by diverse patient groups, including older populations .  \\nThese  directions underscore the need for broader interdisciplinary engagement, \\ndeeper longitudinal insights, and more nuanced epidemiological tools. They also high-\\nlight how lessons from aging research can advance  Long COVID  research , especially \\nregarding risk assessment, symptom evolution, and care strategies [ 135-140]. Future \\nwork could  also focus on  refining how social media data are aggregated and annotated. \\nResearchers often concentrate on static posts  [141,142] , yet new formats, such as \\nephemeral stories [143,144] or live chats  [145,146], offer dynamic insights into how \\nusers articulate their symptoms and need s in real -time. A structured, time -sensitive ap-\\nproach to analyzing these data would help identify swift changes in public sentiment or \\nemerging topics that might otherwise go unnoticed. Complementary initiatives to stand-\\nardize metadata collection could lay the groundwor k for more consistent data sharing \\nacross research gro ups, encouraging broader comparative efforts and mitigating gaps \\nin knowledge  [147-150]. \\nA related priority involves exploring how individuals engage with one another on \\nsocial media beyond simple ‚Äúlike‚Äù or ‚Äúshare‚Äù metrics  in the context of Long COVID -\\nrelated discussions . In many online communities, participants build deep interpersonal \\nnetworks that thrive on mutual trust, and these environments can shape patterns of \\nsymptom reporting and health -seeking behaviors  [151,152]. Future studies could  inves-\\ntigate how trust relationships form and evolve within these networks  and how  different \\nage groups interpret and disseminate healthcare information online. Such inquiries \\ncould highlight  how older adults, who may be managing multiple comorbidities, adapt \\nto social media for guidance and peer support in ways that diverge from younger de-\\nmographics.  Another avenue involves enhancing the accuracy of sentiment and topic 19 \\ndetection by adopting more context -aware algorithms. While many current models fo-\\ncus on keyword frequency or  text structure, there is room for approaches capable of \\ncapturing subtler emotional or cultural nuances. Future works could  consider leverag-\\ning contextual embeddings that adapt to evolving language trends, including new \\nphrases or slang  coined by social media  communities  [153-155]. By calibrating these \\nadvanced tools to different platforms  - whether Reddit, Twitter, or localized forums  - \\ninvestigators could develop a finer -grained picture of how users articulate the long -term \\neffects  of COVID -19 in different social media platforms .  \\nResearchers in this field  could also explore closer collaborations with social media \\nplatforms themselves. Data -access initiatives  and robust privacy safeguards  might fos-\\nter the co -design of tools and dashboards that allow public health professionals and \\ncommunity leaders to monitor and interpret conversations as they unfold. Such partner-\\nships could promote real -time feedback loops, where findings from social media anal-\\nysis guide new research questions and public health messaging, and vice versa. Over \\nthe long term, these initiatives may pave the way for better resource allocation and more \\nprecisely t ailored interventions, ensuring that people suffering fro m Long COVID  have \\nreliable information and consistent support.  \\nThe work presented in this paper  has a couple of limitations . The heterogeneous \\nnature of social media platforms  and varying user demographics and data availability  \\nmay introduce sampling biases. Additionally, the zero -shot classification method, \\nthough novel, relies on pre-trained  language representations and may not fully capture \\ncontextual nuances in highly specialized or region -specific vocabulary as exp ressed on \\nsocial media . \\n6 Conclusion  \\nLong COVID is becoming a complex health -related  problem that requires multiple \\napproaches to comprehensively study the wide range of clinical, long -term effects, and \\npsychosocial aspects associated with it across patients worldwide.  This review high-\\nlights the significant contribution of social media networks in furthering the under-\\nstanding of Long COVID, depicting many facets of how patients experience the condi-\\ntion, the symptoms that typically develop, and how the general population unde rstands \\nthis condition. Social media has allowed for the collection of patient -generated data in \\nreal-time, making it easier to represent the variety of symptoms asso ciated with  Long \\nCOVID . By presenting  a systematic review of studies that rely on user -generated data  \\non social media pl atforms  and by using  a transformer -based zero -shot learning ap-\\nproach, this paper offers new perspectives on how we can capture and categorize th is \\ncomplex research landscape. The review shows  that patients‚Äô online narratives do far \\nmore than supplement clinical findings; they also create a real -time feedback loop that \\nmay guide research questions and shape both public health policies and advocacy ef-\\nforts. These accounts often raise critical qu estions about  persistent symptoms, gaps in \\nhealthcare, and the psychological burden of extended illness. By examining these issues \\nwithin a coherent, data -driven framework, the paper shows how computational methods \\ncan highlight  patterns and themes crucial for this research area . \\nHowever, the significance of this research goes beyond  mapping out social media \\ntrends  and insights . The zero -shot classification pipeline demonstrates that advanced 20 \\nlanguage models  can identify meaningful categories within research papers  on emerg-\\ning conditions, all while bypassing conventional manual labeling workflows. This \\nstreamlined process broadens the scope of discovery, allowing for timely insights in a \\nfield that demands ongoing updates. The insights presented  here confirm a vital need \\nfor refining longitudinal approaches, standardizing both clinical and analytical frame-\\nworks, and exploring how diverse populations experience Long COVID. The future \\ndirections presented in this paper highlight that i nterdisciplinary research should utilize \\nadvanced  computational tools that capture the nuanced language and behavior of social \\nmedia users, especially as public discourse and scientific understanding related to L ong \\nCOVID continue s to evolve. Furthermore , investigators may also delve into multi -\\nmodal social media data - such as images, videos, and real -time audio streams  - to sup-\\nplement textual content. In summary , this paper‚Äôs findings underscore the potential of \\ncombining patient -driven data with advanced analytics to refine our understanding  of \\nLong COVID , while also laying the groundwork for future work in this area  that adapt s \\nto new discoveries and patient needs.  \\nReferences  \\n1. Ciotti, M., Ciccozzi, M., Terrinoni, A., Jiang, W. -C., Wang, C. -B., Bernardini, S.: The \\nCOVID -19 pandemic. Crit. Rev. Clin. Lab. Sci. 57, 365 ‚Äì388 (2020). \\nhttps://doi.org/10.1080/10408363.2020.1783198.  \\n2. Velavan, T.P., Meyer, C.G.: The COVID‚Äê19 epidemic. Trop. Med. Int. Health. 25, 278 ‚Äì\\n280 (2020). https://doi.org/10.1111/tmi.13383.  \\n3. Yesudhas, D., Srivastava, A., Gromiha, M.M.: COVID -19 outbreak: history, mechanism, \\ntransmission, structural studies and therapeutics. Infection. 49, 199 ‚Äì213 (2021). \\nhttps://doi.org/10.1007/s15010 -020-01516 -2. \\n4. Allen, D.W.: Covid -19 lockdown cost/benefits: A critical assessment of the literature. Int. \\nJ. Econ. Bus. 29, 1 ‚Äì32 (2022). https://doi.org/10.1080/13571516.2021.1976051.  \\n5. Cucinotta, D., Vanelli, M.: WHO Declares COVID -19 a Pandemic. Acta Biomed. Ateneo \\nParmense. 91, 157 ‚Äì160 (2020). https://doi.org/10.23750/abm.v91i1.9397.  \\n6. COVID -19 cases, https://covid19.who.int/, last accessed 2024/12/23.  \\n7. Raveendran, A.V., Jayadevan, R., Sashidharan, S.: Long COVID: An overview. Diabetes \\nMetab. Syndr. 15, 869 ‚Äì875 (2021). https://doi.org/10.1016/j.dsx.2021.04.007.  \\n8. Altmann, D.M., Whettlock, E.M., Liu, S., Arachchillage, D.J., Boyton, R.J.: The immu-\\nnology of long COVID. Nat. Rev. Immunol. 23, 618 ‚Äì634 (2023). \\nhttps://doi.org/10.1038/s41577 -023-00904 -7. \\n9. Fern√°ndez -de-las-Pe√±as, C.: Long COVID: current definition. Infection. 50, 285 ‚Äì286 \\n(2022). https://doi.org/10.1007/s15010 -021-01696 -5. \\n10. CDC: Long COVID basics, https://www.cdc.gov/covid/long -term-effects/index.html, last \\naccessed 2024/12/23.  \\n11. Committee on Examining the Working Definition for Long COVID, Board on Health Sci-\\nences Policy, Board on Global Health, Health and Medicine Division, National Academies \\nof Sciences, Engineering, and Medicine: A long COVID definition: A chronic, systemic \\ndisease state with profound consequences, http://dx.doi.org/10.17226/27768, (2024). \\nhttps://doi.org/10.17226/27768.  \\n12. Aiyegbusi, O.L., Hughes, S.E., Turner, G., Rivera, S.C., McMullan, C., Chandan, J.S., \\nHaroon, S., Price, G., Davies, E.H., Nirantharakumar, K., Sapey, E., Calvert, M.J., on be-\\nhalf of the TLC Study Group: Symptoms, complications and management of long COVID : 21 \\na review. J. R. Soc. Med. 114, 428 ‚Äì442 (2021). \\nhttps://doi.org/10.1177/01410768211032850.  \\n13. Subramanian, A., Nirantharakumar, K., Hughes, S., Myles, P., Williams, T., Gokhale, \\nK.M., Taverner, T., Chandan, J.S., Brown, K., Simms -Williams, N., Shah, A.D., Singh, \\nM., Kidy, F., Okoth, K., Hotham, R., Bashir, N., Cockburn, N., Lee, S.I., Turner, G.M.,  \\nGkoutos, G.V., Aiyegbusi, O.L., McMullan, C., Denniston, A.K., Sapey, E., Lord, J.M., \\nWraith, D.C., Leggett, E., Iles, C., Marshall, T., Price, M.J., Marwaha, S., Davies, E.H., \\nJackson, L.J., Matthews, K.L., Camaradou, J., Calvert, M., Haroon, S.: Symptom s and risk \\nfactors for long COVID in non -hospitalized adults. Nat. Med. 28, 1706 ‚Äì1714 (2022). \\nhttps://doi.org/10.1038/s41591 -022-01909 -w. \\n14. Sudre, C.H., Murray, B., Varsavsky, T., Graham, M.S., Penfold, R.S., Bowyer, R.C., Pu-\\njol, J.C., Klaser, K., Antonelli, M., Canas, L.S., Molteni, E., Modat, M., Jorge Cardoso, \\nM., May, A., Ganesh, S., Davies, R., Nguyen, L.H., Drew, D.A., Astley, C.M., Josh i, A.D., \\nMerino, J., Tsereteli, N., Fall, T., Gomez, M.F., Duncan, E.L., Menni, C., Williams, \\nF.M.K., Franks, P.W., Chan, A.T., Wolf, J., Ourselin, S., Spector, T., Steves, C.J.: Attrib-\\nutes and predictors of long COVID. Nat. Med. 27, 626 ‚Äì631 (2021). \\nhttps: //doi.org/10.1038/s41591 -021-01292 -y. \\n15. Notarte, K.I., Catahay, J.A., Velasco, J.V., Pastrana, A., Ver, A.T., Pangilinan, F.C., \\nPeligro, P.J., Casimiro, M., Guerrero, J.J., Gellaco, M.M.L., Lippi, G., Henry, B.M., Fer-\\nn√°ndez -de-las-Pe√±as, C.: Impact of COVID -19 vaccination on the risk of developi ng long -\\nCOVID and on existing long -COVID symptoms: A systematic review. EClinicalMedi-\\ncine. 53, 101624 (2022). https://doi.org/10.1016/j.eclinm.2022.101624.  \\n16. Cabrera Martimbianco, A.L., Pacheco, R.L., Bagattini, √Ç.M., Riera, R.: Frequency, signs \\nand symptoms, and criteria adopted for long COVID‚Äê19: A systematic review. Int. J. Clin. \\nPract. 75, (2021). https://doi.org/10.1111/ijcp.14357.  \\n17. Ayoubkhani, D., Bermingham, C., Pouwels, K.B., Glickman, M., Nafilyan, V., Zaccardi, \\nF., Khunti, K., Alwan, N.A., Walker, A.S.: Trajectory of long covid symptoms after covid -\\n19 vaccination: community based cohort study. BMJ. 377, e069676 (2022). \\nhttps://do i.org/10.1136/bmj -2021 -069676.  \\n18. Davis, H.E., Assaf, G.S., McCorkell, L., Wei, H., Low, R.J., Re‚Äôem, Y., Redfield, S., Aus-\\ntin, J.P., Akrami, A.: Characterizing long COVID in an international cohort: 7 months of \\nsymptoms and their impact. EClinicalMedicine. 38, 101019 (2021). \\nhttps://doi.o rg/10.1016/j.eclinm.2021.101019.  \\n19. Yong, S.J.: Long COVID or post -COVID -19 syndrome: putative pathophysiology, risk \\nfactors, and treatments. Infect. Dis. (Lond.). 53, 737 ‚Äì754 (2021). \\nhttps://doi.org/10.1080/23744235.2021.1924397.  \\n20. Koc, H.C., Xiao, J., Liu, W., Li, Y., Chen, G.: Long COVID and its management. Int. J. \\nBiol. Sci. 18, 4768 ‚Äì4780 (2022). https://doi.org/10.7150/ijbs.75056.  \\n21. Al-Aly, Z., Davis, H., McCorkell, L., Soares, L., Wulf -Hanson, S., Iwasaki, A., Topol, \\nE.J.: Long COVID science, research and policy. Nat. Med. 30, 2148 ‚Äì2164 (2024). \\nhttps://doi.org/10.1038/s41591 -024-03173 -6. \\n22. Tana, C., Bentivegna, E., Cho, S. -J., Harriott, A.M., Garc√≠a -Azor√≠n, D., Labastida -\\nRamirez, A., Ornello, R., Raffaelli, B., Beltr√°n, E.R., Ruscheweyh, R., Martelletti, P.: \\nLong COVID headache. J. Headache Pain. 23, (2022). https://doi.org/10.1186/s10194 -\\n022-01450 -8. \\n23. Peluso, M.J., Deeks, S.G.: Mechanisms of long COVID and the path toward therapeutics. \\nCell. 187, 5500 ‚Äì5529 (2024). https://doi.org/10.1016/j.cell.2024.07.054.  \\n24. Greenhalgh, T., Sivan, M., Perlowski, A., Nikolich, J.≈Ω.: Long COVID: a clinical update. \\nLancet. 404, 707 ‚Äì724 (2024). https://doi.org/10.1016/s0140 -6736(24)01136 -x. \\n25. Sykes, D.L., Holdsworth, L., Jawad, N., Gunasekera, P., Morice, A.H., Crooks, M.G.: \\nPost-COVID -19 symptom burden: What is long -COVID and how should we manage it? \\nLung. 199, 113 ‚Äì119 (2021). https://doi.org/10.1007/s00408 -021-00423 -z. 22 \\n26. Tsao, S. -F., Chen, H., Tisseverasinghe, T., Yang, Y., Li, L., Butt, Z.A.: What social media \\ntold us in the time of COVID -19: a scoping review. Lancet Digit. Health. 3, e175 ‚Äìe194 \\n(2021). https://doi.org/10.1016/s2589 -7500(20)30315 -0. \\n27. Thakur, N.: Social media mining and analysis: A brief review of recent challenges. Infor-\\nmation (Basel). 14, 484 (2023). https://doi.org/10.3390/info14090484.  \\n28. Gottlieb, M., Dyer, S.: Information and disinformation: Social media in the COVID‚Äê19 \\ncrisis. Acad. Emerg. Med. 27, 640 ‚Äì641 (2020). https://doi.org/10.1111/acem.14036.  \\n29. Thakur, N., Han, C.: An exploratory study of tweets about the SARS -CoV -2 Omicron \\nvariant: Insights from sentiment analysis, language interpretation, source tracking, type \\nclassification, and embedded URL detection. COVID. 2, 1026 ‚Äì1049 (2022). \\nhttps://doi.o rg/10.3390/covid2080076.  \\n30. Hussain, W.: Role of social media in COVID -19 pandemic. Int J Front Sci. 4, (2024). \\nhttps://doi.org/10.37978/tijfs.v4i2.144.  \\n31. Thakur, N.: Sentiment analysis and text analysis of the public discourse on Twitter about \\nCOVID -19 and MPox. Big Data Cogn. Comput. 7, 116 (2023). \\nhttps://doi.org/10.3390/bdcc7020116.  \\n32. Shoaei, M.D., Dastani, M.: The role of Twitter during the COVID -19 crisis: A systematic \\nliterature review. Acta Inform. Pragensia. 9, 154 ‚Äì169 (2020). \\nhttps://doi.org/10.18267/j.aip.138.  \\n33. Thakur, N., Cui, S., Khanna, K., Knieling, V., Duggal, Y.N., Shao, M.: Investigation of \\nthe gender -specific discourse about online learning during COVID -19 on Twitter using \\nsentiment analysis, subjectivity analysis, and toxicity analysis. Computers. 12, 22 1 (2023). \\nhttps://doi.org/10.3390/computers12110221.  \\n34. Southwick, L., Guntuku, S.C., Klinger, E.V., Seltzer, E., McCalpin, H.J., Merchant, R.M.: \\nCharacterizing COVID -19 content posted to TikTok: Public sentiment and response dur-\\ning the first phase of the COVID -19 pandemic. J. Adolesc. Health. 69, 234 ‚Äì241 (2021 ). \\nhttps://doi.org/10.1016/j.jadohealth.2021.05.010.  \\n35. Patel, K.A., Thakur, N.: Dissemination of misinformation about COVID -19 on TikTok: A \\nmultimodal analysis. In: Communications in Computer and Information Science. pp. 109 ‚Äì\\n120. Springer Nature Switzerland, Cham (2024).  \\n36. Rovetta, A., Bhagavathula, A.S.: Global infodemiology of COVID -19: Analysis of Google \\nweb searches and Instagram hashtags. J. Med. Internet Res. 22, e20673 (2020). \\nhttps://doi.org/10.2196/20673.  \\n37. Thakur, N.: Five years of COVID -19 discourse on Instagram: A labeled Instagram dataset \\nof over half a million posts for multilingual sentiment analysis. In: 2024 7th International \\nConference on Machine Learning and Natural Language Processing (MLNLP). pp. 1‚Äì10. \\nIEEE (2024).  \\n38. Mejova, Y., Kalimeri, K.: COVID -19 on Facebook ads: Competing agendas around a pub-\\nlic health crisis. In: Proceedings of the 3rd ACM SIGCAS Conference on Computing and \\nSustainable Societies. pp. 22 ‚Äì31. ACM, New York, NY, USA (2020).  \\n39. Perrotta, D., Grow, A., Rampazzo, F., Cimentada, J., Del Fava, E., Gil -Clavel, S., Zagheni, \\nE.: Behaviours and attitudes in response to the COVID -19 pandemic: insights from a cross -\\nnational Facebook survey. EPJ Data Sci. 10, 17 (2021). \\nhttps://doi.org/10.1 140/epjds/s13688 -021-00270 -1. \\n40. Li, H.O. -Y., Bailey, A., Huynh, D., Chan, J.: YouTube as a source of information on \\nCOVID -19: a pandemic of misinformation? BMJ Glob. Health. 5, e002604 (2020). \\nhttps://doi.org/10.1136/bmjgh -2020 -002604.  \\n41. Thakur, N., Cui, S., Knieling, V., Khanna, K., Shao, M.: Investigation of the misinfor-\\nmation about COVID -19 on YouTube using topic modeling, sentiment analysis, and lan-\\nguage analysis. Computation (Basel). 12, 28 (2024). https://doi.org/10.3390/computa-\\ntion1 2020028.  23 \\n42. Veselovsky, V., Anderson, A.: Reddit in the time of COVID. Proceedings of the Interna-\\ntional AAAI Conference on Web and Social Media. 17, 878 ‚Äì889 (2023). \\nhttps://doi.org/10.1609/icwsm.v17i1.22196.  \\n43. Nirmalya, T., Kesha, A.P., Audrey, P., Shuqi, C., Nazif, A., Rishika, S., Riyan, S.: Quan-\\ntifying public response to COVID -19 events: Introducing the Community Sentiment and \\nEngagement Index, http://arxiv.org/abs/2412.16925, (2024).  \\n44. Daglis, T., Tsagarakis, K.P.: A LinkedIn -based analysis of the U.S. dynamic adaptations \\nin healthcare during the COVID -19 pandemic. Healthcare Analytics. 5, 100291 (2024). \\nhttps://doi.org/10.1016/j.health.2023.100291.  \\n45. Pardim, V.I., Pinochet, L.H.C., Souza, C.A., Viana, A.B.N.: The behavior of young people \\nat the beginning of their career through LinkedIn. RAM Rev. Adm. Mackenzie. 23, \\neRAMG220064 (2022). https://doi.org/10.1590/1678 -6971/eramg220064.en.  \\n46. Hinchey, L., Michon, A., Drews, J., Price, M., Christian, J., Pernice, F., Aquila, R.: Club-\\nhouses as essential communities during the COVID -19 pandemic. J. Psychosoc. Rehabil. \\nMent. Health. 9, 149 ‚Äì157 (2022). https://doi.org/10.1007/s40737 -021-00242 -8. \\n47. Junaid, S., Mutschler, C., McShane, K., The Canadian Clubhouse Research Group: The \\nimpact of COVID -19 on clubhouse employment programs. Community Ment. Health J. \\n59, 523 ‚Äì530 (2023). https://doi.org/10.1007/s10597 -022-01036 -3. \\n48. Ayob, M.A., Hadi, N.A., Ezad, M., Pahroraji, H.M., Ismail, B., Saaid, M.N.F.: Promoting \\n‚ÄòDiscord‚Äô as a platform for learning engagement during Covid -19 pandemic. Asian J. Univ. \\nEduc. 18, 663 ‚Äì673 (2022). https://doi.org/10.24191/ajue.v18i3.18953.  \\n49. Ardiyansah, T.Y., Batubara, R.W., Auliya, P.K.: Using discord to facilitate students in \\nteaching learning process during COVID -19 outbreak. Journal of English Teaching, Lit-\\nerature, and Applied Linguistics. 5, 76 (2021). https://doi.org/10.30587/jetlal.v5i1 .2528.  \\n50. Yang, Q., Wang, W., Pierce, L., Vaish, R., Shi, X., Shah, N.: Online communication shifts \\nin the midst of the Covid -19 pandemic: A case study on Snapchat. Proceedings of the \\nInternational AAAI Conference on Web and Social Media. 15, 830 ‚Äì840 (2021). \\nhttps:/ /doi.org/10.1609/icwsm.v15i1.18107.  \\n51. Spieler, B., Batte, C., Mackey, D., Henry, C., Danrad, R., Sabottke, C., Pirtle, C., Mussell, \\nJ., Wallace, E.: Diagnosis in a snap: a pilot study using Snapchat in radiologic didactics. \\nEmerg. Radiol. 28, 93 ‚Äì102 (2021). https://doi.org/10.1007/s10140 -020-01825 -x. \\n52. Yue, Z., Zhang, R., Xiao, J.: Social media use, perceived social support, and well -being: \\nEvidence from two waves of surveys peri - and post -COVID -19 lockdown. J. Soc. Pers. \\nRelat. 41, 1279 ‚Äì1297 (2024). https://doi.org/10.1177/02654075231188185.  \\n53. Thakur, N., Duggal, Y.N., Liu, Z.: Analyzing public reactions, perceptions, and attitudes \\nduring the MPox outbreak: Findings from Topic Modeling of Tweets. Computers. 12, 191 \\n(2023). https://doi.org/10.3390/computers12100191.  \\n54. Thakur, N.: MonkeyPox2022Tweets: A large -scale Twitter dataset on the 2022 Monkey-\\npox outbreak, findings from analysis of Tweets, and open research questions. Infect. Dis. \\nRep. 14, 855 ‚Äì883 (2022). https://doi.org/10.3390/idr14060087.  \\n55. Perego, E.: #LongCovid, https://twitter.com/elisaperego78/sta-\\ntus/1263172084055838721?s=20, last accessed 2024/12/23.  \\n56. Thakur, N., Cho, H., Cheng, H., Lee, H.: Analysis of user diversity -based patterns of pub-\\nlic discourse on twitter about mental health in the context of online learning during \\nCOVID -19. In: Lecture Notes in Computer Science. pp. 367 ‚Äì389. Springer Nature Swi t-\\nzerland, Cham (2023).  \\n57. Schillinger, D., Chittamuru, D., Ram√≠rez, A.S.: From ‚Äúinfodemics‚Äù to health promotion: \\nA novel framework for the role of social media in public health. Am. J. Public Health. 110, \\n1393 ‚Äì1396 (2020). https://doi.org/10.2105/ajph.2020.305746.  \\n58. Thakur, N., Patel, K.A., Poon, A., Shah, R., Azizi, N., Han, C.: A comprehensive analysis \\nand investigation of the public discourse on twitter about exoskeletons from 2017 to 2023. \\nFuture Internet. 15, 346 (2023). https://doi.org/10.3390/fi15100346.  24 \\n59. Pourpanah, F., Abdar, M., Luo, Y., Zhou, X., Wang, R., Lim, C.P., Wang, X. -Z., Wu, \\nQ.M.J.: A review of generalized zero -shot learning methods. IEEE Trans. Pattern Anal. \\nMach. Intell. 45, 1 ‚Äì20 (2022). https://doi.org/10.1109/tpami.2022.3191696.  \\n60. Romera -Paredes, B., Torr, P.H.S.: An embarrassingly simple approach to zero -shot learn-\\ning. ICML. 37, 2152 ‚Äì2161 (07 --09 Jul 2015). https://doi.org/10.1007/978 -3-319-50077 -\\n5_2. \\n61. Wang, W., Zheng, V.W., Yu, H., Miao, C.: A survey of zero -shot learning: Settings, meth-\\nods, and applications. ACM Trans. Intell. Syst. Technol. 10, 1 ‚Äì37 (2019). \\nhttps://doi.org/10.1145/3293318.  \\n62. Fu, Y.: Investigating public perceptions regarding the Long COVID on Twitter using sen-\\ntiment analysis and topic modeling. Med. Data Min. (2022). \\nhttps://doi.org/10.53388/mdm20220520024.  \\n63. Rushforth, A., Ladds, E., Wieringa, S., Taylor, S., Husain, L., Greenhalgh, T.: Long Covid \\n‚Äì The illness narratives. Soc. Sci. Med. 286, 114326 (2021). \\nhttps://doi.org/10.1016/j.socscimed.2021.114326.  \\n64. Russell, D., Spence, N.J., Chase, J. -A.D., Schwartz, T., Tumminello, C.M., Bouldin, E.: \\nSupport amid uncertainty: Long COVID illness experiences and the role of online com-\\nmunities. SSM Qual. Res. Health. 2, 100177 (2022). \\nhttps://doi.org/10.1016/j.ssmqr.20 22.100177.  \\n65. Meledandri, F.: The impact of polarised social media networking communications in the \\n#longcovid debate between ideologies and scientific facts, \\nhttp://dx.doi.org/10.13136/2281 -4582/2024.I23.1450, (2024). \\nhttps://doi.org/10.13136/2281 -4582/2024.I23.1450.  \\n66. Singh, S.M., Reddy, C.: An analysis of self -reported longcovid symptoms on twitter, \\nhttp://dx.doi.org/10.1101/2020.08.14.20175059, (2020). \\nhttps://doi.org/10.1101/2020.08.14.20175059.  \\n67. Ziauddeen, N., Gurdasani, D., O‚ÄôHara, M.E., Hastie, C., Roderick, P., Yao, G., Alwan, \\nN.A.: Characteristics of Long Covid: findings from a social media survey, \\nhttp://dx.doi.org/10.1101/2021.03.21.21253968, (2021). \\nhttps://doi.org/10.1101/2021.03.21.212539 68. \\n68. Sarker, A., Ge, Y.: Long COVID symptoms from Reddit: Characterizing post -COVID \\nsyndrome from patient reports, http://dx.doi.org/10.1101/2021.06.15.21259004, (2021). \\nhttps://doi.org/10.1101/2021.06.15.21259004.  \\n69. Banda, J.M., Adderley, N., Ahmed, W. -U.-R., AlGhoul, H., Alser, O., Alser, M., Areia, \\nC., Cogenur, M., Fi≈°ter, K., Gombar, S., Huser, V., Jonnagaddala, J., Lai, L.Y.H., Leis, \\nA., Mateu, L., Mayer, M.A., Minty, E., Morales, D., Natarajan, K., Paredes, R., P eriyakoil, \\nV.S., Prats -Uribe, A., Ross, E.G., Singh, G., Subbian, V., Vivekanantham, A., Prieto -Al-\\nhambra, D.: Characterization of long -term patient -reported symptoms of COVID -19: an \\nanalysis of social media data, http://dx.doi.org/10.1101/2021.07.13.212604 49, (2021). \\nhttps://doi.org/10.1101/2021.07.13.21260449.  \\n70. Massey, D., Berrent, D., Krumholz, H.: Breakthrough symptomatic COVID -19 infections \\nleading to Long Covid: Report from Long Covid Facebook group poll, \\nhttp://dx.doi.org/10.1101/2021.07.23.21261030, (2021). \\nhttps://doi.org/10.1101/2021.07.23.21261030.  \\n71. Martin, S., Chepo, M., D√©om, N., Khalid, A.F., Vindrola -Padros, C.: ‚Äú#LongCOVID af-\\nfects children too‚Äù: A Twitter analysis of healthcare workers‚Äô sentiment and discourse \\nabout Long COVID in children and young people in the UK, \\nhttp://dx.doi.org/10.1101/2022 .07.20.22277865, (2022). \\nhttps://doi.org/10.1101/2022.07.20.22277865.  \\n72. Dolatabadi, E., Moyano, D., Bales, M., Spasojevic, S., Bhambhoria, R., Bhatti, J., \\nDebnath, S., Hoell, N., Li, X., Leng, C., Nanda, S., Saab, J., Sahak, E., Sie, F., Uppal, S., \\nVadlamudi, N.K., Vladimirova, A., Yakimovich, A., Yang, X., Kocak, S.A., Cheung , 25 \\nA.M.: Using social media to help understand long COVID patient reported health out-\\ncomes: A natural language processing approach, \\nhttp://dx.doi.org/10.1101/2022.12.14.22283419, (2022). \\nhttps://doi.org/10.1101/2022.12.14.22283419.  \\n73. Miao, L., Last, M., Litvak, M.: An interactive analysis of user -reported long COVID symp-\\ntoms using twitter data. In: Hruschka, E., Mitchell, T., Mladenic, D., Grobelnik, M., and \\nBhutani, N. (eds.) Proceedings of the 2nd Workshop on Deriving Insights from U ser-Gen-\\nerated Text. pp. 10 ‚Äì19. Association for Computational Linguistics, Stroudsburg, PA, USA \\n(2022).  \\n74. Guocheng, F., Huaiyu, C., Wei, Q.: Exploring the emotional and mental well -being of \\nindividuals with Long COVID through twitter analysis, http://arxiv.org/abs/2307.07558, \\n(2023).  \\n75. Jordan, A., Park, A.: Understanding the long haulers of COVID -19: Mixed methods anal-\\nysis of YouTube content. JMIR AI. 3, e54501 (2024). https://doi.org/10.2196/54501.  \\n76. Kusuma, I.Y., Suherman, S.: The pulse of long COVID on Twitter: A social network anal-\\nysis. Arch. Iran. Med. 27, 36 ‚Äì43 (2024). https://doi.org/10.34172/aim.2024.06.  \\n77. Thakur, N.: Investigating and analyzing self -reporting of Long COVID on Twitter: Find-\\nings from sentiment analysis. Appl. Syst. Innov. 6, 92 (2023). \\nhttps://doi.org/10.3390/asi6050092.  \\n78. Awoyemi, T., Ebili, U., Olusanya, A., Ogunniyi, K.E., Adejumo, A.V.: Twitter sentiment \\nanalysis of long COVID syndrome. Cureus. 14, e25901 (2022). https://doi.org/10.7759/cu-\\nreus.25901.  \\n79. DIGITAL LONG -HAULER LIFELINES: UNDERSTANDING HOW PEOPLE WITH \\nLONG COVID BUILD COMMUNITY ON REDDIT, https://www.researchgate.net/pub-\\nlication/385720439_Digital_Long -Hauler_Lifelines_Understanding_How_Peo-\\nple_with_Long_Covid_Build_Community_on_Reddit, last acc essed 2024/12/24.  \\n80. Bhattacharyya, A., Seth, A., Rai, S.: The effects of long COVID -19, its severity, and the \\nneed for immediate attention: Analysis of clinical trials and Twitter data. Front. Big Data. \\n5, (2022). https://doi.org/10.3389/fdata.2022.1051386.  \\n81. Matharaarachchi, S., Domaratzki, M., Katz, A., Muthukumarana, S.: Discovering long \\nCOVID symptom patterns: Association rule mining and sentiment analysis in social media \\ntweets. JMIR Form. Res. 6, e37984 (2022). https://doi.org/10.2196/37984.  \\n82. Koss, J., Bohnet -Joschko, S.: Social media mining of long -COVID self -medication re-\\nported by Reddit users: Feasibility study to support drug repurposing. JMIR Form. Res. 6, \\ne39582 (2022). https://doi.org/10.2196/39582.  \\n83. Ayadi, H., Bour, C., Fischer, A., Ghoniem, M., Fagherazzi, G.: The Long COVID experi-\\nence from a patient‚Äôs perspective: a clustering analysis of 27,216 Reddit posts. Front. Pub-\\nlic Health. 11, (2023). https://doi.org/10.3389/fpubh.2023.1227807.  \\n84. Garrett, C., Aghaei, A., Aggarwal, A., Qiao, S.: The role of social media in the experiences \\nof COVID -19 among long -hauler women: Qualitative study. JMIR Hum. Factors. 11, \\ne50443 (2024). https://doi.org/10.2196/50443.  \\n85. Laestadius, L.I., Guidry, J.P.D., Bishop, A., Campos -Castillo, C.: State health department \\ncommunication about long COVID in the United States on Facebook: Risks, prevention, \\nand support. Int. J. Environ. Res. Public Health. 19, 5973 (2022). \\nhttps://doi.or g/10.3390/ijerph19105973.  \\n86. Izquierdo -Condoy, J.S., Fernandez -Naranjo, R., Vasconez -Gonz√°lez, E., Cordovez, S., \\nTello -De-la-Torre, A., Paz, C., Delgado -Moreira, K., Carrington, S., Viscor, G., Ortiz -\\nPrado, E.: Long COVID at different altitudes: A countrywide epidemiological analysis.  \\nInt. J. Environ. Res. Public Health. 19, 14673 (2022). \\nhttps://doi.org/10.3390/ijerph192214673.  \\n87. Santarossa, S., Rapp, A., Sardinas, S., Hussein, J., Ramirez, A., Cassidy -Bushrow, A.E., \\nCheng, P., Yu, E.: Understanding the #longCOVID and #longhaulers conversation on 26 \\nTwitter: Multimethod study. JMIR Infodemiology. 2, e31259 (2022). \\nhttps://doi.org/10.2196/31259.  \\n88. D√©guilhem, A., Malaab, J., Talmatkadi, M., Renner, S., Foulqui√©, P., Fagherazzi, G., Lous-\\nsikian, P., Marty, T., Mebarki, A., Texier, N., Schuck, S.: Identifying profiles and symp-\\ntoms of patients with long COVID in France: Data mining infodemiology study ba sed on \\nsocial media. JMIR Infodemiology. 2, e39849 (2022). https://doi.org/10.2196/39849.  \\n89. Dolatabadi, E., Moyano, D., Bales, M., Spasojevic, S., Bhambhoria, R., Bhatti, J., \\nDebnath, S., Hoell, N., Li, X., Leng, C., Nanda, S., Saab, J., Sahak, E., Sie, F., Uppal, S., \\nVadlamudi, N.K., Vladimirova, A., Yakimovich, A., Yang, X., Kocak, S.A., Cheung , \\nA.M.: Using social media to help understand patient -reported health outcomes of post ‚Äì\\nCOVID -19 condition: Natural language processing approach. J. Med. Internet Res. 25, \\ne45767 (2023). https://doi.org/10.2196/45767.  \\n90. Ziauddeen, N., Gurdasani, D., O‚ÄôHara, M.E., Hastie, C., Roderick, P., Yao, G., Alwan, \\nN.A.: Characteristics and impact of Long Covid: Findings from an online survey. PLoS \\nOne. 17, e0264331 (2022). https://doi.org/10.1371/journal.pone.0264331.  \\n91. Segneri, L., Babina, N., Hammerschmidt, T., Fronzetti Colladon, A., Gloor, P.A.: Too \\nmuch focus on your health might be bad for your health: Reddit user‚Äôs communication \\nstyle predicts their Long COVID likelihood. PLoS One. 19, e0308340 (2024). \\nhttps://doi. org/10.1371/journal.pone.0308340.  \\n92. Singh, S.M., Reddy, S.C., Kathiravan, S.: An analysis of self -reported long COVID -19 \\nsymptoms on twitter. J. Postgrad. Med. Educ. Res. 57, 79 ‚Äì81 (2023). \\nhttps://doi.org/10.5005/jp -journals -10028 -1616.  \\n93. Mining Long -COVID symptoms from Reddit: what we know so far, https://www.re-\\nsearchgate.net/profile/Abeed -Sarker/publication/352208391_Mining_Long -\\nCOVID_symptoms_from_Reddit_what_we_know_so_far/links/60bedc6ca6fdcc22eae8b\\n87a/Mining -Long -COVID -symptoms -from -Reddit -what -we-know -so-far.pdf, last ac-\\ncessed 2024/12/24.  \\n94. Miyake, E., Martin, S.: Long Covid: Online patient narratives, public health communica-\\ntion and vaccine hesitancy. Digit. Health. 7, (2021). \\nhttps://doi.org/10.1177/20552076211059649.  \\n95. Sarker, A., Ge, Y.: Mining long -COVID symptoms from Reddit: characterizing post -\\nCOVID syndrome from patient reports. JAMIA Open. 4, (2021). \\nhttps://doi.org/10.1093/jamiaopen/ooab075.  \\n96. Jordan, A.A.D.: Understanding the plight of covid -19 long haulers through computational \\nanalysis of YouTube content, (2022).  \\n97. Minel, B.J.A.: Using topic modeling and NLP tools for analyzing long Covid coverage by \\nFrench press and Twitter. In: Nagar et al, A. (ed.) Intelligent Sustainable Systems, Lecture \\nNotes in Networks and Systems 817. Springer Nature Singapore, Singapore (202 4). \\nhttps://doi.org/10.1007/978 -981-99-7886 -1_15.  \\n98. Ozduran, E., B√ºy√ºk√ßoban, S.: A content analysis of the reliability and quality of Youtube \\nvideos as a source of information on health -related post -COVID pain. PeerJ. 10, e14089 \\n(2022). https://doi.org/10.7717/peerj.14089.  \\n99. D√©om, N., Khalid, A.F., Martin, S., Chepo, M., Vindrola -Padros, C.: Unlocking the mys-\\nteries of long COVID in children and young people: Insights from a policy review and \\nsocial media analysis in the UK, https://osf.io/preprints/f48yg/, (2023). \\nhttps://doi. org/10.31219/osf.io/f48yg.  \\n100. Jacques, E.T., Basch, C.H., Park, E., Kollia, B., Barry, E.: Long haul COVID -19 videos \\non YouTube: Implications for health communication. J. Community Health. 47, 610 ‚Äì615 \\n(2022). https://doi.org/10.1007/s10900 -022-01086 -4. \\n101. Strain, W.D., Sherwood, O., Banerjee, A., Van der Togt, V., Hishmeh, L., Rossman, J.: \\nThe impact of COVID vaccination on symptoms of long COVID: An international survey 27 \\nof people with lived experience of long COVID. Vaccines (Basel). 10, 652 (2022). \\nhttps://doi.org/10.3390/vaccines10050652.  \\n102. Wongtavavimarn, K.: Social support and narrative sensemaking online: A content analysis \\nof Facebook posts by COVID -19 long haulers, https://uh -ir.tdl.org/bitstream/han-\\ndle/10657/10745/WONGTAVAVIMARN -THESIS -2022.pdf?sequence=1, last accessed \\n2024/12/24.  \\n103. Helmy, Y.A., Fawzy, M., Elaswad, A., Sobieh, A., Kenney, S.P., Shehata, A.A.: The \\nCOVID -19 pandemic: A comprehensive review of taxonomy, genetics, epidemiology, di-\\nagnosis, treatment, and control. J. Clin. Med. 9, 1225 (2020). \\nhttps://doi.org/10.3390/jcm904 1225.  \\n104. Gasser, U., Ienca, M., Scheibner, J., Sleigh, J., Vayena, E.: Digital tools against COVID -\\n19: taxonomy, ethical challenges, and navigation aid. Lancet Digit. Health. 2, e425 ‚Äìe434 \\n(2020). https://doi.org/10.1016/s2589 -7500(20)30137 -0. \\n105. Albahri, O.S., Zaidan, A.A., Albahri, A.S., Zaidan, B.B., Abdulkareem, K.H., Al -qaysi, \\nZ.T., Alamoodi, A.H., Aleesa, A.M., Chyad, M.A., Alesa, R.M., Lim, C.K., Lakulu, M.M., \\nIbrahim, A.B., Rashid, N.A.: Systematic review of artificial intelligence techniqu es in the \\ndetection and classification of COVID -19 medical images in terms of evaluation and \\nbenchmarking: Taxonomy analysis, challenges, future solutions and methodological as-\\npects. J. Infect. Public Health. 13, 1381 ‚Äì1396 (2020). \\nhttps://doi.org/10.1016/j .jiph.2020.06.028.  \\n106. Awassa, L., Jdey, I., Dhahri, H., Hcini, G., Mahmood, A., Othman, E., Haneef, M.: Study \\nof different deep learning methods for Coronavirus (COVID -19) pandemic: Taxonomy, \\nsurvey and insights. Sensors (Basel). 22, 1890 (2022). https://doi.org/10.3390/s220518 90. \\n107. Shahid, Z., Kalayanamitra, R., McClafferty, B., Kepko, D., Ramgobin, D., Patel, R., Ag-\\ngarwal, C.S., Vunnam, R., Sahu, N., Bhatt, D., Jones, K., Golamari, R., Jain, R.: COVID‚Äê\\n19 and older adults: What we know. J. Am. Geriatr. Soc. 68, 926 ‚Äì929 (2020). \\nhttps: //doi.org/10.1111/jgs.16472.  \\n108. Thakur, N., Han, C.Y.: A study of fall detection in assisted living: Identifying and improv-\\ning the optimal machine learning method. J. Sens. Actuator Netw. 10, 39 (2021). \\nhttps://doi.org/10.3390/jsan10030039.  \\n109. Lebrasseur, A., Fortin -B√©dard, N., Lettre, J., Raymond, E., Bussi√®res, E. -L., Lapierre, N., \\nFaieta, J., Vincent, C., Duchesne, L., Ouellet, M. -C., Gagnon, E., Tourigny, A., Lamonta-\\ngne, M. -√à., Routhier, F.: Impact of the COVID -19 pandemic on older adults: R apid re-\\nview. JMIR Aging. 4, e26474 (2021). https://doi.org/10.2196/26474.  \\n110. Thakur, N., Han, C.Y.: Multimodal approaches for Indoor Localization for Ambient As-\\nsisted Living in Smart Homes. Information (Basel). 12, 114 (2021). \\nhttps://doi.org/10.3390/info12030114.  \\n111. Nanda, A., Vura, N.V.R.K., Gravenstein, S.: COVID -19 in older adults. Aging Clin. Exp. \\nRes. 32, 1199 ‚Äì1202 (2020). https://doi.org/10.1007/s40520 -020-01581 -5. \\n112. Miller, E.A.: Protecting and improving the lives of older adults in the COVID -19 era. J. \\nAging Soc. Policy. 32, 297 ‚Äì309 (2020). https://doi.org/10.1080/08959420.2020.1780104.  \\n113. Thakur, N., Han, C.Y.: Indoor localization for personalized ambient assisted living of mul-\\ntiple users in multi -floor smart environments. Big Data Cogn. Comput. 5, 42 (2021). \\nhttps://doi.org/10.3390/bdcc5030042.  \\n114. Saxon, S.V., Mary Jean Etten, EdD, GNP, CMP, FT, Elizabeth A. Perkins, PhD, RNLD, \\nFAAIDD, FGSA: Physical Change and aging, Seventh Edition: A guide for Helping Pro-\\nfessions. Springer Publishing Company (2021).  \\n115. Engelman, M., Jackson, H.: Gradual change, homeostasis, and punctuated equilibrium: \\nReconsidering patterns of health in later life. Demography. 56, 2323 ‚Äì2347 (2019). \\nhttps://doi.org/10.1007/s13524 -019-00826 -x. 28 \\n116. Thakur, N., Han, C.Y.: An intelligent ubiquitous activity aware framework for smart home. \\nIn: Advances in Intelligent Systems and Computing. pp. 296 ‚Äì302. Springer International \\nPublishing, Cham (2021).  \\n117. Busse, P.J., Mathur, S.K.: Age -related changes in immune function: Effect on airway in-\\nflammation. J. Allergy Clin. Immunol. 126, 690 ‚Äì699 (2010). \\nhttps://doi.org/10.1016/j.jaci.2010.08.011.  \\n118. Howcroft, T.K., Campisi, J., Louis, G.B., Smith, M.T., Wise, B., Wyss -Coray, T., Augus-\\ntine, A.D., McElhaney, J.E., Kohanski, R., Sierra, F.: The role of inflammation in age -\\nrelated disease. Aging (Albany NY). 5, 84 ‚Äì93 (2013). https://doi.org/10.18632/ag-\\ning.100531.  \\n119. Pawelec, G., Goldeck, D., Derhovanessian, E.: Inflammation, ageing and chronic disease. \\nCurr. Opin. Immunol. 29, 23 ‚Äì28 (2014). https://doi.org/10.1016/j.coi.2014.03.007.  \\n120. Thakur, N., Han, C.Y.: A framework for facilitating human -human interactions to mitigate \\nloneliness in elderly. In: Advances in Intelligent Systems and Computing. pp. 322 ‚Äì327. \\nSpringer International Publishing, Cham (2021).  \\n121. Novak, M.: Issues in aging. Routledge, Fourth edition. | New York, NY\\u202f: Routledge, 2018. \\n(2018).  \\n122. Silverstein, M.: Meeting the challenges of an aging workforce. Am. J. Ind. Med. 51, 269 ‚Äì\\n280 (2008). https://doi.org/10.1002/ajim.20569.  \\n123. Thakur, N., Y. Han, C.: Pervasive activity logging for indoor localization in smart homes. \\nIn: 2021 4th International Conference on Data Science and Information Technology. pp. \\n246‚Äì255. ACM, New York, NY, USA (2021).  \\n124. Thakur, N., Han, C.Y.: An improved approach for complex activity recognition in smart \\nhomes. In: Lecture Notes in Computer Science. pp. 220 ‚Äì231. Springer International Pub-\\nlishing, Cham (2019).  \\n125. Thakur, N., Han, C.Y.: An activity analysis model for enhancing user experiences in affect \\naware systems. In: 2018 IEEE 5G World Forum (5GWF). pp. 516 ‚Äì519. IEEE (2018).  \\n126. Thakur, N., Han, C.Y.: A context -driven complex activity framework for smart home. In: \\n2018 IEEE 9th Annual Information Technology, Electronics and Mobile Communication \\nConference (IEMCON). pp. 801 ‚Äì806. IEEE (2018).  \\n127. Kogan, A.C., Wilber, K., Mosqueda, L.: Person‚Äêcentered care for older adults with chronic \\nconditions and functional impairment: A systematic literature review. J. Am. Geriatr. Soc. \\n64, (2016). https://doi.org/10.1111/jgs.13873.  \\n128. Matthew -Maich, N., Harris, L., Ploeg, J., Markle -Reid, M., Valaitis, R., Ibrahim, S., Gafni, \\nA., Isaacs, S.: Designing, implementing, and evaluating mobile health technologies for \\nmanaging chronic conditions in older adults: A scoping review. JMIR MHealth UHealth. \\n4, e29 (2016). https://doi.org/10.2196/mhealth.5127.  \\n129. PACC Research Group, Anaby, D., Miller, W.C., Eng, J.J., Jarus, T., Noreau, L.: Partici-\\npation and well -being among older adults living with chronic conditions. Soc. Indic. Res. \\n100, 171 ‚Äì183 (2011). https://doi.org/10.1007/s11205 -010-9611 -x. \\n130. Ferrari, M., Harrison, B., Rawashdeh, O., Hammond, R., Avery, Y., Rawashdeh, M., \\nSa‚Äôdeh, W., Maddens, M.: Clinical feasibility trial of a motion detection system for fall \\nprevention in hospitalized older adult patients. Geriatr. Nurs. 33, 177 ‚Äì183 (2012). \\nhttps://doi.org/10.1016/j.gerinurse.2011.11.011.  \\n131. Thakur, N., Han, C.Y.: A simplistic and cost -effective design for real -world development \\nof an ambient assisted living system for fall detection and indoor localization: Proof -of-\\nconcept. Information (Basel). 13, 363 (2022). https://doi.org/10.3390/info130 80363.  \\n132. Townsley, C.A., Chan, K.K., Pond, G.R., Marquez, C., Siu, L.L., Straus, S.E.: Understand-\\ning the attitudes of the elderly towards enrolment into cancer clinical trials. BMC Cancer. \\n6, (2006). https://doi.org/10.1186/1471 -2407 -6-34. 29 \\n133. Thakur, N., Han, C.Y.: A framework for prediction of cramps during activities of daily \\nliving in elderly. In: 2020 International Conference on Big Data, Artificial Intelligence and \\nInternet of Things Engineering (ICBAIE). pp. 284 ‚Äì287. IEEE (2020).  \\n134. Fontes, A.P., Neri, A.L.: Resilience in aging: literature review. Cien. Saude Colet. 20, \\n1475 ‚Äì1495 (2015). https://doi.org/10.1590/1413 -81232015205.00502014.  \\n135. Thakur, N., Han, C.Y.: A multimodal approach for early detection of cognitive impairment \\nfrom tweets. In: Lecture Notes in Networks and Systems. pp. 11 ‚Äì19. Springer International \\nPublishing, Cham (2022).  \\n136. Stern, Y., Albert, M., Barnes, C.A., Cabeza, R., Pascual -Leone, A., Rapp, P.R.: A frame-\\nwork for concepts of reserve and resilience in aging. Neurobiol. Aging. 124, 100 ‚Äì103 \\n(2023). https://doi.org/10.1016/j.neurobiolaging.2022.10.015.  \\n137. Chen, Z., Yu, J., Song, Y., Chui, D.: Aging Beijing: Challenges and strategies of health \\ncare for the elderly. Ageing Res. Rev. 9, S2 ‚ÄìS5 (2010). \\nhttps://doi.org/10.1016/j.arr.2010.07.001.  \\n138. Thakur, N., Han, C.Y.: Towards a knowledge base for activity recognition of diverse users. \\nIn: Advances in Intelligent Systems and Computing. pp. 303 ‚Äì308. Springer International \\nPublishing, Cham (2021).  \\n139. Dun√©r, A., Nordstr√∂m, M.: Intentions and strategies among elderly people: Coping in eve-\\nryday life. J. Aging Stud. 19, 437 ‚Äì451 (2005). https://doi.org/10.1016/j.jag-\\ning.2004.10.001.  \\n140. Spoorenberg, S.L.W., Uittenbroek, R.J., Middel, B., Kremer, B.P.H., Reijneveld, S.A., \\nWynia, K.: Embrace, a model for integrated elderly care: study protocol of a randomized \\ncontrolled trial on the effectiveness regarding patient outcomes, service use, cos ts, and \\nquality of care. BMC Geriatr. 13, (2013). https://doi.org/10.1186/1471 -2318 -13-62. \\n141 Thakur, N.: A large -scale dataset of Twitter chatter about online learning during the current \\nCOVID -19 Omicron wave. Data (Basel). 7, 109 (2022). \\nhttps://doi.org/10.3390/data7080109.  \\n142 Storey, V.C., O‚ÄôLeary, D.E.: Text analysis of evolving emotions and sentiments in \\nCOVID -19 twitter communication. Cognit. Comput. 16, 1834 ‚Äì1857 (2024). \\nhttps://doi.org/10.1007/s12559 -022-10025 -3. \\n143. Bainotti, L., Caliandro, A., Gandini, A.: From archive cultures to ephemeral content, and \\nback: Studying Instagram Stories with digital methods. New Media Soc. 23, 3656 ‚Äì3676 \\n(2021). https://doi.org/10.1177/1461444820960071.  \\n144. Cardell, K., Douglas, K., Maguire, E.: ‚Äòstories.‚Äô In: Mediating Memory. pp. 157 ‚Äì172. \\nRoutledge (2017).  \\n145. Lin, H.: ‚ÄòLet‚Äôs purchase coloured live chat messages‚Äô: the impact of user engagement with \\nSuper Chat on YouTube. Inf. Commun. Soc. 1 ‚Äì19 (2024). \\nhttps://doi.org/10.1080/1369118x.2024.2442407.  \\n146. Sun, H., Chen, J., Fan, M.: Effect of live chat on traffic‚Äêto‚Äêsales conversion: Evidence from \\nan online marketplace. Prod. Oper. Manag. 30, 1201 ‚Äì1219 (2021). \\nhttps://doi.org/10.1111/poms.13320.  \\n147. Perez, B., Musolesi, M., Stringhini, G.: You are your metadata: Identification and obfus-\\ncation of social media users using metadata information. Proceedings of the International \\nAAAI Conference on Web and Social Media. 12, (2018). https://doi.org/10.1609/i c-\\nwsm.v12i1.15010.  \\n148. Chen, L. -S., Lin, Z. -C., Chang, J. -R.: FIR: An effective scheme for extracting useful \\nmetadata from social media. J. Med. Syst. 39, (2015). https://doi.org/10.1007/s10916 -015-\\n0333 -0. \\n149. Gerber, H.R., Lynch, T.L.: Into the meta: Research methods for moving beyond social \\nmedia surfacing. TechTrends. 61, 263 ‚Äì272 (2017). https://doi.org/10.1007/s11528 -016-\\n0140 -6. 30 \\n150. Jones, S.M., Neblitt -Jones, V., Weigle, M.C., Klein, M., Nelson, M.L.: It‚Äôs all about the \\ncards: Sharing on social media encouraged HTML metadata growth. In: 2021 ACM/IEEE \\nJoint Conference on Digital Libraries (JCDL). pp. 110 ‚Äì119. IEEE (2021).  \\n151. Rolls, K., Hansen, M., Jackson, D., Elliott, D.: How health care professionals use social \\nmedia to create virtual communities: An integrative review. J. Med. Internet Res. 18, e166 \\n(2016). https://doi.org/10.2196/jmir.5312.  \\n152. Lu, Y., Wu, Y., Liu, J., Li, J., Zhang, P.: Understanding health care social media use from \\ndifferent stakeholder perspectives: A content analysis of an online health community. J. \\nMed. Internet Res. 19, e109 (2017). https://doi.org/10.2196/jmir.7087.  \\n153. Sundaram, A., Subramaniam, H., Hamid, S.H.A., Nor, A.M.: A systematic literature re-\\nview on social media slang analytics in contemporary discourse. IEEE Access. 11, \\n132457 ‚Äì132471 (2023). https://doi.org/10.1109/access.2023.3334278.  \\n154. Sundaram, A., Subramaniam, H., Ab Hamid, S.H., Nor, A.M.: A three -step procedural \\nparadigm for domain -specific social media slang analytics. In: 2024 International Confer-\\nence on Trends in Quantum Computing and Emerging Business Technologies. pp. 1 ‚Äì7. \\nIEEE (2024).  \\n155. Matsumoto, K., Ren, F., Matsuoka, M., Yoshida, M., Kita, K.: Slang feature extraction by \\nanalysing topic change on social media. CAAI Trans. Intell. Technol. 4, 64 ‚Äì71 (2019). \\nhttps://doi.org/10.1049/trit.2018.1060.  \\n \"]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d2ea60",
   "metadata": {},
   "source": [
    "# 3. We do that we with all our files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a4b578",
   "metadata": {},
   "source": [
    "I have done a new query at the Arxiv notebook (check the notebook in this same folder!) using the term **Facebook**. Let's locate the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66deb9c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\usuario\\\\ELENA\\\\it-training uzh\\\\it-training uzh\\\\Python for Digital Humanities\\\\Day 2\\\\PDF extraction'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac734fe",
   "metadata": {},
   "source": [
    "Now let's open it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e6e1e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory path where the PDF files are located\n",
    "pdf_dir = 'C:\\\\Users\\\\usuario\\\\ELENA\\\\it-training uzh\\\\it-training uzh\\\\Python for Digital Humanities\\\\Day 2\\\\PDF extraction\\\\arxiv_pdfs_facebook'\n",
    "\n",
    "# Get a list of all files in the directory\n",
    "all_files = os.listdir(pdf_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f7d4047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2407.13549v1.pdf',\n",
       " '2407.16014v1.pdf',\n",
       " '2407.18471v1.pdf',\n",
       " '2408.07322v1.pdf',\n",
       " '2408.08126v1.pdf',\n",
       " '2408.08437v1.pdf',\n",
       " '2408.08964v3.pdf',\n",
       " '2408.09435v1.pdf',\n",
       " '2408.09683v1.pdf',\n",
       " '2408.09725v1.pdf',\n",
       " '2408.12449v2.pdf',\n",
       " '2408.12743v2.pdf',\n",
       " '2408.12753v1.pdf',\n",
       " '2409.01470v1.pdf',\n",
       " '2409.02358v1.pdf',\n",
       " '2409.08405v1.pdf',\n",
       " '2409.13461v1.pdf',\n",
       " '2409.15652v3.pdf',\n",
       " '2409.18393v1.pdf',\n",
       " '2409.18931v1.pdf',\n",
       " '2410.01708v1.pdf',\n",
       " '2410.05401v1.pdf',\n",
       " '2410.06443v1.pdf',\n",
       " '2410.14617v1.pdf',\n",
       " '2410.16977v1.pdf',\n",
       " '2410.17496v1.pdf',\n",
       " '2410.20293v2.pdf',\n",
       " '2410.22716v1.pdf',\n",
       " '2411.04542v1.pdf',\n",
       " '2411.04752v1.pdf',\n",
       " '2411.05043v1.pdf',\n",
       " '2411.05788v1.pdf',\n",
       " '2411.06122v1.pdf',\n",
       " '2411.09214v1.pdf',\n",
       " '2411.11426v1.pdf',\n",
       " '2411.12508v1.pdf',\n",
       " '2411.14613v1.pdf',\n",
       " '2411.16285v1.pdf',\n",
       " '2411.16826v1.pdf',\n",
       " '2412.02349v1.pdf',\n",
       " '2412.04484v1.pdf',\n",
       " '2412.05861v1.pdf',\n",
       " '2412.07550v1.pdf',\n",
       " '2412.08484v1.pdf',\n",
       " '2412.08648v1.pdf',\n",
       " '2412.14985v1.pdf',\n",
       " '2412.15072v1.pdf',\n",
       " '2412.15621v1.pdf',\n",
       " '2412.18779v1.pdf',\n",
       " '2412.20420v1.pdf']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668fb453",
   "metadata": {},
   "source": [
    "There we have our files!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3c919c",
   "metadata": {},
   "source": [
    "Now let's extract all the text inside them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "537b92a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each PDF file\n",
    "extracted_texts = []\n",
    "\n",
    "for file in all_files:\n",
    "    file_path = os.path.join(pdf_dir, file)  # Specify the directory where the files are located\n",
    "    with open(file_path, 'rb') as pdf_file:\n",
    "        # Create a PDF reader object\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "        # Get the number of pages in the PDF\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "\n",
    "        # Initialize an empty string to store the extracted text\n",
    "        extracted_text = ''\n",
    "\n",
    "        # Loop through each page and extract the text\n",
    "        for page_num in range(num_pages):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            extracted_text += page.extract_text()\n",
    "    \n",
    "    # Add the extracted text to the list\n",
    "    extracted_texts.append(['Doc ' + file, extracted_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97049f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Doc 2407.13549v1.pdf',\n",
       "  'Evaluating the effect of viral news on social media\\nengagement\\nEmanuele Sangiorgio1*, Niccol` o Di Marco2, Gabriele Etta2,\\nMatteo Cinelli2, Roy Cerqueti1,3, Walter Quattrociocchi2\\n1*Department of Social Sciences and Economics, Sapienza University of\\nRome, P.le Aldo Moro, 5, 00185, Rome, Italy.\\n2Department of Computer Science, Sapienza University of Rome, Viale\\nRegina Elena, 295, 00161, Rome, Italy.\\n3GRANEM, Universit¬¥ e d‚ÄôAngers, SFR Confluences, F-49000, Angers,\\nFrance.\\n*Corresponding author(s). E-mail(s): emanuele.sangiorgio@uniroma1.it;\\nContributing authors: niccolo.dimarco@uniroma1.it;\\ngabriele.etta@uniroma1.it; matteo.cinelli@uniroma1.it;\\nroy.cerqueti@uniroma1.it; walter.quattrociocchi@uniroma1.it;\\nAbstract\\nThis study examines Facebook and YouTube content from over a thousand news\\noutlets in four European languages from 2018 to 2023, using a Bayesian struc-\\ntural time-series model to evaluate the impact of viral posts. Our results show\\nthat most viral events do not significantly increase engagement and rarely lead\\nto sustained growth. The virality effect usually depends on the engagement trend\\npreceding the viral post, typically reversing it. When news emerges unexpect-\\nedly, viral events enhances users‚Äô engagement, reactivating the collective response\\nprocess. In contrast, when virality manifests after a sustained growth phase, it\\nrepresents the final burst of that growth process, followed by a decline in atten-\\ntion. Moreover, quick viral effects fade faster, while slower processes lead to more\\npersistent growth. These findings highlight the transient effect of viral events and\\nunderscore the importance of consistent, steady attention-building strategies to\\nestablish a solid connection with the user base rather than relying on sudden\\nvisibility spikes.\\nKeywords: Social media |Virality |Attention economy\\n1arXiv:2407.13549v1  [cs.SI]  18 Jul 20241 Introduction\\nThe advent and proliferation of social media have fundamentally altered the informa-\\ntion landscape [1‚Äì3], offering unprecedented opportunities for content to achieve rapid\\nand widespread attention. As these platforms have become integrated into our daily\\nlives [4], transforming into essential tools for information diffusion [5, 6] and personal\\ncommunication [7], they have merged entertainment-driven business models with com-\\nplex social dynamics [8], raising significant concerns about their impact on society.\\nThis complex interplay produced an environment in which information overload is the\\nforemost feature [9] and a wide range of content creators, from news organizations to\\nindividual influencers [10], compete for the limited resource that is users‚Äô attention\\n[11‚Äì14].\\nUnderstanding the attention economy in the digital domain is paramount for nav-\\nigating this competitive information market, whereby the pursuit of virality [15] plays\\na pivotal role in shaping how information sources design their strategies of production\\nand diffusion of content. Characterized by content‚Äôs exceptional reach and engage-\\nment, virality is a core feature of this environment, particularly when referring to viral\\nnews and their potential impact on the public discourse. In today‚Äôs online ecosystem,\\nit is crucial to understand how collective attention responds to abrupt news diffusion\\nand how sudden spikes of visibility reverberate on the subsequent attention captured\\nby the source. While virality has been mainly investigated for its marketing impli-\\ncations and received extensive coverage in the literature [16‚Äì20], the impact of viral\\nnews on collective attention has not obtained as much consideration.\\nTo address this gap in existing literature, this study aims to enhance our compre-\\nhension of the attention economy through a data-driven approach by exploring the\\ndynamics of virality and its effects on users‚Äô engagement on different social media plat-\\nforms. By analyzing data from Facebook and YouTube, we examine attention patterns\\nafter viral events to assess how these events influence users‚Äô interactions over time.\\nIn this study, we use a comparative interrupted time-series (CITS) design imple-\\nmented using a Bayesian structural time series model (BSTS) [21] to evaluate the\\nimpact of viral events on users‚Äô engagement. In our approach, we apply the BSTS by\\nusing increasingly broader time windows to observe the effect of the same viral event\\nfrom a short-term to a long-term perspective. Based on the BSTS‚Äôs results, we conduct\\nour analysis first by examining the magnitude of the impact and then its temporal\\ndynamics to address the following two research questions.\\nRQ1: Does virality induce engagement growth?\\nOur first research question aims to assess whether and how a viral event leads\\nto increased users‚Äô attention received by the source. After that, our second research\\nquestion aims to analyze the temporal dynamics of these effects to evaluate if the\\nrapidity at which they occur influences their longevity.\\nRQ2: Do the faster-manifesting effects persist longer?\\nWhile our first research question aims to determine the actual impact of virality\\nand its magnitude, the second analysis provides valuable insights into whether these\\nevents genuinely contribute to sustained growth or merely act as transient spotlights.\\nOur results indicate the presence of two different types of viral events, a ‚Äòloaded-\\ntype‚Äô and a ‚Äòsudden-type‚Äô virality. When virality follows a sustained growth phase,\\n2it represents the final burst of that growth process, with users‚Äô attention successively\\nstanding on lower levels. Conversely, viral news boosts users‚Äô engagement when occur-\\nring as a sudden event, reactivating the collective response process. While virality can\\ntemporarily boost user engagement, this effect is often short-lived. We observe that\\nquickly emerging viral effects rapidly fade out. On the other hand, content achieving\\nslower and sustained growth tends to show more persistent effects on engagement.\\nThese results emphasize the importance of continuous and consistent content strate-\\ngies in establishing a solid and enduring connection with the user base rather than\\nrelying on viral spikes.\\nThe rest of this paper is organized as follows: Section 2, gives an overview of the\\nrelevant literature. In Section 3, we outline our methodology for virality detection and\\nimpact evaluation. In Section 4 we present our detailed findings on the dynamics of the\\nvirality impact on users‚Äô attention and its persistence through time. In Section 5 we\\ndiscuss the implications of our findings, along with limitations and recommendation\\nfor future research. Section 6 concludes the paper.\\n2 Literature review\\n2.1 Attention economy\\nThe attention economy is central to today‚Äôs digital media landscape. In information\\nmanagement, attention economics applies economic theory to human attention, treat-\\ning it as a scarce resource. The attention economy is defined as a system of agents\\n(senders) who aim to capture the attention of individuals (receivers) by creating and\\nsharing information packages (signals) [11, 22‚Äì25]. Once produced and disseminated,\\nthis information undergoes a cognitive filtering process by the receivers, who select\\nrelevant information and disregard the rest [26‚Äì28]. From a supply-side perspective,\\nthe rise of social media platforms has led to an unprecedented volume of available con-\\ntent and information. This overabundance, often referred to as information overload\\nin the literature [9, 29‚Äì31], is a cardinal characteristic of the current attention market\\nin the digital landscape. In this competitive environment, a wide range of content cre-\\nators, from news organizations to individual influencers [10], compete for the limited\\nresource that is users‚Äô attention. The concept of human attention as a currency entails\\ntwo key features [32]. Unlike real currencies, it can not be accumulated but solely be\\nspent. Additionally, its transient nature makes it hard to trace and measure. Social\\nmedia data help address this challenge, as user interactions with content constitute\\nprecise engagement indicators. The vast amount of tracked and aggregated data allows\\nfor studying large populations, providing insights into collective behavior without the\\nneed for experimental settings [33, 34]. Shifting the focus from individual users to a\\nbroader community perspective highlights the dynamics of collective attention [35, 36].\\n2.2 Human dynamics on social media\\nOriginally designed for entertainment and personal connections, social media platforms\\nhave become indispensable tools for information dissemination [37], raising significant\\n3concerns about their potential impact on social dynamics. Many works in the litera-\\nture highlight how online users are prone to consume information aligning with their\\nexisting beliefs [38‚Äì40] and commonly ignore opposing viewpoints [28, 41, 42]. The\\ncreation and reinforcement of online ‚Äòecho chambers‚Äô [43, 44], where shared narratives\\nare collectively shaped and solidified [39, 45], may exacerbate social divisions and fos-\\nter partisan and polarized communities [46‚Äì48], complicating the landscape of public\\ndiscourse [49] especially during sensitive periods such as global elections [50]. Offering\\nunprecedented opportunities for content to achieve rapid and widespread attention\\n[35], social media have become crucial environments for the spread of information\\nand misinformation during global events [51‚Äì53], political events [54, 55], and discus-\\nsions on emerging technologies such as large language models [56]. The unprecedented\\namount of available content distinguishes today‚Äôs digital ecosystem, considerably com-\\nplicating the search for information and giving rise to the phenomenon of infodemics\\n[57, 58], such as for the COVID-19 pandemic. These platforms may also influence\\npolitical landscapes, potentially affecting public opinion and voter behavior during\\nelections through the rapid dissemination and amplification of political content. How-\\never, this influence is not definitively proven [59, 60]. Research on the interplay between\\nuser behavior and platforms‚Äô byproduct presented both opportunities and challenges\\n[46, 61‚Äì63], unveiling a multifaceted landscape where a prevalence of one over the\\nother has not yet emerged. As users‚Äô behavior could exhibit persistent patterns across\\ndifferent platforms, topics, and contexts [64], the comparative analysis of diverse social\\nmedia can isolate unaltered consistencies of human dynamics in the digital ecosystem\\nor underline algorithmic and platform peculiarities.\\n2.3 Virality on social media\\nOn social media platforms, virality refers to a piece of content having rapid diffu-\\nsion and high levels of users‚Äô engagement. Viral events can differ depending on the\\ndissemination that leads to their emergence. Structural virality distinguishes between\\nbroadcast andviral diffusion based on their spreading patterns [65]. Broadcast dif-\\nfusion follows a pattern where a single, large parent node spreads information to\\nseveral smaller entities. In contrast, viral diffusion involves multiple smaller nodes,\\neach contributing to the spread by generating a limited number of infections.\\nOne of the most complex issues is understanding why certain items achieve sud-\\nden and widespread dissemination while other similar or higher-quality items remain\\noverlooked [66, 67]. Research suggests that the nature of the content is a more signifi-\\ncant factor in driving virality than the characteristics of the spreading source [68]. For\\nexample, the source size in terms of Followers does not affect the probability of going\\nviral on Facebook [14], with the growth dynamics of engagement exhibiting universal\\npatterns in the short run. Concerning content properties, [69] demonstrated that the\\ntype of multimedia content on Twitter affects the volume and speed of retweeting, [70]\\nidentified visual attributes that can predict relative virality using Reddit data, while\\n[71] characterized visual elements distinguishing viral from non-viral memes.\\nOn the other hand, the frantic search for attractiveness may translate into severe\\ndrawbacks for the source itself. For instance, [72] evidenced how readers often view\\nclickbait as a manipulative tactic, which can reduce the perceived competence and\\n4trustworthiness of the publisher. Information overload leads to similar drawbacks also\\nin business and managerial contexts. The widespread practice of consumer-generated\\ncontent can provoke social and brand overload due to its quantity and poor quality,\\nleading to negative consequences such as brand disloyalty [73].\\nBeyond their media features, there is substantial agreement in the literature that\\nemotional resonance is a critical trigger of virality, particularly with negative emotions\\nand extreme or sensitive content. Extreme content, such as sex, nudity, and violence,\\nis more likely to become viral [74]. Hateful content cascades tend to be larger, per-\\nsist longer, and exhibit more significant structural virality [75]. An initial burst of a\\ntopic‚Äôs diffusion often correlates with increased negative reactions from users [76], and\\nnegative messages are typically reposted more quickly and frequently than positive or\\nneutral ones [77].\\nThe spread of misinformation and the rise of political partisanship are critical\\nissues due to their significant social implications. Conspiracy rumor cascades tend to\\nbe more persistent and exhibit a positive relationship between their lifetime and size\\n[39]. During the COVID-19 pandemic, misinformation was more likely to go viral than\\ntruthful information, often due to the use of emotionally charged, other-condemning\\nlanguage [78]. A similar mechanism is observed in political contexts, where posts\\nabout political opponents are more likely to be shared on social media. This out-group\\neffect is more influential than other social media-sharing predictors, such as emotional\\nlanguage [79]. Ultimately, the driving effects of virality stem from the combination and\\nmutual reinforcement of ideological segregation and negative emotional resonance.\\nBesides cases of misinformation and harmful content, viral diffusion may also have\\nsignificant drawbacks in different social contexts. Social media might influence risk\\nperceptions from the general public [80], fueling moral panics and amplifying threats\\nposed by deviant behavior and ideas [81], so much so of having tangible consequences\\nin real contexts like inducing distortions and detrimental effects on financial markets\\n[82, 83].\\n3 Research design\\nWe now outline our research design, starting from the data collection process detailed\\nin Section 3.1. In Section 3.2, we formally define virality along with the measures to\\nquantify it, while Section 3.3 accounts for the virality detection methods. Lastly, in\\nSection 3.4, we first illustrate our approach to evaluate the virality effect, and then\\nwe separately outline our research questions along with their related methods.\\n3.1 Data\\nWe begin by selecting a list of news outlets from NewsGuard [84], an organization\\nknown for monitoring news outlet activities internationally, rating more than 35,000\\nnews and information sources across several countries. We select all the reported news\\noutlets from Germany, France, Italy, and the United Kingdom. NewsGuard provides\\nseveral categories of descriptive metadata, including the URLs of these news sources‚Äô\\nsocial media accounts, if available. For Facebook, after identifying all the news outlets\\nwith active accounts listed on NewsGuard, we use their Facebook URLs to gather data\\n5Table 1 : Dataset overview\\nFacebook YouTube\\n2018-2023 2016-2023\\nCountry Pages Posts Channels Videos\\nGermany 263 9,633,896 242 193,153\\nFrance 252 8,705,928 177 204,665\\nItaly 384 16,259,173 234 271,183\\nUK 227 8,751,788 105 100,794\\nTotal 1,126 43,350,785 758 769,795\\nvia CrowdTangle. This Facebook-owned tool monitors interactions on public content\\nfrom Facebook pages, groups, and verified profiles [85]. Using the ‚ÄòHistorical Data‚Äô\\nfeature in CrowdTangle, we download the complete content history of each page,\\nstarting from January 1, 2018. Similarly, for YouTube, after selecting all available\\naccounts, we use the YouTube Data API [86] to collect their list of published content.\\nGiven the fewer YouTube channels and videos compared to Facebook, we extend the\\nobservation period for YouTube to begin on January 1, 2016.\\nTable 1 provides an overview of the dataset, detailing the total number of Face-\\nbook Pages and YouTube channels used in our analysis for each country, along with\\ntheir respective number of posts and videos. The longitudinal structure and tempo-\\nral continuity of these page observations are crucial for evaluating the impact of viral\\nevents. Therefore, the resulting dataset is formatted consistently for both platforms,\\ncontaining chronological information about each posted and accessible item. This nat-\\nurally excludes any content that may have been removed by users or through platform\\nmoderation at the time of download. Note that in what follows, we will refer to both\\nFacebook pages and YouTube channels as (information) sources and to Facebook posts\\nand YouTube videos as either content or posts, specifying the platform in the latter\\ncase.\\n3.2 Defining virality\\nOn social media platforms, virality represents the propensity of content to achieve\\nrapid diffusion and high interactions levels from users [15]. To quantify virality, we use\\na bivariate approach based on two measures, spreading and interactions. The following\\nsubsections provide the definition of these two measures for each social media platform.\\n3.2.1 Spreading\\nFor Facebook data, we adopt the metric provided by [87]. Originally defined on Twitter\\ndata, the proposed metric can be applied to any social media platform with equivalent\\nRetweets and Followers metrics. On Facebook, Shares are equivalent to Retweets,\\nrepresenting the number of users sharing certain content. On both platforms, Followers\\nrepresent the number of users subscribed to a given page at the time of posting.\\n6Therefore, on Facebook, we can define the Spreading Sof a given post jof a Page ias:\\nSijt=ln\\x12Shares ij\\nFollowers it\\x13\\n(1)\\nwhere Followers itindicates the number of Followers of the Page iat time t, i.e. the\\ntime of posting.\\nFor YouTube, we can define it using the video Views metric, which specifically\\nmeasures content spreading. Therefore, on YouTube, we define the Spreading Sof a\\ngiven video jpublished by a Channel iat time tas:\\nSijt=ln(V iews ij). (2)\\n3.2.2 Interactions\\nOn Facebook, interactions are measured by the Total Interactions, encompassing the\\ntotal number of users‚Äô interactions with the post (the sum of Likes, Comments, and\\nShares). Therefore, on Facebook, we define the Total Interactions TIof a given post\\njof a Page ias:\\nTIijt=ln(Likes ij+Comments ij+Shares ij) (3)\\nwhere tis the date in which post jappears.\\nSince YouTube does not provide a share count (and consequently its Shares metric),\\nwe define the Total Interactions TIof a given video jof a Channel ipublished at day\\ntas:\\nTIijt=ln(Likes ij+Comments ij). (4)\\n3.3 Detecting virality\\nWe measure virality on social media as an (exceptional) over-performance of content\\nregarding spreading and users‚Äô interactions. To detect virality, we first apply a z‚àíscore\\nnormalization to both SijtandTIijtfor each piece of content jfrom a source i. On\\nFacebook, a post is considered viral if both its z‚àíscores exceed 3. We set the threshold\\nfor YouTube at 2.5 to ensure a comparable number of videos and channels as in\\nFacebook.\\nThe distributions of the two z‚àíscores for both platforms are shown in Fig. 1. The\\ninset plots within each panel zoom on the subset of values that exceed the threshold\\nfor the displayed metric, distinguishing between viral content (which surpasses both\\nthresholds) and non-viral content (which exceeds the threshold only in the displayed\\none).\\nThese insets reveal that the subset resulting from the combination of overperfor-\\nmances constitutes a small fraction of the distribution tails, especially on Facebook.\\nIn other words, content may induce high interactions levels without widespread diffu-\\nsion, or conversely, it may fail to engage users despite extensive reach. This observation\\nfurther emphasizes the exceptional rarity of virality.\\n7101102103104105106107\\n‚àí4‚àí3‚àí2‚àí101234567\\nSpreading z‚àíscoreFrequency050001000015000\\n3.03.54.04.55.0Viral\\nFALSE\\nTRUE\\n101102103104105106107\\n‚àí2‚àí101234567\\nInteractions z‚àíscore 05000100001500020000\\n3.03.54.04.55.0Viral\\nFALSE\\nTRUE Facebook A)\\n101102103104105\\n‚àí3‚àí2‚àí10123456\\nSpreading z‚àíscoreFrequency025050075010001250\\n2.53.03.54.0Viral\\nFALSE\\nTRUE\\n101102103104105\\n‚àí1 0 1 2 3 4 5\\nInteractions z‚àíscore 025050075010001250\\n2.53.03.54.0Viral\\nFALSE\\nTRUE YouTube B) Distributions of Spreading and Interactions z‚àíscoresFig. 1 : Distributions of Spreading and Engagement z‚àíscores for Facebook and\\nYouTube. The inset plots of each chart display the subset of values exceeding the\\nthreshold for the given metric, showing its breakdown between viral contents (which\\nexceed the threshold in both metrics) and non-viral ones (hence exceeding the thresh-\\nold only in the displayed metric).\\nFig. 2 presents the distributions of viral posts per source for Facebook and\\nYouTube. As the Figure shows, despite the differences in sample sizes, both platforms\\nexhibit similar heavy-tailed distributions over different scales reflecting the peculiar\\ndynamics of content virality. If, on the one hand, a limited number of sources suc-\\nceeded in achieving virality multiple times, it represented a unique or rare event for\\nmost of them.\\n3.4 Evaluating virality impact\\nAfter defining the procedure for detecting viral posts, we can now delve deep into our\\napproach for evaluating the impact of viral events on the performance of Facebook\\npages and YouTube channels.\\nIn terms of attention received, we quantify the performance of a given source by\\ncalculating its daily engagement. We define the Engagement Eitof a source iat day t\\nas the sum of the Total Interactions TI(as in Eq. 3, and 4) of all its jposts published\\nthat day, i.e.:\\nEit=X\\njTIijt.\\n8110100\\n1 10 100\\nViral PostsNumber of PagesPlatform\\nFacebook\\nYouTubeFig. 2 : Distributions of viral posts per source on Facebook and YouTube. Facebook\\nsample sizes: Pages = 613, Posts = 9514; YouTube sample sizes: Channels = 161,\\nVideos = 3624.\\nIn assessing the total attention a news outlet receives, we interpret higher engagement\\nas indicative of greater user attention, regardless of the number of posts published. This\\napproach is based on the premise that increasing the content volume only results in\\nheightened engagement if the content effectively captures users‚Äô attention. Conversely,\\nhigh engagement across several posts implies high users‚Äô attention, and calculating its\\naverage value would potentially underestimate this effect [14].\\nOur analysis aims to evaluate the after-effects of a viral event on the source‚Äôs\\nperformance in terms of attention caught. To inspect the attention dynamics following\\nthe event, we first assess whether the engagement received significantly changes after it.\\nTo detect a significant variation (either positive or negative) on the engagement after a\\nviral event, we use a comparative interrupted time-series (CITS) design implemented\\nusing a Bayesian structural time series model (BSTS, hereafter) [21], which has also\\nbeen used in previous research [88]. In our approach, we apply BSTS to each viral\\nevent, using time windows associated to nweeks ahead and before, with n= 2,3,4,5,6,\\nconstructed as follows: for any given n, we exclude the day of the viral post and\\ncompare the engagement trend during the nweeks following the event to the expected\\ntrend based on the nweeks preceding it ‚Äì hence, having a windows of 2 n√ó7 days ‚Äì and\\ncontrolling for the presence of other viral posts. Based on the BSTS‚Äôs results, we then\\nconduct our analysis to address our two research questions outlined in the following\\nsections, first examining the magnitude of the impact and then its temporal dynamics.\\n93.4.1 Research Question 1: Does virality induce engagement\\ngrowth?\\nOur first research question aims to assess whether - and how - a viral event increases\\nusers‚Äô attention received by the source after its occurrence. We start from the premise\\nthat each viral post can have a positive, null, or negative effect on the subsequent\\nengagement in the considered time window.\\nTo evaluate the effect, we use two variables provided by the BSTS: 1) the Average\\nAbsolute Effect, which indicates whether the impact was positive or negative, along\\nwith its magnitude, and 2) the statistical significance of the effect, which is captured\\nby the p-value pŒ±of BSTS at a confidence level Œ±. For the analysis, we set Œ±= 0.05.\\nIf the effect has no statistical significance at a confidence level Œ±, we consider that\\nvirality did not have a discernible impact on users‚Äô engagement - which we refer to as\\nNo Effect . Otherwise, if the effect is statistically significant and the observed engage-\\nment significantly deviates from the expected trend based on the examined nweeks, we\\nconsider that virality had a Growth orDecrease effect on users‚Äô attention as quantified\\nby the Average Absolute Effect. For our analyses, we use the BSTS implementation\\nfrom the CausalImpact R package [89]. This research question is addressed in Sec. 4.1.\\n3.4.2 Research Question 2: Do the faster-manifesting effects persist\\nlonger?\\nTo continue our investigation of virality dynamics, we aim to examine the relationship\\nbetween the speed at which effects manifest and their persistency over time. By using\\nincreasingly wider time windows, we can observe the effect of the same viral event\\nfrom a short-term to a long-term perspective.\\nFor each detected effect, its emergence and its subsequent persistency can be deter-\\nmined through a classification based on the output from the BSTS. For each viral post\\njof a page i, we define the time of emergence of its effect, denoted as œÑ(em)\\nij, as follows:\\nœÑ(em)\\nij = min {n= 2,3,4,5 :pij(n)< pŒ±}, (5)\\nwhere pij(n) is the p-value of BSTS applied to the time window associated to week n.\\nSimilarly, the fade-out time, œÑ(f‚àío)\\nij , is identified as the earliest time window in which\\nthe previously detected effect no longer manifests, i.e.,\\nœÑ(f‚àío)\\nij = min {n > œÑ(em)\\nij :pij(n)> pŒ±}. (6)\\nWe notice that the sets associated to the definitions of œÑ(em)\\nij andœÑ(f‚àío)\\nij are not empty\\nin the cases treated in this paper.\\nNotice also that the œÑ‚Äôs lead to a partition of the collection of the posts with the\\nrelated pages. Specifically, fixed ¬Ø n, we define\\nT(em)\\n¬Øn={(i, j)‚àà I √ó J :œÑ(em)\\nij = ¬Øn} (7)\\n10and\\nT(f‚àío)\\n¬Øn ={(i, j)‚àà I √ó J :œÑ(f‚àío)\\nij = ¬Øn}. (8)\\nWe define the persistency œïkhas the fraction of posts still having persistent effects at\\na given week hafter their emergence time k, i.e.,\\nœïkh=|T(em)\\nk| ‚àíPh\\n‚Ñì=k+1|T(f‚àío)\\n‚Ñì|\\n|T(em)\\nk|, (9)\\nwhere | ‚Ä¢ |is the cardinality of set ‚Ä¢, for each k‚àà[2,5] and h‚àà[k+ 1,6]. See Sec. 4.2\\nfor further details, in which we address the Research Question 2.\\nBy analyzing the persistency rates across all cases for each specified emergence\\ntime, we can evaluate the relationship between the speed of manifestation and the\\nlongevity of the effects. This analysis will help determine whether effects that emerge\\nmore quickly tend to persist longer or, conversely, if they vanish more rapidly than\\nslow-emerging ones.\\nThis approach helps us to understand the temporal behavior of viral effects, as we\\ncan further extrapolate the observed dynamics to their unobservable earlier phase by\\ntracing back to the time of the event and evaluating the dynamics as we approach it\\nasymptotically. This analysis provides valuable insights into how collective attention\\nresponds to sudden stress conditions, like viral news events, and to determine whether\\nthese events genuinely contribute to sustained growth or merely act as transient\\nspotlights.\\n4 Results\\nBefore discussing our research questions in depth, we first inspect the BSTS‚Äôs results,\\nwhich provide us with a useful initial overview to help delve into the analysis. We begin\\nby examining the proportion of cases that exhibit positive, null, or negative impacts\\nand assess their consistency across various time windows.\\nThe results are reported in Fig 3, which shows the percentage of Growth, No\\nEffect, and Decrease cases on the y-axis for each time scale on the x-axis. The diagram\\nincludes links representing the effects flow between consecutive weeks, color-coded\\naccording to the first observed effect to maintain a clear visual trajectory through the\\ndata stream. A significant observation from this analysis regards the high percentage\\nof cases showing no statistically significant impact on engagement performance. This\\nfirst finding suggests that virality does not necessarily enhance engagement, as it often\\nresults in an indiscernible impact.\\nSecondly, the percentages of Growth and Decrease cases are comparable, with\\nslightly higher occurrences of negative impacts. This suggests that, apart from being\\ninfrequent, the impact of a viral event on users‚Äô attention can even be detrimental.\\nContrary to common expectations [15, 90, 91], virality rarely induces engagement\\ngrowth. Furthermore, we observe that effects typically emerge or shift between the\\nsecond and fourth weeks, with the most significant transitions happening from the\\nthird to the fourth week. Beyond this period, the consistency of effects is broadly\\nstable. Given the inherently short-term nature of virality, effects observed in larger\\n11DecreaseNo EffectGrowth\\nDecreaseNo EffectGrowth\\nDecreaseNo EffectGrowth\\nDecreaseNo EffectGrowth\\nDecreaseNo EffectGrowth\\n0.000.250.500.751.00\\n2 3 4 5 6\\nWeeks from ViralityFrequencyFacebook A)\\nDecreaseNo EffectGrowth\\nDecreaseNo EffectGrowth\\nDecreaseNo EffectGrowth\\nDecreaseNo EffectGrowth\\nDecreaseNo EffectGrowth\\n0.000.250.500.751.00\\n2 3 4 5 6\\nWeeks from ViralityFrequencyYouTube B)2nd Week Effect\\nGrowth\\nNo Effect\\nDecreaseFig. 3 : Effects after virality with increasing time windows. The y-axis represents the\\npercentage of Growth, No Effect, and Decrease cases, for each time scale on the x-axis.\\nLinks represent the effects flow between consecutive weeks, color-coded according to\\nthe first observed effect (i.e., in the 2-week window).\\ntime scales, such as 5- and 6-week windows, are less likely to be directly attributable\\nto the viral event. These longer time windows are primarily aimed at assessing the\\npersistency of earlier effects, enabling us to analyze the dynamics from a short-term\\nto a long-term perspective and to distinguish the boundary between these periods.\\n4.1 RQ1: Dynamics of virality effect on Engagement\\nWe now deepen the dynamics of the virality effect on Engagement by focusing\\nexclusively on the Growth and Decrease cases, as defined in Sec. 3.4.1.\\nIn Fig. 4, we present the joint density of the slopes of the attention trends preceding\\nthe viral posts and the absolute effects on engagement after the event.\\nThe ‚ÄòTrend Pre Virality‚Äô on the y-axis represents the Œ≤1coefficient of the regression\\nestimated by the BSTS for the nweeks preceding the viral event. On the x-axis, we\\nshow the Average Absolute Effect on Engagement in the examined n-week window\\nafter virality, accounting for its previous trend.\\n12œÅœÅ = ‚àí0.92\\n‚àí0.20.00.2\\n‚àí2 ‚àí1 0 1 2\\nAverage Absolute EffectTrend Pre ViralityFacebook A)\\nœÅœÅ = ‚àí0.95\\n‚àí0.8‚àí0.40.00.40.8\\n‚àí2 ‚àí1 0 1 2\\nAverage Absolute EffectTrend Pre ViralityYouTube B) 2‚àíweek time windowFig. 4 : Density of the trend preceding the viral post and the average absolute effect\\non engagement for the 2-week time window. Trend Pre Virality is the Œ≤1 coefficient\\nof the regression estimated by the BSTS on the weeks preceding virality. Given its\\nprevious trend, the average Absolute Effect is the average effect on the Engagement\\nafter the viral event. Only events with a statistically significant effect on Engagement\\nare shown.\\nFig. 4 reports the values for the 2-week window as an example, while Fig. 6 in\\nSI Appendix shows the results for other timescales on both platforms which display\\nconsistent results.\\nAs Fig. 4 shows, the density is split into two opposing quadrants: from growth to\\ndecrease and from decrease to growth, highlighting a significant negative correlation\\nbetween the preceding trend and the consequent absolute effect. This dynamic shows\\nconsistency on both platforms and across their timescales, as shown in Tab 2, which\\nreports the Spearman‚Äôs correlation coefficients between the Œ≤1coefficient of the trend\\npreceding virality and the Average Absolute Effect on Engagement.\\nTherefore, virality positively impacts users‚Äô engagement when occurring as a sud-\\nden event on a declining collective attention. Conversely, when virality manifests\\nfollowing a sustained growth phase, it represents the final burst of that growth process,\\nwith users‚Äô attention successively standing on lower levels than its previous phase.\\nThese results shed light on the bounded yet elastic nature of collective attention.\\nWhile additional growth following a sustained growth phase is extremely rare, viral\\nevents act as a booster when users‚Äô attention is likely to nearing its lower bound,\\nreactivating the collective response process.\\nBy focusing on viral news, these results potentially indicate the presence of two\\ndifferent types of viral events. The first is a ‚Äòloaded-type‚Äô virality, where attention pro-\\ngressively increases, culminating in the viral event. This type could occur in scenarios\\nwhere information is gradually revealed, such as sequences of rumors, confirmations,\\n13Table 2 : Spearman‚Äôs correlation coeffi-\\ncients between the Œ≤1 coefficient of the\\ntrend preceding virality and the Absolute\\nEffect on Engagement for different plat-\\nforms and time windows. Posts and Videos\\nrepresent the number of events for which\\na Growth or Decrease effect is detected,\\naccording to the BSTS.\\nFacebook YouTube\\nTimescale œÅ Posts œÅ Videos\\n2 Weeks -0.92 3015 -0.95 1100\\n3 Weeks -0.91 2953 -0.93 1048\\n4 Weeks -0.90 3134 -0.93 1049\\n5 Weeks -0.88 3279 -0.93 1028\\n6 Weeks -0.87 3399 -0.93 1016\\nand official announcements. The second type represents a ‚Äòsudden-type‚Äô virality, with\\nthe news emerging unexpectedly as an exogenous event. Based on the release patterns\\nof information, this interpretation could explain the differences between the two types,\\nalong with the inverse relationship between the preceding attention pattern and the\\nafter-effect of the viral news.\\n4.2 RQ2: Emergence and persistency of the virality effect\\nAfter assessing how virality affects users‚Äô engagement and its extent, we now inspect\\nthe temporal dynamics of the effect by examining the relationship between its manifes-\\ntation and longevity. In this section, the analysis will specifically focus on the effect‚Äôs\\npersistency based on its timing of emergence and fade-out.\\nIn Eq. 5, we defined the time of œÑ(em)\\nij as the shortest time window during which\\na Growth or Decrease effect first becomes significant. For example, if the impact of a\\nviral event is significant within a 2-week time window, we assign its emergence time as\\n2. Conversely, if the impact is not significant within the 2-week window but becomes\\nsignificant in the 3-week window, the emergence time is then 3.\\nSimilarly, we examine the duration for which the effect remains unchanged and\\ndetermine the window in which it ceases to be consistent. As we defined in Eq. 6,\\nthefade-out œÑ(f‚àío)\\nij of the effect corresponds to the first window where it no longer\\nmanifests. For example, if an event consistently exhibits a Growth effect from the 2-\\nweek to the 4-week window and then ceases to manifest in the 5-week window, the\\nemergence time would be 2, and the fade-out time would be 5.\\nConsequently, as we defined in Eq. 9, the persistency œïkhis the fraction of still\\npersistent effects at hweeks after their given emergence time k, to which we generally\\nrefer here. Hence, we evaluate the effect‚Äôs decay by calculating the fraction of still\\npersistent effects after each week, grouped by their time of emergence. For instance,\\nover the total number of cases where the impact appears two weeks after virality, we\\ncalculate the proportion of persistent effects in the 3rd, 4th, 5th, and 6th week. This\\nmethod allows us to derive the effect‚Äôs decay curve for each time of emergence k‚àà[2,5].\\n14At this stage, we are able to observe the persistency rate up to a minimum of two\\nweeks after the viral event. By fitting a negative exponential function of kand power\\nlaw in h, we can estimate the variation of the exponent of the decay curves based on\\ntheir time of emergence kas:\\nœïkh=f(k, h) =h‚àíŒª+k\\nŒ≤. (10)\\nThis procedure allows us to describe the decay behavior of the effects based on\\ntheir emergence and to extend it to their unobservable earlier phases as if we approach\\nthem asymptotically.\\nThe results of the fitting procedure are reported in Tab. 3, while Fig. 5 shows the\\ngraphical representation of the decay curves for each emergence time. Solid lines repre-\\nsent the estimated decay for the observed curves - from the 2nd week to the 5th week of\\nemergence time - along with their observed persistency. The dotted lines represent the\\ncorresponding extrapolated decay curves for 0 and 1 week after virality as emergence\\ntime. The curves representing extrapolations inherently involve higher uncertainty\\nand should be considered useful asymptotic approximations for understanding the\\ndynamics during their unobservable phases.\\nTable 3 : Paramaters estimation\\nPlatform Parameter Estimate Std. error p-value\\nFacebook Œª 1.09 0.02 <0.001\\nFacebook Œ≤ 8.84 0.44 <0.001\\nYouTube Œª 1.09 0.05 <0.001\\nYouTube Œ≤ 17.44 4.55 0.0024\\n0.000.250.500.751.00\\n0 1 2 3 4 5 6\\nTimePersistencyFacebook\\n0.000.250.500.751.00\\n0 1 2 3 4 5 6\\nTimePersistencyYouTube\\nEmergence \\n Time\\n0*\\n1*\\n2\\n3\\n4\\n5\\nFig. 5 : Persistency of the virality effect based on its time of emergence. Solid lines\\nrepresent the estimated decay for the observed curves - from the 2nd week to the 5th\\nweek of emergence time - along with their observed values. The dotted lines represent\\nthe corresponding extrapolated decay curves for 0 and 1 week after virality as emer-\\ngence time.\\n15According to the estimations, in about 50% of cases, the impact either fades out\\nwithin the first week following the viral event or does not occur. Moreover, the data\\nconsistently show that earlier emergences of viral impacts are associated with faster\\ndecay across both platforms. This indicates that faster processes tend to fade quicker,\\nwhile slower ones exhibit more persistence. This finding highlights the elastic nature of\\ncollective attention when stretched to its limits. The volatile and fluctuating nature of\\nattention prevents it from being steadily focused, resulting in a trade-off between the\\nrapidity of the effect and its durability. These observations have critical implications\\nfor content producers in the digital realm, underscoring a clear distinction between\\nshort-term and long-term dynamics of collective attention.\\nFrom a probabilistic perspective, sudden and disproportionate growth is rare and\\nrarely leads to a noticeable positive impact on engagement. Even when it occurs, its\\neffects tend to fade away swiftly. Conversely, organic and sustained growth, though\\nslower to manifest, tends to have more enduring effects. This contrast emphasizes\\nthe transient nature of viral events compared to the lasting effectiveness of consistent\\nengagement, highlighting the importance of gradual and continuous attention-building\\nstrategies rather than relying on abrupt surges of visibility.\\n5 Discussion\\n5.1 Implications\\nOur findings highlight collective attention‚Äôs bounded and elastic nature under sud-\\nden stress conditions, such as viral events, and have significant implications for the\\ninformation sources ecosystem in the digital domain. Firstly, our analysis reveals that\\nviral events rarely lead to engagement growth, suggesting that the frantic pursuit of\\nsudden visibility is often unproductive. The study distinguishes between two primary\\ntypes of virality, each corresponding to a different mechanism of collective attention\\nresponse. The first type, ‚Äòloaded-type virality,‚Äô is characterized by a gradual increase\\nof engagement that culminates in a final burst of attention in the viral event. The\\nsecond type, ‚Äòsudden-type virality,‚Äô occurs when news emerges unexpectedly, similar\\nto an exogenous event, and is the only scenario in which virality leads to an increased\\nusers‚Äô response. Regardless of whether the impact is positive or negative, our findings\\nindicate that effects emerging more quickly tend to fade faster, while slower-emerging\\nprocesses are more persistent over time. As well as being an extremely rare event,\\nvirality does not turn out to be an effective long-term growth strategy. These insights\\nunderscore the advantages of organic and continued growth strategies in establishing\\na solid and enduring connection with the user base, yielding positive and sustained\\nresponses regarding collective attention.\\n5.2 Limitations and Future Research\\nThe findings of this study are specifically tied to the platforms analyzed, the type\\nof pages examined, and the period considered. Although the results are consistent\\nwith general human dynamics observed in other collective attention studies, they\\ncannot be generalized across all social media platforms. Each platform has distinct\\n16user bases and algorithmic characteristics that influence how content is spread and\\nengages users. Additionally, the specificity of the sources analyzed ‚Äî news outlets - and\\ntheir content type must be considered. Focusing exclusively on news items offers the\\nadvantage of dealing with content inherently tied to specific topics or events, providing\\na concrete subject matter. However, the dynamics and peculiarities of the news agenda\\nmean that these results may not necessarily apply to the broader spectrum of content\\ncreators. Future research should explore various social media platforms and content\\ntypes to understand better the dynamics of virality and collective attention across\\ndifferent contexts. A more comprehensive picture of how virality functions in the\\ndigital landscape can be developed by expanding the study to include a broader range\\nof content creators and user engagement patterns.\\n6 Conclusions\\nThis study offers a detailed analysis of virality dynamics on social media platforms.\\nIt examines how viral events affect users‚Äô engagement and the relationship between\\ntheir rapid emergence and subsequent persistency. Our findings challenge the common\\nassumption that virality regularly enhances user engagement. While viral events may\\nmomentarily capture attention, our evaluation reveals that they rarely foster sustained\\nengagement growth. Indeed, the impact of virality typically depends on the preceding\\ngrowth trend, with a pronounced negative correlation observed between the two. We\\ncategorize viral events into two distinct types: ‚Äòloaded-type‚Äô virality, characterized by\\na gradual build-up of attention culminating in a viral burst, and ‚Äòsudden-type‚Äô virality,\\nwhich appears unexpectedly like an exogenous shock.\\nWhen virality occurs following an ongoing growth phase, it represents its final\\npeak, resulting in diminished attention levels. In contrast, a viral event reactivates the\\ncollective response process when arising suddenly after a declining attention phase.\\nAdditionally, our study highlights the elastic nature of collective attention, demon-\\nstrating that effects emerging swiftly tend to fade quickly, whereas slower-emerging\\neffects show greater persistency. This finding highlights the critical role of content pro-\\nducers in fostering consistent, high-quality engagement rather than depending solely\\non transient viral spikes. The rapid dissipation of viral impacts, particularly those that\\nemerge quickly, illustrates the volatility and fluctuation of collective attention. This\\nvolatility implies a trade-off between the rapidity of an impact‚Äôs emergence and its last-\\ning presence, emphasizing content producers‚Äô challenges in capturing and maintaining\\nusers‚Äô engagement. In conclusion, although virality can temporarily surge users‚Äô atten-\\ntion, its effects are usually short-lived, making the frantic pursuit of sudden visibility\\nan often fruitless strategy.\\n7 Author contributions statement\\n‚Ä¢Emanuele Sangiorgio Conceptualization, Methodology, Validation, Data collec-\\ntion, Data curation, Data analysis, Visualization, Writing.\\n‚Ä¢Niccol` o Di Marco Methodology, Validation, Data analysis, Writing.\\n‚Ä¢Gabriele Etta Data collection, Data curation, Data analysis, Writing.\\n‚Ä¢Matteo Cinelli Conceptualization, Methodology, Supervision, Writing.\\n17‚Ä¢Roy Cerqueti Conceptualization, Methodology, Supervision, Writing.\\n‚Ä¢Walter Quattrociocchi Conceptualization, Supervision, Writing.\\nDeclaration of Competing Interest\\nThe authors declare no competing interest.\\n8 Data availability\\nThe data collection and analysis process are compliant with the terms and conditions\\nimposed by Crowdtangle [85]. Therefore, the results described in this paper cannot be\\nexploited to infer the identity of the accounts involved. CrowdTangle does not include\\npaid ads unless those ads began as organic, non-paid posts that were subsequently\\n‚Äúboosted‚Äù using Facebook‚Äôs advertising tools. It also does not include activity on\\nprivate accounts, or posts made visible only to specific groups of followers.\\n9 SI Appendix Section\\nReferences\\n[1] Bakshy, E., Rosenn, I., Marlow, C., Adamic, L.: The role of social networks in\\ninformation diffusion. In: Proceedings of the 21st International Conference on\\nWorld Wide Web, pp. 519‚Äì528 (2012)\\n[2] Schmidt, A.L., Zollo, F., Vicario, M.D., Bessi, A., Scala, A., Caldarelli, G.,\\nStanley, H.E., Quattrociocchi, W.: Anatomy of news consumption on facebook.\\nProceedings of the National Academy of Sciences 114(12), 3035‚Äì3039 (2017)\\n[3] Bergstr¬® om, A., Belfrage, M.J.: News in social media. Digital Journalism 6(5),\\n583‚Äì598 (2018)\\n[4] Grover, P., Kar, A.K., Dwivedi, Y.: The evolution of social media influence-\\na literature review and research agenda. International Journal of Information\\nManagement Data Insights 2(2), 100116 (2022)\\n[5] Pentina, I., Tarafdar, M.: From ‚Äúinformation‚Äù to ‚Äúknowing‚Äù: Exploring the role of\\nsocial media in contemporary news consumption. Computers in human behavior\\n35, 211‚Äì223 (2014)\\n[6] Levy, R.: Social media, news consumption, and polarization: Evidence from a\\nfield experiment. American economic review 111(3), 831‚Äì870 (2021)\\n[7] Boase, J.: Personal networks and the personal communication system: Using mul-\\ntiple media to connect. Information, communication & society 11(4), 490‚Äì508\\n(2008)\\n18[8] Bail, C.: Breaking the Social Media Prism: How to Make Our Platforms Less\\nPolarizing. Princeton University Press, Princeton (2022)\\n[9] Bawden, D., Robinson, L.: Information overload: An overview. In: Oxford\\nEncyclopedia of Political Decision Making. Oxford University Press, Oxford\\n(2020)\\n[10] Harrigan, P., Daly, T.M., Coussement, K., Lee, J.A., Soutar, G.N., Evers, U.:\\nIdentifying influencers on social media. International Journal of Information\\nManagement 56, 102246 (2021)\\n[11] Falkinger, J.: Limited attention as a scarce resource in information-rich economies.\\nThe Economic Journal 118(532), 1596‚Äì1620 (2008)\\n[12] Bell, G., Hey, T., Szalay, A.: Beyond the data deluge. Science 323(5919), 1297‚Äì\\n1298 (2009)\\n[13] Anderson, S.P., De Palma, A.: Competition for attention in the information\\n(overload) age. The RAND Journal of Economics 43(1), 1‚Äì25 (2012)\\n[14] Sangiorgio, E., Cinelli, M., Cerqueti, R., Quattrociocchi, W.: Followers do not\\ndictate the virality of news outlets on social media. PNAS Nexus 3(7), 257 (2024)\\n[15] Al-Rawi, A.: Viral news on social media. Digital journalism 7(1), 63‚Äì79 (2019)\\n[16] Scott, D.M.: The New Rules of Marketing and PR: How to Use Social Media,\\nBlogs, News Releases, Online Video, and Viral Marketing to Reach Buyers\\nDirectly. John Wiley & Sons, ??? (2009)\\n[17] Miller, R., Lammas, N.: Social media and its implications for viral marketing.\\nAsia Pacific Public Relations Journal 11(1), 1‚Äì9 (2010)\\n[18] Kaplan, A.M., Haenlein, M.: Two hearts in three-quarter time: How to waltz the\\nsocial media/viral marketing dance. Business horizons 54(3), 253‚Äì263 (2011)\\n[19] Petrescu, M., Korgaonkar, P.: Viral advertising: Definitional review and synthesis.\\nJournal of Internet Commerce 10(3), 208‚Äì226 (2011)\\n[20] Borges-Tiago, M.T., Tiago, F., Cosme, C.: Exploring users‚Äô motivations to par-\\nticipate in viral communication on social media. Journal of Business Research\\n101, 574‚Äì582 (2019)\\n[21] Scott, S.L., Varian, H.R.: Predicting the present with bayesian structural time\\nseries. International Journal of Mathematical Modelling and Numerical Optimi-\\nsation 5(1-2), 4‚Äì23 (2014)\\n[22] Simon, H., Greenberger, M.: Computers, communications and the public inter-\\nest. Computers, communications, and the public interest. Johns Hopkins Press,\\n19Baltimore, 40‚Äì41 (1971)\\n[23] Simon, H.A.: Designing organizations for an information-rich world. International\\nLibrary of Critical Writings in Economics 70, 187‚Äì202 (1996)\\n[24] Davenport, T.H., Beck, J.C.: The attention economy. Ubiquity 2001 (May), 1\\n(2001)\\n[25] Falkinger, J.: Attention economies. Journal of Economic Theory 133(1), 266‚Äì294\\n(2007)\\n[26] Treisman, A.M.: Strategies and models of selective attention. Psychological review\\n76(3), 282 (1969)\\n[27] Kahneman, D.: Attention and Effort. Prentice-Hall, ??? (1973)\\n[28] Zollo, F., Bessi, A., Del Vicario, M., Scala, A., Caldarelli, G., Shekhtman, L.,\\nHavlin, S., Quattrociocchi, W.: Debunking in a world of tribes. PloS one 12(7),\\n0181821 (2017)\\n[29] Jacoby, J.: Perspectives on information overload. Journal of consumer research\\n10(4), 432‚Äì435 (1984)\\n[30] Edmunds, A., Morris, A.: The problem of information overload in business\\norganisations: a review of the literature. International journal of information\\nmanagement 20(1), 17‚Äì28 (2000)\\n[31] White, M., Dorman, S.M.: Confronting information overload. Journal of School\\nHealth 70(4), 160‚Äì160 (2000)\\n[32] Hendricks, V.F., Vestergaard, M.: The Attention Economy, pp. 1‚Äì17. Springer,\\nCham (2019)\\n[33] Wu, F., Huberman, B.A.: Novelty and collective attention. Proceedings of the\\nNational Academy of Sciences 104(45), 17599‚Äì17601 (2007)\\n[34] Lazer, D., Pentland, A., Adamic, L., Aral, S., Barab¬¥ asi, A.-L., Brewer, D., Chris-\\ntakis, N., Contractor, N., Fowler, J., Gutmann, M., Jebara, T., King, G., Macy,\\nM., Roy, D., Alstyne, M.V.: Computational social science. Science 323(5915),\\n721‚Äì723 (2009)\\n[35] Mocanu, D., Rossi, L., Zhang, Q., Karsai, M., Quattrociocchi, W.: Collective\\nattention in the age of (mis) information. Computers in Human Behavior 51,\\n1198‚Äì1204 (2015)\\n[36] Lorenz-Spreen, P., M√∏nsted, B.M., H¬® ovel, P., Lehmann, S.: Accelerating dynamics\\nof collective attention. Nature communications 10(1), 1759 (2019)\\n20[37] Alipour, S., Di Marco, N., Avalle, M., Etta, G., Cinelli, M., Quattrociocchi, W.:\\nThe drivers of global news spreading patterns. Scientific Reports 14(1), 1519\\n(2024)\\n[38] Cinelli, M., Brugnoli, E., Schmidt, A.L., Zollo, F., Quattrociocchi, W., Scala, A.:\\nSelective exposure shapes the facebook news diet. PloS one 15(3), 0229129 (2020)\\n[39] Del Vicario, M., Bessi, A., Zollo, F., Petroni, F., Scala, A., Caldarelli, G., Stanley,\\nH.E., Quattrociocchi, W.: The spreading of misinformation online. Proceedings\\nof the national academy of Sciences 113(3), 554‚Äì559 (2016)\\n[40] Marco, N.D., Cinelli, M., Alipour, S., Quattrociocchi, W.: Users volatility on\\nreddit and voat. IEEE Transactions on Computational Social Systems, 1‚Äì9 (2024)\\n[41] Bessi, A., Coletto, M., Davidescu, G.A., Scala, A., Caldarelli, G., Quattrociocchi,\\nW.: Science vs conspiracy: Collective narratives in the age of misinformation.\\nPloS one 10(2), 0118093 (2015)\\n[42] Sultana, T., Dhillon, G., Oliveira, T.: The effect of fear and situational motivation\\non online information avoidance: The case of covid-19. International Journal of\\nInformation Management 69, 102596 (2023)\\n[43] Del Vicario, M., Vivaldo, G., Bessi, A., Zollo, F., Scala, A., Caldarelli, G., Quat-\\ntrociocchi, W.: Echo chambers: Emotional contagion and group polarization on\\nfacebook. Scientific reports 6(1), 37825 (2016)\\n[44] Terren, L.T.L., Borge-Bravo, R.B.-B.R.: Echo chambers on social media: A\\nsystematic review of the literature. Review of Communication Research 9(2021)\\n[45] Nyhan, B., Settle, J., Thorson, E., Wojcieszak, M., Barber¬¥ a, P., Chen, A.Y.,\\nAllcott, H., Brown, T., Crespo-Tenorio, A., Dimmery, D., et al. : Like-minded\\nsources on facebook are prevalent but not polarizing. Nature 620(7972), 137‚Äì144\\n(2023)\\n[46] Cinelli, M., De Francisci Morales, G., Galeazzi, A., Quattrociocchi, W., Starnini,\\nM.: The echo chamber effect on social media. Proceedings of the National\\nAcademy of Sciences 118(9), 2023301118 (2021)\\n[47] Falkenberg, M., Galeazzi, A., Torricelli, M., Di Marco, N., Larosa, F., Sas, M.,\\nMekacher, A., Pearce, W., Zollo, F., Quattrociocchi, W., et al. : Growing polar-\\nization around climate change on social media. Nature Climate Change 12(12),\\n1114‚Äì1121 (2022)\\n[48] Allcott, H., Gentzkow, M., Mason, W., Wilkins, A., Barber¬¥ a, P., Brown, T., Cis-\\nneros, J.C., Crespo-Tenorio, A., Dimmery, D., Freelon, D., et al. : The effects\\nof facebook and instagram on the 2020 election: A deactivation experiment.\\nProceedings of the National Academy of Sciences 121(21), 2321584121 (2024)\\n21[49] Flaxman, S., Goel, S., Rao, J.M.: Filter bubbles, echo chambers, and online news\\nconsumption. Public opinion quarterly 80(S1), 298‚Äì320 (2016)\\n[50] Pecile, G., Di Marco, N., Cinelli, M., Quattrociocchi, W.: Mapping the global\\nelection landscape on social media in 2024. arXiv preprint arXiv:2406.04962\\n(2024)\\n[51] Kim, A., Moravec, P.L., Dennis, A.R.: When do details matter? news source evalu-\\nation summaries and details against misinformation on social media. International\\nJournal of Information Management 72, 102666 (2023)\\n[52] Zhong, B.: Going beyond fact-checking to fight health misinformation: A multi-\\nlevel analysis of the twitter response to health news stories. International Journal\\nof Information Management 70, 102626 (2023)\\n[53] Shahbazi, M., Bunker, D.: Social media trust: Fighting misinformation in the time\\nof crisis. International Journal of Information Management 77, 102780 (2024)\\n[54] Etta, G., Cinelli, M., Di Marco, N., Avalle, M., Panconesi, A., Quattrociocchi, W.:\\nA topology-based approach for predicting toxic outcomes on twitter and youtube.\\nIEEE Transactions on Network Science and Engineering (2024)\\n[55] Flamino, J., Galeazzi, A., Feldman, S., Macy, M.W., Cross, B., Zhou, Z., Serafino,\\nM., Bovet, A., Makse, H.A., Szymanski, B.K.: Political polarization of news media\\nand influencers on twitter in the 2016 and 2020 us presidential elections. Nature\\nHuman Behaviour 7(6), 904‚Äì916 (2023)\\n[56] Alipour, S., Galeazzi, A., Sangiorgio, E., Avalle, M., Bojic, L., Cinelli, M.,\\nQuattrociocchi, W.: Cross-platform social dynamics: an analysis of chatgpt and\\ncovid-19 vaccine conversations. Scientific Reports 14(1), 2789 (2024)\\n[57] Cinelli, M., Quattrociocchi, W., Galeazzi, A., Valensise, C.M., Brugnoli, E.,\\nSchmidt, A.L., Zola, P., Zollo, F., Scala, A.: The covid-19 social media infodemic.\\nScientific reports 10(1), 1‚Äì10 (2020)\\n[58] Briand, S.C., Cinelli, M., Nguyen, T., Lewis, R., Prybylski, D., Valensise, C.M.,\\nColizza, V., Tozzi, A.E., Perra, N., Baronchelli, A., et al. : Infodemics: A new\\nchallenge for public health. Cell 184(25), 6010‚Äì6014 (2021)\\n[59] Bovet, A., Makse, H.A.: Influence of fake news in twitter during the 2016 us\\npresidential election. Nature communications 10(1), 7 (2019)\\n[60] Gonz¬¥ alez-Bail¬¥ on, S., Lazer, D., Barber¬¥ a, P., Zhang, M., Allcott, H., Brown, T.,\\nCrespo-Tenorio, A., Freelon, D., Gentzkow, M., Guess, A.M., Iyengar, S., Kim,\\nY.M., Malhotra, N., Moehler, D., Nyhan, B., Pan, J., Rivera, C.V., Settle, J.,\\nThorson, E., Tromble, R., Wilkins, A., Wojcieszak, M., Jonge, C.K., Franco, A.,\\nMason, W., Stroud, N.J., Tucker, J.A.: Asymmetric ideological segregation in\\n22exposure to political news on facebook. Science 381(6656), 392‚Äì398 (2023)\\n[61] Gonz¬¥ alez-Bail¬¥ on, S., Lelkes, Y.: Do social media undermine social cohesion? a\\ncritical review. Social Issues and Policy Review 17(1), 155‚Äì180 (2023)\\n[62] Guess, A.M., Malhotra, N., Pan, J., Barber¬¥ a, P., Allcott, H., Brown, T., Crespo-\\nTenorio, A., Dimmery, D., Freelon, D., Gentzkow, M., et al. : How do social media\\nfeed algorithms affect attitudes and behavior in an election campaign? Science\\n381(6656), 398‚Äì404 (2023)\\n[63] Guess, A.M., Malhotra, N., Pan, J., Barber¬¥ a, P., Allcott, H., Brown, T., Crespo-\\nTenorio, A., Dimmery, D., Freelon, D., Gentzkow, M., et al. : Reshares on social\\nmedia amplify political news but do not detectably affect beliefs or opinions.\\nScience 381(6656), 404‚Äì408 (2023)\\n[64] Avalle, M., Di Marco, N., Etta, G., Sangiorgio, E., Alipour, S., Bonetti, A., Alvisi,\\nL., Scala, A., Baronchelli, A., Cinelli, M., et al. : Persistent interaction patterns\\nacross social media platforms and over time. Nature 628(8008), 582‚Äì589 (2024)\\n[65] Goel, S., Anderson, A., Hofman, J., Watts, D.J.: The structural virality of online\\ndiffusion. Management Science 62(1), 180‚Äì196 (2016)\\n[66] Berger, J., Milkman, K.L.: What makes online content viral? Journal of marketing\\nresearch 49(2), 192‚Äì205 (2012)\\n[67] Rathje, S., Robertson, C., Brady, W.J., Van Bavel, J.J.: People think that social\\nmedia platforms do (but should not) amplify divisive content. Perspectives on\\nPsychological Science, 17456916231190392 (2023)\\n[68] Guerini, M., Strapparava, C., Ozbal, G.: Exploring text virality in social networks.\\nProceedings of the International AAAI Conference on Web and Social Media 5(1),\\n506‚Äì509 (2021)\\n[69] Bruni, L., Francalanci, C., Giacomazzi, P.: The role of multimedia content in\\ndetermining the virality of social media information. Information 3(3), 278‚Äì289\\n(2012)\\n[70] Deza, A., Parikh, D.: Understanding image virality. In: Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition, pp. 1818‚Äì1826 (2015)\\n[71] Ling, C., AbuHilal, I., Blackburn, J., De Cristofaro, E., Zannettou, S., Stringhini,\\nG.: Dissecting the meme magic: Understanding indicators of virality in image\\nmemes. Proceedings of the ACM on human-computer interaction 5(CSCW1),\\n1‚Äì24 (2021)\\n[72] Mukherjee, P., Dutta, S., De Bruyn, A.: Did clickbait crack the code on virality?\\nJournal of the Academy of Marketing Science 50(3), 482‚Äì502 (2022)\\n23[73] Lin, X., Wang, X.: Following too much on facebook brand page: A con-\\ncept of brand overload and its validation. International Journal of Information\\nManagement 73, 102682 (2023)\\n[74] Lance, P., Guy J, G.: From subservient chickens to brawny men: A comparison of\\nviral advertising to television advertising. Journal of interactive advertising 6(2),\\n4‚Äì33 (2006)\\n[75] Maarouf, A., Pr¬® ollochs, N., Feuerriegel, S.: The virality of hate speech on social\\nmedia. Proceedings of the ACM on Human-Computer Interaction 8(CSCW1),\\n1‚Äì22 (2024)\\n[76] Etta, G., Sangiorgio, E., Di Marco, N., Avalle, M., Scala, A., Cinelli, M., Quat-\\ntrociocchi, W.: Characterizing engagement dynamics across topics on facebook.\\nPlos one 18(6), 0286150 (2023)\\n[77] Tsugawa, S., Ohsaki, H.: On the relation between message sentiment and its\\nvirality on social media. Social network analysis and mining 7, 1‚Äì14 (2017)\\n[78] Solovev, K., Pr¬® ollochs, N.: Moral emotions shape the virality of covid-19 misin-\\nformation on social media. In: Proceedings of the ACM Web Conference 2022,\\npp. 3706‚Äì3717 (2022)\\n[79] Rathje, S., Van Bavel, J.J., Van Der Linden, S.: Out-group animosity drives\\nengagement on social media. Proceedings of the National Academy of Sciences\\n118(26), 2024292118 (2021)\\n[80] Vijaykumar, S., Jin, Y., Nowak, G.: Social media and the virality of risk: The risk\\namplification through media spread (rams) model. Journal of homeland security\\nand emergency management 12(3), 653‚Äì677 (2015)\\n[81] Puryear, C., Vandello, J.A., Gray, K.: Moral panics on social media are fueled by\\nsignals of virality. Journal of Personality and Social Psychology (2024)\\n[82] Mancini, A., Desiderio, A., Di Clemente, R., Cimini, G.: Self-induced consensus of\\nreddit users to characterise the gamestop short squeeze. Scientific reports 12(1),\\n13780 (2022)\\n[83] Campbell, B., Drake, M., Thornock, J., Twedt, B.: Earnings virality. Journal of\\nAccounting and Economics 75(1), 101517 (2023)\\n[84] NewsGuard. https://www.newsguardtech.com (2024)\\n[85] CrowdTangle. CrowdTangle Team. Facebook, Menlo Park, California, United\\nStates (2024)\\n[86] YouTube: YouTube Data API. https://developers.google.com/youtube/v3\\n(2024)\\n24[87] Elmas, T., Stephane, S., Houssiaux, C.: Measuring and detecting virality on social\\nmedia: the case of twitter‚Äôs viral tweets topic. In: Companion Proceedings of the\\nACM Web Conference 2023, pp. 314‚Äì317 (2023)\\n[88] Trujillo, A., Cresci, S.: Make reddit great again: assessing community effects of\\nmoderation interventions on r/the donald. Proceedings of the ACM on Human-\\ncomputer Interaction 6(CSCW2), 1‚Äì28 (2022)\\n[89] Brodersen, K.H., Gallusser, F., Koehler, J., Remy, N., Scott, S.L.: Inferring causal\\nimpact using Bayesian structural time-series models. The Annals of Applied\\nStatistics 9(1), 247‚Äì274 (2015)\\n[90] Tellis, G.J., MacInnis, D.J., Tirunillai, S., Zhang, Y.: What drives virality (shar-\\ning) of online digital content? the critical role of information, emotion, and brand\\nprominence. Journal of marketing 83(4), 1‚Äì20 (2019)\\n[91] Malodia, S., Dhir, A., Bilgihan, A., Sinha, P., Tikoo, T.: Meme marketing:\\nHow can marketers drive better engagement using viral memes? Psychology &\\nMarketing 39(9), 1775‚Äì1801 (2022)\\n25‚àí0.2‚àí0.10.00.10.2\\n‚àí2 ‚àí1 0 1 2\\n Trend Pre Virality\\n‚àí0.50‚àí0.250.000.250.50\\n‚àí2‚àí1 012\\n  3 Weeks\\n‚àí0.2‚àí0.10.00.10.2\\n‚àí2 ‚àí1 0 1 2\\n Trend Pre Virality\\n‚àí0.4‚àí0.20.00.20.4\\n‚àí2‚àí1 012\\n  4 Weeks\\n‚àí0.10‚àí0.050.000.050.10\\n‚àí2 ‚àí1 0 1 2\\n Trend Pre Virality\\n‚àí0.20.00.2\\n‚àí2‚àí1 012\\n  5 Weeks\\n‚àí0.10‚àí0.050.000.050.10\\n‚àí2 ‚àí1 0 1 2\\nAverage Absolute EffectTrend Pre Virality\\n‚àí0.2‚àí0.10.00.10.2\\n‚àí2‚àí1 012\\nAverage Absolute Effect 6 WeeksFacebook                                                    YouTubeFig. 6 : Density of the trend preceding the viral post and the average absolute effect on\\nengagement for the 3, 4, 5, and 6-week timescales. We recall that Trend Pre Virality\\nis the Œ≤1 coefficient of the regression estimated by the BSTS on the weeks preceding\\nvirality, and that the Average Absolute Effect is the average effect on the Engagement\\nafter virality estimated by the BSTS, given its previous trend. Only events with a\\nstatistically significant effect on engagement are shown.26']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_texts[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2caa5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Doc 2407.16014v1.pdf',\n",
       "  'Political Elites in the Attention Economy: Visibility Over Civility and Credibility?\\nAhana Biswas1, Yu-Ru Lin1*, Yuehong Cassandra Tai2, Bruce A. Desmarais2\\n1University of Pittsburgh\\n2Pennsylvania State University\\nahana.biswas@pitt.edu, yurulin@pitt.edu, yhcasstai@psu.edu, bdesmarais@psu.edu\\nAbstract\\nElected officials have privileged roles in public communica-\\ntion. In contrast to national politicians, whose posting content\\nis more likely to be closely scrutinized by a robust ecosystem\\nof nationally focused media outlets, sub-national politicians\\nare more likely to openly disseminate harmful content with\\nlimited media scrutiny. In this paper, we analyze the factors\\nthat explain the online visibility of over 6.5K unique state leg-\\nislators in the US and how their visibility might be impacted\\nby posting low-credibility or uncivil content. We conducted a\\nstudy of posting on Twitter and Facebook (FB) during 2020-\\n21 to analyze how legislators engage with users on these plat-\\nforms. The results indicate that distributing content with low-\\ncredibility information attracts greater attention from users on\\nFB and Twitter for Republicans. Conversely, posting content\\nthat is considered uncivil on Twitter receives less attention.\\nA noticeable scarcity of posts containing uncivil content was\\nobserved on FB, which may be attributed to the different com-\\nmunication patterns of legislators on these platforms. In most\\ncases, the effect is more pronounced among the most ideolog-\\nically extreme legislators. Our research explores the influence\\nexerted by state legislators on online political conversations,\\nwith Twitter and FB serving as case studies. Furthermore, it\\nsheds light on the differences in the conduct of political ac-\\ntors on these platforms. This study contributes to a better un-\\nderstanding of the role that political figures play in shaping\\nonline political discourse.\\n1 Introduction\\nSocial media channels are effective for communication due\\nto the ease of information dissemination irrespective of the\\nquality of the information. Politicians also leverage social\\nmedia due to the wide reach of these platforms. While pub-\\nlic officials can promote constructive dialogue, they can also\\nspread harmful content online. Government officials are of-\\nten subjected to less stringent content moderation rules (Pel-\\nletier et al. 2021), and have higher followings than average\\nusers (Grant et al. 2010) which makes it easier for them to\\nendorse and propagate harmful content online.\\nPolitical communication online is often geared toward the\\ntarget audience and platform characteristics (Kreiss 2016;\\nEnli and Skogerb√∏ 2013; Stier et al. 2020; Kelm 2020). For\\n*Corresponding author.\\nCopyright ¬© 2022, Association for the Advancement of Artificial\\nIntelligence (www.aaai.org). All rights reserved.instance, Stier et al. (2020) found that social media messages\\nof both candidates and their audiences focused on distinct\\ntopics compared to the general audience during 2013 Ger-\\nman federal elections. More broadly, politicians often tailor\\ntheir content to achieve maximum political gains, and prior\\nstudies have shown that social media plays a significant role\\nin shaping political agendas, influencing public opinion, and\\npotentially affecting electoral outcomes (Kreiss 2016; Bovet\\net al. 2019; Boulianne and Larsson 2023).\\nTo understand how politicians use social media to sway\\npublic opinion, it is critical to examine what makes them vis-\\nible online. In this study, we use the term visibility to refer to\\nthe level of public engagement or interactions individuals re-\\nceive on social media, which is often utilized to gauge their\\noutreach and influence on these platforms (Eberl et al. 2020)\\n. Party membership, demographic information (such as state,\\nrace, and gender), and platform-level characteristics (such as\\nfollower count, posting activity, and content style) are poten-\\ntial contributors. We are particularly interested in determin-\\ning if politicians‚Äô visibility is increased by the dissemina-\\ntion of harmful content, which includes toxic and uncivil\\nlanguage, as well as untrustworthy or non-credible infor-\\nmation. Both uncivil and low-credibility content have been\\nassociated with a decline in the quality of democratic dis-\\ncourse (Goovaerts et al. 2020; Bennett et al. 2018). We ask,\\ndoes posting uncivil and low-credibility content increase the\\nvisibility of politicians? The answer to this question is criti-\\ncal as prior research has linked harmful content online to vi-\\nolent offline incidents, increased affective polarization, dis-\\ntrust in institutions, and so on (Johnson 2018; Serrano-Puche\\n2021; Coe et al. 2014). Harmful content originating from or\\nendorsed by politicians may further exacerbate these nega-\\ntive outcomes due to their larger audience base and higher\\ntrustability owing to partisan preferences.\\nThis work focuses on how US state legislators cultivate\\nand exert their influence online. In contrast to national politi-\\ncians, who are closely scrutinized by media outlets (Kyriaki-\\ndou et al. 2021), sub-national politicians are more likely to\\ndisseminate harmful content with limited scrutiny (Mihai-\\nlidis et al. 2021). State legislators are responsible for laws\\nacross all policy areas within state jurisdiction, making their\\nrole crucial in the U.S. political system. Given the limited\\nmedia coverage of state legislators (Squire et al. 2019), these\\nsocial media platforms serve as important mediums for com-arXiv:2407.16014v1  [cs.SI]  22 Jul 2024municating their ideological and political positions to their\\nvoters.\\nWe study the dynamics of legislators‚Äô visibility, examin-\\ning the different factors that influenced the attention they re-\\nceived on Twitter and Facebook (FB) during the two-year\\nperiod spanning 2020-2021. We focus on these two years\\nowing to the surge of harmful content online due to signifi-\\ncant events such as the US Presidential elections, COVID-\\n19, Capitol Riots, and BLM protest movements (Ferrara\\net al. 2020; Cuan-Baltazar et al. 2020; Toraman et al. 2022).\\nStudying the visibility dynamics over a time period has cer-\\ntain challenges. Apart from individual attributes (e.g., post-\\ning frequency, party, demographics) or volume of harmful\\ncontent posted, the politician‚Äôs visibility may also vary by\\ntime (e.g., during elections) or due to the particular topics\\nthey post. We tackle these challenges in our study, our main\\ncontributions are as follows :\\n‚Ä¢Factors Associated with Visibility. We present a large-\\nscale, longitudinal study on political elites‚Äô online visibil-\\nity in the US by comparing differences in their platform\\nvisibility based on party, socio-demographic factors, and\\nposting activity (RQ1, RQ2; See Section 3) . Republi-\\ncans and men tend to have a higher level of visibility on\\nFB, while Democrats tend to have higher visibility on\\nTwitter. Posting uncivil content on Twitter and similarly\\nlow-credibility content on FB is also correlated with their\\nplatform visibility. Legislators‚Äô visibility on posting low-\\ncredibility content, however, varies by party on Twitter.\\nOur thorough analysis of legislators who post on both\\nplatforms reveals notable platform differences associated\\nwith their social media activities.\\n‚Ä¢Methodological Contribution. We conduct a causal in-\\nference study to examine how legislators‚Äô social media\\nposts affect their visibility, particularly when the content\\nis uncivil or less credible (RQ3; See Section 3) . To en-\\nsure that our findings are not influenced by potential con-\\nfounding factors, such as temporal and topical correla-\\ntions that are common in dynamic text data and can bias\\nthe results, we leverage deep learning of potential out-\\ncome and matching techniques. Our analytical method\\nhelps disambiguate the effect of posting activities.\\n‚Ä¢Impact of Harmful Content. The results, based on ob-\\nservational data using causal inference (RQ3; See Sec-\\ntion 3) , have revealed significant and novel patterns. Our\\nstudy found that posting uncivil content on Twitter led to\\na decrease in visibility. It was observed that Republicans\\nposting low-credibility content on both platforms have an\\nincreased visibility, while Democrats posting the same\\nhave lower visibility. The effect is more pronounced for\\nideologically extreme legislators in most cases. Overall,\\nour findings contribute to the understanding of politi-\\ncians‚Äô online visibility, shedding light on cross-platform\\ndifferences and partisan asymmetries.\\n2 Related Work\\nPolitical Elites‚Äô Online Behaviors. Social media are used\\nby politicians for both broadcasting as well as having dia-\\nlogue with voters. The effect of Twitter and Facebook useon election campaigns has been studied extensively (Kreiss\\n2016; Jungherr 2016; Boulianne and Larsson 2023; Sahly\\net al. 2019). Kreiss (2016) looked at how Twitter was used\\nby political party staffers to shape the perspectives of jour-\\nnalists and influence dedicated voters. V oters engaging in\\npolitical discussion online have demonstrated increased in-\\nterest and engagement in political affairs (Bode et al. 2016).\\nSocial media, thus, serves as a powerful tool for politicians\\nto influence public opinion and/or convey their stance re-\\ngarding several important issues. Despite a large body of\\nwork on political communication on social media, there is\\nno clear understanding of which factors influence the on-\\nline visibility of legislators, especially, how politicians post-\\ningharmful content is viewed by the audience. Our research\\naims to close this gap by examining the impact of uncivil\\nandlow-credibility content on legislators‚Äô visibility by per-\\nforming a cross-platform study‚Äîafter accounting for sev-\\neral confounding factors related to their personal attributes,\\ntemporal and topical variations.\\nMisinformation and Virality. There exists a large body\\nof literature characterizing the diffusion of low-credibility\\ncontent online (V osoughi et al. 2018; Friggeri et al. 2014;\\nZollo et al. 2015). V osoughi et al. (2018) found that false-\\nhoods spread significantly faster, and reached a broader au-\\ndience as compared to true news on Twitter. Friggeri et al.\\n(2014) found that rumor cascades on FB tend to penetrate\\ndeeper into the social network compared to general reshare\\ncascades. Prior works suggest that the sentiment towards\\nmisinformation is primarily negative which could be respon-\\nsible for the variations seen in the diffusion (V osoughi et al.\\n2018; Zollo et al. 2015).\\nA significant body of research has examined the impact of\\nmisinformation on the 2016 and 2020 US presidential elec-\\ntions (Bovet et al. 2019; Pennycook and Rand 2021). Prior\\nresearch has also shown that online misinformation tends to\\nbe directed more frequently toward conservative users (Rao\\net al. 2022; Yang et al. 2023) making them more likely to en-\\ngage in misinformation. Misinformation may have a higher\\nreach on social media platforms which could be leveraged\\nby politicians to gain visibility, and the extent may vary\\nacross ideologies. However, the question of how misinfor-\\nmation originating from public figures is reacted by audi-\\nences is less explored. In this work, we aim to illuminate\\nthe impact of posting low-credibility content on legislators‚Äô\\nvisibility.\\nThe Attention Economy and Toxic or Controversial Con-\\ntent. There is no clear consensus on how uncivil content\\nspreads on online platforms (Shmargad et al. 2022; Gervais\\n2015). Prior works have studied the nature of incivility in on-\\nline political communication suggesting that engaging in un-\\ncivil discourse may have certain benefits for politicians such\\nas political opinion polarization (Bodrunova and Blekanov\\n2021) or empowerment by voicing criticism against author-\\nities (Bodrunova et al. 2021). Uncivil content was found to\\nbe associated with emotionally loaded language which gen-\\nerated strong responses from the audiences (Mutz 2007). Ir-\\nrespective of the kind of response, this may lead to higher\\nvisibility. Coe et al. (2014) found that uncivil commentson news websites received more negative reactions. Thus,\\neven though engaging in uncivil discourse may have cer-\\ntain political benefits, it is unclear how that is perceived by\\naudiences‚Äîa gap that we address in this study.\\nConfounding with Textual Data. Causal inference with\\ntext data is particularly challenging since the assumptions of\\ncausal inference (positivity, conditional ignorability, consis-\\ntency) may not hold when confounding, treatment or out-\\ncomes are encoded in text (Feder et al. 2022). For instance,\\nposts on certain topics may be more likely to contain mis-\\ninformation and also receive higher engagement from the\\naudience. Prior works on extracting confounding from text\\nhave utilized unsupervised dimensionality reduction meth-\\nods (Roberts et al. 2020; Sridhar and Getoor 2019). Re-\\ncent works leverage neural networks to automatically ex-\\ntract features especially when the confounders in text are\\nnot explicitly known (Koch et al. 2021). To address this\\nproblem, some works have added transformer layers for text\\nprocessing to TARNet or Dragonnet (Veitch et al. 2020;\\nPryzant et al. 2020). We have extended the state-of-the-art\\ntechniques to address the confounding factors that are com-\\nmonly seen in dynamic social media content, such as textual\\nand temporal correlations due to similar topics, events, and\\npersonal attributes. To generate content representations, we\\nleverage contextual RoBERTa embeddings (Liu et al. 2019)\\nwith other post attributes. We then utilize a fine-tuned Drag-\\nonnet model to produce content embeddings that isolate the\\nconfounding factors.\\n3 Study Design\\nIt is crucial to understand how politicians develop influence\\nthrough online media and factors associated with the in-\\nfluence. This work focuses on state legislators specifically\\nsince they may be more likely than national-level politicians\\nto disseminate harmful content owing to limited scrutiny. We\\nask the following research questions (RQs):\\nRQ1 How does the legislators‚Äô visibility, as measured by the\\nattention they receive, vary based on party affiliation, and\\nindividual attributes such as gender, ethnicity, state rep-\\nresentation, and social media activity?\\nRQ2 What attributes of legislators and their posts are associ-\\nated with their visibility?\\nRQ3 How does low-credibility or incivility impact the visibil-\\nity of legislators‚Äô posts?\\nIn RQ1 we analyze whether the attention received by leg-\\nislators varies based on party affiliation, basic demograph-\\nics, and posting activity. RQ2 examines what characteristics\\nof legislators are most strongly correlated with their online\\nvisibility change. RQ1 and RQ2 characterize how visibility\\nvaries by legislators‚Äô attributes and provide an understand-\\ning of factors that may potentially impact their visibility dy-\\nnamics at the account level. To examine the impact of post-\\ning harmful content, RQ3 investigates how low-credibility\\nand incivility impacts the attention received by individual\\nposts. More specifically, we study whether low-credibility\\nor incivility increases or decreases a post‚Äôs visibility where\\nvisibility is measured in terms of expected interaction rate.Since the user attention on social media platforms may vary\\nby legislators‚Äô attributes (RQ1), and there may be other fac-\\ntors associated with the visibility (RQ2), in RQ3 we estimate\\nthe impact of incivility or low-credibility by controlling for\\nthese variables as well as temporal and topical variations.\\n3.1 Datasets\\nWe collect Twitter and FB posts from all US state represen-\\ntatives and senators who held office during 2020-2021 (i.e.,\\neach legislator has been in office at least for a certain time\\nbetween 2020 and 2021) . We focus on Twitter and FB due\\nto the vast amounts1of content produced by legislators on\\nthese platforms. Of the 8,028 legislators, 5,712 (64%) leg-\\nislators, comprising 2,943 (61%) Democrats, 2,740 (48.2%)\\nRepublicans, and 29 Independents had at least one Twitter\\naccount (Kim et al. 2022). For FB, 5,1472(64.1%) legis-\\nlators, comprising 2,215 (48.2%) Democrats, 2,515 (56.0%)\\nRepublicans, and 418 other party members, had either an of-\\nficial or a campaign account. We collect all their Twitter and\\nFB posts during 2020-21. It is important to note that many\\nof these accounts were either dormant or inaccessible during\\nour data collection period3.\\nTwitter. Our Twitter dataset4comprises around 4M tweets\\nposted by 3,551 (44.2%) US state legislators during 2020-\\n2021. The coverage of state legislators ranges from 29.4 to\\n96.5%, with a mean of 71.6%. This suggests that our dataset\\ncomprises a representative legislator population across\\nmost US states. We only analyze tweets by Democrats and\\nRepublicans due to the insufficient number of posts from\\nIndependent legislators (N=29). We calculate the interaction\\na tweet receives as the sum of likes, retweets, replies, and\\nquotes. Table 1 shows the number of legislators, posts, and\\nmedian5interactions on post. Democrats are more active\\nand receive higher engagement on posts as compared to\\nRepublicans. The comparatively higher volume of posts by\\nDemocrats indicates that Twitter is a more preferred com-\\nmunication platform for them compared to Republicans. We\\nalso construct the intra-legislator follower network, where a\\ndirected edge from legislator A‚ÜíBindicates Afollows B.\\nFB.We collect all FB posts6by US state legislators during\\n2020-21. This yields over 493K posts from 5,147 (64.1%)\\nlegislators. The coverage of state legislators ranges from\\n23.5 to 80.6%, with a mean of 57.7%. We similarly focus\\nonly on posts from Republicans and Democrats due to a\\nsmall number of posts from other party members. We use the\\ninteraction metric returned by the CrowdTangle API which\\nis the sum of all reactions (‚ÄòLikes‚Äô, ‚ÄòLove‚Äô, ‚ÄòWow‚Äô, ‚ÄòHaha‚Äô,\\n‚ÄòSad‚Äô, ‚ÄòAngry‚Äô, ‚ÄòCare‚Äô), comments, and shares for a post.\\nTable 1 shows the number of legislators, posts, and median\\n1https://www.pewresearch.org/internet/2020/07/16/congress-\\nsoars-to-new-heights-on-social-media/\\n2FB account information was crawled from Ballotpedia\\n3See Appendix for further details on data collection\\n4Collected using Twitter API v2 before March 2023\\n5The interactions received on posts have a skewed distribution,\\nso we report the median instead of mean.\\n6Collected using CrowdTangle APITable 1: Descriptive statistics for Twitter and FB datasets\\nshowing the number of legislators, posts, and median inter-\\nactions received per post by party.\\nTwitter FB\\nparty #users #tweets Int #users #posts Int\\nDem 1677 2.25M 8.0 2211 224K 58.0\\nRep 1412 889K 6.0 2501 218K 101.0\\ninteractions on post. The posting frequency is similar for\\nDemocrats and Republicans on FB unlike on Twitter. Inter-\\nestingly, Republicans receive almost double the interaction\\nas compared to Democrats, suggesting that Republicans may\\nhave a larger audience base and hence a larger reach on FB.\\nWe do not use the follower count for FB data since some\\nof the legislator accounts are official accounts while others\\nare Pages, due to which the follower counts across these dif-\\nferent account types vary widely. We are unable to analyze\\nFB‚Äôs network data due to the lack of access.\\n3.2 Individuals‚Äô attributes\\nWe characterize legislators based on their platform pres-\\nence and individual-level characteristics. For Twitter-based\\nattributes, we include their post count, follower count, and\\nin-degree centrality in the intra-legislator follower network.\\nThe post count serves as a proxy for measuring how ac-\\ntively the legislators use the platform and follower count\\nindicates the size of their audience base. Both post and fol-\\nlower counts are likely to have an impact on online visibil-\\nity (Hasan et al. 2022). Legislator‚Äôs position in their peer net-\\nwork may also impact their visibility (larger follower base,\\ngreater potential for content virality, or seniority). To gauge\\nthe influence of the legislators among their peers concern-\\ning the immediate connections, we leverage the in-degree\\ncentrality measure. For FB-based attributes, we include the\\npost count. The individual-level attributes include party af-\\nfiliation (Republican vs. Democratic), state, gender (Men\\nvs. Women), ethnicity (White vs. Non-White), and ideology\\nscores. We leverage the ideology scores constructed by Shor\\nand McCarty (2011). Around 99.7% (N=3074) and 70.2%7\\n(N=3306) of legislators are mapped8to their attributes on\\nTwitter and FB, and only those legislators with attributes are\\nanalyzed in our study . The final dataset comprises around\\n62.2% and 53.8% White, and 68.2% and 67.7% men on\\nTwitter and FB respectively, indicating that men and White\\nlegislators outnumber women and Non-Whites respectively.\\nThere are 2,131 (26.5%) overlapping users (OL) (i.e., legis-\\nlators having accounts on both Twitter and FB) across Twit-\\nter and FB. Figure 8 in Appendix shows the breakdown of\\nethnicity and gender by party and for OL. Republicans have\\nfewer women users compared to Democrats for both Twitter\\nand FB and both platforms have more Republican men. The\\nrepresentation of Non-White Republicans is higher on FB\\nthan on Twitter.\\n7See Data Collection section in Appendix\\n8Ethnicity and gender are mapped using Ballotpedia. Binary\\ngenders are used due to insufficient data about non-binary genders.4 Measuring Posts‚Äô Civility, Credibility, and\\nLegislators‚Äô Visibility\\n4.1 Assessing posts‚Äô civility\\nWe assess the civility of a post based on the toxicity of their\\nlanguage which is a common practice in literature (Frimer\\net al. 2023; Kim et al. 2021). Incivility in the context of\\npolitical speech is commonly associated with rudeness ac-\\ncording to the study by Stryker et al. (2016). We follow the\\ndefinition of toxic language provided by Google Perspec-\\ntive9:‚Äúrude, disrespectful, or unreasonable comment that\\nis likely to make someone leave discussion‚Äù . Based on this\\ndefinition, it would be reasonable to assume that the toxicity\\nclassifier is able to identify the markers of political incivil-\\nity. We determine the level of toxicity using the ‚Äúoriginal‚Äù\\nmodel10from Detoxify11(Hanu and Unitary team 2020). We\\nchoose the cutoff for toxicity score as 0.82 based on manual\\nannotation (see Appendix), i.e., posts having a score above\\n0.82 are considered uncivil12. This yields 24,242 (0.8%) and\\n277 (See Appendix) uncivil posts on Twitter and FB.\\nTable 2 shows the number of legislators posting uncivil\\ncontent, posts, and median interactions received, by party.\\nThis may be attributed to politicians‚Äô different communica-\\ntion styles across these platforms as observed in prior re-\\nsearch, suggesting FB is used more for broadcasting pur-\\nposes compared to Twitter which is leveraged more for hav-\\ning dialogue (Enli and Skogerb√∏ 2013). For our analysis on\\nuncivil content in the following sections, we only focus on\\nTwitter owing to the small number of uncivil posts on FB.\\nAround 47.1% of Democrats and 32.6% Republican legisla-\\ntors post uncivil content on Twitter. Democrats post almost\\ndouble the number of uncivil content compared to Republi-\\ncans (on average) on Twitter. This could be due to the higher\\ninteraction received on such posts by Democratic legisla-\\ntors. The interaction on uncivil content is higher compared\\nto the baseline interaction (in Table 1) for both parties, with\\na larger difference for Democrats.\\nFigure 1A shows the rate of uncivil posts across years by\\nparty and platform. More uncivil content was posted dur-\\ning 2020 on Twitter, with Democrats having a higher rate\\nof posting uncivil content. Figure 2A shows the rate of un-\\ncivil posts across states by party and by platform. Interest-\\ningly, we find that Republicans posted more uncivil content\\non FB compared to Democrats across almost all the states,\\nbut state-wise differences exist for Twitter.\\n4.2 Assessing posts‚Äô credibility\\nWe identify low-credibility content based on the credibility\\nof URL in the post, which is a common practice in liter-\\nature (Lasser et al. 2022). It is important to note that this\\n9https://perspectiveapi.com/how-it-works/\\n10The ‚Äúoriginal‚Äù model had the best performance when evalu-\\nated against our manual annotation labels compared to the other\\nDetoxify models, namely, ‚Äúdebiased‚Äù and ‚Äúmultilingual‚Äù.\\n11We choose Detoxify over Google Perspective API since\\nDetoxify has better or comparable performance (Arhin et al. 2021)\\nand is faster\\n12See Appendix for examples of uncivil postsTable 2: Number of uncivil posts, legislators posting, and\\nmedian interactions received per post on uncivil content on\\nTwitter and FB, by party.\\nTwitter FB\\nparty #users #tweets (%) Int #users #posts (%) Int\\nDem 782 18111 (0.9%) 14.0 55 89 (0.1%) 131\\nRep 461 6131 (0.7%) 7.0 54 188 (0.1%) 120\\nparty Rep DemFB Twitter\\n2020 2021 2020 20210.000.250.500.75\\nyearA. Incivility (%)FB Twitter\\n2020 2021 2020 20210.00.51.01.5\\nyearB. Low-Credibility (%)\\nFigure 1: Percentage of uncivil and low-credibility posts\\nacross years, by party, by platform. Republicans have a\\nhigher rate of posting low-credibility content on both plat-\\nforms and across years. The yearly posting rates of uncivil\\ncontent are comparable across parties.\\nmethod of identifying low-credibility posts does not dis-\\ncriminate between posts endorsing or debunking such in-\\nformation. In this study, we are interested in looking at the\\nvisibility of both, i.e., posts promoting or debunking low-\\ncredibility information since both types of posts are expos-\\ning the audience to harmful information. Prior research has\\nshown that exposure to misinformation has an impact on be-\\nlieving and subsequent sharing of such information (Halpern\\net al. 2019). Thus, irrespective of the author‚Äôs intent, the au-\\ndience may be susceptible to believing and/or sharing such\\nlow-credibility content. Moreover, the URL sharing patterns\\nare similar for Democrats and Republicans on both Twitter\\nand FB13, so it is unlikely that any biases are introduced in\\nthe study due to our method of using URLs to identify low-\\ncredibility posts. We use the low-fact URL domain refer-\\nences provided by Tai et al. (2023) to identify low-credibility\\nposts. Tai et al. (2023) refined Media Bias/Fact Check‚Äôs14\\n(MBFC) original ratings to include URLs that contain mis-\\nleading information and not just politically biased ones. This\\nrestrictive framing may undermine the true scale of misin-\\nformation on these platforms but it offers a higher preci-\\nsion (vs. recall) in identifying low-credibility content. This\\nyields 6,848 (0.2%) and 4,141 (1.0%) low-credibility posts\\non Twitter and FB respectively suggesting that legislators\\npost more low-credibility content on FB.\\nTable 3 shows the number of legislators posting low-\\n13Around 19.2% posts by Republicans and 17.4% by Democrats\\ncontain URLs on Twitter, and 14.6% and 18.3% posts by Republi-\\ncans and Democrats on FB contain URLs.\\n14https://mediabiasfactcheck.com/\\nFB Twitter\\n0 1 2 3 4 0.0 0.5 1.0 1.52.0WYWVWIWAVTVAUTTXTNSDSCRIPAOROKOHNYNVNMNJNHNENDNCMTMSMOMNMIMEMDMALAKYKSINILIDIAHIGAFLDECTCOCAAZARALAK\\n(%)A. IncivilityFB Twitter\\n0 5 10 0 1 2 3 4 5WYWVWIWAVTVAUTTXTNSDSCRIPAOROKOHNYNVNMNJNHNENDNCMTMSMOMNMIMEMDMALAKYKSINILIDIAHIGAFLDECTCOCAAZARALAK\\n(%)B. Low-CredibilityDem Rep partyFigure 2: Percentage of uncivil and low-credibility posts\\nfrom states, by party, by platform. Republicans have a\\nhigher rate of posting low-credibility content across all\\nstates, on both platforms. Moreover, Republican legislators\\nfrom most states have a higher rate of posting uncivil con-\\ntent on FB. State and party-wise differences exist for posting\\nrates of uncivil content on Twitter. Note that the x-axis scale\\nhas been adjusted to better visualize party differences.\\ncredibility content, posts, and median interactions received,\\nby party. Around 5.2% of Democrats and 36.7% Republi-\\ncan legislators post low-credibility on FB. On Twitter, only\\na handful of accounts are responsible for spreading low-\\ncredibility content across both parties (1.1% for Democrats\\nand 2.8% for Republicans). Republicans post more low-\\ncredibility content compared to Democrats on both plat-\\nforms. Similar to uncivil content, posts containing low-fact\\nURLs receive higher interaction except for Democrats on\\nTwitter. The median interaction for low-credibility content\\nis almost three times on Twitter and double on FB for Re-\\npublicans compared to their baseline interaction15.\\nFigure 1B shows the rate of low-credibility posts across\\nyears by party and platform. The prevalence was higher dur-\\ning 2021 on both platforms and it was mainly driven by Re-\\npublicans. Figure 2B shows the rate of low-credibility posts\\nacross states by party and by platform. Low-credibility con-\\ntent is driven by Republicans across all the states, with the\\nhighest rate from Arizona, on both Twitter and FB. The low-\\ncredibility information from Arizona is mostly related to the\\n15For Twitter, the median interaction on posts with and with-\\nout URLs are 9.0 and 7.0 respectively whereas for FB, the median\\ninteractions are 62.0 and 81.0, i.e., there is no clear pattern as to\\nwhether having a URL increases or decreases the interaction of\\nposts, suggesting that the results in Table 3 could potentially be\\ndue to the low-credibility of URLs.Table 3: Number of posts containing low-fact URLs, legis-\\nlators posting, and median interactions received per post on\\nlow-credibility content on Twitter and FB, by party.\\nTwitter FB\\nparty #users #tweets (%) Int #users #posts (%) Int\\nDem 19 188 (0.0%) 4.0 83 114 (0.1%) 78.5\\nRep 40 6660 (0.8%) 20.0 567 4027 (2.6%) 239.0\\n2020 US Presidential elections. In particular, we find that\\nsome Republican legislators frequently shared posts from\\nunreliable information outlets, especially during Arizona‚Äôs\\naudit of the 2020 election, which contributed to a significant\\namount of low-credibility posts from Arizona.\\nThe differences in interactions received by low-credibility\\nand uncivil posts may be attributed to the differences in post\\ntopics, timing, or attributes of authors‚Äîwe address this in\\nthe following sections.\\n4.3 Measuring legislator‚Äôs visibility\\nSocial media is being increasingly used as a tool by politi-\\ncians to enhance their visibility and outreach to the public\\n(Bahramirad 2022). The measure of a legislator‚Äôs visibility\\nis typically based on the interactions received on their posts.\\nOur approach to measuring visibility is inspired by the met-\\nrics used on Twitter and Facebook to calculate a post‚Äôs ex-\\npected engagement or virality potential (Twitter-team 2024;\\nCrowdtangle 2024). Our objective is to measure the overall\\noutreach of a post or legislator on the platform. Therefore,\\nwe consider all the interactions received on a post or by leg-\\nislators instead of focusing on a single type of engagement\\nsuch as ‚ÄúLikes.‚Äù It‚Äôs important to note that the visibility met-\\nrics are platform-specific and may not be comparable across\\nplatforms even if they share the same names. For instance,\\naudiences may engage with ‚ÄúLike‚Äù feature on Twitter dif-\\nferently than ‚ÄúLike‚Äù on Facebook due to different interface\\ndesigns (see details in Appendix). However, our visibility\\nmetrics are designed to capture the overall level of engage-\\nment a post or legislator receives on a specific platform.\\nMoreover, we do not distinguish between positive and\\nnegative reactions received from the audience (e.g., ‚Äòlove‚Äô\\nvs. ‚Äòangry‚Äô on FB). We are concerned with the visibility of\\nthe legislators and both positive and negative reactions con-\\ntribute to their overall outreach on the platform. As shown in\\nTables 2 and 3, the interactions received on low-credibility\\nand uncivil content are noticeably different from the over-\\nall interactions received by legislators, suggesting that audi-\\nences‚Äô reactions differ based on content (or other related fac-\\ntors). Thus, using interactions as a proxy to measure legisla-\\ntors‚Äô online visibility allows us to capture these differences\\nand get insights into factors contributing to their visibility.\\nThe visibility can be measured at both account and post\\nlevel. For post level, the visibility of ithpost by legislator u,\\nvuiis simply the interaction received on that post. Further-\\nmore, we examine how visibility changes in relation to other\\nvariables. Instead of measuring aggregated interactions over\\nthe audience of their posts, we measure the legislator‚Äôs vis-\\nibility ( V) by interaction rate, i.e., the number of interac-tions per post or audience size. We consider three different\\nways to adjust the quantity of the interaction rate, since plat-\\nform reach tends to be correlated with the number of follow-\\ners (Hasan et al. 2022), resulting in the following dependent\\nvariables (DVs): (1) interaction normalized by post count\\n(VIP), (2) interaction normalized by follower count ( VIF),\\nand (3) interaction normalized by follower and post count\\n(VIP,F). Therefore, the visibility of legislator u,Vu, is given\\nby aggregating the visibility of all u‚Äôs posts normalized by\\npost and/or follower count.\\n5 Methods\\nIn this section, we describe the methods used to answer our\\nRQs. In addition to the legislator behavior on Twitter and\\nFB, we analyze the overlapping legislators (OL) to under-\\nstand whether the differences observed across these plat-\\nforms are due to different legislator populations or different\\naudience/platform characteristics on Twitter and FB.\\n5.1 Analyzing Legislators‚Äô Visibility\\nIn RQ1, we analyze whether legislators‚Äô visibility varies\\nbased on demographics, party, and posting activity. For at-\\ntention disparity based on posting activity, we compare visi-\\nbility of legislators having above the median number of posts\\nwith those posting less than or equal to median16. Using\\nMann-Whitney U test, we find differences in the platform\\nvisibility of legislators based on party, gender, ethnicity, and\\nposting frequency. We incorporate additional DV variants,\\nnamely, 25 th, 50th, and 75 thpercentile of the legislator‚Äôs\\npost interactions together with the mean interaction, i.e.,\\nVIPto ensure robustness of our results. For states-wise dif-\\nferences, we visualize the mean visibility across states.\\nFor RQ2, we study the factors significantly correlated\\nwith the platform visibility of legislators. In addition to the\\nindividuals‚Äô attributes (Section 3.2), we measure the associ-\\nation between low-credibility and uncivil post volumes and\\ntheir visibility. Since low-credibility content is targeted more\\ntowards conservative users (Rao et al. 2022; Yang et al.\\n2023), we also consider the interaction between party and\\nlow-credibility content posted. Additionally, the legislator‚Äôs\\nvisibility may be influenced by the visibility of their peers\\nif their peers (re)post similar content often. To measure this\\nnetwork visibility, we use the median of the visibilities of\\nlegislator‚Äôs in-degree neighbors in the intra-legislator fol-\\nlower network. Unfortunately, we are unable to account for\\nthe network effects on FB. We use the following model,\\nVu=Œ≤0+Œ≤PParty u+Œ≤GGender u+Œ≤EEthnicity u+\\nŒ≤NPosts u+Œ≤FFollowers u+Œ≤CCentrality u+\\nŒ≤SState u+Œ≤TUncivil u+Œ≤MLowCredible u+\\nŒ≤NetNVu+Œ≤eParty u‚àóLowCredible u\\n(1)\\nwhere Vuis the visibility of legislator u,Uncivil u\\nand LowCredible u are the count of uncivil and\\n16We chose median as the threshold due to the skewed posting\\nactivity distribution.low-credibility posts, Party u, Gender u, Ethnicity uand\\nState uare dummy variables, Posts uis the post count,\\nFollowers uis the follower count17(Twitter specific),\\nCentrality uis the indegree centrality in intra-legislator fol-\\nlower network (Twitter specific), and NVuis the network\\nvisibility. To address the correlated errors across and within\\nstates, we incorporate a random effect on the state variable.\\nThe effect of each factor is estimated using a linear mixed ef-\\nfects regression model18with standardized continuous vari-\\nables. To ensure robust results for the analysis of RQ1 and\\nRQ2, we conduct separate analyses for the years 2020 and\\n2021, to determine if the visibility trends observed were con-\\nsistent across both years19. Moreover, we analyze the topi-\\ncal20distributions (e.g., COVID-19, BLM, elections) for our\\ndataset and find that our data is not skewed towards any par-\\nticular topic, suggesting limited bias due to specific topic(s).\\n5.2 Analyzing the Impact of Low-credibility or\\nUncivil Content\\nMeasuring outcome. Posting low-credibility and uncivil\\ncontent may have an impact on how politicians are perceived\\nonline. In particular, we analyze whether the presence of\\nincivility or low-fact URLs increases/decreases their posts‚Äô\\nvisibility. To characterize the change in visibility, we ana-\\nlyze the engagement on a post considering the post author‚Äôs\\nexpected visibility. Our metric is inspired by the overper-\\nforming metric used at CrowdTangle (Crowdtangle 2024).\\nThe overperforming score for post iis calculated as follows,\\nOuip=vuip\\nbup+thres p(2)\\nwhere buis the median interaction for legislator u‚Äôs posts\\non the platform in previous w-days and a threshold ( thres )\\nfor the minimum number of interactions on a post to be con-\\nsidered as overperforming, with pdenoting platform. The\\ntermbupis used to adjust the outcome with respect to the\\nlevel of expected interactions with the post authors on plat-\\nform p. We choose the thres = 10 for Twitter (i.e., a post\\nmust have at least 10 interactions to be considered overper-\\nforming on Twitter) and 100for FB. We estimate the ideal\\nwindow w, based on legislators‚Äô daily posting rates on these\\nplatforms (see Appendix for thres ,westimation). To get\\na reliable estimate of the overperforming metric, we choose\\nw= 14 day rolling window. This allows us to calculate the\\noverperforming score over a sufficient number of posts per\\nlegislator, while also accounting for the temporal variation.\\nFigure 3 shows the post overperforming scores on Twit-\\nter and FB21. The distribution of overperforming scores are\\n17Only included for DV not normalized by follower count\\n18To satisfy assumptions of linear regression, all variables are\\nsuitably transformed to be close to normal distributions (See Ap-\\npendix). Ideology score is dropped since it is correlated with party.\\n19Our findings revealed that the trends were similar for both\\nyears. Therefore, we have reported the overall results for the en-\\ntire study period.\\n20Identified using keywords.\\n21Our scores for FB posts are highly correlated (Spearman\\n102\\n1011040.000.250.500.751.00A. T witter\\n101\\n1011030.000.250.500.751.00B. FBproportionRep\\nDemFigure 3: ECDF plots of overperforming score distribu-\\ntions by party, by platform. Distributions are similar for\\nTwitter but posts by Republicans overperform more on FB.\\nsimilar for Republicans and Democrats on Twitter. On FB,\\nposts by Republican legislators overperform more compared\\nto posts by Democrats. This suggests that posts by Repub-\\nlican legislators have a higher tendency to be viral on FB\\ncompared to Democrats. For estimating the causal impact,\\nwe are interested in analyzing whether a post overperforms\\nor not due to its incivility or low-credibility, hence we bina-\\nrize the overperforming score ( outcome ) at cutoff 1, i.e., a\\npost is overperforming if it has a score >1.\\nEstimating causal impact. It is necessary to control for\\ntheconfounders that affect both the treatment andoutcome\\nvariables to differentiate between spurious correlation and\\ncausation . One of the possible confounders is the topic‚Äî\\nposts on certain topics (e.g., COVID-19) may be more likely\\nto contain misinformation, and these topics may get higher\\nvisibility. Another possible confounder could be the tone‚Äî\\nposts having certain tones (e.g., assertive) may be more\\nlikely to contain uncivil language, and also more likely to\\ngenerate stronger responses from the audience. Apart from\\ntextual content, other confounding variables can include leg-\\nislators‚Äô personal traits and the timing of their posts (e.g.,\\nduring elections there might be a rise in harmful content as\\nwell as an increase in legislators‚Äô visibility).\\nWe control for the individual‚Äôs attributes mentioned in\\nSection 3.2 (excluding ideological scores), post content, and\\ntime, i.e., count of days since the content was posted, start-\\ning from 2020-01-01. To control for the content, we leverage\\nembeddings generated by the pre-trained RoBERTa model.\\nFor low-credibility content, we also include the URL head-\\nlines along with post content because we assume that both\\nthe text and news headlines are visible to the viewers. The\\ntextual embeddings22are concatenated with individual‚Äôs at-\\ntributes, and time variable to get the final embedding of each\\npost. The confounders we choose to control for are based on\\nprior literature (Hasan et al. 2022; Sahly et al. 2019) and\\ntheir feasibility of being measured. Apart from these con-\\nfounders, there could also be certain other confounders (e.g.,\\nexternal events, algorithmic promotion effects, effect of ads)\\nwhich we are unable to measure and thus account for in this\\nrho=0.997, p < 0.001) with the overperforming score returned\\nby the CrowdTangle API, suggesting that our method successfully\\nidentifies posts that are overperforming.\\n22Only posts having a minimum of 10 words are considered for\\nthis part of the analysis. This accounts for 5,405 (78.9%) low-\\ncredibility and 15,883 (65.5%) uncivil posts on Twitter, and 2,427\\n(58.6%) low-credibility posts on FB.study. Our causal effect estimation has two steps: potential\\noutcome prediction and matching.\\nPotential outcome prediction . The confounding in our case\\nis time-varying and encoded in complex textual data, so we\\nleverage the non-parametric nature of neural networks to\\nlearn deconfounded, low-dimensional representations of the\\nhigh-dimensional data (Koch et al. 2021). We leverage the\\nDragonnet23model proposed by Shi et al. (2019) which uses\\nfeed-forward neural networks to learn balanced24represen-\\ntations of the data such that each head models a separate po-\\ntential outcome, a third propensity head predicts the propen-\\nsity (œÄ) of being treated and a free nudge parameter œµ(see\\nAppendix for model description).\\nWe fine-tune the Dragonnet model25by adding more hid-\\nden layers and using cross-entropy loss. The treatment vari-\\nables in our case are low-credibility and incivility respec-\\ntively. We use a 5-fold cross-validation setting for training,\\nwith a 1:1 ratio of treated vs. non-treated random samples\\n(see Appendix for model performance). For low-credibility\\nposts, we select corresponding non-treated posts that con-\\ntain at least one URL and similarly include the URL head-\\nlines to minimize the confounding from text (e.g., presence\\nof URL). Figure 4 shows the effectiveness of our decon-\\nfounding for incivility on Twitter.\\nA. Before Deconfounding\\nRep\\nDem\\nuncivil\\ncivil\\nB. After Deconfounding\\nRep\\nDem\\nuncivil\\ncivil\\nFigure 4: Effectiveness of deconfounding for uncivil vs.\\ncivil tweets. (A) shows the T-SNE space of content embed-\\ndings for civil vs. uncivil tweets by party. (B) shows the\\nrepresentation of the deconfounded embeddings returned by\\nDragonnet. After deconfounding, the representation space\\nfor treated and control covariate distributions ( party in this\\nexample) can not be distinguished.\\nMatching. For Conditional Average Treatment Effect\\n(CATE) estimation with Dragonnet predictions, we find\\nthat the covariates are not balanced after propensity score\\nreweighting. To improve balance, we further use matching.\\nWe match the treated and untreated subjects based on the\\ndeconfounded Dragonnet embeddings (see Appendix). The\\ncovariate balance for matching is shown in Figure 5. All the\\ncovariates are balanced for Twitter and FB low-credibility\\n23Dragonnet is chosen over other deep learning models for\\ncausal inference (S-learner, T-learner, TARNet) due to its ‚ÄúTar-\\ngeted Regularization‚Äù procedure which allows for statistical guar-\\nantees (Koch et al. 2021)\\n24Balancing is a treatment adjustment strategy that forces the\\ntreated and non-treated covariate distributions closer to deconfound\\nthe treatment from outcome (Johansson et al. 2016)\\n25Models trained on a single NVIDIA A100 40GB PCIe GPU\\n#follower#postscentralitydaysethnicitygenderpartystate\\n0.0 0.2 0.4\\nStandardized DifferencesTwitter uncivil A\\n#follower#postscentralitydaysethnicitygenderpartystate\\n0.0 0.5 1.0 1.5 2.0\\nStandardized DifferencesTwitter non-credible B\\n#postsdaysethnicitygenderpartystate\\n0.0 0.5 1.0 1.5\\nStandardized DifferencesFB non-credible C\\n before after matchingFigure 5: Covariate balance after matching. All covari-\\nates, except for party, and state in Twitter uncivil model, are\\nbalanced (i.e., absolute standardized difference <0.1) after\\nmatching on deconfounded embeddings.\\nmodels. For Twitter incivility, all the covariates except for\\nstate and party are balanced after matching. The final CATE\\nis calculated based on the matched samples as follows,\\nCATE =1\\nN‚Ä≤N‚Ä≤X\\n(Yi(1)‚àíYi‚Ä≤(0)) (3)\\nwhere Y(T)is the outcome for treatment TandN‚Ä≤is\\nthe number of matched samples. We estimate the CATE\\nfor Democrats and Republicans separately to study potential\\nasymmetries in receptivity across their audiences. Further-\\nmore, we look at the CATE for ideologically extreme leg-\\nislators to understand whether audiences engage differently\\nwith legislators at the extreme. We consider legislators hav-\\ning top 25% conservative and top 25% liberal ideological\\nscores as Extreme Republicans and Extreme Democrats.\\nFurthermore, our analysis ensures that outliers, a common\\noccurrence in social networks, do not significantly impact\\nour results. (See the Appendix for more details.)\\n6 Results\\n6.1 RQ1: Legislators‚Äô Visibility by Party, Gender,\\nEthnicity, Posting Frequency, State\\nTable 4 shows the effect sizes for the Mann-Whitney U test.\\nWe only report the results for VIPhere, the results for 25 th,\\n50th, and 75 thpercentile are added in the Appendix along\\nwith 95% CI for Table 4. Overall, the visibility of legisla-\\ntors differs significantly based on party, gender, and post-\\ning frequency on both platforms and also for ethnicity on\\nTwitter. Interestingly, Democrats and women appear to have\\nhigher visibility on Twitter but Republicans and men have\\nhigher visibility on FB ( p <0.001). White legislators also\\nreceive more attention on Twitter. On both platforms, leg-\\nislators with higher posting activity have higher visibility.\\nFigure 6 shows the mean VIPacross US states for Twitter\\nand FB. the visibility of legislators also differs based on their\\nstate representation. The most visible state on Twitter is New\\nMexico and Mississippi on FB. The posting rate of legisla-\\ntors is the second highest for New Mexico on Twitter which\\ncould be a possible explanation for the high visibility26.\\n26For instance on FB, Mississippi Republican senator Chris Mc-\\nDaniel has a remarkably high engagement which dominates the vis-Table 4: RQ1 Effect sizes\\nOverlapping (OL)\\nIVs Twitter FB Twitter FB\\nParty 0.239‚àó‚àó‚àó-0.299‚àó‚àó‚àó0.189‚àó‚àó‚àó-0.347‚àó‚àó‚àó\\n(Rep vs. Dem)\\nGender 0.076‚àó‚àó‚àó-0.089‚àó‚àó‚àó0.009 -0.128‚àó‚àó‚àó\\n(Men vs. Women)\\nEthnicity -0.067‚àó‚àó-0.031 -0.023 -0.062\\n(White vs. Non-White)\\nPosting Freq. 0.457‚àó‚àó‚àó0.435‚àó‚àó‚àó0.368‚àó‚àó‚àó0.418‚àó‚àó‚àó\\n(‚â§vs.>median)\\n.p <0.1,‚àóp <0.05,‚àó‚àóp <0.01,‚àó‚àó‚àóp <0.001\\nWe also look at the visibility of overlapping users (OL)\\non these platforms. Similar to prior results, Republicans and\\nmen receive higher engagement on FB and Democrats on\\nTwitter. However, we do not find any significant difference\\nacross gender and ethnicity on Twitter for OL. Our results\\nsuggest that there exists cross-platform differences in how\\naudiences engage with political content.\\n0.00 0.25 0.50 0.75 1.00Visibility (Twitter)\\n0.00 0.25 0.50 0.75 1.00Visibility (FB)\\nFigure 6: Mean visibility of legislators from states. The\\nvisibility ( VIP) (normalized between 0-1) of legislators dif-\\nfer based on their state representation and platform.\\n6.2 RQ2: Factors Related to Visibility\\nIn RQ2 we look at factors related to legislators‚Äô platform\\nvisibility. The results for all DVs are similar but we only\\nreport results for VIPin Table 5 (See Appendix for 95%\\nCI). We are unable to estimate the interaction effect be-\\ntween party and low-credibility posts for FB due to insuf-\\nficient low-credibility posts from Democrats. Party, gender,\\nand post frequency are significantly correlated with visibil-\\nity after controlling for other variables on both platforms.\\nRepublicans and men are more likely to garner higher visi-\\nbility on FB whereas the opposite is true for Twitter. Higher\\nposting activity is also related to higher visibility on both\\nplatforms. On Twitter, the number of followers and central-\\nity in the intra-legislator network are not correlated with the\\nlegislators‚Äô visibility. Interestingly, there is a positive rela-\\ntion between legislators‚Äô network visibility and visibility.\\nAs shown in Table 5, the volume of low-credibility posts\\nis positively associated with legislators‚Äô visibility on FB\\nibility term for the state.Table 5: RQ2 Regression Results\\nOverlapping (OL)\\nIVs Twitter FB Twitter FB\\nParty [Rep] -0.133‚àó‚àó0.423‚àó‚àó‚àó-0.142‚àó0.512‚àó‚àó‚àó\\nGender [Men] -0.078‚àó0.090‚àó‚àó-0.011 0.113‚àó‚àó\\nEthnicity [White] 0.020 0.015 0.020 0.040\\n#posts 0.401‚àó‚àó‚àó0.477‚àó‚àó‚àó0.409‚àó‚àó‚àó0.499‚àó‚àó‚àó\\n#followers -0.028 - -0.028 -\\nCentrality -0.020 - -0.029 -\\nNetwork visibility 0.040‚àó0.027 -\\n#Low-Credible -0.268‚àó‚àó0.079‚àó‚àó‚àó-0.335‚àó-0.037‚àó\\n#Uncivil 0.148‚àó‚àó‚àó- 0.152‚àó‚àó‚àó-\\nParty [Rep] x\\n#Low-Credible 0.299‚àó‚àó- 0.351‚àó‚àó-\\nR20.289 0.382 0.291 0.386\\n.p <0.1,‚àóp <0.05,‚àó‚àóp <0.01,‚àó‚àó‚àóp <0.001\\n(p <0.001) but has an opposite effect ( p <0.01) on Twitter.\\nHowever, visibility is positively correlated with the interac-\\ntion between party and low-credibility posts on Twitter, i.e.,\\none standard deviation increase in low-credibility posts by\\nRepublicans is associated with a 0.299 standard deviation\\nincrease in visibility . Thus, posting low-credibility content\\nis related to higher visibility for only Republicans, otherwise\\nit has a negative association on Twitter. Posting more uncivil\\ncontent also increases the visibility of legislators on Twitter\\n(p <0.001). These results suggest that posting harmful con-\\ntent is associated with legislators‚Äô platform visibility.\\nWe find similar results for OL, i.e., men and Republicans\\nrelate to higher visibility on FB, and Democrats on Twit-\\nter, again suggesting the cross-platform differences. Posting\\nuncivil content on Twitter is positively associated with in-\\ncreased visibility, while posting low-credibility content is\\nnegatively associated with it. Moreover, visibility is posi-\\ntively associated with the interaction term between party and\\nlow-credibility posts. Interestingly however, posting low-\\ncredibility content is related to decreased visibility for the\\nOL on FB similar to Twitter. Therefore, legislators who\\npost content on both platforms have different communica-\\ntion strategies in comparison to the general legislator popu-\\nlations on those platforms which may be attributed to audi-\\nence preferences across platforms.\\nThus, posting harmful content is related to the visibility of\\nthe legislators. But the observed correlation may be a spuri-\\nousone due to confounders, such as content in similar top-\\nics. Next, we analyze whether incivility or low-credibility of\\na post impacts its visibility.\\n6.3 RQ3: Observed Causal Impact of Incivility\\nand Low-credibility on Visibility\\nFigure 7 shows the CATE estimates and 95% CI after match-\\ning. CATE is the expected change in the overperformance\\nof a post if it contains low-credibility or incivility. For\\nFB, we find that Republican legislators receive higher atten-\\ntion on posting low-credibility content. Similar results hold\\nwhen we look at Extreme and OL Republicans. Interest-FB non-credible Twitter non-credible Twitter uncivil\\n-0.50 -0.25 0.00 0.25 0.50 -0.50 -0.25 0.00 0.25 0.50 -0.50 -0.25 0.00 0.25 0.50Extreme DemDemDem (OL)Extreme RepRepRep (OL)\\nCATEFigure 7: Observed causal impact of low-credibility and\\nincivility on legislators‚Äô content visibility. After control-\\nling for confounders, low-credibility has a positive impact\\non content visibility for Republicans on both platforms, but a\\nnegative effect for Democrats on FB. Incivility significantly\\nhinders content visibility for all subgroups on Twitter.\\ningly, the visibility of Democrats decreases when they post\\nlow-credibility content on FB. There are no effects for Ex-\\ntreme and OL Democrat subgroups.\\nFor Twitter, similar to FB, Republicans, including their\\nOL and Extreme subgroups receive higher visibility on post-\\ning low-credibility content. The effect size is higher for Ex-\\ntreme Republicans, i.e., the more conservative a legislator\\nis the higher attention they receive on posting misinforma-\\ntion. For Democrats, however, we do not find any significant\\neffects. The difference in outcomes for Democrats across\\nTwitter and FB could either be due to their distinct behav-\\niors (e.g., content) and/or audience preferences across these\\nplatforms. The average number of posts containing low-\\ncredibility content by Democratic legislators is higher on\\nTwitter than on FB as shown in Table 3. This may suggest\\nthat Democrats post less low-credibility content on FB since\\ntheir visibility is penalized and/or vice-versa.\\nThe cross-platform analysis results suggest that there are\\nasymmetries in how the Republican and Democratic party\\naudiences engage with low-credibility content online and\\nthese asymmetries hold across platforms within and/or be-\\ntween parties. The former are more likely to engage with\\nmisinformation as shown by our results.\\nFor uncivil content on Twitter, we find that both\\nDemocrats and Republicans, including their subgroups, re-\\nceive lower engagement on posts containing uncivil lan-\\nguage. The negative effects are higher for Democratic legis-\\nlators compared to Republicans. The effects are also higher\\nfor Extreme legislators from both parties when compared to\\ntheir party baselines. This implies that audiences irrespective\\nof their partisan preferences engage less with uncivil content\\nposted by legislators on Twitter and the legislators towards\\nthe extreme political spectra are penalized more.\\n7 Discussion\\nWe analyzed the factors influencing the visibility dynam-\\nics of US state legislators by conducting a cross-platform\\nanalysis across Twitter and FB to understand different audi-\\nence behaviors across these platforms. We showed that leg-\\nislators‚Äô visibility varies based on their demographics, party,and posting frequency. Democrats have higher visibility on\\nTwitter whereas Republicans have higher visibility on FB.\\nMoreover, the regression analysis showed that a strong cor-\\nrelation exists between party and visibility, i.e., Democrats\\nare associated with higher engagement on Twitter and Re-\\npublicans on FB. These results also hold for the overlapping\\nlegislators, suggesting that audiences across these platforms\\nengage with political content differently. Posting harmful\\ncontent is also associated with legislators‚Äô visibility. Low-\\ncredibility content is related to increased visibility on FB,\\nbut decreased visibility on Twitter. Taking the effect of party\\ninto account, low-credibility posting correlates with higher\\nvisibility for Republicans. Uncivil posting is also correlated\\nwith higher account-level visibility on Twitter.\\nWe further analyzed whether the increased visibility is\\ndue to posting harmful content after controlling for con-\\nfounding factors such as demographics, party, topics, and\\ntime. Low-credibility garners more attention for Repub-\\nlicans on both platforms. However for Democrats, low-\\ncredibility reduces their content visibility on Twitter. These\\nresults highlight the partisan asymmetries in how low-\\ncredibility content receives attention online. Existing works\\nhave shown population asymmetries (Rao et al. 2022); our\\nstudy further reveals attention disparity due to political ac-\\ntors‚Äô party affiliation. Higher online visibility provides a\\ngreater opportunity to influence public opinion (e.g., by\\ngaining followers, impacting ranking algorithms). Politi-\\ncians may post higher low-credibility content for political\\ngains which may have implications for platform moderation.\\nUnlike low-credibility, incivility decreases the visibility\\nof legislators‚Äô posts on Twitter for both Republicans and\\nDemocrats. The negative effects are more pronounced for\\nDemocrats compared to Republicans as well as for Ex-\\ntreme legislators, suggesting that audiences prefer to engage\\nless with uncivil content irrespective of partisan preferences.\\nPrior research has shown that uncivil content receives more\\nnegative reactions (Coe et al. 2014) owing to its emotionally\\ncharged language. The lower visibility may be attributed to\\nthe lack of expressing negative sentiments on Twitter, but\\nfurther research is needed to confirm this. Our results high-\\nlight the cross-platform differences as well as asymmetries\\nin how Democratic and Republican party audiences engage\\nwith political content online which is aligned with previous\\nliterature (Kelm 2020; Sahly et al. 2019) .\\nWe show that harmful content is associated with legisla-\\ntors‚Äô online visibility. Such observed associations may be\\nspurious, and other factors may contribute to their visibil-\\nity. For instance, posting uncivil content has a positive as-\\nsociation with visibility on Twitter but incivility has a neg-\\native impact on content visibility after controlling for con-\\nfounders. Other factors may include the topics of their posts\\nor simply the post timing. External factors (e.g., offline cam-\\npaigns, media presence) can also contribute to their online\\nvisibility which is out of scope for this study. Moreover,\\nthere may be spillover effects from legislators‚Äô network vis-\\nibility as hinted in our RQ2 results. Nevertheless, this study\\nsheds light on some of the factors influencing legislators‚Äô\\noverall as well as content visibility, but more research is\\nneeded to fully understand their online visibility dynamics.Limitations and Future Work. Our study has certain\\nlimitations. We only look at the years 2020 and 2021‚Äî\\nthe generalizability to other periods remains uncertain. Our\\nmethod of identifying low-credibility content was conserva-\\ntive which could have led to certain biases in our sampling.\\nWe demonstrated the feasibility of addressing the represen-\\ntation biases in Section 5; however, it is uncertain whether\\nwe were able to fully correct for them. Furthermore, we only\\nidentified uncivil and low-credibility posts based on the tex-\\ntual content but do not consider other forms of content (e.g.,\\nimages) which may also contain harmful information.\\nWe only looked at the visibility based on total interac-\\ntions received on posts without discriminating between pos-\\nitive and negative reactions. Future work can study the im-\\npact of harmful content on positive and negative visibility\\nseparately to get a more nuanced understanding of public\\nreceptivity. It would also be interesting to look at the impact\\nof posting harmful content on different types of engagement\\n(e.g., Likes vs. Retweets). Another possible extension could\\nbe adapting our causal study design for continuous treatment\\n(e.g., how visibility is affected by the degree of incivility).\\n8 Ethical Considerations\\nData. We collect data from two sources, Twitter and FB.\\nFor Twitter, the data was collected using Twitter‚Äôs Official\\nAPI v2.0 before rate limitations were imposed (i.e., March\\n2023). For FB, we collect data using CrowdTangle‚Äôs official\\nAPI by following their terms of service. All the data are pub-\\nlicly posted and available for viewing without restrictions.\\nWe ensure that the Dragonnet model can effectively de-\\nconfound the covariate representation space for treated and\\nnon-treated samples based on our qualitative analysis and\\nperformance metrics. The classification errors from Drag-\\nonnet model are less likely to affect our results since we do\\nnot use the model predictions to calculate CATE. However,\\nmisclassification may still impact the deconfounding qual-\\nity, resulting in confounding from textual content that we\\ncannot measure, unlike other covariates. Our study suggests\\nthat public figures sharing harmful content on social media\\nhas significant consequences. We showed that when low-\\ncredibility content is posted by public figures, the combi-\\nnation of user behavior (interacting with the posts) and plat-\\nform mechanisms (e.g., feed recommendation algorithms)\\ncan result in increased visibility for such content. Our find-\\nings should not be interpreted as an encouragement to spread\\nsuch low-credibility content; instead, they should serve as a\\nwarning that there may be incentives for political or elite ac-\\ntors to do so. Moreover, our study has focused on the behav-\\nior of US subnational politicians on two specific platforms‚Äî\\nTwitter and FB. The results may not be generalizable to\\nother platforms and social media users including the ac-\\ntivities of political opinion leaders and media elites from\\nother countries, or even US national politicians, due to sev-\\neral factors‚Äîmedia scrutiny, platform moderation rules, and\\npublic perception to name some. More research is needed to\\nunderstand whether our results generalize to other settings.Acknowledgement\\nThe authors would like to acknowledge support from\\nAFOSR, ONR, Minerva, NSF #2318461, and Pitt Cyber In-\\nstitute‚Äôs PCAG awards. The research was partly supported\\nby Pitt‚Äôs CRC resources (RRID:SCR 022735 through NIH\\n#S10OD028483). Any opinions, findings, and conclusions\\nor recommendations expressed in this material do not nec-\\nessarily reflect the views of the funding sources.\\nReferences\\nArhin, K.; et al. 2021. Ground-Truth, Whose Truth?‚Äì\\nExamining the Challenges with Annotating Toxic Text\\nDatasets. arXiv preprint arXiv:2112.03529 .\\nBahramirad, S. 2022. Virtual forums for public accountabil-\\nity: How internet and communication technologies are influ-\\nencing citizen interactions with a local government. CJAS ,\\n39(3): 313‚Äì327.\\nBennett, W. L.; et al. 2018. The disinformation order: Dis-\\nruptive communication and the decline of democratic insti-\\ntutions. European journal of communication , 33(2).\\nBode, L.; et al. 2016. Politics in 140 characters or less:\\nCampaign communication, network interaction, and politi-\\ncal participation on Twitter. Journal of Political Marketing ,\\n15(4): 311‚Äì332.\\nBodrunova, S. S.; and Blekanov, I. S. 2021. A self-critical\\npublic: Cumulation of opinion on Belarusian oppositional\\nYouTube before the 2020 protests. Social Media+ Society ,\\n7(4): 20563051211063464.\\nBodrunova, S. S.; et al. 2021. Constructive aggression? Mul-\\ntiple roles of aggressive content in political discourse on\\nRussian YouTube. Media and Communication , 9: 181‚Äì194.\\nBoulianne, S.; and Larsson, A. O. 2023. Engagement with\\ncandidate posts on Twitter, Instagram, and Facebook during\\nthe 2019 election. New Media & Society , 25(1): 119‚Äì140.\\nBovet, A.; et al. 2019. Influence of fake news in Twitter\\nduring the 2016 US presidential election. Nature communi-\\ncations , 10(1): 7.\\nCoe, K.; et al. 2014. Online and uncivil? Patterns and deter-\\nminants of incivility in newspaper website comments. Jour-\\nnal of communication , 64(4): 658‚Äì679.\\nCrowdtangle. 2024. How do you calculate overperforming\\nscores? https://help.crowdtangle.com/en/articles/2013937-\\nhow-do-you-calculate-overperforming-scores.\\nCuan-Baltazar, J. Y .; et al. 2020. Misinformation of COVID-\\n19 on the internet: infodemiology study. JMIR public health\\nand surveillance , 6(2): e18444.\\nEberl, J.-M.; et al. 2020. What‚Äôs in a post? How sentiment\\nand issue salience affect users‚Äô emotional reactions on Face-\\nbook. Journal of Information Technology & Politics , 17(1).\\nEnli, G. S.; and Skogerb√∏, E. 2013. Personalized campaigns\\nin party-centred politics: Twitter and Facebook as arenas for\\npolitical communication. Information, communication & so-\\nciety , 16(5): 757‚Äì774.\\nFeder, A.; et al. 2022. Causal inference in natural lan-\\nguage processing: Estimation, prediction, interpretation and\\nbeyond. Transactions of the ACL , 10: 1138‚Äì1158.\\nFerrara, E.; et al. 2020. Characterizing social media manip-\\nulation in the 2020 US presidential election. First Monday .Friggeri, A.; et al. 2014. Rumor cascades. In ICWSM , vol-\\nume 8, 101‚Äì110.\\nFrimer, J. A.; et al. 2023. Incivility is rising among Ameri-\\ncan politicians on Twitter. SPPS , 14(2): 259‚Äì269.\\nGervais, B. T. 2015. Incivility online: Affective and be-\\nhavioral reactions to uncivil political posts in a web-based\\nexperiment. Journal of Information Technology & Politics ,\\n12(2): 167‚Äì185.\\nGoovaerts, I.; et al. 2020. Uncivil communication and sim-\\nplistic argumentation: Decreasing political trust, increasing\\npersuasive power? Political Communication , 37(6).\\nGrant, W. J.; et al. 2010. Digital dialogue? Australian politi-\\ncians‚Äô use of the social network tool Twitter. Australian jour-\\nnal of political science , 45(4): 579‚Äì604.\\nHalpern, D.; et al. 2019. From belief in conspiracy theories\\nto trust in others: Which factors influence exposure, believ-\\ning and sharing fake news. In SCSM 2019 . Springer.\\nHanu, L.; and Unitary team. 2020. Detoxify. Github.\\nhttps://github.com/unitaryai/detoxify.\\nHasan, R.; et al. 2022. The Impact of Viral Posts on Visibil-\\nity and Behavior of Professionals: A Longitudinal Study of\\nScientists on Twitter. In ICWSM , volume 16, 323‚Äì334.\\nHua, Y .; et al. 2020. Characterizing twitter users who engage\\nin adversarial interactions against political candidates. In\\nCHI 2020 , 1‚Äì13.\\nJohansson, F.; et al. 2016. Learning representations for\\ncounterfactual inference. In ICML , 3020‚Äì3029. PMLR.\\nJohnson, J. 2018. The self-radicalization of white\\nmen:‚ÄúFake news‚Äù and the affective networking of paranoia.\\nCommunication Culture & Critique , 11(1): 100‚Äì115.\\nJungherr, A. 2016. Twitter use in election campaigns: A sys-\\ntematic literature review. Journal of information technology\\n& politics , 13(1): 72‚Äì91.\\nKelm, O. 2020. Why do politicians use Facebook and Twit-\\nter the way they do? The influence of perceived audience\\nexpectations. SCM Studies in Communication and Media ,\\n9(1): 8‚Äì34.\\nKim, J. W.; et al. 2021. The distorting prism of social me-\\ndia: How self-selection and exposure to incivility fuel online\\ncomment toxicity. Journal of Communication , 71(6).\\nKim, T.; et al. 2022. Attention to the COVID-19 Pandemic\\non Twitter: Partisan Differences Among US State Legisla-\\ntors. Legislative studies quarterly , 47(4): 1023‚Äì1041.\\nKoch, B.; et al. 2021. Deep Learning for Causal Inference.\\nKreiss, D. 2016. Seizing the moment: The presidential cam-\\npaigns‚Äô use of Twitter during the 2012 electoral cycle. New\\nmedia & society , 18(8): 1473‚Äì1490.\\nKyriakidou, M.; et al. 2021. Journalistic responses to mis-\\ninformation. The Routledge Companion to Media Disinfor-\\nmation and Populism , 529‚Äì537.\\nLasser, J.; et al. 2022. Social media sharing of low-quality\\nnews sources by political elites. PNAS nexus , 1(4): pgac186.\\nLiu, Y .; et al. 2019. Roberta: A robustly optimized bert pre-\\ntraining approach. arXiv preprint arXiv:1907.11692 .\\nMihailidis, P.; et al. 2021. The cost of disbelief: Fracturing\\nnews ecosystems in an age of rampant media cynicism. ABS,\\n65(4): 616‚Äì631.\\nMutz, D. C. 2007. Effects of ‚Äúin-your-face‚Äù television dis-\\ncourse on perceptions of a legitimate opposition. APSR ,101(4): 621‚Äì635.\\nPelletier, M. J.; et al. 2021. Fexit: The effect of political\\nand promotional communication from friends and family on\\nFacebook exiting intentions. Journal of Business Research ,\\n122: 321‚Äì334.\\nPennycook, G.; and Rand, D. G. 2021. Examining false be-\\nliefs about voter fraud in the wake of the 2020 Presidential\\nElection. The HKS Misinformation Review .\\nPryzant, R.; et al. 2020. Causal effects of linguistic proper-\\nties. arXiv preprint arXiv:2010.12919 .\\nRao, A.; et al. 2022. Partisan asymmetries in exposure to\\nmisinformation. Scientific Reports , 12(1): 15671.\\nRoberts, M. E.; et al. 2020. Adjusting for confounding with\\ntext matching. AJPS , 64(4): 887‚Äì903.\\nSahly, A.; et al. 2019. Social media for political campaigns:\\nAn examination of Trump‚Äôs and Clinton‚Äôs frame building\\nand its effect on audience engagement. Social Media+ So-\\nciety , 5(2): 2056305119855141.\\nSerrano-Puche, J. 2021. Digital disinformation and emo-\\ntions: exploring the social risks of affective polarization. In-\\nternational Review of Sociology , 31(2): 231‚Äì245.\\nShi, C.; et al. 2019. Adapting neural networks for the esti-\\nmation of treatment effects. NeurIPS , 32.\\nShmargad, Y .; et al. 2022. Social norms and the dynamics\\nof online incivility. Social Science Computer Review , 40(3).\\nShor, B.; and McCarty, N. 2011. The ideological mapping\\nof American legislatures. APSR , 105(3): 530‚Äì551.\\nSquire, P.; et al. 2019. State legislatures today: Politics un-\\nder the domes . Rowman & Littlefield.\\nSridhar, D.; and Getoor, L. 2019. Estimating causal effects\\nof tone in online debates. arXiv preprint arXiv:1906.04177 .\\nStier, S.; et al. 2020. Election campaigning on social media:\\nPoliticians, audiences, and the mediation of political com-\\nmunication on Facebook and Twitter. In Studying Politics\\nAcross Media , 50‚Äì74. Routledge.\\nStryker, R.; et al. 2016. What is political incivility? Com-\\nmunication monographs , 83(4): 535‚Äì556.\\nTai, Y . C.; et al. 2023. Official yet questionable: examining\\nmisinformation in US state legislators‚Äô tweets. Journal of\\nInformation Technology & Politics , 1‚Äì13.\\nToraman, C.; et al. 2022. BlackLivesMatter 2020: An anal-\\nysis of deleted and suspended users in Twitter. In WebSci\\n2022 , 290‚Äì295.\\nTwitter-team. 2024. the-algorithm. https://github.com/\\ntwitter/the-algorithm?tab=readme-ov-file.\\nVeitch, V .; et al. 2020. Adapting text embeddings for causal\\ninference. In UAI, 919‚Äì928. PMLR.\\nV osoughi, S.; et al. 2018. The spread of true and false news\\nonline. science , 359(6380): 1146‚Äì1151.\\nYang, Y .; et al. 2023. Visual misinformation on Facebook.\\nJournal of Communication , jqac051.\\nZollo, F.; et al. 2015. Emotional dynamics in the age of\\nmisinformation. PloS one , 10(9): e0138740.Appendix\\nData Collection: For Twitter data, we employed a com-\\nprehensive approach, drawing from various sources, includ-\\ning existing datasets (Kim et al. 2022), and conducting\\nsearches on Google, Twitter, Wikipedia, legislators‚Äô official\\nwebsites, campaign sites, and Ballotpedia, to compile the\\naccounts and demographic information of state legislators.\\nThis meticulous strategy facilitated the manual identifica-\\ntion and verification of legislators‚Äô Twitter accounts. Sub-\\nsequently, we refined our dataset to only include legislators\\nwho served between 2020 and 2021, determined by their\\ntenure in office. It is important to acknowledge that the com-\\npleteness of our data was affected by factors such as inactive\\nor inaccessible accounts after legislators left office.\\nOur initial approach to collecting FB data involved gath-\\nering posts from accounts bearing names indicative of be-\\nlonging to state legislators. Subsequent verification against\\ninformation from Ballotpedia allowed us to filter out non-\\nlegislator accounts. To address data gaps, we conducted\\nthree successive rounds of data recollection in April 2022,\\nMarch 2023, and April 2023. The successive rounds allowed\\nus to capture posts from accounts previously overlooked.\\nHowever, similar to Twitter data, numerous accounts had\\nbecome inaccessible, largely due to campaign or official ac-\\ncounts no longer being active.\\nDespite these efforts, we encountered a significant chal-\\nlenge with FB data collection concerning accounts that were\\nnot listed on Ballotpedia. Although we attempted to iden-\\ntify missing accounts using keyword searches, achieving a\\nperfect match with the legislator information on Ballotpedia\\nwas difficult. This limitation resulted in a higher rate of mis-\\nmatch in the mapped attributes of FB accounts, i.e., around\\n70.2% of legislators could be mapped to their attributes for\\nFB. Table 7 shows the statistics of our FB dataset after map-\\nping the legislators to their attributes. The trends are similar\\nto that in Table 2 which suggests that no or minimal biases\\nwere introduced during our mapping process.\\nFigure 8 shows the breakdown of ethnicity and gender by\\nparty and for OL.\\n32% 30%17% 23%\\n29% 25%25% 21%\\n30% 30%17% 23%Twitter FB OLethnicity\\n0 0 0WhiteNon-White\\n36% 32%\\n13% 21%41% 27%\\n13% 19%35% 32%\\n12% 21%gender\\n0 1K 2K 3K 4K0 1K 2K 3K 4K0 1K 2K 3K 4KWomenMen\\nDem Rep\\nFigure 8: Ethnicity and gender by party, platform, and for\\noverlapping users. Our dataset has higher representation of\\nmen and White legislators on both platforms.\\nAssessing Post‚Äôs Civility. For a study like ours it is hard\\nto interpret the continuous toxicity scores returned by the\\nDetoxify model. So we follow the common practice in lit-\\nerature to convert the toxicity scores to binary based on a\\nthreshold (Hua et al. 2020). It is important to have a thresh-old for the toxicity for our particular dataset because un-\\ncivil language may evolve over different topics and time.\\nTo estimate an ideal cutoff for the toxicity score, we man-\\nually annotate a sample of 300 posts as uncivil or civil us-\\ning stratified sampling such that more samples are included\\nat higher toxicity scores. Three annotators labeled all 300\\nsamples based on the aforementioned definition of toxic lan-\\nguage. Since Cohen‚Äôs Kappa suffers from imbalanced label\\ndistributions, we compute the pairwise agreement scores for\\nthe percentage of total samples agreed upon by the annota-\\ntors, which ranges between 66.3-85.3%. The final labels are\\ndecided according to the majority vote. Based on the ROC\\n(AUC =0.81), we choose the cutoff for toxicity as 0.82,\\ni.e., posts having a score above 0.82 are considered uncivil.\\nThis is similar to previous works using Detoxify or Perspec-\\ntive API which have a threshold between 0.5-0.9 (Hua et al.\\n2020), though we acknowledge that our threshold is more\\ntowards the conservative side which is done to ensure that\\nuncivil posts are selected with high precision. Table 6 shows\\nexamples of uncivil posts by legislators on Twitter and FB.\\nThe number of uncivil posts on FB27is much lesser com-\\npared to Twitter which shows that the political communica-\\ntion styles are different on two platforms. This finding also\\nresonates with previous studies which have suggested that\\nFB is used more for broadcasting purposes whereas Twitter\\nis used more for direct communication (Enli and Skogerb√∏\\n2013). Based on this, it is reasonable that there are very less\\nuncivil posts on FB since the language used on FB tends\\nto be more formal. We further verify this by measuring the\\nreadability scores of author‚Äôs posts on these two platforms\\nusing Flesch‚ÄìKincaid grade level. The median readability of\\nlegislators is 11.04 (i.e., the text is written at a level suitable\\nfor someone who has completed the 11th grade in the US\\neducation system) on FB and 9.55 on Twitter. This shows\\nthat the language used on FB is indeed more formal and po-\\ntentially the reason why it is more civil.\\nMeasuring Visibility. The visibility metric is designed to\\ncapture the overall engagement on the platforms and hence\\nour metric includes all/most of the elements used to calculate\\nengagement on the respective platforms. We further analyze\\nthe contribution of individual visibility elements on each\\nplatform. On Twitter, Likes contribute 2.3% and Retweets\\ncontribute 97.7% to the overall interactions. Reply and quote\\nconsist of a small percentage of the interactions. On FB how-\\never, Likes contribute 51.8%, Shares contribute 17.0% and\\nComments contribute 13.2% to the overall interactions. This\\nsuggests that audiences engage differently with content on\\nTwitter and FB, e.g., retweeting is most dominant form of\\nengagement on Twitter whereas Liking for FB. Moreover,\\nthe interpretation of individual elements may also be dif-\\nferent across platforms, for instance, audiences may engage\\nwith Like on Twitter differently than Like on FB simply due\\nto different icons, therefore a direct comparison of individ-\\nual engagement metrics may not be justified. So, by only\\nincluding individual elements, the visibility metric may not\\nbe able to capture the level of engagement on these plat-\\n27The number of uncivil posts is still low at other cutoffs, for\\ne.g., cutoff = 0.1 yields around 2,690 uncivil postsTable 6: Examples of Uncivil Posts on Twitter and FB, by party\\nplatform party text\\nFB Rep While millions of Americans have yet to receive their stimulus checks, a new study reveals that $4.38 billion of\\nthe new round will go right into the pockets of illegal immigrants. Another dumb idea and stupid stupid policy.\\nWhat is wrong with these people?\\nFB Dem ‚ÄúI didn‚Äôt think it would be this ridiculous. It‚Äôs embarrassing to be a state senator at this point, ‚Äù Paul Boyer said\\nof partisan recount. Yes it does make you look like idiots. Wasting time & resources on #TrumpsBigLie\\nTwitter Rep We are at the start of one of the LARGEST recessions in American history, which will DESTROY many lives,\\nand people are still in favor of partial lockdownshow stupid could you possibly be! Bunch of damn fools.\\nTwitter Dem You are a blithering idiot. Who gives a shit about the VP . Vote for Trump and thousands upon thousands will\\ndie.\\nTable 7: Descriptive statistics for FB dataset after mapping,\\nshowing the number of legislators, posts, and median inter-\\nactions received per post by party.\\nFB\\nparty #users #tweets Int\\nDem 1588 171K 61.0\\nRep 1718 152K 114.0\\nTable 8: 95% CI for Table 4\\nOL\\nIVs Twitter [CI] FB [CI] Twitter [CI] FB [CI]\\nParty 0.239 -0.299 0.189 -0.347\\n[0.202, 0.277] [-0.337, -0.259] [0.141, 0.240] [-0.397, -0.298]\\nGender 0.076 -0.089 0.009 -0.128\\n[0.033, 0.117] [-0.132, -0.046] [-0.044, 0.059] [-0.182, -0.072]\\nEthnicity -0.067 -0.031 -0.023 -0.062\\n[-0.108, -0.027] [-0.079, 0.015] [-0.073, 0.029] [-0.128, 0.001]\\nPosting 0.457 0.435 0.368 0.418\\nFreq. [0.427, 0.492] [0.398, 0.470] [0.323, 0.414] [0.370, 0.461]\\nforms, making the interpretation of the metric harder for a\\ncross-platform study.\\nWe further examine the possibility of the visibility metric\\nbeing dominated by a single element by testing if the under-\\nlying distributions are similar for the total interactions and\\nthe most dominant element. According to our KS tests, for\\nboth Likes on FB and Retweets on Twitter, we find that the\\ndistributions are significantly different (p-value <0.05) com-\\npared to the total interactions, suggesting that other elements\\nalso contribute to the overall engagement on both platforms\\nand hence need to be included in the visibility measure.\\nDV Transformation for RQ2. To satisfy assumptions of\\nlinear regression in RQ2, we transform all variables to be\\nclose to normal distributions using the ‚ÄúbestNormalize‚Äù R\\npackage. For Twitter, we transform variables as follows:\\nVIP(Yeo-Johnson), #posts (Box Cox), #Misinfo (sqrt),\\n#Uncivil (sqrt), Network visibility (Center+scale), #follow-\\ners (Box Cox), Centrality (None). For FB, we transform\\nvariables as follows: VIP(Yeo-Johnson), #posts (Box Cox),\\n#Misinfo (sqrt).Table 9: Robustness analysis of RQ1 results for 25 th, 50th,\\nand 75 thpercentile visibility on Twitter\\nIVs 25th50th75th\\nParty 0.120‚àó‚àó‚àó0.160‚àó‚àó‚àó0.160‚àó‚àó‚àó\\nGender 0.016 0.036 .0.060‚àó‚àó\\nEthnicity 0.004 -0.027 -0.054‚àó‚àó\\nPosting Freq. 0.255‚àó‚àó‚àó0.354‚àó‚àó‚àó0.386‚àó‚àó‚àó\\nTable 10: Robustness analysis of RQ1 results for 25 th, 50th,\\nand 75 thpercentile visibility on FB\\nIVs 25th50th75th\\nParty -0.293‚àó‚àó‚àó-0.300‚àó‚àó‚àó-0.291‚àó‚àó‚àó\\nGender -0.078‚àó‚àó‚àó-0.088‚àó‚àó‚àó-0.088‚àó‚àó‚àó\\nEthnicity -0.048 -0.047 . -0.032\\nPosting Freq. 0.376‚àó‚àó‚àó0.402‚àó‚àó‚àó0.424‚àó‚àó‚àó\\nRQ1 Tables. Table 8 shows the 95% CI for Table 4. Ta-\\nbles 9 and 10 show the results for 25 th, 50th, and 75 th\\npercentile visibility for Twitter and FB respectively.\\nRQ2 Tables. Table 12 shows the 95% CI for Table 5.\\nBenchmarks for thres ,w.Figure 9 shows the ECDF\\nplots for daily mean interactions received by legislators on\\nTwitter and FB, by party. For Twitter and FB, the medians\\nare close to 10 and 100 respectively. So we select thres =\\n10for Twitter and thres = 100 for FB. We do not have dif-\\nferent thres across parties since medians are similar across\\nparties on both platforms.\\n1001021041060.00.20.40.60.81.0A. T witter\\nparty\\nRep\\nDem\\n1011031050.00.20.40.60.81.0B. FB\\nparty\\nRep\\nDem\\ndaily average interactions received by legislatorsproportion\\nFigure 9: ECDF plots for daily mean interactions received\\nby legislators on Twitter and FB, by party.\\nFigure 10 shows the ECDF plots for daily number of posts\\nby legislators on Twitter and FB, by party. Again the median\\nposting rates are similar for Republicans and Democrats on1001011020.00.20.40.60.81.0A. T witter\\nparty\\nRep\\nDem\\n1001011020.00.20.40.60.81.0B. FB\\nparty\\nRep\\nDem\\ndaily #posts by legislatorsproportionFigure 10: ECDF plots for daily number of posts by legisla-\\ntors on Twitter and FB, by party.\\nTable 11: Dragonnet performance\\nAUC Macro F1\\noverall Extreme overall Extreme\\nTwitter uncivil 0.74 0.72 0.69 0.66\\nTwitter non-credible 0.73 0.74 0.66 0.68\\nFB non-credible 0.80 0.88 0.74 0.82\\nboth Twitter and FB. The median daily post count on Twitter\\nis 2 and 1 on FB. To ensure that we have a sufficient num-\\nber of posts per legislator to compute the overperforming\\nscores and simultaneously account for temporal variation in\\nour data, we select w= 14 for both Twitter and FB.\\nDragonnet Model Description. Dragonnet uses feed-\\nforward neural networks to learn balanced representations\\nof the data such that each head models a separate poten-\\ntial outcome, a third propensity head predicts the propen-\\nsity (œÄ) of being treated, and a free nudge parameter œµ. The\\nœÄandœµparameters are used to re-weight the outcomes to\\nprovide lower biased estimates of the Conditional Average\\nTreatment Effect (CATE). The error gradients from the two\\noutcome modeling heads are propagated back to the shared\\nrepresentation layers of the Dragonnet model to learn the\\ncovariate representation, i.e., œï(X). The representation lay-\\ners learn a balanced representation of the data since the\\nmodel objective is to predict both outcomes and each out-\\ncome modeling head learns a function of the transformed co-\\nvariate representation, i.e., Y(T) =h(œï(x), T). The CATE\\nfrom Dragonnet predictions is estimated as follows,\\nCATE =1\\nNNX\\ni(Y‚àó\\ni(1)‚àíY‚àó\\ni(0)) (4)\\nwhere,\\nY‚àó\\ni=ÀÜYi+ (Ti\\nœÄ(œï(Xi),1)‚àí1‚àíTi\\nœÄ(œï(Xi),0))√óœµ (5)\\nwhere ÀÜY(1)andÀÜY(0)are predictions returned by the\\ntwo outcome modeling heads respectively, œÄis the predicted\\npropensity of a sample being treated, and sample size N.\\nDragonnet Model Performance. Figure 11 shows the\\nROC curves for Twitter incivility, Twitter low-credibility\\nand FB low-credibility Dragonnet models. The AUC and\\nMacro F1-scores28are reported in Table 11.\\n28F1-scores reported at optimal cutoff\\n0.0 0.2 0.4 0.6 0.8 1.0\\nFPR0.00.20.40.60.81.0TPRT witter uncivil\\nOverall (AUC: 0.74)\\nHeadO=0 (AUC: 0.78)\\nHeadO=1 (AUC: 0.69)\\n0.0 0.2 0.4 0.6 0.8 1.0\\nFPR0.00.20.40.60.81.0TPRT witter non-credible\\nOverall (AUC: 0.73)\\nHeadO=0 (AUC: 0.67)\\nHeadO=1 (AUC: 0.77)\\n0.0 0.2 0.4 0.6 0.8 1.0\\nFPR0.00.20.40.60.81.0TPRFB non-credible\\nOverall (AUC: 0.80)\\nHeadO=0 (AUC: 0.75)\\nHeadO=1 (AUC: 0.87)Figure 11: ROC curves showing Dragonnet performance of\\noverall as well as two outcome modeling heads, for Twitter\\nincivility, Twitter and FB low-credibility respectively.\\nTable 12: 95% CI for Table 5\\nOL\\nIVs Twitter FB Twitter FB\\nParty [Rep] -0.133 0.423 -0.142 0.512\\n[-0.219, -0.046] [0.359, 0.487] [-0.252, -0.029] [0.431, 0.594]\\nGender [Men] -0.078 0.090 -0.011 0.113\\n[-0.149, -0.007] [0.029, 0.152] [-0.101, 0.079] [0.033, 0.194]\\nEthnicity [White] 0.020 0.015 0.020 0.040\\n[-0.061, 0.100] [-0.056, 0.086] [-0.079, 0.118] [-0.054, 0.133]\\n#posts 0.401 0.477 0.409 0.499\\n[0.345, 0.455] [0.443, 0.510] [0.337, 0.481] [0.459, 0.538]\\n#followers -0.028 - -0.028 -\\n[-0.076, 0.021] [-0.091, 0.034]\\nCentrality -0.020 - -0.029 -\\n[-0.060, 0.019] [-0.079, 0.021]\\nNetwork visibility 0.040 0.027 -\\n[0.002, 0.080] [-0.020, 0.076]\\n#Low-Credible -0.268 0.079 -0.335 -0.037\\n[-0.466, -0.070] [0.047, 0.110] [-0.601, -0.069][-0.074, 0.000]\\n#Uncivil 0.148 - 0.152 -\\n[0.101, 0.196] [0.092, 0.213]\\nParty [Rep] x 0.299 - 0.351 -\\n#Low-Credible [0.103, 0.496] [0.087, 0.614]\\nR20.289 0.382 0.291 0.386\\nCovariate Balance for Matching. We employ 1:1 match-\\ning such that each treated sample is matched to one un-\\ntreated sample. We use Nearest Neighbour matching based\\non Euclidean distance between the deconfounded tweet em-\\nbeddings. We find matches for 9677 (64.0%) uncivil tweets,\\n3957 (73.5%) low-credibility tweets, and 1583 (61.1%) low-\\ncredibility FB posts using a distance cutoff of 0.1 to maxi-\\nmize the covariate overlap. This gives us balanced represen-\\ntations of the observed covariates across the treated and un-\\ntreated samples as shown in Figure 6. We compute the stan-\\ndardized differences for each of the covariates before and\\nafter matching. A score between -0.1-0.1 indicates balance.\\nEffect of Outliers Outliers are common in social net-\\nworks, but their impact on analysis results and conclusions\\ncan vary. In our dataset, outliers may exist in terms of post-\\ning volume and engagement received due to the scale-free\\ndistributions (refer to Fig 12). However, we have taken mea-\\nsures to ensure these outliers do not significantly impact our\\nresults.\\nFor RQ1, we used a non-parametric statistical test, the\\nMann-Whitney U test, to compare distributions, which isrobust to outliers. For RQ2, we used non-linear transforma-\\ntions on study variables to minimize the impact of outliers\\nin the regression analysis.\\nIn RQ3, we matched accounts with similar characteris-\\ntics in the de-confounded embedding space, such as simi-\\nlar posting rates. This process either discarded outliers if no\\nadequate match was found or retained them if an adequate\\nmatch was identified. This matching step ensured the bal-\\nance of the covariates before running the estimation of the\\nConditional Average Treatment Effect (CATE), further re-\\nducing the impact of outliers.\\n1001011021031040.00.20.40.60.81.0ProportionA. T witter posting volume\\nparty\\nRepublican\\nDemocratic\\n101\\n1001011021031040.00.20.40.60.81.0ProportionB. Visibility on T witter\\nparty\\nRepublican\\nDemocratic\\nFigure 12: Scale-free distributions for (A) Posting volumes\\nand (B) Visibility of legislators on Twitter. The distributions\\nare also similar for FB.']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_texts[1:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69b8df9",
   "metadata": {},
   "source": [
    "# 4. We save that into our computer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74887289",
   "metadata": {},
   "source": [
    "First we extract the headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1bd64684",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [item[0] for item in extracted_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c348cd99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Doc 2407.13549v1.pdf',\n",
       " 'Doc 2407.16014v1.pdf',\n",
       " 'Doc 2407.18471v1.pdf',\n",
       " 'Doc 2408.07322v1.pdf',\n",
       " 'Doc 2408.08126v1.pdf',\n",
       " 'Doc 2408.08437v1.pdf',\n",
       " 'Doc 2408.08964v3.pdf',\n",
       " 'Doc 2408.09435v1.pdf',\n",
       " 'Doc 2408.09683v1.pdf',\n",
       " 'Doc 2408.09725v1.pdf',\n",
       " 'Doc 2408.12449v2.pdf',\n",
       " 'Doc 2408.12743v2.pdf',\n",
       " 'Doc 2408.12753v1.pdf',\n",
       " 'Doc 2409.01470v1.pdf',\n",
       " 'Doc 2409.02358v1.pdf',\n",
       " 'Doc 2409.08405v1.pdf',\n",
       " 'Doc 2409.13461v1.pdf',\n",
       " 'Doc 2409.15652v3.pdf',\n",
       " 'Doc 2409.18393v1.pdf',\n",
       " 'Doc 2409.18931v1.pdf',\n",
       " 'Doc 2410.01708v1.pdf',\n",
       " 'Doc 2410.05401v1.pdf',\n",
       " 'Doc 2410.06443v1.pdf',\n",
       " 'Doc 2410.14617v1.pdf',\n",
       " 'Doc 2410.16977v1.pdf',\n",
       " 'Doc 2410.17496v1.pdf',\n",
       " 'Doc 2410.20293v2.pdf',\n",
       " 'Doc 2410.22716v1.pdf',\n",
       " 'Doc 2411.04542v1.pdf',\n",
       " 'Doc 2411.04752v1.pdf',\n",
       " 'Doc 2411.05043v1.pdf',\n",
       " 'Doc 2411.05788v1.pdf',\n",
       " 'Doc 2411.06122v1.pdf',\n",
       " 'Doc 2411.09214v1.pdf',\n",
       " 'Doc 2411.11426v1.pdf',\n",
       " 'Doc 2411.12508v1.pdf',\n",
       " 'Doc 2411.14613v1.pdf',\n",
       " 'Doc 2411.16285v1.pdf',\n",
       " 'Doc 2411.16826v1.pdf',\n",
       " 'Doc 2412.02349v1.pdf',\n",
       " 'Doc 2412.04484v1.pdf',\n",
       " 'Doc 2412.05861v1.pdf',\n",
       " 'Doc 2412.07550v1.pdf',\n",
       " 'Doc 2412.08484v1.pdf',\n",
       " 'Doc 2412.08648v1.pdf',\n",
       " 'Doc 2412.14985v1.pdf',\n",
       " 'Doc 2412.15072v1.pdf',\n",
       " 'Doc 2412.15621v1.pdf',\n",
       " 'Doc 2412.18779v1.pdf',\n",
       " 'Doc 2412.20420v1.pdf']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "236b5756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf61d6a",
   "metadata": {},
   "source": [
    "And now we extract the text of the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a139804",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_elements = [item[1] for item in extracted_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9063dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Evaluating the effect of viral news on social media\\nengagement\\nEmanuele Sangiorgio1*, Niccol` o Di Marco2, Gabriele Etta2,\\nMatteo Cinelli2, Roy Cerqueti1,3, Walter Quattrociocchi2\\n1*Department of Social Sciences and Economics, Sapienza University of\\nRome, P.le Aldo Moro, 5, 00185, Rome, Italy.\\n2Department of Computer Science, Sapienza University of Rome, Viale\\nRegina Elena, 295, 00161, Rome, Italy.\\n3GRANEM, Universit¬¥ e d‚ÄôAngers, SFR Confluences, F-49000, Angers,\\nFrance.\\n*Corresponding author(s). E-mail(s): emanuele.sangiorgio@uniroma1.it;\\nContributing authors: niccolo.dimarco@uniroma1.it;\\ngabriele.etta@uniroma1.it; matteo.cinelli@uniroma1.it;\\nroy.cerqueti@uniroma1.it; walter.quattrociocchi@uniroma1.it;\\nAbstract\\nThis study examines Facebook and YouTube content from over a thousand news\\noutlets in four European languages from 2018 to 2023, using a Bayesian struc-\\ntural time-series model to evaluate the impact of viral posts. Our results show\\nthat most viral events do not significantly increase engagement and rarely lead\\nto sustained growth. The virality effect usually depends on the engagement trend\\npreceding the viral post, typically reversing it. When news emerges unexpect-\\nedly, viral events enhances users‚Äô engagement, reactivating the collective response\\nprocess. In contrast, when virality manifests after a sustained growth phase, it\\nrepresents the final burst of that growth process, followed by a decline in atten-\\ntion. Moreover, quick viral effects fade faster, while slower processes lead to more\\npersistent growth. These findings highlight the transient effect of viral events and\\nunderscore the importance of consistent, steady attention-building strategies to\\nestablish a solid connection with the user base rather than relying on sudden\\nvisibility spikes.\\nKeywords: Social media |Virality |Attention economy\\n1arXiv:2407.13549v1  [cs.SI]  18 Jul 20241 Introduction\\nThe advent and proliferation of social media have fundamentally altered the informa-\\ntion landscape [1‚Äì3], offering unprecedented opportunities for content to achieve rapid\\nand widespread attention. As these platforms have become integrated into our daily\\nlives [4], transforming into essential tools for information diffusion [5, 6] and personal\\ncommunication [7], they have merged entertainment-driven business models with com-\\nplex social dynamics [8], raising significant concerns about their impact on society.\\nThis complex interplay produced an environment in which information overload is the\\nforemost feature [9] and a wide range of content creators, from news organizations to\\nindividual influencers [10], compete for the limited resource that is users‚Äô attention\\n[11‚Äì14].\\nUnderstanding the attention economy in the digital domain is paramount for nav-\\nigating this competitive information market, whereby the pursuit of virality [15] plays\\na pivotal role in shaping how information sources design their strategies of production\\nand diffusion of content. Characterized by content‚Äôs exceptional reach and engage-\\nment, virality is a core feature of this environment, particularly when referring to viral\\nnews and their potential impact on the public discourse. In today‚Äôs online ecosystem,\\nit is crucial to understand how collective attention responds to abrupt news diffusion\\nand how sudden spikes of visibility reverberate on the subsequent attention captured\\nby the source. While virality has been mainly investigated for its marketing impli-\\ncations and received extensive coverage in the literature [16‚Äì20], the impact of viral\\nnews on collective attention has not obtained as much consideration.\\nTo address this gap in existing literature, this study aims to enhance our compre-\\nhension of the attention economy through a data-driven approach by exploring the\\ndynamics of virality and its effects on users‚Äô engagement on different social media plat-\\nforms. By analyzing data from Facebook and YouTube, we examine attention patterns\\nafter viral events to assess how these events influence users‚Äô interactions over time.\\nIn this study, we use a comparative interrupted time-series (CITS) design imple-\\nmented using a Bayesian structural time series model (BSTS) [21] to evaluate the\\nimpact of viral events on users‚Äô engagement. In our approach, we apply the BSTS by\\nusing increasingly broader time windows to observe the effect of the same viral event\\nfrom a short-term to a long-term perspective. Based on the BSTS‚Äôs results, we conduct\\nour analysis first by examining the magnitude of the impact and then its temporal\\ndynamics to address the following two research questions.\\nRQ1: Does virality induce engagement growth?\\nOur first research question aims to assess whether and how a viral event leads\\nto increased users‚Äô attention received by the source. After that, our second research\\nquestion aims to analyze the temporal dynamics of these effects to evaluate if the\\nrapidity at which they occur influences their longevity.\\nRQ2: Do the faster-manifesting effects persist longer?\\nWhile our first research question aims to determine the actual impact of virality\\nand its magnitude, the second analysis provides valuable insights into whether these\\nevents genuinely contribute to sustained growth or merely act as transient spotlights.\\nOur results indicate the presence of two different types of viral events, a ‚Äòloaded-\\ntype‚Äô and a ‚Äòsudden-type‚Äô virality. When virality follows a sustained growth phase,\\n2it represents the final burst of that growth process, with users‚Äô attention successively\\nstanding on lower levels. Conversely, viral news boosts users‚Äô engagement when occur-\\nring as a sudden event, reactivating the collective response process. While virality can\\ntemporarily boost user engagement, this effect is often short-lived. We observe that\\nquickly emerging viral effects rapidly fade out. On the other hand, content achieving\\nslower and sustained growth tends to show more persistent effects on engagement.\\nThese results emphasize the importance of continuous and consistent content strate-\\ngies in establishing a solid and enduring connection with the user base rather than\\nrelying on viral spikes.\\nThe rest of this paper is organized as follows: Section 2, gives an overview of the\\nrelevant literature. In Section 3, we outline our methodology for virality detection and\\nimpact evaluation. In Section 4 we present our detailed findings on the dynamics of the\\nvirality impact on users‚Äô attention and its persistence through time. In Section 5 we\\ndiscuss the implications of our findings, along with limitations and recommendation\\nfor future research. Section 6 concludes the paper.\\n2 Literature review\\n2.1 Attention economy\\nThe attention economy is central to today‚Äôs digital media landscape. In information\\nmanagement, attention economics applies economic theory to human attention, treat-\\ning it as a scarce resource. The attention economy is defined as a system of agents\\n(senders) who aim to capture the attention of individuals (receivers) by creating and\\nsharing information packages (signals) [11, 22‚Äì25]. Once produced and disseminated,\\nthis information undergoes a cognitive filtering process by the receivers, who select\\nrelevant information and disregard the rest [26‚Äì28]. From a supply-side perspective,\\nthe rise of social media platforms has led to an unprecedented volume of available con-\\ntent and information. This overabundance, often referred to as information overload\\nin the literature [9, 29‚Äì31], is a cardinal characteristic of the current attention market\\nin the digital landscape. In this competitive environment, a wide range of content cre-\\nators, from news organizations to individual influencers [10], compete for the limited\\nresource that is users‚Äô attention. The concept of human attention as a currency entails\\ntwo key features [32]. Unlike real currencies, it can not be accumulated but solely be\\nspent. Additionally, its transient nature makes it hard to trace and measure. Social\\nmedia data help address this challenge, as user interactions with content constitute\\nprecise engagement indicators. The vast amount of tracked and aggregated data allows\\nfor studying large populations, providing insights into collective behavior without the\\nneed for experimental settings [33, 34]. Shifting the focus from individual users to a\\nbroader community perspective highlights the dynamics of collective attention [35, 36].\\n2.2 Human dynamics on social media\\nOriginally designed for entertainment and personal connections, social media platforms\\nhave become indispensable tools for information dissemination [37], raising significant\\n3concerns about their potential impact on social dynamics. Many works in the litera-\\nture highlight how online users are prone to consume information aligning with their\\nexisting beliefs [38‚Äì40] and commonly ignore opposing viewpoints [28, 41, 42]. The\\ncreation and reinforcement of online ‚Äòecho chambers‚Äô [43, 44], where shared narratives\\nare collectively shaped and solidified [39, 45], may exacerbate social divisions and fos-\\nter partisan and polarized communities [46‚Äì48], complicating the landscape of public\\ndiscourse [49] especially during sensitive periods such as global elections [50]. Offering\\nunprecedented opportunities for content to achieve rapid and widespread attention\\n[35], social media have become crucial environments for the spread of information\\nand misinformation during global events [51‚Äì53], political events [54, 55], and discus-\\nsions on emerging technologies such as large language models [56]. The unprecedented\\namount of available content distinguishes today‚Äôs digital ecosystem, considerably com-\\nplicating the search for information and giving rise to the phenomenon of infodemics\\n[57, 58], such as for the COVID-19 pandemic. These platforms may also influence\\npolitical landscapes, potentially affecting public opinion and voter behavior during\\nelections through the rapid dissemination and amplification of political content. How-\\never, this influence is not definitively proven [59, 60]. Research on the interplay between\\nuser behavior and platforms‚Äô byproduct presented both opportunities and challenges\\n[46, 61‚Äì63], unveiling a multifaceted landscape where a prevalence of one over the\\nother has not yet emerged. As users‚Äô behavior could exhibit persistent patterns across\\ndifferent platforms, topics, and contexts [64], the comparative analysis of diverse social\\nmedia can isolate unaltered consistencies of human dynamics in the digital ecosystem\\nor underline algorithmic and platform peculiarities.\\n2.3 Virality on social media\\nOn social media platforms, virality refers to a piece of content having rapid diffu-\\nsion and high levels of users‚Äô engagement. Viral events can differ depending on the\\ndissemination that leads to their emergence. Structural virality distinguishes between\\nbroadcast andviral diffusion based on their spreading patterns [65]. Broadcast dif-\\nfusion follows a pattern where a single, large parent node spreads information to\\nseveral smaller entities. In contrast, viral diffusion involves multiple smaller nodes,\\neach contributing to the spread by generating a limited number of infections.\\nOne of the most complex issues is understanding why certain items achieve sud-\\nden and widespread dissemination while other similar or higher-quality items remain\\noverlooked [66, 67]. Research suggests that the nature of the content is a more signifi-\\ncant factor in driving virality than the characteristics of the spreading source [68]. For\\nexample, the source size in terms of Followers does not affect the probability of going\\nviral on Facebook [14], with the growth dynamics of engagement exhibiting universal\\npatterns in the short run. Concerning content properties, [69] demonstrated that the\\ntype of multimedia content on Twitter affects the volume and speed of retweeting, [70]\\nidentified visual attributes that can predict relative virality using Reddit data, while\\n[71] characterized visual elements distinguishing viral from non-viral memes.\\nOn the other hand, the frantic search for attractiveness may translate into severe\\ndrawbacks for the source itself. For instance, [72] evidenced how readers often view\\nclickbait as a manipulative tactic, which can reduce the perceived competence and\\n4trustworthiness of the publisher. Information overload leads to similar drawbacks also\\nin business and managerial contexts. The widespread practice of consumer-generated\\ncontent can provoke social and brand overload due to its quantity and poor quality,\\nleading to negative consequences such as brand disloyalty [73].\\nBeyond their media features, there is substantial agreement in the literature that\\nemotional resonance is a critical trigger of virality, particularly with negative emotions\\nand extreme or sensitive content. Extreme content, such as sex, nudity, and violence,\\nis more likely to become viral [74]. Hateful content cascades tend to be larger, per-\\nsist longer, and exhibit more significant structural virality [75]. An initial burst of a\\ntopic‚Äôs diffusion often correlates with increased negative reactions from users [76], and\\nnegative messages are typically reposted more quickly and frequently than positive or\\nneutral ones [77].\\nThe spread of misinformation and the rise of political partisanship are critical\\nissues due to their significant social implications. Conspiracy rumor cascades tend to\\nbe more persistent and exhibit a positive relationship between their lifetime and size\\n[39]. During the COVID-19 pandemic, misinformation was more likely to go viral than\\ntruthful information, often due to the use of emotionally charged, other-condemning\\nlanguage [78]. A similar mechanism is observed in political contexts, where posts\\nabout political opponents are more likely to be shared on social media. This out-group\\neffect is more influential than other social media-sharing predictors, such as emotional\\nlanguage [79]. Ultimately, the driving effects of virality stem from the combination and\\nmutual reinforcement of ideological segregation and negative emotional resonance.\\nBesides cases of misinformation and harmful content, viral diffusion may also have\\nsignificant drawbacks in different social contexts. Social media might influence risk\\nperceptions from the general public [80], fueling moral panics and amplifying threats\\nposed by deviant behavior and ideas [81], so much so of having tangible consequences\\nin real contexts like inducing distortions and detrimental effects on financial markets\\n[82, 83].\\n3 Research design\\nWe now outline our research design, starting from the data collection process detailed\\nin Section 3.1. In Section 3.2, we formally define virality along with the measures to\\nquantify it, while Section 3.3 accounts for the virality detection methods. Lastly, in\\nSection 3.4, we first illustrate our approach to evaluate the virality effect, and then\\nwe separately outline our research questions along with their related methods.\\n3.1 Data\\nWe begin by selecting a list of news outlets from NewsGuard [84], an organization\\nknown for monitoring news outlet activities internationally, rating more than 35,000\\nnews and information sources across several countries. We select all the reported news\\noutlets from Germany, France, Italy, and the United Kingdom. NewsGuard provides\\nseveral categories of descriptive metadata, including the URLs of these news sources‚Äô\\nsocial media accounts, if available. For Facebook, after identifying all the news outlets\\nwith active accounts listed on NewsGuard, we use their Facebook URLs to gather data\\n5Table 1 : Dataset overview\\nFacebook YouTube\\n2018-2023 2016-2023\\nCountry Pages Posts Channels Videos\\nGermany 263 9,633,896 242 193,153\\nFrance 252 8,705,928 177 204,665\\nItaly 384 16,259,173 234 271,183\\nUK 227 8,751,788 105 100,794\\nTotal 1,126 43,350,785 758 769,795\\nvia CrowdTangle. This Facebook-owned tool monitors interactions on public content\\nfrom Facebook pages, groups, and verified profiles [85]. Using the ‚ÄòHistorical Data‚Äô\\nfeature in CrowdTangle, we download the complete content history of each page,\\nstarting from January 1, 2018. Similarly, for YouTube, after selecting all available\\naccounts, we use the YouTube Data API [86] to collect their list of published content.\\nGiven the fewer YouTube channels and videos compared to Facebook, we extend the\\nobservation period for YouTube to begin on January 1, 2016.\\nTable 1 provides an overview of the dataset, detailing the total number of Face-\\nbook Pages and YouTube channels used in our analysis for each country, along with\\ntheir respective number of posts and videos. The longitudinal structure and tempo-\\nral continuity of these page observations are crucial for evaluating the impact of viral\\nevents. Therefore, the resulting dataset is formatted consistently for both platforms,\\ncontaining chronological information about each posted and accessible item. This nat-\\nurally excludes any content that may have been removed by users or through platform\\nmoderation at the time of download. Note that in what follows, we will refer to both\\nFacebook pages and YouTube channels as (information) sources and to Facebook posts\\nand YouTube videos as either content or posts, specifying the platform in the latter\\ncase.\\n3.2 Defining virality\\nOn social media platforms, virality represents the propensity of content to achieve\\nrapid diffusion and high interactions levels from users [15]. To quantify virality, we use\\na bivariate approach based on two measures, spreading and interactions. The following\\nsubsections provide the definition of these two measures for each social media platform.\\n3.2.1 Spreading\\nFor Facebook data, we adopt the metric provided by [87]. Originally defined on Twitter\\ndata, the proposed metric can be applied to any social media platform with equivalent\\nRetweets and Followers metrics. On Facebook, Shares are equivalent to Retweets,\\nrepresenting the number of users sharing certain content. On both platforms, Followers\\nrepresent the number of users subscribed to a given page at the time of posting.\\n6Therefore, on Facebook, we can define the Spreading Sof a given post jof a Page ias:\\nSijt=ln\\x12Shares ij\\nFollowers it\\x13\\n(1)\\nwhere Followers itindicates the number of Followers of the Page iat time t, i.e. the\\ntime of posting.\\nFor YouTube, we can define it using the video Views metric, which specifically\\nmeasures content spreading. Therefore, on YouTube, we define the Spreading Sof a\\ngiven video jpublished by a Channel iat time tas:\\nSijt=ln(V iews ij). (2)\\n3.2.2 Interactions\\nOn Facebook, interactions are measured by the Total Interactions, encompassing the\\ntotal number of users‚Äô interactions with the post (the sum of Likes, Comments, and\\nShares). Therefore, on Facebook, we define the Total Interactions TIof a given post\\njof a Page ias:\\nTIijt=ln(Likes ij+Comments ij+Shares ij) (3)\\nwhere tis the date in which post jappears.\\nSince YouTube does not provide a share count (and consequently its Shares metric),\\nwe define the Total Interactions TIof a given video jof a Channel ipublished at day\\ntas:\\nTIijt=ln(Likes ij+Comments ij). (4)\\n3.3 Detecting virality\\nWe measure virality on social media as an (exceptional) over-performance of content\\nregarding spreading and users‚Äô interactions. To detect virality, we first apply a z‚àíscore\\nnormalization to both SijtandTIijtfor each piece of content jfrom a source i. On\\nFacebook, a post is considered viral if both its z‚àíscores exceed 3. We set the threshold\\nfor YouTube at 2.5 to ensure a comparable number of videos and channels as in\\nFacebook.\\nThe distributions of the two z‚àíscores for both platforms are shown in Fig. 1. The\\ninset plots within each panel zoom on the subset of values that exceed the threshold\\nfor the displayed metric, distinguishing between viral content (which surpasses both\\nthresholds) and non-viral content (which exceeds the threshold only in the displayed\\none).\\nThese insets reveal that the subset resulting from the combination of overperfor-\\nmances constitutes a small fraction of the distribution tails, especially on Facebook.\\nIn other words, content may induce high interactions levels without widespread diffu-\\nsion, or conversely, it may fail to engage users despite extensive reach. This observation\\nfurther emphasizes the exceptional rarity of virality.\\n7101102103104105106107\\n‚àí4‚àí3‚àí2‚àí101234567\\nSpreading z‚àíscoreFrequency050001000015000\\n3.03.54.04.55.0Viral\\nFALSE\\nTRUE\\n101102103104105106107\\n‚àí2‚àí101234567\\nInteractions z‚àíscore 05000100001500020000\\n3.03.54.04.55.0Viral\\nFALSE\\nTRUE Facebook A)\\n101102103104105\\n‚àí3‚àí2‚àí10123456\\nSpreading z‚àíscoreFrequency025050075010001250\\n2.53.03.54.0Viral\\nFALSE\\nTRUE\\n101102103104105\\n‚àí1 0 1 2 3 4 5\\nInteractions z‚àíscore 025050075010001250\\n2.53.03.54.0Viral\\nFALSE\\nTRUE YouTube B) Distributions of Spreading and Interactions z‚àíscoresFig. 1 : Distributions of Spreading and Engagement z‚àíscores for Facebook and\\nYouTube. The inset plots of each chart display the subset of values exceeding the\\nthreshold for the given metric, showing its breakdown between viral contents (which\\nexceed the threshold in both metrics) and non-viral ones (hence exceeding the thresh-\\nold only in the displayed metric).\\nFig. 2 presents the distributions of viral posts per source for Facebook and\\nYouTube. As the Figure shows, despite the differences in sample sizes, both platforms\\nexhibit similar heavy-tailed distributions over different scales reflecting the peculiar\\ndynamics of content virality. If, on the one hand, a limited number of sources suc-\\nceeded in achieving virality multiple times, it represented a unique or rare event for\\nmost of them.\\n3.4 Evaluating virality impact\\nAfter defining the procedure for detecting viral posts, we can now delve deep into our\\napproach for evaluating the impact of viral events on the performance of Facebook\\npages and YouTube channels.\\nIn terms of attention received, we quantify the performance of a given source by\\ncalculating its daily engagement. We define the Engagement Eitof a source iat day t\\nas the sum of the Total Interactions TI(as in Eq. 3, and 4) of all its jposts published\\nthat day, i.e.:\\nEit=X\\njTIijt.\\n8110100\\n1 10 100\\nViral PostsNumber of PagesPlatform\\nFacebook\\nYouTubeFig. 2 : Distributions of viral posts per source on Facebook and YouTube. Facebook\\nsample sizes: Pages = 613, Posts = 9514; YouTube sample sizes: Channels = 161,\\nVideos = 3624.\\nIn assessing the total attention a news outlet receives, we interpret higher engagement\\nas indicative of greater user attention, regardless of the number of posts published. This\\napproach is based on the premise that increasing the content volume only results in\\nheightened engagement if the content effectively captures users‚Äô attention. Conversely,\\nhigh engagement across several posts implies high users‚Äô attention, and calculating its\\naverage value would potentially underestimate this effect [14].\\nOur analysis aims to evaluate the after-effects of a viral event on the source‚Äôs\\nperformance in terms of attention caught. To inspect the attention dynamics following\\nthe event, we first assess whether the engagement received significantly changes after it.\\nTo detect a significant variation (either positive or negative) on the engagement after a\\nviral event, we use a comparative interrupted time-series (CITS) design implemented\\nusing a Bayesian structural time series model (BSTS, hereafter) [21], which has also\\nbeen used in previous research [88]. In our approach, we apply BSTS to each viral\\nevent, using time windows associated to nweeks ahead and before, with n= 2,3,4,5,6,\\nconstructed as follows: for any given n, we exclude the day of the viral post and\\ncompare the engagement trend during the nweeks following the event to the expected\\ntrend based on the nweeks preceding it ‚Äì hence, having a windows of 2 n√ó7 days ‚Äì and\\ncontrolling for the presence of other viral posts. Based on the BSTS‚Äôs results, we then\\nconduct our analysis to address our two research questions outlined in the following\\nsections, first examining the magnitude of the impact and then its temporal dynamics.\\n93.4.1 Research Question 1: Does virality induce engagement\\ngrowth?\\nOur first research question aims to assess whether - and how - a viral event increases\\nusers‚Äô attention received by the source after its occurrence. We start from the premise\\nthat each viral post can have a positive, null, or negative effect on the subsequent\\nengagement in the considered time window.\\nTo evaluate the effect, we use two variables provided by the BSTS: 1) the Average\\nAbsolute Effect, which indicates whether the impact was positive or negative, along\\nwith its magnitude, and 2) the statistical significance of the effect, which is captured\\nby the p-value pŒ±of BSTS at a confidence level Œ±. For the analysis, we set Œ±= 0.05.\\nIf the effect has no statistical significance at a confidence level Œ±, we consider that\\nvirality did not have a discernible impact on users‚Äô engagement - which we refer to as\\nNo Effect . Otherwise, if the effect is statistically significant and the observed engage-\\nment significantly deviates from the expected trend based on the examined nweeks, we\\nconsider that virality had a Growth orDecrease effect on users‚Äô attention as quantified\\nby the Average Absolute Effect. For our analyses, we use the BSTS implementation\\nfrom the CausalImpact R package [89]. This research question is addressed in Sec. 4.1.\\n3.4.2 Research Question 2: Do the faster-manifesting effects persist\\nlonger?\\nTo continue our investigation of virality dynamics, we aim to examine the relationship\\nbetween the speed at which effects manifest and their persistency over time. By using\\nincreasingly wider time windows, we can observe the effect of the same viral event\\nfrom a short-term to a long-term perspective.\\nFor each detected effect, its emergence and its subsequent persistency can be deter-\\nmined through a classification based on the output from the BSTS. For each viral post\\njof a page i, we define the time of emergence of its effect, denoted as œÑ(em)\\nij, as follows:\\nœÑ(em)\\nij = min {n= 2,3,4,5 :pij(n)< pŒ±}, (5)\\nwhere pij(n) is the p-value of BSTS applied to the time window associated to week n.\\nSimilarly, the fade-out time, œÑ(f‚àío)\\nij , is identified as the earliest time window in which\\nthe previously detected effect no longer manifests, i.e.,\\nœÑ(f‚àío)\\nij = min {n > œÑ(em)\\nij :pij(n)> pŒ±}. (6)\\nWe notice that the sets associated to the definitions of œÑ(em)\\nij andœÑ(f‚àío)\\nij are not empty\\nin the cases treated in this paper.\\nNotice also that the œÑ‚Äôs lead to a partition of the collection of the posts with the\\nrelated pages. Specifically, fixed ¬Ø n, we define\\nT(em)\\n¬Øn={(i, j)‚àà I √ó J :œÑ(em)\\nij = ¬Øn} (7)\\n10and\\nT(f‚àío)\\n¬Øn ={(i, j)‚àà I √ó J :œÑ(f‚àío)\\nij = ¬Øn}. (8)\\nWe define the persistency œïkhas the fraction of posts still having persistent effects at\\na given week hafter their emergence time k, i.e.,\\nœïkh=|T(em)\\nk| ‚àíPh\\n‚Ñì=k+1|T(f‚àío)\\n‚Ñì|\\n|T(em)\\nk|, (9)\\nwhere | ‚Ä¢ |is the cardinality of set ‚Ä¢, for each k‚àà[2,5] and h‚àà[k+ 1,6]. See Sec. 4.2\\nfor further details, in which we address the Research Question 2.\\nBy analyzing the persistency rates across all cases for each specified emergence\\ntime, we can evaluate the relationship between the speed of manifestation and the\\nlongevity of the effects. This analysis will help determine whether effects that emerge\\nmore quickly tend to persist longer or, conversely, if they vanish more rapidly than\\nslow-emerging ones.\\nThis approach helps us to understand the temporal behavior of viral effects, as we\\ncan further extrapolate the observed dynamics to their unobservable earlier phase by\\ntracing back to the time of the event and evaluating the dynamics as we approach it\\nasymptotically. This analysis provides valuable insights into how collective attention\\nresponds to sudden stress conditions, like viral news events, and to determine whether\\nthese events genuinely contribute to sustained growth or merely act as transient\\nspotlights.\\n4 Results\\nBefore discussing our research questions in depth, we first inspect the BSTS‚Äôs results,\\nwhich provide us with a useful initial overview to help delve into the analysis. We begin\\nby examining the proportion of cases that exhibit positive, null, or negative impacts\\nand assess their consistency across various time windows.\\nThe results are reported in Fig 3, which shows the percentage of Growth, No\\nEffect, and Decrease cases on the y-axis for each time scale on the x-axis. The diagram\\nincludes links representing the effects flow between consecutive weeks, color-coded\\naccording to the first observed effect to maintain a clear visual trajectory through the\\ndata stream. A significant observation from this analysis regards the high percentage\\nof cases showing no statistically significant impact on engagement performance. This\\nfirst finding suggests that virality does not necessarily enhance engagement, as it often\\nresults in an indiscernible impact.\\nSecondly, the percentages of Growth and Decrease cases are comparable, with\\nslightly higher occurrences of negative impacts. This suggests that, apart from being\\ninfrequent, the impact of a viral event on users‚Äô attention can even be detrimental.\\nContrary to common expectations [15, 90, 91], virality rarely induces engagement\\ngrowth. Furthermore, we observe that effects typically emerge or shift between the\\nsecond and fourth weeks, with the most significant transitions happening from the\\nthird to the fourth week. Beyond this period, the consistency of effects is broadly\\nstable. Given the inherently short-term nature of virality, effects observed in larger\\n11DecreaseNo EffectGrowth\\nDecreaseNo EffectGrowth\\nDecreaseNo EffectGrowth\\nDecreaseNo EffectGrowth\\nDecreaseNo EffectGrowth\\n0.000.250.500.751.00\\n2 3 4 5 6\\nWeeks from ViralityFrequencyFacebook A)\\nDecreaseNo EffectGrowth\\nDecreaseNo EffectGrowth\\nDecreaseNo EffectGrowth\\nDecreaseNo EffectGrowth\\nDecreaseNo EffectGrowth\\n0.000.250.500.751.00\\n2 3 4 5 6\\nWeeks from ViralityFrequencyYouTube B)2nd Week Effect\\nGrowth\\nNo Effect\\nDecreaseFig. 3 : Effects after virality with increasing time windows. The y-axis represents the\\npercentage of Growth, No Effect, and Decrease cases, for each time scale on the x-axis.\\nLinks represent the effects flow between consecutive weeks, color-coded according to\\nthe first observed effect (i.e., in the 2-week window).\\ntime scales, such as 5- and 6-week windows, are less likely to be directly attributable\\nto the viral event. These longer time windows are primarily aimed at assessing the\\npersistency of earlier effects, enabling us to analyze the dynamics from a short-term\\nto a long-term perspective and to distinguish the boundary between these periods.\\n4.1 RQ1: Dynamics of virality effect on Engagement\\nWe now deepen the dynamics of the virality effect on Engagement by focusing\\nexclusively on the Growth and Decrease cases, as defined in Sec. 3.4.1.\\nIn Fig. 4, we present the joint density of the slopes of the attention trends preceding\\nthe viral posts and the absolute effects on engagement after the event.\\nThe ‚ÄòTrend Pre Virality‚Äô on the y-axis represents the Œ≤1coefficient of the regression\\nestimated by the BSTS for the nweeks preceding the viral event. On the x-axis, we\\nshow the Average Absolute Effect on Engagement in the examined n-week window\\nafter virality, accounting for its previous trend.\\n12œÅœÅ = ‚àí0.92\\n‚àí0.20.00.2\\n‚àí2 ‚àí1 0 1 2\\nAverage Absolute EffectTrend Pre ViralityFacebook A)\\nœÅœÅ = ‚àí0.95\\n‚àí0.8‚àí0.40.00.40.8\\n‚àí2 ‚àí1 0 1 2\\nAverage Absolute EffectTrend Pre ViralityYouTube B) 2‚àíweek time windowFig. 4 : Density of the trend preceding the viral post and the average absolute effect\\non engagement for the 2-week time window. Trend Pre Virality is the Œ≤1 coefficient\\nof the regression estimated by the BSTS on the weeks preceding virality. Given its\\nprevious trend, the average Absolute Effect is the average effect on the Engagement\\nafter the viral event. Only events with a statistically significant effect on Engagement\\nare shown.\\nFig. 4 reports the values for the 2-week window as an example, while Fig. 6 in\\nSI Appendix shows the results for other timescales on both platforms which display\\nconsistent results.\\nAs Fig. 4 shows, the density is split into two opposing quadrants: from growth to\\ndecrease and from decrease to growth, highlighting a significant negative correlation\\nbetween the preceding trend and the consequent absolute effect. This dynamic shows\\nconsistency on both platforms and across their timescales, as shown in Tab 2, which\\nreports the Spearman‚Äôs correlation coefficients between the Œ≤1coefficient of the trend\\npreceding virality and the Average Absolute Effect on Engagement.\\nTherefore, virality positively impacts users‚Äô engagement when occurring as a sud-\\nden event on a declining collective attention. Conversely, when virality manifests\\nfollowing a sustained growth phase, it represents the final burst of that growth process,\\nwith users‚Äô attention successively standing on lower levels than its previous phase.\\nThese results shed light on the bounded yet elastic nature of collective attention.\\nWhile additional growth following a sustained growth phase is extremely rare, viral\\nevents act as a booster when users‚Äô attention is likely to nearing its lower bound,\\nreactivating the collective response process.\\nBy focusing on viral news, these results potentially indicate the presence of two\\ndifferent types of viral events. The first is a ‚Äòloaded-type‚Äô virality, where attention pro-\\ngressively increases, culminating in the viral event. This type could occur in scenarios\\nwhere information is gradually revealed, such as sequences of rumors, confirmations,\\n13Table 2 : Spearman‚Äôs correlation coeffi-\\ncients between the Œ≤1 coefficient of the\\ntrend preceding virality and the Absolute\\nEffect on Engagement for different plat-\\nforms and time windows. Posts and Videos\\nrepresent the number of events for which\\na Growth or Decrease effect is detected,\\naccording to the BSTS.\\nFacebook YouTube\\nTimescale œÅ Posts œÅ Videos\\n2 Weeks -0.92 3015 -0.95 1100\\n3 Weeks -0.91 2953 -0.93 1048\\n4 Weeks -0.90 3134 -0.93 1049\\n5 Weeks -0.88 3279 -0.93 1028\\n6 Weeks -0.87 3399 -0.93 1016\\nand official announcements. The second type represents a ‚Äòsudden-type‚Äô virality, with\\nthe news emerging unexpectedly as an exogenous event. Based on the release patterns\\nof information, this interpretation could explain the differences between the two types,\\nalong with the inverse relationship between the preceding attention pattern and the\\nafter-effect of the viral news.\\n4.2 RQ2: Emergence and persistency of the virality effect\\nAfter assessing how virality affects users‚Äô engagement and its extent, we now inspect\\nthe temporal dynamics of the effect by examining the relationship between its manifes-\\ntation and longevity. In this section, the analysis will specifically focus on the effect‚Äôs\\npersistency based on its timing of emergence and fade-out.\\nIn Eq. 5, we defined the time of œÑ(em)\\nij as the shortest time window during which\\na Growth or Decrease effect first becomes significant. For example, if the impact of a\\nviral event is significant within a 2-week time window, we assign its emergence time as\\n2. Conversely, if the impact is not significant within the 2-week window but becomes\\nsignificant in the 3-week window, the emergence time is then 3.\\nSimilarly, we examine the duration for which the effect remains unchanged and\\ndetermine the window in which it ceases to be consistent. As we defined in Eq. 6,\\nthefade-out œÑ(f‚àío)\\nij of the effect corresponds to the first window where it no longer\\nmanifests. For example, if an event consistently exhibits a Growth effect from the 2-\\nweek to the 4-week window and then ceases to manifest in the 5-week window, the\\nemergence time would be 2, and the fade-out time would be 5.\\nConsequently, as we defined in Eq. 9, the persistency œïkhis the fraction of still\\npersistent effects at hweeks after their given emergence time k, to which we generally\\nrefer here. Hence, we evaluate the effect‚Äôs decay by calculating the fraction of still\\npersistent effects after each week, grouped by their time of emergence. For instance,\\nover the total number of cases where the impact appears two weeks after virality, we\\ncalculate the proportion of persistent effects in the 3rd, 4th, 5th, and 6th week. This\\nmethod allows us to derive the effect‚Äôs decay curve for each time of emergence k‚àà[2,5].\\n14At this stage, we are able to observe the persistency rate up to a minimum of two\\nweeks after the viral event. By fitting a negative exponential function of kand power\\nlaw in h, we can estimate the variation of the exponent of the decay curves based on\\ntheir time of emergence kas:\\nœïkh=f(k, h) =h‚àíŒª+k\\nŒ≤. (10)\\nThis procedure allows us to describe the decay behavior of the effects based on\\ntheir emergence and to extend it to their unobservable earlier phases as if we approach\\nthem asymptotically.\\nThe results of the fitting procedure are reported in Tab. 3, while Fig. 5 shows the\\ngraphical representation of the decay curves for each emergence time. Solid lines repre-\\nsent the estimated decay for the observed curves - from the 2nd week to the 5th week of\\nemergence time - along with their observed persistency. The dotted lines represent the\\ncorresponding extrapolated decay curves for 0 and 1 week after virality as emergence\\ntime. The curves representing extrapolations inherently involve higher uncertainty\\nand should be considered useful asymptotic approximations for understanding the\\ndynamics during their unobservable phases.\\nTable 3 : Paramaters estimation\\nPlatform Parameter Estimate Std. error p-value\\nFacebook Œª 1.09 0.02 <0.001\\nFacebook Œ≤ 8.84 0.44 <0.001\\nYouTube Œª 1.09 0.05 <0.001\\nYouTube Œ≤ 17.44 4.55 0.0024\\n0.000.250.500.751.00\\n0 1 2 3 4 5 6\\nTimePersistencyFacebook\\n0.000.250.500.751.00\\n0 1 2 3 4 5 6\\nTimePersistencyYouTube\\nEmergence \\n Time\\n0*\\n1*\\n2\\n3\\n4\\n5\\nFig. 5 : Persistency of the virality effect based on its time of emergence. Solid lines\\nrepresent the estimated decay for the observed curves - from the 2nd week to the 5th\\nweek of emergence time - along with their observed values. The dotted lines represent\\nthe corresponding extrapolated decay curves for 0 and 1 week after virality as emer-\\ngence time.\\n15According to the estimations, in about 50% of cases, the impact either fades out\\nwithin the first week following the viral event or does not occur. Moreover, the data\\nconsistently show that earlier emergences of viral impacts are associated with faster\\ndecay across both platforms. This indicates that faster processes tend to fade quicker,\\nwhile slower ones exhibit more persistence. This finding highlights the elastic nature of\\ncollective attention when stretched to its limits. The volatile and fluctuating nature of\\nattention prevents it from being steadily focused, resulting in a trade-off between the\\nrapidity of the effect and its durability. These observations have critical implications\\nfor content producers in the digital realm, underscoring a clear distinction between\\nshort-term and long-term dynamics of collective attention.\\nFrom a probabilistic perspective, sudden and disproportionate growth is rare and\\nrarely leads to a noticeable positive impact on engagement. Even when it occurs, its\\neffects tend to fade away swiftly. Conversely, organic and sustained growth, though\\nslower to manifest, tends to have more enduring effects. This contrast emphasizes\\nthe transient nature of viral events compared to the lasting effectiveness of consistent\\nengagement, highlighting the importance of gradual and continuous attention-building\\nstrategies rather than relying on abrupt surges of visibility.\\n5 Discussion\\n5.1 Implications\\nOur findings highlight collective attention‚Äôs bounded and elastic nature under sud-\\nden stress conditions, such as viral events, and have significant implications for the\\ninformation sources ecosystem in the digital domain. Firstly, our analysis reveals that\\nviral events rarely lead to engagement growth, suggesting that the frantic pursuit of\\nsudden visibility is often unproductive. The study distinguishes between two primary\\ntypes of virality, each corresponding to a different mechanism of collective attention\\nresponse. The first type, ‚Äòloaded-type virality,‚Äô is characterized by a gradual increase\\nof engagement that culminates in a final burst of attention in the viral event. The\\nsecond type, ‚Äòsudden-type virality,‚Äô occurs when news emerges unexpectedly, similar\\nto an exogenous event, and is the only scenario in which virality leads to an increased\\nusers‚Äô response. Regardless of whether the impact is positive or negative, our findings\\nindicate that effects emerging more quickly tend to fade faster, while slower-emerging\\nprocesses are more persistent over time. As well as being an extremely rare event,\\nvirality does not turn out to be an effective long-term growth strategy. These insights\\nunderscore the advantages of organic and continued growth strategies in establishing\\na solid and enduring connection with the user base, yielding positive and sustained\\nresponses regarding collective attention.\\n5.2 Limitations and Future Research\\nThe findings of this study are specifically tied to the platforms analyzed, the type\\nof pages examined, and the period considered. Although the results are consistent\\nwith general human dynamics observed in other collective attention studies, they\\ncannot be generalized across all social media platforms. Each platform has distinct\\n16user bases and algorithmic characteristics that influence how content is spread and\\nengages users. Additionally, the specificity of the sources analyzed ‚Äî news outlets - and\\ntheir content type must be considered. Focusing exclusively on news items offers the\\nadvantage of dealing with content inherently tied to specific topics or events, providing\\na concrete subject matter. However, the dynamics and peculiarities of the news agenda\\nmean that these results may not necessarily apply to the broader spectrum of content\\ncreators. Future research should explore various social media platforms and content\\ntypes to understand better the dynamics of virality and collective attention across\\ndifferent contexts. A more comprehensive picture of how virality functions in the\\ndigital landscape can be developed by expanding the study to include a broader range\\nof content creators and user engagement patterns.\\n6 Conclusions\\nThis study offers a detailed analysis of virality dynamics on social media platforms.\\nIt examines how viral events affect users‚Äô engagement and the relationship between\\ntheir rapid emergence and subsequent persistency. Our findings challenge the common\\nassumption that virality regularly enhances user engagement. While viral events may\\nmomentarily capture attention, our evaluation reveals that they rarely foster sustained\\nengagement growth. Indeed, the impact of virality typically depends on the preceding\\ngrowth trend, with a pronounced negative correlation observed between the two. We\\ncategorize viral events into two distinct types: ‚Äòloaded-type‚Äô virality, characterized by\\na gradual build-up of attention culminating in a viral burst, and ‚Äòsudden-type‚Äô virality,\\nwhich appears unexpectedly like an exogenous shock.\\nWhen virality occurs following an ongoing growth phase, it represents its final\\npeak, resulting in diminished attention levels. In contrast, a viral event reactivates the\\ncollective response process when arising suddenly after a declining attention phase.\\nAdditionally, our study highlights the elastic nature of collective attention, demon-\\nstrating that effects emerging swiftly tend to fade quickly, whereas slower-emerging\\neffects show greater persistency. This finding highlights the critical role of content pro-\\nducers in fostering consistent, high-quality engagement rather than depending solely\\non transient viral spikes. The rapid dissipation of viral impacts, particularly those that\\nemerge quickly, illustrates the volatility and fluctuation of collective attention. This\\nvolatility implies a trade-off between the rapidity of an impact‚Äôs emergence and its last-\\ning presence, emphasizing content producers‚Äô challenges in capturing and maintaining\\nusers‚Äô engagement. In conclusion, although virality can temporarily surge users‚Äô atten-\\ntion, its effects are usually short-lived, making the frantic pursuit of sudden visibility\\nan often fruitless strategy.\\n7 Author contributions statement\\n‚Ä¢Emanuele Sangiorgio Conceptualization, Methodology, Validation, Data collec-\\ntion, Data curation, Data analysis, Visualization, Writing.\\n‚Ä¢Niccol` o Di Marco Methodology, Validation, Data analysis, Writing.\\n‚Ä¢Gabriele Etta Data collection, Data curation, Data analysis, Writing.\\n‚Ä¢Matteo Cinelli Conceptualization, Methodology, Supervision, Writing.\\n17‚Ä¢Roy Cerqueti Conceptualization, Methodology, Supervision, Writing.\\n‚Ä¢Walter Quattrociocchi Conceptualization, Supervision, Writing.\\nDeclaration of Competing Interest\\nThe authors declare no competing interest.\\n8 Data availability\\nThe data collection and analysis process are compliant with the terms and conditions\\nimposed by Crowdtangle [85]. Therefore, the results described in this paper cannot be\\nexploited to infer the identity of the accounts involved. CrowdTangle does not include\\npaid ads unless those ads began as organic, non-paid posts that were subsequently\\n‚Äúboosted‚Äù using Facebook‚Äôs advertising tools. It also does not include activity on\\nprivate accounts, or posts made visible only to specific groups of followers.\\n9 SI Appendix Section\\nReferences\\n[1] Bakshy, E., Rosenn, I., Marlow, C., Adamic, L.: The role of social networks in\\ninformation diffusion. In: Proceedings of the 21st International Conference on\\nWorld Wide Web, pp. 519‚Äì528 (2012)\\n[2] Schmidt, A.L., Zollo, F., Vicario, M.D., Bessi, A., Scala, A., Caldarelli, G.,\\nStanley, H.E., Quattrociocchi, W.: Anatomy of news consumption on facebook.\\nProceedings of the National Academy of Sciences 114(12), 3035‚Äì3039 (2017)\\n[3] Bergstr¬® om, A., Belfrage, M.J.: News in social media. Digital Journalism 6(5),\\n583‚Äì598 (2018)\\n[4] Grover, P., Kar, A.K., Dwivedi, Y.: The evolution of social media influence-\\na literature review and research agenda. International Journal of Information\\nManagement Data Insights 2(2), 100116 (2022)\\n[5] Pentina, I., Tarafdar, M.: From ‚Äúinformation‚Äù to ‚Äúknowing‚Äù: Exploring the role of\\nsocial media in contemporary news consumption. Computers in human behavior\\n35, 211‚Äì223 (2014)\\n[6] Levy, R.: Social media, news consumption, and polarization: Evidence from a\\nfield experiment. American economic review 111(3), 831‚Äì870 (2021)\\n[7] Boase, J.: Personal networks and the personal communication system: Using mul-\\ntiple media to connect. Information, communication & society 11(4), 490‚Äì508\\n(2008)\\n18[8] Bail, C.: Breaking the Social Media Prism: How to Make Our Platforms Less\\nPolarizing. Princeton University Press, Princeton (2022)\\n[9] Bawden, D., Robinson, L.: Information overload: An overview. In: Oxford\\nEncyclopedia of Political Decision Making. Oxford University Press, Oxford\\n(2020)\\n[10] Harrigan, P., Daly, T.M., Coussement, K., Lee, J.A., Soutar, G.N., Evers, U.:\\nIdentifying influencers on social media. International Journal of Information\\nManagement 56, 102246 (2021)\\n[11] Falkinger, J.: Limited attention as a scarce resource in information-rich economies.\\nThe Economic Journal 118(532), 1596‚Äì1620 (2008)\\n[12] Bell, G., Hey, T., Szalay, A.: Beyond the data deluge. Science 323(5919), 1297‚Äì\\n1298 (2009)\\n[13] Anderson, S.P., De Palma, A.: Competition for attention in the information\\n(overload) age. The RAND Journal of Economics 43(1), 1‚Äì25 (2012)\\n[14] Sangiorgio, E., Cinelli, M., Cerqueti, R., Quattrociocchi, W.: Followers do not\\ndictate the virality of news outlets on social media. PNAS Nexus 3(7), 257 (2024)\\n[15] Al-Rawi, A.: Viral news on social media. Digital journalism 7(1), 63‚Äì79 (2019)\\n[16] Scott, D.M.: The New Rules of Marketing and PR: How to Use Social Media,\\nBlogs, News Releases, Online Video, and Viral Marketing to Reach Buyers\\nDirectly. John Wiley & Sons, ??? (2009)\\n[17] Miller, R., Lammas, N.: Social media and its implications for viral marketing.\\nAsia Pacific Public Relations Journal 11(1), 1‚Äì9 (2010)\\n[18] Kaplan, A.M., Haenlein, M.: Two hearts in three-quarter time: How to waltz the\\nsocial media/viral marketing dance. Business horizons 54(3), 253‚Äì263 (2011)\\n[19] Petrescu, M., Korgaonkar, P.: Viral advertising: Definitional review and synthesis.\\nJournal of Internet Commerce 10(3), 208‚Äì226 (2011)\\n[20] Borges-Tiago, M.T., Tiago, F., Cosme, C.: Exploring users‚Äô motivations to par-\\nticipate in viral communication on social media. Journal of Business Research\\n101, 574‚Äì582 (2019)\\n[21] Scott, S.L., Varian, H.R.: Predicting the present with bayesian structural time\\nseries. International Journal of Mathematical Modelling and Numerical Optimi-\\nsation 5(1-2), 4‚Äì23 (2014)\\n[22] Simon, H., Greenberger, M.: Computers, communications and the public inter-\\nest. Computers, communications, and the public interest. Johns Hopkins Press,\\n19Baltimore, 40‚Äì41 (1971)\\n[23] Simon, H.A.: Designing organizations for an information-rich world. International\\nLibrary of Critical Writings in Economics 70, 187‚Äì202 (1996)\\n[24] Davenport, T.H., Beck, J.C.: The attention economy. Ubiquity 2001 (May), 1\\n(2001)\\n[25] Falkinger, J.: Attention economies. Journal of Economic Theory 133(1), 266‚Äì294\\n(2007)\\n[26] Treisman, A.M.: Strategies and models of selective attention. Psychological review\\n76(3), 282 (1969)\\n[27] Kahneman, D.: Attention and Effort. Prentice-Hall, ??? (1973)\\n[28] Zollo, F., Bessi, A., Del Vicario, M., Scala, A., Caldarelli, G., Shekhtman, L.,\\nHavlin, S., Quattrociocchi, W.: Debunking in a world of tribes. PloS one 12(7),\\n0181821 (2017)\\n[29] Jacoby, J.: Perspectives on information overload. Journal of consumer research\\n10(4), 432‚Äì435 (1984)\\n[30] Edmunds, A., Morris, A.: The problem of information overload in business\\norganisations: a review of the literature. International journal of information\\nmanagement 20(1), 17‚Äì28 (2000)\\n[31] White, M., Dorman, S.M.: Confronting information overload. Journal of School\\nHealth 70(4), 160‚Äì160 (2000)\\n[32] Hendricks, V.F., Vestergaard, M.: The Attention Economy, pp. 1‚Äì17. Springer,\\nCham (2019)\\n[33] Wu, F., Huberman, B.A.: Novelty and collective attention. Proceedings of the\\nNational Academy of Sciences 104(45), 17599‚Äì17601 (2007)\\n[34] Lazer, D., Pentland, A., Adamic, L., Aral, S., Barab¬¥ asi, A.-L., Brewer, D., Chris-\\ntakis, N., Contractor, N., Fowler, J., Gutmann, M., Jebara, T., King, G., Macy,\\nM., Roy, D., Alstyne, M.V.: Computational social science. Science 323(5915),\\n721‚Äì723 (2009)\\n[35] Mocanu, D., Rossi, L., Zhang, Q., Karsai, M., Quattrociocchi, W.: Collective\\nattention in the age of (mis) information. Computers in Human Behavior 51,\\n1198‚Äì1204 (2015)\\n[36] Lorenz-Spreen, P., M√∏nsted, B.M., H¬® ovel, P., Lehmann, S.: Accelerating dynamics\\nof collective attention. Nature communications 10(1), 1759 (2019)\\n20[37] Alipour, S., Di Marco, N., Avalle, M., Etta, G., Cinelli, M., Quattrociocchi, W.:\\nThe drivers of global news spreading patterns. Scientific Reports 14(1), 1519\\n(2024)\\n[38] Cinelli, M., Brugnoli, E., Schmidt, A.L., Zollo, F., Quattrociocchi, W., Scala, A.:\\nSelective exposure shapes the facebook news diet. PloS one 15(3), 0229129 (2020)\\n[39] Del Vicario, M., Bessi, A., Zollo, F., Petroni, F., Scala, A., Caldarelli, G., Stanley,\\nH.E., Quattrociocchi, W.: The spreading of misinformation online. Proceedings\\nof the national academy of Sciences 113(3), 554‚Äì559 (2016)\\n[40] Marco, N.D., Cinelli, M., Alipour, S., Quattrociocchi, W.: Users volatility on\\nreddit and voat. IEEE Transactions on Computational Social Systems, 1‚Äì9 (2024)\\n[41] Bessi, A., Coletto, M., Davidescu, G.A., Scala, A., Caldarelli, G., Quattrociocchi,\\nW.: Science vs conspiracy: Collective narratives in the age of misinformation.\\nPloS one 10(2), 0118093 (2015)\\n[42] Sultana, T., Dhillon, G., Oliveira, T.: The effect of fear and situational motivation\\non online information avoidance: The case of covid-19. International Journal of\\nInformation Management 69, 102596 (2023)\\n[43] Del Vicario, M., Vivaldo, G., Bessi, A., Zollo, F., Scala, A., Caldarelli, G., Quat-\\ntrociocchi, W.: Echo chambers: Emotional contagion and group polarization on\\nfacebook. Scientific reports 6(1), 37825 (2016)\\n[44] Terren, L.T.L., Borge-Bravo, R.B.-B.R.: Echo chambers on social media: A\\nsystematic review of the literature. Review of Communication Research 9(2021)\\n[45] Nyhan, B., Settle, J., Thorson, E., Wojcieszak, M., Barber¬¥ a, P., Chen, A.Y.,\\nAllcott, H., Brown, T., Crespo-Tenorio, A., Dimmery, D., et al. : Like-minded\\nsources on facebook are prevalent but not polarizing. Nature 620(7972), 137‚Äì144\\n(2023)\\n[46] Cinelli, M., De Francisci Morales, G., Galeazzi, A., Quattrociocchi, W., Starnini,\\nM.: The echo chamber effect on social media. Proceedings of the National\\nAcademy of Sciences 118(9), 2023301118 (2021)\\n[47] Falkenberg, M., Galeazzi, A., Torricelli, M., Di Marco, N., Larosa, F., Sas, M.,\\nMekacher, A., Pearce, W., Zollo, F., Quattrociocchi, W., et al. : Growing polar-\\nization around climate change on social media. Nature Climate Change 12(12),\\n1114‚Äì1121 (2022)\\n[48] Allcott, H., Gentzkow, M., Mason, W., Wilkins, A., Barber¬¥ a, P., Brown, T., Cis-\\nneros, J.C., Crespo-Tenorio, A., Dimmery, D., Freelon, D., et al. : The effects\\nof facebook and instagram on the 2020 election: A deactivation experiment.\\nProceedings of the National Academy of Sciences 121(21), 2321584121 (2024)\\n21[49] Flaxman, S., Goel, S., Rao, J.M.: Filter bubbles, echo chambers, and online news\\nconsumption. Public opinion quarterly 80(S1), 298‚Äì320 (2016)\\n[50] Pecile, G., Di Marco, N., Cinelli, M., Quattrociocchi, W.: Mapping the global\\nelection landscape on social media in 2024. arXiv preprint arXiv:2406.04962\\n(2024)\\n[51] Kim, A., Moravec, P.L., Dennis, A.R.: When do details matter? news source evalu-\\nation summaries and details against misinformation on social media. International\\nJournal of Information Management 72, 102666 (2023)\\n[52] Zhong, B.: Going beyond fact-checking to fight health misinformation: A multi-\\nlevel analysis of the twitter response to health news stories. International Journal\\nof Information Management 70, 102626 (2023)\\n[53] Shahbazi, M., Bunker, D.: Social media trust: Fighting misinformation in the time\\nof crisis. International Journal of Information Management 77, 102780 (2024)\\n[54] Etta, G., Cinelli, M., Di Marco, N., Avalle, M., Panconesi, A., Quattrociocchi, W.:\\nA topology-based approach for predicting toxic outcomes on twitter and youtube.\\nIEEE Transactions on Network Science and Engineering (2024)\\n[55] Flamino, J., Galeazzi, A., Feldman, S., Macy, M.W., Cross, B., Zhou, Z., Serafino,\\nM., Bovet, A., Makse, H.A., Szymanski, B.K.: Political polarization of news media\\nand influencers on twitter in the 2016 and 2020 us presidential elections. Nature\\nHuman Behaviour 7(6), 904‚Äì916 (2023)\\n[56] Alipour, S., Galeazzi, A., Sangiorgio, E., Avalle, M., Bojic, L., Cinelli, M.,\\nQuattrociocchi, W.: Cross-platform social dynamics: an analysis of chatgpt and\\ncovid-19 vaccine conversations. Scientific Reports 14(1), 2789 (2024)\\n[57] Cinelli, M., Quattrociocchi, W., Galeazzi, A., Valensise, C.M., Brugnoli, E.,\\nSchmidt, A.L., Zola, P., Zollo, F., Scala, A.: The covid-19 social media infodemic.\\nScientific reports 10(1), 1‚Äì10 (2020)\\n[58] Briand, S.C., Cinelli, M., Nguyen, T., Lewis, R., Prybylski, D., Valensise, C.M.,\\nColizza, V., Tozzi, A.E., Perra, N., Baronchelli, A., et al. : Infodemics: A new\\nchallenge for public health. Cell 184(25), 6010‚Äì6014 (2021)\\n[59] Bovet, A., Makse, H.A.: Influence of fake news in twitter during the 2016 us\\npresidential election. Nature communications 10(1), 7 (2019)\\n[60] Gonz¬¥ alez-Bail¬¥ on, S., Lazer, D., Barber¬¥ a, P., Zhang, M., Allcott, H., Brown, T.,\\nCrespo-Tenorio, A., Freelon, D., Gentzkow, M., Guess, A.M., Iyengar, S., Kim,\\nY.M., Malhotra, N., Moehler, D., Nyhan, B., Pan, J., Rivera, C.V., Settle, J.,\\nThorson, E., Tromble, R., Wilkins, A., Wojcieszak, M., Jonge, C.K., Franco, A.,\\nMason, W., Stroud, N.J., Tucker, J.A.: Asymmetric ideological segregation in\\n22exposure to political news on facebook. Science 381(6656), 392‚Äì398 (2023)\\n[61] Gonz¬¥ alez-Bail¬¥ on, S., Lelkes, Y.: Do social media undermine social cohesion? a\\ncritical review. Social Issues and Policy Review 17(1), 155‚Äì180 (2023)\\n[62] Guess, A.M., Malhotra, N., Pan, J., Barber¬¥ a, P., Allcott, H., Brown, T., Crespo-\\nTenorio, A., Dimmery, D., Freelon, D., Gentzkow, M., et al. : How do social media\\nfeed algorithms affect attitudes and behavior in an election campaign? Science\\n381(6656), 398‚Äì404 (2023)\\n[63] Guess, A.M., Malhotra, N., Pan, J., Barber¬¥ a, P., Allcott, H., Brown, T., Crespo-\\nTenorio, A., Dimmery, D., Freelon, D., Gentzkow, M., et al. : Reshares on social\\nmedia amplify political news but do not detectably affect beliefs or opinions.\\nScience 381(6656), 404‚Äì408 (2023)\\n[64] Avalle, M., Di Marco, N., Etta, G., Sangiorgio, E., Alipour, S., Bonetti, A., Alvisi,\\nL., Scala, A., Baronchelli, A., Cinelli, M., et al. : Persistent interaction patterns\\nacross social media platforms and over time. Nature 628(8008), 582‚Äì589 (2024)\\n[65] Goel, S., Anderson, A., Hofman, J., Watts, D.J.: The structural virality of online\\ndiffusion. Management Science 62(1), 180‚Äì196 (2016)\\n[66] Berger, J., Milkman, K.L.: What makes online content viral? Journal of marketing\\nresearch 49(2), 192‚Äì205 (2012)\\n[67] Rathje, S., Robertson, C., Brady, W.J., Van Bavel, J.J.: People think that social\\nmedia platforms do (but should not) amplify divisive content. Perspectives on\\nPsychological Science, 17456916231190392 (2023)\\n[68] Guerini, M., Strapparava, C., Ozbal, G.: Exploring text virality in social networks.\\nProceedings of the International AAAI Conference on Web and Social Media 5(1),\\n506‚Äì509 (2021)\\n[69] Bruni, L., Francalanci, C., Giacomazzi, P.: The role of multimedia content in\\ndetermining the virality of social media information. Information 3(3), 278‚Äì289\\n(2012)\\n[70] Deza, A., Parikh, D.: Understanding image virality. In: Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition, pp. 1818‚Äì1826 (2015)\\n[71] Ling, C., AbuHilal, I., Blackburn, J., De Cristofaro, E., Zannettou, S., Stringhini,\\nG.: Dissecting the meme magic: Understanding indicators of virality in image\\nmemes. Proceedings of the ACM on human-computer interaction 5(CSCW1),\\n1‚Äì24 (2021)\\n[72] Mukherjee, P., Dutta, S., De Bruyn, A.: Did clickbait crack the code on virality?\\nJournal of the Academy of Marketing Science 50(3), 482‚Äì502 (2022)\\n23[73] Lin, X., Wang, X.: Following too much on facebook brand page: A con-\\ncept of brand overload and its validation. International Journal of Information\\nManagement 73, 102682 (2023)\\n[74] Lance, P., Guy J, G.: From subservient chickens to brawny men: A comparison of\\nviral advertising to television advertising. Journal of interactive advertising 6(2),\\n4‚Äì33 (2006)\\n[75] Maarouf, A., Pr¬® ollochs, N., Feuerriegel, S.: The virality of hate speech on social\\nmedia. Proceedings of the ACM on Human-Computer Interaction 8(CSCW1),\\n1‚Äì22 (2024)\\n[76] Etta, G., Sangiorgio, E., Di Marco, N., Avalle, M., Scala, A., Cinelli, M., Quat-\\ntrociocchi, W.: Characterizing engagement dynamics across topics on facebook.\\nPlos one 18(6), 0286150 (2023)\\n[77] Tsugawa, S., Ohsaki, H.: On the relation between message sentiment and its\\nvirality on social media. Social network analysis and mining 7, 1‚Äì14 (2017)\\n[78] Solovev, K., Pr¬® ollochs, N.: Moral emotions shape the virality of covid-19 misin-\\nformation on social media. In: Proceedings of the ACM Web Conference 2022,\\npp. 3706‚Äì3717 (2022)\\n[79] Rathje, S., Van Bavel, J.J., Van Der Linden, S.: Out-group animosity drives\\nengagement on social media. Proceedings of the National Academy of Sciences\\n118(26), 2024292118 (2021)\\n[80] Vijaykumar, S., Jin, Y., Nowak, G.: Social media and the virality of risk: The risk\\namplification through media spread (rams) model. Journal of homeland security\\nand emergency management 12(3), 653‚Äì677 (2015)\\n[81] Puryear, C., Vandello, J.A., Gray, K.: Moral panics on social media are fueled by\\nsignals of virality. Journal of Personality and Social Psychology (2024)\\n[82] Mancini, A., Desiderio, A., Di Clemente, R., Cimini, G.: Self-induced consensus of\\nreddit users to characterise the gamestop short squeeze. Scientific reports 12(1),\\n13780 (2022)\\n[83] Campbell, B., Drake, M., Thornock, J., Twedt, B.: Earnings virality. Journal of\\nAccounting and Economics 75(1), 101517 (2023)\\n[84] NewsGuard. https://www.newsguardtech.com (2024)\\n[85] CrowdTangle. CrowdTangle Team. Facebook, Menlo Park, California, United\\nStates (2024)\\n[86] YouTube: YouTube Data API. https://developers.google.com/youtube/v3\\n(2024)\\n24[87] Elmas, T., Stephane, S., Houssiaux, C.: Measuring and detecting virality on social\\nmedia: the case of twitter‚Äôs viral tweets topic. In: Companion Proceedings of the\\nACM Web Conference 2023, pp. 314‚Äì317 (2023)\\n[88] Trujillo, A., Cresci, S.: Make reddit great again: assessing community effects of\\nmoderation interventions on r/the donald. Proceedings of the ACM on Human-\\ncomputer Interaction 6(CSCW2), 1‚Äì28 (2022)\\n[89] Brodersen, K.H., Gallusser, F., Koehler, J., Remy, N., Scott, S.L.: Inferring causal\\nimpact using Bayesian structural time-series models. The Annals of Applied\\nStatistics 9(1), 247‚Äì274 (2015)\\n[90] Tellis, G.J., MacInnis, D.J., Tirunillai, S., Zhang, Y.: What drives virality (shar-\\ning) of online digital content? the critical role of information, emotion, and brand\\nprominence. Journal of marketing 83(4), 1‚Äì20 (2019)\\n[91] Malodia, S., Dhir, A., Bilgihan, A., Sinha, P., Tikoo, T.: Meme marketing:\\nHow can marketers drive better engagement using viral memes? Psychology &\\nMarketing 39(9), 1775‚Äì1801 (2022)\\n25‚àí0.2‚àí0.10.00.10.2\\n‚àí2 ‚àí1 0 1 2\\n Trend Pre Virality\\n‚àí0.50‚àí0.250.000.250.50\\n‚àí2‚àí1 012\\n  3 Weeks\\n‚àí0.2‚àí0.10.00.10.2\\n‚àí2 ‚àí1 0 1 2\\n Trend Pre Virality\\n‚àí0.4‚àí0.20.00.20.4\\n‚àí2‚àí1 012\\n  4 Weeks\\n‚àí0.10‚àí0.050.000.050.10\\n‚àí2 ‚àí1 0 1 2\\n Trend Pre Virality\\n‚àí0.20.00.2\\n‚àí2‚àí1 012\\n  5 Weeks\\n‚àí0.10‚àí0.050.000.050.10\\n‚àí2 ‚àí1 0 1 2\\nAverage Absolute EffectTrend Pre Virality\\n‚àí0.2‚àí0.10.00.10.2\\n‚àí2‚àí1 012\\nAverage Absolute Effect 6 WeeksFacebook                                                    YouTubeFig. 6 : Density of the trend preceding the viral post and the average absolute effect on\\nengagement for the 3, 4, 5, and 6-week timescales. We recall that Trend Pre Virality\\nis the Œ≤1 coefficient of the regression estimated by the BSTS on the weeks preceding\\nvirality, and that the Average Absolute Effect is the average effect on the Engagement\\nafter virality estimated by the BSTS, given its previous trend. Only events with a\\nstatistically significant effect on engagement are shown.26',\n",
       " 'Political Elites in the Attention Economy: Visibility Over Civility and Credibility?\\nAhana Biswas1, Yu-Ru Lin1*, Yuehong Cassandra Tai2, Bruce A. Desmarais2\\n1University of Pittsburgh\\n2Pennsylvania State University\\nahana.biswas@pitt.edu, yurulin@pitt.edu, yhcasstai@psu.edu, bdesmarais@psu.edu\\nAbstract\\nElected officials have privileged roles in public communica-\\ntion. In contrast to national politicians, whose posting content\\nis more likely to be closely scrutinized by a robust ecosystem\\nof nationally focused media outlets, sub-national politicians\\nare more likely to openly disseminate harmful content with\\nlimited media scrutiny. In this paper, we analyze the factors\\nthat explain the online visibility of over 6.5K unique state leg-\\nislators in the US and how their visibility might be impacted\\nby posting low-credibility or uncivil content. We conducted a\\nstudy of posting on Twitter and Facebook (FB) during 2020-\\n21 to analyze how legislators engage with users on these plat-\\nforms. The results indicate that distributing content with low-\\ncredibility information attracts greater attention from users on\\nFB and Twitter for Republicans. Conversely, posting content\\nthat is considered uncivil on Twitter receives less attention.\\nA noticeable scarcity of posts containing uncivil content was\\nobserved on FB, which may be attributed to the different com-\\nmunication patterns of legislators on these platforms. In most\\ncases, the effect is more pronounced among the most ideolog-\\nically extreme legislators. Our research explores the influence\\nexerted by state legislators on online political conversations,\\nwith Twitter and FB serving as case studies. Furthermore, it\\nsheds light on the differences in the conduct of political ac-\\ntors on these platforms. This study contributes to a better un-\\nderstanding of the role that political figures play in shaping\\nonline political discourse.\\n1 Introduction\\nSocial media channels are effective for communication due\\nto the ease of information dissemination irrespective of the\\nquality of the information. Politicians also leverage social\\nmedia due to the wide reach of these platforms. While pub-\\nlic officials can promote constructive dialogue, they can also\\nspread harmful content online. Government officials are of-\\nten subjected to less stringent content moderation rules (Pel-\\nletier et al. 2021), and have higher followings than average\\nusers (Grant et al. 2010) which makes it easier for them to\\nendorse and propagate harmful content online.\\nPolitical communication online is often geared toward the\\ntarget audience and platform characteristics (Kreiss 2016;\\nEnli and Skogerb√∏ 2013; Stier et al. 2020; Kelm 2020). For\\n*Corresponding author.\\nCopyright ¬© 2022, Association for the Advancement of Artificial\\nIntelligence (www.aaai.org). All rights reserved.instance, Stier et al. (2020) found that social media messages\\nof both candidates and their audiences focused on distinct\\ntopics compared to the general audience during 2013 Ger-\\nman federal elections. More broadly, politicians often tailor\\ntheir content to achieve maximum political gains, and prior\\nstudies have shown that social media plays a significant role\\nin shaping political agendas, influencing public opinion, and\\npotentially affecting electoral outcomes (Kreiss 2016; Bovet\\net al. 2019; Boulianne and Larsson 2023).\\nTo understand how politicians use social media to sway\\npublic opinion, it is critical to examine what makes them vis-\\nible online. In this study, we use the term visibility to refer to\\nthe level of public engagement or interactions individuals re-\\nceive on social media, which is often utilized to gauge their\\noutreach and influence on these platforms (Eberl et al. 2020)\\n. Party membership, demographic information (such as state,\\nrace, and gender), and platform-level characteristics (such as\\nfollower count, posting activity, and content style) are poten-\\ntial contributors. We are particularly interested in determin-\\ning if politicians‚Äô visibility is increased by the dissemina-\\ntion of harmful content, which includes toxic and uncivil\\nlanguage, as well as untrustworthy or non-credible infor-\\nmation. Both uncivil and low-credibility content have been\\nassociated with a decline in the quality of democratic dis-\\ncourse (Goovaerts et al. 2020; Bennett et al. 2018). We ask,\\ndoes posting uncivil and low-credibility content increase the\\nvisibility of politicians? The answer to this question is criti-\\ncal as prior research has linked harmful content online to vi-\\nolent offline incidents, increased affective polarization, dis-\\ntrust in institutions, and so on (Johnson 2018; Serrano-Puche\\n2021; Coe et al. 2014). Harmful content originating from or\\nendorsed by politicians may further exacerbate these nega-\\ntive outcomes due to their larger audience base and higher\\ntrustability owing to partisan preferences.\\nThis work focuses on how US state legislators cultivate\\nand exert their influence online. In contrast to national politi-\\ncians, who are closely scrutinized by media outlets (Kyriaki-\\ndou et al. 2021), sub-national politicians are more likely to\\ndisseminate harmful content with limited scrutiny (Mihai-\\nlidis et al. 2021). State legislators are responsible for laws\\nacross all policy areas within state jurisdiction, making their\\nrole crucial in the U.S. political system. Given the limited\\nmedia coverage of state legislators (Squire et al. 2019), these\\nsocial media platforms serve as important mediums for com-arXiv:2407.16014v1  [cs.SI]  22 Jul 2024municating their ideological and political positions to their\\nvoters.\\nWe study the dynamics of legislators‚Äô visibility, examin-\\ning the different factors that influenced the attention they re-\\nceived on Twitter and Facebook (FB) during the two-year\\nperiod spanning 2020-2021. We focus on these two years\\nowing to the surge of harmful content online due to signifi-\\ncant events such as the US Presidential elections, COVID-\\n19, Capitol Riots, and BLM protest movements (Ferrara\\net al. 2020; Cuan-Baltazar et al. 2020; Toraman et al. 2022).\\nStudying the visibility dynamics over a time period has cer-\\ntain challenges. Apart from individual attributes (e.g., post-\\ning frequency, party, demographics) or volume of harmful\\ncontent posted, the politician‚Äôs visibility may also vary by\\ntime (e.g., during elections) or due to the particular topics\\nthey post. We tackle these challenges in our study, our main\\ncontributions are as follows :\\n‚Ä¢Factors Associated with Visibility. We present a large-\\nscale, longitudinal study on political elites‚Äô online visibil-\\nity in the US by comparing differences in their platform\\nvisibility based on party, socio-demographic factors, and\\nposting activity (RQ1, RQ2; See Section 3) . Republi-\\ncans and men tend to have a higher level of visibility on\\nFB, while Democrats tend to have higher visibility on\\nTwitter. Posting uncivil content on Twitter and similarly\\nlow-credibility content on FB is also correlated with their\\nplatform visibility. Legislators‚Äô visibility on posting low-\\ncredibility content, however, varies by party on Twitter.\\nOur thorough analysis of legislators who post on both\\nplatforms reveals notable platform differences associated\\nwith their social media activities.\\n‚Ä¢Methodological Contribution. We conduct a causal in-\\nference study to examine how legislators‚Äô social media\\nposts affect their visibility, particularly when the content\\nis uncivil or less credible (RQ3; See Section 3) . To en-\\nsure that our findings are not influenced by potential con-\\nfounding factors, such as temporal and topical correla-\\ntions that are common in dynamic text data and can bias\\nthe results, we leverage deep learning of potential out-\\ncome and matching techniques. Our analytical method\\nhelps disambiguate the effect of posting activities.\\n‚Ä¢Impact of Harmful Content. The results, based on ob-\\nservational data using causal inference (RQ3; See Sec-\\ntion 3) , have revealed significant and novel patterns. Our\\nstudy found that posting uncivil content on Twitter led to\\na decrease in visibility. It was observed that Republicans\\nposting low-credibility content on both platforms have an\\nincreased visibility, while Democrats posting the same\\nhave lower visibility. The effect is more pronounced for\\nideologically extreme legislators in most cases. Overall,\\nour findings contribute to the understanding of politi-\\ncians‚Äô online visibility, shedding light on cross-platform\\ndifferences and partisan asymmetries.\\n2 Related Work\\nPolitical Elites‚Äô Online Behaviors. Social media are used\\nby politicians for both broadcasting as well as having dia-\\nlogue with voters. The effect of Twitter and Facebook useon election campaigns has been studied extensively (Kreiss\\n2016; Jungherr 2016; Boulianne and Larsson 2023; Sahly\\net al. 2019). Kreiss (2016) looked at how Twitter was used\\nby political party staffers to shape the perspectives of jour-\\nnalists and influence dedicated voters. V oters engaging in\\npolitical discussion online have demonstrated increased in-\\nterest and engagement in political affairs (Bode et al. 2016).\\nSocial media, thus, serves as a powerful tool for politicians\\nto influence public opinion and/or convey their stance re-\\ngarding several important issues. Despite a large body of\\nwork on political communication on social media, there is\\nno clear understanding of which factors influence the on-\\nline visibility of legislators, especially, how politicians post-\\ningharmful content is viewed by the audience. Our research\\naims to close this gap by examining the impact of uncivil\\nandlow-credibility content on legislators‚Äô visibility by per-\\nforming a cross-platform study‚Äîafter accounting for sev-\\neral confounding factors related to their personal attributes,\\ntemporal and topical variations.\\nMisinformation and Virality. There exists a large body\\nof literature characterizing the diffusion of low-credibility\\ncontent online (V osoughi et al. 2018; Friggeri et al. 2014;\\nZollo et al. 2015). V osoughi et al. (2018) found that false-\\nhoods spread significantly faster, and reached a broader au-\\ndience as compared to true news on Twitter. Friggeri et al.\\n(2014) found that rumor cascades on FB tend to penetrate\\ndeeper into the social network compared to general reshare\\ncascades. Prior works suggest that the sentiment towards\\nmisinformation is primarily negative which could be respon-\\nsible for the variations seen in the diffusion (V osoughi et al.\\n2018; Zollo et al. 2015).\\nA significant body of research has examined the impact of\\nmisinformation on the 2016 and 2020 US presidential elec-\\ntions (Bovet et al. 2019; Pennycook and Rand 2021). Prior\\nresearch has also shown that online misinformation tends to\\nbe directed more frequently toward conservative users (Rao\\net al. 2022; Yang et al. 2023) making them more likely to en-\\ngage in misinformation. Misinformation may have a higher\\nreach on social media platforms which could be leveraged\\nby politicians to gain visibility, and the extent may vary\\nacross ideologies. However, the question of how misinfor-\\nmation originating from public figures is reacted by audi-\\nences is less explored. In this work, we aim to illuminate\\nthe impact of posting low-credibility content on legislators‚Äô\\nvisibility.\\nThe Attention Economy and Toxic or Controversial Con-\\ntent. There is no clear consensus on how uncivil content\\nspreads on online platforms (Shmargad et al. 2022; Gervais\\n2015). Prior works have studied the nature of incivility in on-\\nline political communication suggesting that engaging in un-\\ncivil discourse may have certain benefits for politicians such\\nas political opinion polarization (Bodrunova and Blekanov\\n2021) or empowerment by voicing criticism against author-\\nities (Bodrunova et al. 2021). Uncivil content was found to\\nbe associated with emotionally loaded language which gen-\\nerated strong responses from the audiences (Mutz 2007). Ir-\\nrespective of the kind of response, this may lead to higher\\nvisibility. Coe et al. (2014) found that uncivil commentson news websites received more negative reactions. Thus,\\neven though engaging in uncivil discourse may have cer-\\ntain political benefits, it is unclear how that is perceived by\\naudiences‚Äîa gap that we address in this study.\\nConfounding with Textual Data. Causal inference with\\ntext data is particularly challenging since the assumptions of\\ncausal inference (positivity, conditional ignorability, consis-\\ntency) may not hold when confounding, treatment or out-\\ncomes are encoded in text (Feder et al. 2022). For instance,\\nposts on certain topics may be more likely to contain mis-\\ninformation and also receive higher engagement from the\\naudience. Prior works on extracting confounding from text\\nhave utilized unsupervised dimensionality reduction meth-\\nods (Roberts et al. 2020; Sridhar and Getoor 2019). Re-\\ncent works leverage neural networks to automatically ex-\\ntract features especially when the confounders in text are\\nnot explicitly known (Koch et al. 2021). To address this\\nproblem, some works have added transformer layers for text\\nprocessing to TARNet or Dragonnet (Veitch et al. 2020;\\nPryzant et al. 2020). We have extended the state-of-the-art\\ntechniques to address the confounding factors that are com-\\nmonly seen in dynamic social media content, such as textual\\nand temporal correlations due to similar topics, events, and\\npersonal attributes. To generate content representations, we\\nleverage contextual RoBERTa embeddings (Liu et al. 2019)\\nwith other post attributes. We then utilize a fine-tuned Drag-\\nonnet model to produce content embeddings that isolate the\\nconfounding factors.\\n3 Study Design\\nIt is crucial to understand how politicians develop influence\\nthrough online media and factors associated with the in-\\nfluence. This work focuses on state legislators specifically\\nsince they may be more likely than national-level politicians\\nto disseminate harmful content owing to limited scrutiny. We\\nask the following research questions (RQs):\\nRQ1 How does the legislators‚Äô visibility, as measured by the\\nattention they receive, vary based on party affiliation, and\\nindividual attributes such as gender, ethnicity, state rep-\\nresentation, and social media activity?\\nRQ2 What attributes of legislators and their posts are associ-\\nated with their visibility?\\nRQ3 How does low-credibility or incivility impact the visibil-\\nity of legislators‚Äô posts?\\nIn RQ1 we analyze whether the attention received by leg-\\nislators varies based on party affiliation, basic demograph-\\nics, and posting activity. RQ2 examines what characteristics\\nof legislators are most strongly correlated with their online\\nvisibility change. RQ1 and RQ2 characterize how visibility\\nvaries by legislators‚Äô attributes and provide an understand-\\ning of factors that may potentially impact their visibility dy-\\nnamics at the account level. To examine the impact of post-\\ning harmful content, RQ3 investigates how low-credibility\\nand incivility impacts the attention received by individual\\nposts. More specifically, we study whether low-credibility\\nor incivility increases or decreases a post‚Äôs visibility where\\nvisibility is measured in terms of expected interaction rate.Since the user attention on social media platforms may vary\\nby legislators‚Äô attributes (RQ1), and there may be other fac-\\ntors associated with the visibility (RQ2), in RQ3 we estimate\\nthe impact of incivility or low-credibility by controlling for\\nthese variables as well as temporal and topical variations.\\n3.1 Datasets\\nWe collect Twitter and FB posts from all US state represen-\\ntatives and senators who held office during 2020-2021 (i.e.,\\neach legislator has been in office at least for a certain time\\nbetween 2020 and 2021) . We focus on Twitter and FB due\\nto the vast amounts1of content produced by legislators on\\nthese platforms. Of the 8,028 legislators, 5,712 (64%) leg-\\nislators, comprising 2,943 (61%) Democrats, 2,740 (48.2%)\\nRepublicans, and 29 Independents had at least one Twitter\\naccount (Kim et al. 2022). For FB, 5,1472(64.1%) legis-\\nlators, comprising 2,215 (48.2%) Democrats, 2,515 (56.0%)\\nRepublicans, and 418 other party members, had either an of-\\nficial or a campaign account. We collect all their Twitter and\\nFB posts during 2020-21. It is important to note that many\\nof these accounts were either dormant or inaccessible during\\nour data collection period3.\\nTwitter. Our Twitter dataset4comprises around 4M tweets\\nposted by 3,551 (44.2%) US state legislators during 2020-\\n2021. The coverage of state legislators ranges from 29.4 to\\n96.5%, with a mean of 71.6%. This suggests that our dataset\\ncomprises a representative legislator population across\\nmost US states. We only analyze tweets by Democrats and\\nRepublicans due to the insufficient number of posts from\\nIndependent legislators (N=29). We calculate the interaction\\na tweet receives as the sum of likes, retweets, replies, and\\nquotes. Table 1 shows the number of legislators, posts, and\\nmedian5interactions on post. Democrats are more active\\nand receive higher engagement on posts as compared to\\nRepublicans. The comparatively higher volume of posts by\\nDemocrats indicates that Twitter is a more preferred com-\\nmunication platform for them compared to Republicans. We\\nalso construct the intra-legislator follower network, where a\\ndirected edge from legislator A‚ÜíBindicates Afollows B.\\nFB.We collect all FB posts6by US state legislators during\\n2020-21. This yields over 493K posts from 5,147 (64.1%)\\nlegislators. The coverage of state legislators ranges from\\n23.5 to 80.6%, with a mean of 57.7%. We similarly focus\\nonly on posts from Republicans and Democrats due to a\\nsmall number of posts from other party members. We use the\\ninteraction metric returned by the CrowdTangle API which\\nis the sum of all reactions (‚ÄòLikes‚Äô, ‚ÄòLove‚Äô, ‚ÄòWow‚Äô, ‚ÄòHaha‚Äô,\\n‚ÄòSad‚Äô, ‚ÄòAngry‚Äô, ‚ÄòCare‚Äô), comments, and shares for a post.\\nTable 1 shows the number of legislators, posts, and median\\n1https://www.pewresearch.org/internet/2020/07/16/congress-\\nsoars-to-new-heights-on-social-media/\\n2FB account information was crawled from Ballotpedia\\n3See Appendix for further details on data collection\\n4Collected using Twitter API v2 before March 2023\\n5The interactions received on posts have a skewed distribution,\\nso we report the median instead of mean.\\n6Collected using CrowdTangle APITable 1: Descriptive statistics for Twitter and FB datasets\\nshowing the number of legislators, posts, and median inter-\\nactions received per post by party.\\nTwitter FB\\nparty #users #tweets Int #users #posts Int\\nDem 1677 2.25M 8.0 2211 224K 58.0\\nRep 1412 889K 6.0 2501 218K 101.0\\ninteractions on post. The posting frequency is similar for\\nDemocrats and Republicans on FB unlike on Twitter. Inter-\\nestingly, Republicans receive almost double the interaction\\nas compared to Democrats, suggesting that Republicans may\\nhave a larger audience base and hence a larger reach on FB.\\nWe do not use the follower count for FB data since some\\nof the legislator accounts are official accounts while others\\nare Pages, due to which the follower counts across these dif-\\nferent account types vary widely. We are unable to analyze\\nFB‚Äôs network data due to the lack of access.\\n3.2 Individuals‚Äô attributes\\nWe characterize legislators based on their platform pres-\\nence and individual-level characteristics. For Twitter-based\\nattributes, we include their post count, follower count, and\\nin-degree centrality in the intra-legislator follower network.\\nThe post count serves as a proxy for measuring how ac-\\ntively the legislators use the platform and follower count\\nindicates the size of their audience base. Both post and fol-\\nlower counts are likely to have an impact on online visibil-\\nity (Hasan et al. 2022). Legislator‚Äôs position in their peer net-\\nwork may also impact their visibility (larger follower base,\\ngreater potential for content virality, or seniority). To gauge\\nthe influence of the legislators among their peers concern-\\ning the immediate connections, we leverage the in-degree\\ncentrality measure. For FB-based attributes, we include the\\npost count. The individual-level attributes include party af-\\nfiliation (Republican vs. Democratic), state, gender (Men\\nvs. Women), ethnicity (White vs. Non-White), and ideology\\nscores. We leverage the ideology scores constructed by Shor\\nand McCarty (2011). Around 99.7% (N=3074) and 70.2%7\\n(N=3306) of legislators are mapped8to their attributes on\\nTwitter and FB, and only those legislators with attributes are\\nanalyzed in our study . The final dataset comprises around\\n62.2% and 53.8% White, and 68.2% and 67.7% men on\\nTwitter and FB respectively, indicating that men and White\\nlegislators outnumber women and Non-Whites respectively.\\nThere are 2,131 (26.5%) overlapping users (OL) (i.e., legis-\\nlators having accounts on both Twitter and FB) across Twit-\\nter and FB. Figure 8 in Appendix shows the breakdown of\\nethnicity and gender by party and for OL. Republicans have\\nfewer women users compared to Democrats for both Twitter\\nand FB and both platforms have more Republican men. The\\nrepresentation of Non-White Republicans is higher on FB\\nthan on Twitter.\\n7See Data Collection section in Appendix\\n8Ethnicity and gender are mapped using Ballotpedia. Binary\\ngenders are used due to insufficient data about non-binary genders.4 Measuring Posts‚Äô Civility, Credibility, and\\nLegislators‚Äô Visibility\\n4.1 Assessing posts‚Äô civility\\nWe assess the civility of a post based on the toxicity of their\\nlanguage which is a common practice in literature (Frimer\\net al. 2023; Kim et al. 2021). Incivility in the context of\\npolitical speech is commonly associated with rudeness ac-\\ncording to the study by Stryker et al. (2016). We follow the\\ndefinition of toxic language provided by Google Perspec-\\ntive9:‚Äúrude, disrespectful, or unreasonable comment that\\nis likely to make someone leave discussion‚Äù . Based on this\\ndefinition, it would be reasonable to assume that the toxicity\\nclassifier is able to identify the markers of political incivil-\\nity. We determine the level of toxicity using the ‚Äúoriginal‚Äù\\nmodel10from Detoxify11(Hanu and Unitary team 2020). We\\nchoose the cutoff for toxicity score as 0.82 based on manual\\nannotation (see Appendix), i.e., posts having a score above\\n0.82 are considered uncivil12. This yields 24,242 (0.8%) and\\n277 (See Appendix) uncivil posts on Twitter and FB.\\nTable 2 shows the number of legislators posting uncivil\\ncontent, posts, and median interactions received, by party.\\nThis may be attributed to politicians‚Äô different communica-\\ntion styles across these platforms as observed in prior re-\\nsearch, suggesting FB is used more for broadcasting pur-\\nposes compared to Twitter which is leveraged more for hav-\\ning dialogue (Enli and Skogerb√∏ 2013). For our analysis on\\nuncivil content in the following sections, we only focus on\\nTwitter owing to the small number of uncivil posts on FB.\\nAround 47.1% of Democrats and 32.6% Republican legisla-\\ntors post uncivil content on Twitter. Democrats post almost\\ndouble the number of uncivil content compared to Republi-\\ncans (on average) on Twitter. This could be due to the higher\\ninteraction received on such posts by Democratic legisla-\\ntors. The interaction on uncivil content is higher compared\\nto the baseline interaction (in Table 1) for both parties, with\\na larger difference for Democrats.\\nFigure 1A shows the rate of uncivil posts across years by\\nparty and platform. More uncivil content was posted dur-\\ning 2020 on Twitter, with Democrats having a higher rate\\nof posting uncivil content. Figure 2A shows the rate of un-\\ncivil posts across states by party and by platform. Interest-\\ningly, we find that Republicans posted more uncivil content\\non FB compared to Democrats across almost all the states,\\nbut state-wise differences exist for Twitter.\\n4.2 Assessing posts‚Äô credibility\\nWe identify low-credibility content based on the credibility\\nof URL in the post, which is a common practice in liter-\\nature (Lasser et al. 2022). It is important to note that this\\n9https://perspectiveapi.com/how-it-works/\\n10The ‚Äúoriginal‚Äù model had the best performance when evalu-\\nated against our manual annotation labels compared to the other\\nDetoxify models, namely, ‚Äúdebiased‚Äù and ‚Äúmultilingual‚Äù.\\n11We choose Detoxify over Google Perspective API since\\nDetoxify has better or comparable performance (Arhin et al. 2021)\\nand is faster\\n12See Appendix for examples of uncivil postsTable 2: Number of uncivil posts, legislators posting, and\\nmedian interactions received per post on uncivil content on\\nTwitter and FB, by party.\\nTwitter FB\\nparty #users #tweets (%) Int #users #posts (%) Int\\nDem 782 18111 (0.9%) 14.0 55 89 (0.1%) 131\\nRep 461 6131 (0.7%) 7.0 54 188 (0.1%) 120\\nparty Rep DemFB Twitter\\n2020 2021 2020 20210.000.250.500.75\\nyearA. Incivility (%)FB Twitter\\n2020 2021 2020 20210.00.51.01.5\\nyearB. Low-Credibility (%)\\nFigure 1: Percentage of uncivil and low-credibility posts\\nacross years, by party, by platform. Republicans have a\\nhigher rate of posting low-credibility content on both plat-\\nforms and across years. The yearly posting rates of uncivil\\ncontent are comparable across parties.\\nmethod of identifying low-credibility posts does not dis-\\ncriminate between posts endorsing or debunking such in-\\nformation. In this study, we are interested in looking at the\\nvisibility of both, i.e., posts promoting or debunking low-\\ncredibility information since both types of posts are expos-\\ning the audience to harmful information. Prior research has\\nshown that exposure to misinformation has an impact on be-\\nlieving and subsequent sharing of such information (Halpern\\net al. 2019). Thus, irrespective of the author‚Äôs intent, the au-\\ndience may be susceptible to believing and/or sharing such\\nlow-credibility content. Moreover, the URL sharing patterns\\nare similar for Democrats and Republicans on both Twitter\\nand FB13, so it is unlikely that any biases are introduced in\\nthe study due to our method of using URLs to identify low-\\ncredibility posts. We use the low-fact URL domain refer-\\nences provided by Tai et al. (2023) to identify low-credibility\\nposts. Tai et al. (2023) refined Media Bias/Fact Check‚Äôs14\\n(MBFC) original ratings to include URLs that contain mis-\\nleading information and not just politically biased ones. This\\nrestrictive framing may undermine the true scale of misin-\\nformation on these platforms but it offers a higher preci-\\nsion (vs. recall) in identifying low-credibility content. This\\nyields 6,848 (0.2%) and 4,141 (1.0%) low-credibility posts\\non Twitter and FB respectively suggesting that legislators\\npost more low-credibility content on FB.\\nTable 3 shows the number of legislators posting low-\\n13Around 19.2% posts by Republicans and 17.4% by Democrats\\ncontain URLs on Twitter, and 14.6% and 18.3% posts by Republi-\\ncans and Democrats on FB contain URLs.\\n14https://mediabiasfactcheck.com/\\nFB Twitter\\n0 1 2 3 4 0.0 0.5 1.0 1.52.0WYWVWIWAVTVAUTTXTNSDSCRIPAOROKOHNYNVNMNJNHNENDNCMTMSMOMNMIMEMDMALAKYKSINILIDIAHIGAFLDECTCOCAAZARALAK\\n(%)A. IncivilityFB Twitter\\n0 5 10 0 1 2 3 4 5WYWVWIWAVTVAUTTXTNSDSCRIPAOROKOHNYNVNMNJNHNENDNCMTMSMOMNMIMEMDMALAKYKSINILIDIAHIGAFLDECTCOCAAZARALAK\\n(%)B. Low-CredibilityDem Rep partyFigure 2: Percentage of uncivil and low-credibility posts\\nfrom states, by party, by platform. Republicans have a\\nhigher rate of posting low-credibility content across all\\nstates, on both platforms. Moreover, Republican legislators\\nfrom most states have a higher rate of posting uncivil con-\\ntent on FB. State and party-wise differences exist for posting\\nrates of uncivil content on Twitter. Note that the x-axis scale\\nhas been adjusted to better visualize party differences.\\ncredibility content, posts, and median interactions received,\\nby party. Around 5.2% of Democrats and 36.7% Republi-\\ncan legislators post low-credibility on FB. On Twitter, only\\na handful of accounts are responsible for spreading low-\\ncredibility content across both parties (1.1% for Democrats\\nand 2.8% for Republicans). Republicans post more low-\\ncredibility content compared to Democrats on both plat-\\nforms. Similar to uncivil content, posts containing low-fact\\nURLs receive higher interaction except for Democrats on\\nTwitter. The median interaction for low-credibility content\\nis almost three times on Twitter and double on FB for Re-\\npublicans compared to their baseline interaction15.\\nFigure 1B shows the rate of low-credibility posts across\\nyears by party and platform. The prevalence was higher dur-\\ning 2021 on both platforms and it was mainly driven by Re-\\npublicans. Figure 2B shows the rate of low-credibility posts\\nacross states by party and by platform. Low-credibility con-\\ntent is driven by Republicans across all the states, with the\\nhighest rate from Arizona, on both Twitter and FB. The low-\\ncredibility information from Arizona is mostly related to the\\n15For Twitter, the median interaction on posts with and with-\\nout URLs are 9.0 and 7.0 respectively whereas for FB, the median\\ninteractions are 62.0 and 81.0, i.e., there is no clear pattern as to\\nwhether having a URL increases or decreases the interaction of\\nposts, suggesting that the results in Table 3 could potentially be\\ndue to the low-credibility of URLs.Table 3: Number of posts containing low-fact URLs, legis-\\nlators posting, and median interactions received per post on\\nlow-credibility content on Twitter and FB, by party.\\nTwitter FB\\nparty #users #tweets (%) Int #users #posts (%) Int\\nDem 19 188 (0.0%) 4.0 83 114 (0.1%) 78.5\\nRep 40 6660 (0.8%) 20.0 567 4027 (2.6%) 239.0\\n2020 US Presidential elections. In particular, we find that\\nsome Republican legislators frequently shared posts from\\nunreliable information outlets, especially during Arizona‚Äôs\\naudit of the 2020 election, which contributed to a significant\\namount of low-credibility posts from Arizona.\\nThe differences in interactions received by low-credibility\\nand uncivil posts may be attributed to the differences in post\\ntopics, timing, or attributes of authors‚Äîwe address this in\\nthe following sections.\\n4.3 Measuring legislator‚Äôs visibility\\nSocial media is being increasingly used as a tool by politi-\\ncians to enhance their visibility and outreach to the public\\n(Bahramirad 2022). The measure of a legislator‚Äôs visibility\\nis typically based on the interactions received on their posts.\\nOur approach to measuring visibility is inspired by the met-\\nrics used on Twitter and Facebook to calculate a post‚Äôs ex-\\npected engagement or virality potential (Twitter-team 2024;\\nCrowdtangle 2024). Our objective is to measure the overall\\noutreach of a post or legislator on the platform. Therefore,\\nwe consider all the interactions received on a post or by leg-\\nislators instead of focusing on a single type of engagement\\nsuch as ‚ÄúLikes.‚Äù It‚Äôs important to note that the visibility met-\\nrics are platform-specific and may not be comparable across\\nplatforms even if they share the same names. For instance,\\naudiences may engage with ‚ÄúLike‚Äù feature on Twitter dif-\\nferently than ‚ÄúLike‚Äù on Facebook due to different interface\\ndesigns (see details in Appendix). However, our visibility\\nmetrics are designed to capture the overall level of engage-\\nment a post or legislator receives on a specific platform.\\nMoreover, we do not distinguish between positive and\\nnegative reactions received from the audience (e.g., ‚Äòlove‚Äô\\nvs. ‚Äòangry‚Äô on FB). We are concerned with the visibility of\\nthe legislators and both positive and negative reactions con-\\ntribute to their overall outreach on the platform. As shown in\\nTables 2 and 3, the interactions received on low-credibility\\nand uncivil content are noticeably different from the over-\\nall interactions received by legislators, suggesting that audi-\\nences‚Äô reactions differ based on content (or other related fac-\\ntors). Thus, using interactions as a proxy to measure legisla-\\ntors‚Äô online visibility allows us to capture these differences\\nand get insights into factors contributing to their visibility.\\nThe visibility can be measured at both account and post\\nlevel. For post level, the visibility of ithpost by legislator u,\\nvuiis simply the interaction received on that post. Further-\\nmore, we examine how visibility changes in relation to other\\nvariables. Instead of measuring aggregated interactions over\\nthe audience of their posts, we measure the legislator‚Äôs vis-\\nibility ( V) by interaction rate, i.e., the number of interac-tions per post or audience size. We consider three different\\nways to adjust the quantity of the interaction rate, since plat-\\nform reach tends to be correlated with the number of follow-\\ners (Hasan et al. 2022), resulting in the following dependent\\nvariables (DVs): (1) interaction normalized by post count\\n(VIP), (2) interaction normalized by follower count ( VIF),\\nand (3) interaction normalized by follower and post count\\n(VIP,F). Therefore, the visibility of legislator u,Vu, is given\\nby aggregating the visibility of all u‚Äôs posts normalized by\\npost and/or follower count.\\n5 Methods\\nIn this section, we describe the methods used to answer our\\nRQs. In addition to the legislator behavior on Twitter and\\nFB, we analyze the overlapping legislators (OL) to under-\\nstand whether the differences observed across these plat-\\nforms are due to different legislator populations or different\\naudience/platform characteristics on Twitter and FB.\\n5.1 Analyzing Legislators‚Äô Visibility\\nIn RQ1, we analyze whether legislators‚Äô visibility varies\\nbased on demographics, party, and posting activity. For at-\\ntention disparity based on posting activity, we compare visi-\\nbility of legislators having above the median number of posts\\nwith those posting less than or equal to median16. Using\\nMann-Whitney U test, we find differences in the platform\\nvisibility of legislators based on party, gender, ethnicity, and\\nposting frequency. We incorporate additional DV variants,\\nnamely, 25 th, 50th, and 75 thpercentile of the legislator‚Äôs\\npost interactions together with the mean interaction, i.e.,\\nVIPto ensure robustness of our results. For states-wise dif-\\nferences, we visualize the mean visibility across states.\\nFor RQ2, we study the factors significantly correlated\\nwith the platform visibility of legislators. In addition to the\\nindividuals‚Äô attributes (Section 3.2), we measure the associ-\\nation between low-credibility and uncivil post volumes and\\ntheir visibility. Since low-credibility content is targeted more\\ntowards conservative users (Rao et al. 2022; Yang et al.\\n2023), we also consider the interaction between party and\\nlow-credibility content posted. Additionally, the legislator‚Äôs\\nvisibility may be influenced by the visibility of their peers\\nif their peers (re)post similar content often. To measure this\\nnetwork visibility, we use the median of the visibilities of\\nlegislator‚Äôs in-degree neighbors in the intra-legislator fol-\\nlower network. Unfortunately, we are unable to account for\\nthe network effects on FB. We use the following model,\\nVu=Œ≤0+Œ≤PParty u+Œ≤GGender u+Œ≤EEthnicity u+\\nŒ≤NPosts u+Œ≤FFollowers u+Œ≤CCentrality u+\\nŒ≤SState u+Œ≤TUncivil u+Œ≤MLowCredible u+\\nŒ≤NetNVu+Œ≤eParty u‚àóLowCredible u\\n(1)\\nwhere Vuis the visibility of legislator u,Uncivil u\\nand LowCredible u are the count of uncivil and\\n16We chose median as the threshold due to the skewed posting\\nactivity distribution.low-credibility posts, Party u, Gender u, Ethnicity uand\\nState uare dummy variables, Posts uis the post count,\\nFollowers uis the follower count17(Twitter specific),\\nCentrality uis the indegree centrality in intra-legislator fol-\\nlower network (Twitter specific), and NVuis the network\\nvisibility. To address the correlated errors across and within\\nstates, we incorporate a random effect on the state variable.\\nThe effect of each factor is estimated using a linear mixed ef-\\nfects regression model18with standardized continuous vari-\\nables. To ensure robust results for the analysis of RQ1 and\\nRQ2, we conduct separate analyses for the years 2020 and\\n2021, to determine if the visibility trends observed were con-\\nsistent across both years19. Moreover, we analyze the topi-\\ncal20distributions (e.g., COVID-19, BLM, elections) for our\\ndataset and find that our data is not skewed towards any par-\\nticular topic, suggesting limited bias due to specific topic(s).\\n5.2 Analyzing the Impact of Low-credibility or\\nUncivil Content\\nMeasuring outcome. Posting low-credibility and uncivil\\ncontent may have an impact on how politicians are perceived\\nonline. In particular, we analyze whether the presence of\\nincivility or low-fact URLs increases/decreases their posts‚Äô\\nvisibility. To characterize the change in visibility, we ana-\\nlyze the engagement on a post considering the post author‚Äôs\\nexpected visibility. Our metric is inspired by the overper-\\nforming metric used at CrowdTangle (Crowdtangle 2024).\\nThe overperforming score for post iis calculated as follows,\\nOuip=vuip\\nbup+thres p(2)\\nwhere buis the median interaction for legislator u‚Äôs posts\\non the platform in previous w-days and a threshold ( thres )\\nfor the minimum number of interactions on a post to be con-\\nsidered as overperforming, with pdenoting platform. The\\ntermbupis used to adjust the outcome with respect to the\\nlevel of expected interactions with the post authors on plat-\\nform p. We choose the thres = 10 for Twitter (i.e., a post\\nmust have at least 10 interactions to be considered overper-\\nforming on Twitter) and 100for FB. We estimate the ideal\\nwindow w, based on legislators‚Äô daily posting rates on these\\nplatforms (see Appendix for thres ,westimation). To get\\na reliable estimate of the overperforming metric, we choose\\nw= 14 day rolling window. This allows us to calculate the\\noverperforming score over a sufficient number of posts per\\nlegislator, while also accounting for the temporal variation.\\nFigure 3 shows the post overperforming scores on Twit-\\nter and FB21. The distribution of overperforming scores are\\n17Only included for DV not normalized by follower count\\n18To satisfy assumptions of linear regression, all variables are\\nsuitably transformed to be close to normal distributions (See Ap-\\npendix). Ideology score is dropped since it is correlated with party.\\n19Our findings revealed that the trends were similar for both\\nyears. Therefore, we have reported the overall results for the en-\\ntire study period.\\n20Identified using keywords.\\n21Our scores for FB posts are highly correlated (Spearman\\n102\\n1011040.000.250.500.751.00A. T witter\\n101\\n1011030.000.250.500.751.00B. FBproportionRep\\nDemFigure 3: ECDF plots of overperforming score distribu-\\ntions by party, by platform. Distributions are similar for\\nTwitter but posts by Republicans overperform more on FB.\\nsimilar for Republicans and Democrats on Twitter. On FB,\\nposts by Republican legislators overperform more compared\\nto posts by Democrats. This suggests that posts by Repub-\\nlican legislators have a higher tendency to be viral on FB\\ncompared to Democrats. For estimating the causal impact,\\nwe are interested in analyzing whether a post overperforms\\nor not due to its incivility or low-credibility, hence we bina-\\nrize the overperforming score ( outcome ) at cutoff 1, i.e., a\\npost is overperforming if it has a score >1.\\nEstimating causal impact. It is necessary to control for\\ntheconfounders that affect both the treatment andoutcome\\nvariables to differentiate between spurious correlation and\\ncausation . One of the possible confounders is the topic‚Äî\\nposts on certain topics (e.g., COVID-19) may be more likely\\nto contain misinformation, and these topics may get higher\\nvisibility. Another possible confounder could be the tone‚Äî\\nposts having certain tones (e.g., assertive) may be more\\nlikely to contain uncivil language, and also more likely to\\ngenerate stronger responses from the audience. Apart from\\ntextual content, other confounding variables can include leg-\\nislators‚Äô personal traits and the timing of their posts (e.g.,\\nduring elections there might be a rise in harmful content as\\nwell as an increase in legislators‚Äô visibility).\\nWe control for the individual‚Äôs attributes mentioned in\\nSection 3.2 (excluding ideological scores), post content, and\\ntime, i.e., count of days since the content was posted, start-\\ning from 2020-01-01. To control for the content, we leverage\\nembeddings generated by the pre-trained RoBERTa model.\\nFor low-credibility content, we also include the URL head-\\nlines along with post content because we assume that both\\nthe text and news headlines are visible to the viewers. The\\ntextual embeddings22are concatenated with individual‚Äôs at-\\ntributes, and time variable to get the final embedding of each\\npost. The confounders we choose to control for are based on\\nprior literature (Hasan et al. 2022; Sahly et al. 2019) and\\ntheir feasibility of being measured. Apart from these con-\\nfounders, there could also be certain other confounders (e.g.,\\nexternal events, algorithmic promotion effects, effect of ads)\\nwhich we are unable to measure and thus account for in this\\nrho=0.997, p < 0.001) with the overperforming score returned\\nby the CrowdTangle API, suggesting that our method successfully\\nidentifies posts that are overperforming.\\n22Only posts having a minimum of 10 words are considered for\\nthis part of the analysis. This accounts for 5,405 (78.9%) low-\\ncredibility and 15,883 (65.5%) uncivil posts on Twitter, and 2,427\\n(58.6%) low-credibility posts on FB.study. Our causal effect estimation has two steps: potential\\noutcome prediction and matching.\\nPotential outcome prediction . The confounding in our case\\nis time-varying and encoded in complex textual data, so we\\nleverage the non-parametric nature of neural networks to\\nlearn deconfounded, low-dimensional representations of the\\nhigh-dimensional data (Koch et al. 2021). We leverage the\\nDragonnet23model proposed by Shi et al. (2019) which uses\\nfeed-forward neural networks to learn balanced24represen-\\ntations of the data such that each head models a separate po-\\ntential outcome, a third propensity head predicts the propen-\\nsity (œÄ) of being treated and a free nudge parameter œµ(see\\nAppendix for model description).\\nWe fine-tune the Dragonnet model25by adding more hid-\\nden layers and using cross-entropy loss. The treatment vari-\\nables in our case are low-credibility and incivility respec-\\ntively. We use a 5-fold cross-validation setting for training,\\nwith a 1:1 ratio of treated vs. non-treated random samples\\n(see Appendix for model performance). For low-credibility\\nposts, we select corresponding non-treated posts that con-\\ntain at least one URL and similarly include the URL head-\\nlines to minimize the confounding from text (e.g., presence\\nof URL). Figure 4 shows the effectiveness of our decon-\\nfounding for incivility on Twitter.\\nA. Before Deconfounding\\nRep\\nDem\\nuncivil\\ncivil\\nB. After Deconfounding\\nRep\\nDem\\nuncivil\\ncivil\\nFigure 4: Effectiveness of deconfounding for uncivil vs.\\ncivil tweets. (A) shows the T-SNE space of content embed-\\ndings for civil vs. uncivil tweets by party. (B) shows the\\nrepresentation of the deconfounded embeddings returned by\\nDragonnet. After deconfounding, the representation space\\nfor treated and control covariate distributions ( party in this\\nexample) can not be distinguished.\\nMatching. For Conditional Average Treatment Effect\\n(CATE) estimation with Dragonnet predictions, we find\\nthat the covariates are not balanced after propensity score\\nreweighting. To improve balance, we further use matching.\\nWe match the treated and untreated subjects based on the\\ndeconfounded Dragonnet embeddings (see Appendix). The\\ncovariate balance for matching is shown in Figure 5. All the\\ncovariates are balanced for Twitter and FB low-credibility\\n23Dragonnet is chosen over other deep learning models for\\ncausal inference (S-learner, T-learner, TARNet) due to its ‚ÄúTar-\\ngeted Regularization‚Äù procedure which allows for statistical guar-\\nantees (Koch et al. 2021)\\n24Balancing is a treatment adjustment strategy that forces the\\ntreated and non-treated covariate distributions closer to deconfound\\nthe treatment from outcome (Johansson et al. 2016)\\n25Models trained on a single NVIDIA A100 40GB PCIe GPU\\n#follower#postscentralitydaysethnicitygenderpartystate\\n0.0 0.2 0.4\\nStandardized DifferencesTwitter uncivil A\\n#follower#postscentralitydaysethnicitygenderpartystate\\n0.0 0.5 1.0 1.5 2.0\\nStandardized DifferencesTwitter non-credible B\\n#postsdaysethnicitygenderpartystate\\n0.0 0.5 1.0 1.5\\nStandardized DifferencesFB non-credible C\\n before after matchingFigure 5: Covariate balance after matching. All covari-\\nates, except for party, and state in Twitter uncivil model, are\\nbalanced (i.e., absolute standardized difference <0.1) after\\nmatching on deconfounded embeddings.\\nmodels. For Twitter incivility, all the covariates except for\\nstate and party are balanced after matching. The final CATE\\nis calculated based on the matched samples as follows,\\nCATE =1\\nN‚Ä≤N‚Ä≤X\\n(Yi(1)‚àíYi‚Ä≤(0)) (3)\\nwhere Y(T)is the outcome for treatment TandN‚Ä≤is\\nthe number of matched samples. We estimate the CATE\\nfor Democrats and Republicans separately to study potential\\nasymmetries in receptivity across their audiences. Further-\\nmore, we look at the CATE for ideologically extreme leg-\\nislators to understand whether audiences engage differently\\nwith legislators at the extreme. We consider legislators hav-\\ning top 25% conservative and top 25% liberal ideological\\nscores as Extreme Republicans and Extreme Democrats.\\nFurthermore, our analysis ensures that outliers, a common\\noccurrence in social networks, do not significantly impact\\nour results. (See the Appendix for more details.)\\n6 Results\\n6.1 RQ1: Legislators‚Äô Visibility by Party, Gender,\\nEthnicity, Posting Frequency, State\\nTable 4 shows the effect sizes for the Mann-Whitney U test.\\nWe only report the results for VIPhere, the results for 25 th,\\n50th, and 75 thpercentile are added in the Appendix along\\nwith 95% CI for Table 4. Overall, the visibility of legisla-\\ntors differs significantly based on party, gender, and post-\\ning frequency on both platforms and also for ethnicity on\\nTwitter. Interestingly, Democrats and women appear to have\\nhigher visibility on Twitter but Republicans and men have\\nhigher visibility on FB ( p <0.001). White legislators also\\nreceive more attention on Twitter. On both platforms, leg-\\nislators with higher posting activity have higher visibility.\\nFigure 6 shows the mean VIPacross US states for Twitter\\nand FB. the visibility of legislators also differs based on their\\nstate representation. The most visible state on Twitter is New\\nMexico and Mississippi on FB. The posting rate of legisla-\\ntors is the second highest for New Mexico on Twitter which\\ncould be a possible explanation for the high visibility26.\\n26For instance on FB, Mississippi Republican senator Chris Mc-\\nDaniel has a remarkably high engagement which dominates the vis-Table 4: RQ1 Effect sizes\\nOverlapping (OL)\\nIVs Twitter FB Twitter FB\\nParty 0.239‚àó‚àó‚àó-0.299‚àó‚àó‚àó0.189‚àó‚àó‚àó-0.347‚àó‚àó‚àó\\n(Rep vs. Dem)\\nGender 0.076‚àó‚àó‚àó-0.089‚àó‚àó‚àó0.009 -0.128‚àó‚àó‚àó\\n(Men vs. Women)\\nEthnicity -0.067‚àó‚àó-0.031 -0.023 -0.062\\n(White vs. Non-White)\\nPosting Freq. 0.457‚àó‚àó‚àó0.435‚àó‚àó‚àó0.368‚àó‚àó‚àó0.418‚àó‚àó‚àó\\n(‚â§vs.>median)\\n.p <0.1,‚àóp <0.05,‚àó‚àóp <0.01,‚àó‚àó‚àóp <0.001\\nWe also look at the visibility of overlapping users (OL)\\non these platforms. Similar to prior results, Republicans and\\nmen receive higher engagement on FB and Democrats on\\nTwitter. However, we do not find any significant difference\\nacross gender and ethnicity on Twitter for OL. Our results\\nsuggest that there exists cross-platform differences in how\\naudiences engage with political content.\\n0.00 0.25 0.50 0.75 1.00Visibility (Twitter)\\n0.00 0.25 0.50 0.75 1.00Visibility (FB)\\nFigure 6: Mean visibility of legislators from states. The\\nvisibility ( VIP) (normalized between 0-1) of legislators dif-\\nfer based on their state representation and platform.\\n6.2 RQ2: Factors Related to Visibility\\nIn RQ2 we look at factors related to legislators‚Äô platform\\nvisibility. The results for all DVs are similar but we only\\nreport results for VIPin Table 5 (See Appendix for 95%\\nCI). We are unable to estimate the interaction effect be-\\ntween party and low-credibility posts for FB due to insuf-\\nficient low-credibility posts from Democrats. Party, gender,\\nand post frequency are significantly correlated with visibil-\\nity after controlling for other variables on both platforms.\\nRepublicans and men are more likely to garner higher visi-\\nbility on FB whereas the opposite is true for Twitter. Higher\\nposting activity is also related to higher visibility on both\\nplatforms. On Twitter, the number of followers and central-\\nity in the intra-legislator network are not correlated with the\\nlegislators‚Äô visibility. Interestingly, there is a positive rela-\\ntion between legislators‚Äô network visibility and visibility.\\nAs shown in Table 5, the volume of low-credibility posts\\nis positively associated with legislators‚Äô visibility on FB\\nibility term for the state.Table 5: RQ2 Regression Results\\nOverlapping (OL)\\nIVs Twitter FB Twitter FB\\nParty [Rep] -0.133‚àó‚àó0.423‚àó‚àó‚àó-0.142‚àó0.512‚àó‚àó‚àó\\nGender [Men] -0.078‚àó0.090‚àó‚àó-0.011 0.113‚àó‚àó\\nEthnicity [White] 0.020 0.015 0.020 0.040\\n#posts 0.401‚àó‚àó‚àó0.477‚àó‚àó‚àó0.409‚àó‚àó‚àó0.499‚àó‚àó‚àó\\n#followers -0.028 - -0.028 -\\nCentrality -0.020 - -0.029 -\\nNetwork visibility 0.040‚àó0.027 -\\n#Low-Credible -0.268‚àó‚àó0.079‚àó‚àó‚àó-0.335‚àó-0.037‚àó\\n#Uncivil 0.148‚àó‚àó‚àó- 0.152‚àó‚àó‚àó-\\nParty [Rep] x\\n#Low-Credible 0.299‚àó‚àó- 0.351‚àó‚àó-\\nR20.289 0.382 0.291 0.386\\n.p <0.1,‚àóp <0.05,‚àó‚àóp <0.01,‚àó‚àó‚àóp <0.001\\n(p <0.001) but has an opposite effect ( p <0.01) on Twitter.\\nHowever, visibility is positively correlated with the interac-\\ntion between party and low-credibility posts on Twitter, i.e.,\\none standard deviation increase in low-credibility posts by\\nRepublicans is associated with a 0.299 standard deviation\\nincrease in visibility . Thus, posting low-credibility content\\nis related to higher visibility for only Republicans, otherwise\\nit has a negative association on Twitter. Posting more uncivil\\ncontent also increases the visibility of legislators on Twitter\\n(p <0.001). These results suggest that posting harmful con-\\ntent is associated with legislators‚Äô platform visibility.\\nWe find similar results for OL, i.e., men and Republicans\\nrelate to higher visibility on FB, and Democrats on Twit-\\nter, again suggesting the cross-platform differences. Posting\\nuncivil content on Twitter is positively associated with in-\\ncreased visibility, while posting low-credibility content is\\nnegatively associated with it. Moreover, visibility is posi-\\ntively associated with the interaction term between party and\\nlow-credibility posts. Interestingly however, posting low-\\ncredibility content is related to decreased visibility for the\\nOL on FB similar to Twitter. Therefore, legislators who\\npost content on both platforms have different communica-\\ntion strategies in comparison to the general legislator popu-\\nlations on those platforms which may be attributed to audi-\\nence preferences across platforms.\\nThus, posting harmful content is related to the visibility of\\nthe legislators. But the observed correlation may be a spuri-\\nousone due to confounders, such as content in similar top-\\nics. Next, we analyze whether incivility or low-credibility of\\na post impacts its visibility.\\n6.3 RQ3: Observed Causal Impact of Incivility\\nand Low-credibility on Visibility\\nFigure 7 shows the CATE estimates and 95% CI after match-\\ning. CATE is the expected change in the overperformance\\nof a post if it contains low-credibility or incivility. For\\nFB, we find that Republican legislators receive higher atten-\\ntion on posting low-credibility content. Similar results hold\\nwhen we look at Extreme and OL Republicans. Interest-FB non-credible Twitter non-credible Twitter uncivil\\n-0.50 -0.25 0.00 0.25 0.50 -0.50 -0.25 0.00 0.25 0.50 -0.50 -0.25 0.00 0.25 0.50Extreme DemDemDem (OL)Extreme RepRepRep (OL)\\nCATEFigure 7: Observed causal impact of low-credibility and\\nincivility on legislators‚Äô content visibility. After control-\\nling for confounders, low-credibility has a positive impact\\non content visibility for Republicans on both platforms, but a\\nnegative effect for Democrats on FB. Incivility significantly\\nhinders content visibility for all subgroups on Twitter.\\ningly, the visibility of Democrats decreases when they post\\nlow-credibility content on FB. There are no effects for Ex-\\ntreme and OL Democrat subgroups.\\nFor Twitter, similar to FB, Republicans, including their\\nOL and Extreme subgroups receive higher visibility on post-\\ning low-credibility content. The effect size is higher for Ex-\\ntreme Republicans, i.e., the more conservative a legislator\\nis the higher attention they receive on posting misinforma-\\ntion. For Democrats, however, we do not find any significant\\neffects. The difference in outcomes for Democrats across\\nTwitter and FB could either be due to their distinct behav-\\niors (e.g., content) and/or audience preferences across these\\nplatforms. The average number of posts containing low-\\ncredibility content by Democratic legislators is higher on\\nTwitter than on FB as shown in Table 3. This may suggest\\nthat Democrats post less low-credibility content on FB since\\ntheir visibility is penalized and/or vice-versa.\\nThe cross-platform analysis results suggest that there are\\nasymmetries in how the Republican and Democratic party\\naudiences engage with low-credibility content online and\\nthese asymmetries hold across platforms within and/or be-\\ntween parties. The former are more likely to engage with\\nmisinformation as shown by our results.\\nFor uncivil content on Twitter, we find that both\\nDemocrats and Republicans, including their subgroups, re-\\nceive lower engagement on posts containing uncivil lan-\\nguage. The negative effects are higher for Democratic legis-\\nlators compared to Republicans. The effects are also higher\\nfor Extreme legislators from both parties when compared to\\ntheir party baselines. This implies that audiences irrespective\\nof their partisan preferences engage less with uncivil content\\nposted by legislators on Twitter and the legislators towards\\nthe extreme political spectra are penalized more.\\n7 Discussion\\nWe analyzed the factors influencing the visibility dynam-\\nics of US state legislators by conducting a cross-platform\\nanalysis across Twitter and FB to understand different audi-\\nence behaviors across these platforms. We showed that leg-\\nislators‚Äô visibility varies based on their demographics, party,and posting frequency. Democrats have higher visibility on\\nTwitter whereas Republicans have higher visibility on FB.\\nMoreover, the regression analysis showed that a strong cor-\\nrelation exists between party and visibility, i.e., Democrats\\nare associated with higher engagement on Twitter and Re-\\npublicans on FB. These results also hold for the overlapping\\nlegislators, suggesting that audiences across these platforms\\nengage with political content differently. Posting harmful\\ncontent is also associated with legislators‚Äô visibility. Low-\\ncredibility content is related to increased visibility on FB,\\nbut decreased visibility on Twitter. Taking the effect of party\\ninto account, low-credibility posting correlates with higher\\nvisibility for Republicans. Uncivil posting is also correlated\\nwith higher account-level visibility on Twitter.\\nWe further analyzed whether the increased visibility is\\ndue to posting harmful content after controlling for con-\\nfounding factors such as demographics, party, topics, and\\ntime. Low-credibility garners more attention for Repub-\\nlicans on both platforms. However for Democrats, low-\\ncredibility reduces their content visibility on Twitter. These\\nresults highlight the partisan asymmetries in how low-\\ncredibility content receives attention online. Existing works\\nhave shown population asymmetries (Rao et al. 2022); our\\nstudy further reveals attention disparity due to political ac-\\ntors‚Äô party affiliation. Higher online visibility provides a\\ngreater opportunity to influence public opinion (e.g., by\\ngaining followers, impacting ranking algorithms). Politi-\\ncians may post higher low-credibility content for political\\ngains which may have implications for platform moderation.\\nUnlike low-credibility, incivility decreases the visibility\\nof legislators‚Äô posts on Twitter for both Republicans and\\nDemocrats. The negative effects are more pronounced for\\nDemocrats compared to Republicans as well as for Ex-\\ntreme legislators, suggesting that audiences prefer to engage\\nless with uncivil content irrespective of partisan preferences.\\nPrior research has shown that uncivil content receives more\\nnegative reactions (Coe et al. 2014) owing to its emotionally\\ncharged language. The lower visibility may be attributed to\\nthe lack of expressing negative sentiments on Twitter, but\\nfurther research is needed to confirm this. Our results high-\\nlight the cross-platform differences as well as asymmetries\\nin how Democratic and Republican party audiences engage\\nwith political content online which is aligned with previous\\nliterature (Kelm 2020; Sahly et al. 2019) .\\nWe show that harmful content is associated with legisla-\\ntors‚Äô online visibility. Such observed associations may be\\nspurious, and other factors may contribute to their visibil-\\nity. For instance, posting uncivil content has a positive as-\\nsociation with visibility on Twitter but incivility has a neg-\\native impact on content visibility after controlling for con-\\nfounders. Other factors may include the topics of their posts\\nor simply the post timing. External factors (e.g., offline cam-\\npaigns, media presence) can also contribute to their online\\nvisibility which is out of scope for this study. Moreover,\\nthere may be spillover effects from legislators‚Äô network vis-\\nibility as hinted in our RQ2 results. Nevertheless, this study\\nsheds light on some of the factors influencing legislators‚Äô\\noverall as well as content visibility, but more research is\\nneeded to fully understand their online visibility dynamics.Limitations and Future Work. Our study has certain\\nlimitations. We only look at the years 2020 and 2021‚Äî\\nthe generalizability to other periods remains uncertain. Our\\nmethod of identifying low-credibility content was conserva-\\ntive which could have led to certain biases in our sampling.\\nWe demonstrated the feasibility of addressing the represen-\\ntation biases in Section 5; however, it is uncertain whether\\nwe were able to fully correct for them. Furthermore, we only\\nidentified uncivil and low-credibility posts based on the tex-\\ntual content but do not consider other forms of content (e.g.,\\nimages) which may also contain harmful information.\\nWe only looked at the visibility based on total interac-\\ntions received on posts without discriminating between pos-\\nitive and negative reactions. Future work can study the im-\\npact of harmful content on positive and negative visibility\\nseparately to get a more nuanced understanding of public\\nreceptivity. It would also be interesting to look at the impact\\nof posting harmful content on different types of engagement\\n(e.g., Likes vs. Retweets). Another possible extension could\\nbe adapting our causal study design for continuous treatment\\n(e.g., how visibility is affected by the degree of incivility).\\n8 Ethical Considerations\\nData. We collect data from two sources, Twitter and FB.\\nFor Twitter, the data was collected using Twitter‚Äôs Official\\nAPI v2.0 before rate limitations were imposed (i.e., March\\n2023). For FB, we collect data using CrowdTangle‚Äôs official\\nAPI by following their terms of service. All the data are pub-\\nlicly posted and available for viewing without restrictions.\\nWe ensure that the Dragonnet model can effectively de-\\nconfound the covariate representation space for treated and\\nnon-treated samples based on our qualitative analysis and\\nperformance metrics. The classification errors from Drag-\\nonnet model are less likely to affect our results since we do\\nnot use the model predictions to calculate CATE. However,\\nmisclassification may still impact the deconfounding qual-\\nity, resulting in confounding from textual content that we\\ncannot measure, unlike other covariates. Our study suggests\\nthat public figures sharing harmful content on social media\\nhas significant consequences. We showed that when low-\\ncredibility content is posted by public figures, the combi-\\nnation of user behavior (interacting with the posts) and plat-\\nform mechanisms (e.g., feed recommendation algorithms)\\ncan result in increased visibility for such content. Our find-\\nings should not be interpreted as an encouragement to spread\\nsuch low-credibility content; instead, they should serve as a\\nwarning that there may be incentives for political or elite ac-\\ntors to do so. Moreover, our study has focused on the behav-\\nior of US subnational politicians on two specific platforms‚Äî\\nTwitter and FB. The results may not be generalizable to\\nother platforms and social media users including the ac-\\ntivities of political opinion leaders and media elites from\\nother countries, or even US national politicians, due to sev-\\neral factors‚Äîmedia scrutiny, platform moderation rules, and\\npublic perception to name some. More research is needed to\\nunderstand whether our results generalize to other settings.Acknowledgement\\nThe authors would like to acknowledge support from\\nAFOSR, ONR, Minerva, NSF #2318461, and Pitt Cyber In-\\nstitute‚Äôs PCAG awards. The research was partly supported\\nby Pitt‚Äôs CRC resources (RRID:SCR 022735 through NIH\\n#S10OD028483). Any opinions, findings, and conclusions\\nor recommendations expressed in this material do not nec-\\nessarily reflect the views of the funding sources.\\nReferences\\nArhin, K.; et al. 2021. Ground-Truth, Whose Truth?‚Äì\\nExamining the Challenges with Annotating Toxic Text\\nDatasets. arXiv preprint arXiv:2112.03529 .\\nBahramirad, S. 2022. Virtual forums for public accountabil-\\nity: How internet and communication technologies are influ-\\nencing citizen interactions with a local government. CJAS ,\\n39(3): 313‚Äì327.\\nBennett, W. L.; et al. 2018. The disinformation order: Dis-\\nruptive communication and the decline of democratic insti-\\ntutions. European journal of communication , 33(2).\\nBode, L.; et al. 2016. Politics in 140 characters or less:\\nCampaign communication, network interaction, and politi-\\ncal participation on Twitter. Journal of Political Marketing ,\\n15(4): 311‚Äì332.\\nBodrunova, S. S.; and Blekanov, I. S. 2021. A self-critical\\npublic: Cumulation of opinion on Belarusian oppositional\\nYouTube before the 2020 protests. Social Media+ Society ,\\n7(4): 20563051211063464.\\nBodrunova, S. S.; et al. 2021. Constructive aggression? Mul-\\ntiple roles of aggressive content in political discourse on\\nRussian YouTube. Media and Communication , 9: 181‚Äì194.\\nBoulianne, S.; and Larsson, A. O. 2023. Engagement with\\ncandidate posts on Twitter, Instagram, and Facebook during\\nthe 2019 election. New Media & Society , 25(1): 119‚Äì140.\\nBovet, A.; et al. 2019. Influence of fake news in Twitter\\nduring the 2016 US presidential election. Nature communi-\\ncations , 10(1): 7.\\nCoe, K.; et al. 2014. Online and uncivil? Patterns and deter-\\nminants of incivility in newspaper website comments. Jour-\\nnal of communication , 64(4): 658‚Äì679.\\nCrowdtangle. 2024. How do you calculate overperforming\\nscores? https://help.crowdtangle.com/en/articles/2013937-\\nhow-do-you-calculate-overperforming-scores.\\nCuan-Baltazar, J. Y .; et al. 2020. Misinformation of COVID-\\n19 on the internet: infodemiology study. JMIR public health\\nand surveillance , 6(2): e18444.\\nEberl, J.-M.; et al. 2020. What‚Äôs in a post? How sentiment\\nand issue salience affect users‚Äô emotional reactions on Face-\\nbook. Journal of Information Technology & Politics , 17(1).\\nEnli, G. S.; and Skogerb√∏, E. 2013. Personalized campaigns\\nin party-centred politics: Twitter and Facebook as arenas for\\npolitical communication. Information, communication & so-\\nciety , 16(5): 757‚Äì774.\\nFeder, A.; et al. 2022. Causal inference in natural lan-\\nguage processing: Estimation, prediction, interpretation and\\nbeyond. Transactions of the ACL , 10: 1138‚Äì1158.\\nFerrara, E.; et al. 2020. Characterizing social media manip-\\nulation in the 2020 US presidential election. First Monday .Friggeri, A.; et al. 2014. Rumor cascades. In ICWSM , vol-\\nume 8, 101‚Äì110.\\nFrimer, J. A.; et al. 2023. Incivility is rising among Ameri-\\ncan politicians on Twitter. SPPS , 14(2): 259‚Äì269.\\nGervais, B. T. 2015. Incivility online: Affective and be-\\nhavioral reactions to uncivil political posts in a web-based\\nexperiment. Journal of Information Technology & Politics ,\\n12(2): 167‚Äì185.\\nGoovaerts, I.; et al. 2020. Uncivil communication and sim-\\nplistic argumentation: Decreasing political trust, increasing\\npersuasive power? Political Communication , 37(6).\\nGrant, W. J.; et al. 2010. Digital dialogue? Australian politi-\\ncians‚Äô use of the social network tool Twitter. Australian jour-\\nnal of political science , 45(4): 579‚Äì604.\\nHalpern, D.; et al. 2019. From belief in conspiracy theories\\nto trust in others: Which factors influence exposure, believ-\\ning and sharing fake news. In SCSM 2019 . Springer.\\nHanu, L.; and Unitary team. 2020. Detoxify. Github.\\nhttps://github.com/unitaryai/detoxify.\\nHasan, R.; et al. 2022. The Impact of Viral Posts on Visibil-\\nity and Behavior of Professionals: A Longitudinal Study of\\nScientists on Twitter. In ICWSM , volume 16, 323‚Äì334.\\nHua, Y .; et al. 2020. Characterizing twitter users who engage\\nin adversarial interactions against political candidates. In\\nCHI 2020 , 1‚Äì13.\\nJohansson, F.; et al. 2016. Learning representations for\\ncounterfactual inference. In ICML , 3020‚Äì3029. PMLR.\\nJohnson, J. 2018. The self-radicalization of white\\nmen:‚ÄúFake news‚Äù and the affective networking of paranoia.\\nCommunication Culture & Critique , 11(1): 100‚Äì115.\\nJungherr, A. 2016. Twitter use in election campaigns: A sys-\\ntematic literature review. Journal of information technology\\n& politics , 13(1): 72‚Äì91.\\nKelm, O. 2020. Why do politicians use Facebook and Twit-\\nter the way they do? The influence of perceived audience\\nexpectations. SCM Studies in Communication and Media ,\\n9(1): 8‚Äì34.\\nKim, J. W.; et al. 2021. The distorting prism of social me-\\ndia: How self-selection and exposure to incivility fuel online\\ncomment toxicity. Journal of Communication , 71(6).\\nKim, T.; et al. 2022. Attention to the COVID-19 Pandemic\\non Twitter: Partisan Differences Among US State Legisla-\\ntors. Legislative studies quarterly , 47(4): 1023‚Äì1041.\\nKoch, B.; et al. 2021. Deep Learning for Causal Inference.\\nKreiss, D. 2016. Seizing the moment: The presidential cam-\\npaigns‚Äô use of Twitter during the 2012 electoral cycle. New\\nmedia & society , 18(8): 1473‚Äì1490.\\nKyriakidou, M.; et al. 2021. Journalistic responses to mis-\\ninformation. The Routledge Companion to Media Disinfor-\\nmation and Populism , 529‚Äì537.\\nLasser, J.; et al. 2022. Social media sharing of low-quality\\nnews sources by political elites. PNAS nexus , 1(4): pgac186.\\nLiu, Y .; et al. 2019. Roberta: A robustly optimized bert pre-\\ntraining approach. arXiv preprint arXiv:1907.11692 .\\nMihailidis, P.; et al. 2021. The cost of disbelief: Fracturing\\nnews ecosystems in an age of rampant media cynicism. ABS,\\n65(4): 616‚Äì631.\\nMutz, D. C. 2007. Effects of ‚Äúin-your-face‚Äù television dis-\\ncourse on perceptions of a legitimate opposition. APSR ,101(4): 621‚Äì635.\\nPelletier, M. J.; et al. 2021. Fexit: The effect of political\\nand promotional communication from friends and family on\\nFacebook exiting intentions. Journal of Business Research ,\\n122: 321‚Äì334.\\nPennycook, G.; and Rand, D. G. 2021. Examining false be-\\nliefs about voter fraud in the wake of the 2020 Presidential\\nElection. The HKS Misinformation Review .\\nPryzant, R.; et al. 2020. Causal effects of linguistic proper-\\nties. arXiv preprint arXiv:2010.12919 .\\nRao, A.; et al. 2022. Partisan asymmetries in exposure to\\nmisinformation. Scientific Reports , 12(1): 15671.\\nRoberts, M. E.; et al. 2020. Adjusting for confounding with\\ntext matching. AJPS , 64(4): 887‚Äì903.\\nSahly, A.; et al. 2019. Social media for political campaigns:\\nAn examination of Trump‚Äôs and Clinton‚Äôs frame building\\nand its effect on audience engagement. Social Media+ So-\\nciety , 5(2): 2056305119855141.\\nSerrano-Puche, J. 2021. Digital disinformation and emo-\\ntions: exploring the social risks of affective polarization. In-\\nternational Review of Sociology , 31(2): 231‚Äì245.\\nShi, C.; et al. 2019. Adapting neural networks for the esti-\\nmation of treatment effects. NeurIPS , 32.\\nShmargad, Y .; et al. 2022. Social norms and the dynamics\\nof online incivility. Social Science Computer Review , 40(3).\\nShor, B.; and McCarty, N. 2011. The ideological mapping\\nof American legislatures. APSR , 105(3): 530‚Äì551.\\nSquire, P.; et al. 2019. State legislatures today: Politics un-\\nder the domes . Rowman & Littlefield.\\nSridhar, D.; and Getoor, L. 2019. Estimating causal effects\\nof tone in online debates. arXiv preprint arXiv:1906.04177 .\\nStier, S.; et al. 2020. Election campaigning on social media:\\nPoliticians, audiences, and the mediation of political com-\\nmunication on Facebook and Twitter. In Studying Politics\\nAcross Media , 50‚Äì74. Routledge.\\nStryker, R.; et al. 2016. What is political incivility? Com-\\nmunication monographs , 83(4): 535‚Äì556.\\nTai, Y . C.; et al. 2023. Official yet questionable: examining\\nmisinformation in US state legislators‚Äô tweets. Journal of\\nInformation Technology & Politics , 1‚Äì13.\\nToraman, C.; et al. 2022. BlackLivesMatter 2020: An anal-\\nysis of deleted and suspended users in Twitter. In WebSci\\n2022 , 290‚Äì295.\\nTwitter-team. 2024. the-algorithm. https://github.com/\\ntwitter/the-algorithm?tab=readme-ov-file.\\nVeitch, V .; et al. 2020. Adapting text embeddings for causal\\ninference. In UAI, 919‚Äì928. PMLR.\\nV osoughi, S.; et al. 2018. The spread of true and false news\\nonline. science , 359(6380): 1146‚Äì1151.\\nYang, Y .; et al. 2023. Visual misinformation on Facebook.\\nJournal of Communication , jqac051.\\nZollo, F.; et al. 2015. Emotional dynamics in the age of\\nmisinformation. PloS one , 10(9): e0138740.Appendix\\nData Collection: For Twitter data, we employed a com-\\nprehensive approach, drawing from various sources, includ-\\ning existing datasets (Kim et al. 2022), and conducting\\nsearches on Google, Twitter, Wikipedia, legislators‚Äô official\\nwebsites, campaign sites, and Ballotpedia, to compile the\\naccounts and demographic information of state legislators.\\nThis meticulous strategy facilitated the manual identifica-\\ntion and verification of legislators‚Äô Twitter accounts. Sub-\\nsequently, we refined our dataset to only include legislators\\nwho served between 2020 and 2021, determined by their\\ntenure in office. It is important to acknowledge that the com-\\npleteness of our data was affected by factors such as inactive\\nor inaccessible accounts after legislators left office.\\nOur initial approach to collecting FB data involved gath-\\nering posts from accounts bearing names indicative of be-\\nlonging to state legislators. Subsequent verification against\\ninformation from Ballotpedia allowed us to filter out non-\\nlegislator accounts. To address data gaps, we conducted\\nthree successive rounds of data recollection in April 2022,\\nMarch 2023, and April 2023. The successive rounds allowed\\nus to capture posts from accounts previously overlooked.\\nHowever, similar to Twitter data, numerous accounts had\\nbecome inaccessible, largely due to campaign or official ac-\\ncounts no longer being active.\\nDespite these efforts, we encountered a significant chal-\\nlenge with FB data collection concerning accounts that were\\nnot listed on Ballotpedia. Although we attempted to iden-\\ntify missing accounts using keyword searches, achieving a\\nperfect match with the legislator information on Ballotpedia\\nwas difficult. This limitation resulted in a higher rate of mis-\\nmatch in the mapped attributes of FB accounts, i.e., around\\n70.2% of legislators could be mapped to their attributes for\\nFB. Table 7 shows the statistics of our FB dataset after map-\\nping the legislators to their attributes. The trends are similar\\nto that in Table 2 which suggests that no or minimal biases\\nwere introduced during our mapping process.\\nFigure 8 shows the breakdown of ethnicity and gender by\\nparty and for OL.\\n32% 30%17% 23%\\n29% 25%25% 21%\\n30% 30%17% 23%Twitter FB OLethnicity\\n0 0 0WhiteNon-White\\n36% 32%\\n13% 21%41% 27%\\n13% 19%35% 32%\\n12% 21%gender\\n0 1K 2K 3K 4K0 1K 2K 3K 4K0 1K 2K 3K 4KWomenMen\\nDem Rep\\nFigure 8: Ethnicity and gender by party, platform, and for\\noverlapping users. Our dataset has higher representation of\\nmen and White legislators on both platforms.\\nAssessing Post‚Äôs Civility. For a study like ours it is hard\\nto interpret the continuous toxicity scores returned by the\\nDetoxify model. So we follow the common practice in lit-\\nerature to convert the toxicity scores to binary based on a\\nthreshold (Hua et al. 2020). It is important to have a thresh-old for the toxicity for our particular dataset because un-\\ncivil language may evolve over different topics and time.\\nTo estimate an ideal cutoff for the toxicity score, we man-\\nually annotate a sample of 300 posts as uncivil or civil us-\\ning stratified sampling such that more samples are included\\nat higher toxicity scores. Three annotators labeled all 300\\nsamples based on the aforementioned definition of toxic lan-\\nguage. Since Cohen‚Äôs Kappa suffers from imbalanced label\\ndistributions, we compute the pairwise agreement scores for\\nthe percentage of total samples agreed upon by the annota-\\ntors, which ranges between 66.3-85.3%. The final labels are\\ndecided according to the majority vote. Based on the ROC\\n(AUC =0.81), we choose the cutoff for toxicity as 0.82,\\ni.e., posts having a score above 0.82 are considered uncivil.\\nThis is similar to previous works using Detoxify or Perspec-\\ntive API which have a threshold between 0.5-0.9 (Hua et al.\\n2020), though we acknowledge that our threshold is more\\ntowards the conservative side which is done to ensure that\\nuncivil posts are selected with high precision. Table 6 shows\\nexamples of uncivil posts by legislators on Twitter and FB.\\nThe number of uncivil posts on FB27is much lesser com-\\npared to Twitter which shows that the political communica-\\ntion styles are different on two platforms. This finding also\\nresonates with previous studies which have suggested that\\nFB is used more for broadcasting purposes whereas Twitter\\nis used more for direct communication (Enli and Skogerb√∏\\n2013). Based on this, it is reasonable that there are very less\\nuncivil posts on FB since the language used on FB tends\\nto be more formal. We further verify this by measuring the\\nreadability scores of author‚Äôs posts on these two platforms\\nusing Flesch‚ÄìKincaid grade level. The median readability of\\nlegislators is 11.04 (i.e., the text is written at a level suitable\\nfor someone who has completed the 11th grade in the US\\neducation system) on FB and 9.55 on Twitter. This shows\\nthat the language used on FB is indeed more formal and po-\\ntentially the reason why it is more civil.\\nMeasuring Visibility. The visibility metric is designed to\\ncapture the overall engagement on the platforms and hence\\nour metric includes all/most of the elements used to calculate\\nengagement on the respective platforms. We further analyze\\nthe contribution of individual visibility elements on each\\nplatform. On Twitter, Likes contribute 2.3% and Retweets\\ncontribute 97.7% to the overall interactions. Reply and quote\\nconsist of a small percentage of the interactions. On FB how-\\never, Likes contribute 51.8%, Shares contribute 17.0% and\\nComments contribute 13.2% to the overall interactions. This\\nsuggests that audiences engage differently with content on\\nTwitter and FB, e.g., retweeting is most dominant form of\\nengagement on Twitter whereas Liking for FB. Moreover,\\nthe interpretation of individual elements may also be dif-\\nferent across platforms, for instance, audiences may engage\\nwith Like on Twitter differently than Like on FB simply due\\nto different icons, therefore a direct comparison of individ-\\nual engagement metrics may not be justified. So, by only\\nincluding individual elements, the visibility metric may not\\nbe able to capture the level of engagement on these plat-\\n27The number of uncivil posts is still low at other cutoffs, for\\ne.g., cutoff = 0.1 yields around 2,690 uncivil postsTable 6: Examples of Uncivil Posts on Twitter and FB, by party\\nplatform party text\\nFB Rep While millions of Americans have yet to receive their stimulus checks, a new study reveals that $4.38 billion of\\nthe new round will go right into the pockets of illegal immigrants. Another dumb idea and stupid stupid policy.\\nWhat is wrong with these people?\\nFB Dem ‚ÄúI didn‚Äôt think it would be this ridiculous. It‚Äôs embarrassing to be a state senator at this point, ‚Äù Paul Boyer said\\nof partisan recount. Yes it does make you look like idiots. Wasting time & resources on #TrumpsBigLie\\nTwitter Rep We are at the start of one of the LARGEST recessions in American history, which will DESTROY many lives,\\nand people are still in favor of partial lockdownshow stupid could you possibly be! Bunch of damn fools.\\nTwitter Dem You are a blithering idiot. Who gives a shit about the VP . Vote for Trump and thousands upon thousands will\\ndie.\\nTable 7: Descriptive statistics for FB dataset after mapping,\\nshowing the number of legislators, posts, and median inter-\\nactions received per post by party.\\nFB\\nparty #users #tweets Int\\nDem 1588 171K 61.0\\nRep 1718 152K 114.0\\nTable 8: 95% CI for Table 4\\nOL\\nIVs Twitter [CI] FB [CI] Twitter [CI] FB [CI]\\nParty 0.239 -0.299 0.189 -0.347\\n[0.202, 0.277] [-0.337, -0.259] [0.141, 0.240] [-0.397, -0.298]\\nGender 0.076 -0.089 0.009 -0.128\\n[0.033, 0.117] [-0.132, -0.046] [-0.044, 0.059] [-0.182, -0.072]\\nEthnicity -0.067 -0.031 -0.023 -0.062\\n[-0.108, -0.027] [-0.079, 0.015] [-0.073, 0.029] [-0.128, 0.001]\\nPosting 0.457 0.435 0.368 0.418\\nFreq. [0.427, 0.492] [0.398, 0.470] [0.323, 0.414] [0.370, 0.461]\\nforms, making the interpretation of the metric harder for a\\ncross-platform study.\\nWe further examine the possibility of the visibility metric\\nbeing dominated by a single element by testing if the under-\\nlying distributions are similar for the total interactions and\\nthe most dominant element. According to our KS tests, for\\nboth Likes on FB and Retweets on Twitter, we find that the\\ndistributions are significantly different (p-value <0.05) com-\\npared to the total interactions, suggesting that other elements\\nalso contribute to the overall engagement on both platforms\\nand hence need to be included in the visibility measure.\\nDV Transformation for RQ2. To satisfy assumptions of\\nlinear regression in RQ2, we transform all variables to be\\nclose to normal distributions using the ‚ÄúbestNormalize‚Äù R\\npackage. For Twitter, we transform variables as follows:\\nVIP(Yeo-Johnson), #posts (Box Cox), #Misinfo (sqrt),\\n#Uncivil (sqrt), Network visibility (Center+scale), #follow-\\ners (Box Cox), Centrality (None). For FB, we transform\\nvariables as follows: VIP(Yeo-Johnson), #posts (Box Cox),\\n#Misinfo (sqrt).Table 9: Robustness analysis of RQ1 results for 25 th, 50th,\\nand 75 thpercentile visibility on Twitter\\nIVs 25th50th75th\\nParty 0.120‚àó‚àó‚àó0.160‚àó‚àó‚àó0.160‚àó‚àó‚àó\\nGender 0.016 0.036 .0.060‚àó‚àó\\nEthnicity 0.004 -0.027 -0.054‚àó‚àó\\nPosting Freq. 0.255‚àó‚àó‚àó0.354‚àó‚àó‚àó0.386‚àó‚àó‚àó\\nTable 10: Robustness analysis of RQ1 results for 25 th, 50th,\\nand 75 thpercentile visibility on FB\\nIVs 25th50th75th\\nParty -0.293‚àó‚àó‚àó-0.300‚àó‚àó‚àó-0.291‚àó‚àó‚àó\\nGender -0.078‚àó‚àó‚àó-0.088‚àó‚àó‚àó-0.088‚àó‚àó‚àó\\nEthnicity -0.048 -0.047 . -0.032\\nPosting Freq. 0.376‚àó‚àó‚àó0.402‚àó‚àó‚àó0.424‚àó‚àó‚àó\\nRQ1 Tables. Table 8 shows the 95% CI for Table 4. Ta-\\nbles 9 and 10 show the results for 25 th, 50th, and 75 th\\npercentile visibility for Twitter and FB respectively.\\nRQ2 Tables. Table 12 shows the 95% CI for Table 5.\\nBenchmarks for thres ,w.Figure 9 shows the ECDF\\nplots for daily mean interactions received by legislators on\\nTwitter and FB, by party. For Twitter and FB, the medians\\nare close to 10 and 100 respectively. So we select thres =\\n10for Twitter and thres = 100 for FB. We do not have dif-\\nferent thres across parties since medians are similar across\\nparties on both platforms.\\n1001021041060.00.20.40.60.81.0A. T witter\\nparty\\nRep\\nDem\\n1011031050.00.20.40.60.81.0B. FB\\nparty\\nRep\\nDem\\ndaily average interactions received by legislatorsproportion\\nFigure 9: ECDF plots for daily mean interactions received\\nby legislators on Twitter and FB, by party.\\nFigure 10 shows the ECDF plots for daily number of posts\\nby legislators on Twitter and FB, by party. Again the median\\nposting rates are similar for Republicans and Democrats on1001011020.00.20.40.60.81.0A. T witter\\nparty\\nRep\\nDem\\n1001011020.00.20.40.60.81.0B. FB\\nparty\\nRep\\nDem\\ndaily #posts by legislatorsproportionFigure 10: ECDF plots for daily number of posts by legisla-\\ntors on Twitter and FB, by party.\\nTable 11: Dragonnet performance\\nAUC Macro F1\\noverall Extreme overall Extreme\\nTwitter uncivil 0.74 0.72 0.69 0.66\\nTwitter non-credible 0.73 0.74 0.66 0.68\\nFB non-credible 0.80 0.88 0.74 0.82\\nboth Twitter and FB. The median daily post count on Twitter\\nis 2 and 1 on FB. To ensure that we have a sufficient num-\\nber of posts per legislator to compute the overperforming\\nscores and simultaneously account for temporal variation in\\nour data, we select w= 14 for both Twitter and FB.\\nDragonnet Model Description. Dragonnet uses feed-\\nforward neural networks to learn balanced representations\\nof the data such that each head models a separate poten-\\ntial outcome, a third propensity head predicts the propen-\\nsity (œÄ) of being treated, and a free nudge parameter œµ. The\\nœÄandœµparameters are used to re-weight the outcomes to\\nprovide lower biased estimates of the Conditional Average\\nTreatment Effect (CATE). The error gradients from the two\\noutcome modeling heads are propagated back to the shared\\nrepresentation layers of the Dragonnet model to learn the\\ncovariate representation, i.e., œï(X). The representation lay-\\ners learn a balanced representation of the data since the\\nmodel objective is to predict both outcomes and each out-\\ncome modeling head learns a function of the transformed co-\\nvariate representation, i.e., Y(T) =h(œï(x), T). The CATE\\nfrom Dragonnet predictions is estimated as follows,\\nCATE =1\\nNNX\\ni(Y‚àó\\ni(1)‚àíY‚àó\\ni(0)) (4)\\nwhere,\\nY‚àó\\ni=ÀÜYi+ (Ti\\nœÄ(œï(Xi),1)‚àí1‚àíTi\\nœÄ(œï(Xi),0))√óœµ (5)\\nwhere ÀÜY(1)andÀÜY(0)are predictions returned by the\\ntwo outcome modeling heads respectively, œÄis the predicted\\npropensity of a sample being treated, and sample size N.\\nDragonnet Model Performance. Figure 11 shows the\\nROC curves for Twitter incivility, Twitter low-credibility\\nand FB low-credibility Dragonnet models. The AUC and\\nMacro F1-scores28are reported in Table 11.\\n28F1-scores reported at optimal cutoff\\n0.0 0.2 0.4 0.6 0.8 1.0\\nFPR0.00.20.40.60.81.0TPRT witter uncivil\\nOverall (AUC: 0.74)\\nHeadO=0 (AUC: 0.78)\\nHeadO=1 (AUC: 0.69)\\n0.0 0.2 0.4 0.6 0.8 1.0\\nFPR0.00.20.40.60.81.0TPRT witter non-credible\\nOverall (AUC: 0.73)\\nHeadO=0 (AUC: 0.67)\\nHeadO=1 (AUC: 0.77)\\n0.0 0.2 0.4 0.6 0.8 1.0\\nFPR0.00.20.40.60.81.0TPRFB non-credible\\nOverall (AUC: 0.80)\\nHeadO=0 (AUC: 0.75)\\nHeadO=1 (AUC: 0.87)Figure 11: ROC curves showing Dragonnet performance of\\noverall as well as two outcome modeling heads, for Twitter\\nincivility, Twitter and FB low-credibility respectively.\\nTable 12: 95% CI for Table 5\\nOL\\nIVs Twitter FB Twitter FB\\nParty [Rep] -0.133 0.423 -0.142 0.512\\n[-0.219, -0.046] [0.359, 0.487] [-0.252, -0.029] [0.431, 0.594]\\nGender [Men] -0.078 0.090 -0.011 0.113\\n[-0.149, -0.007] [0.029, 0.152] [-0.101, 0.079] [0.033, 0.194]\\nEthnicity [White] 0.020 0.015 0.020 0.040\\n[-0.061, 0.100] [-0.056, 0.086] [-0.079, 0.118] [-0.054, 0.133]\\n#posts 0.401 0.477 0.409 0.499\\n[0.345, 0.455] [0.443, 0.510] [0.337, 0.481] [0.459, 0.538]\\n#followers -0.028 - -0.028 -\\n[-0.076, 0.021] [-0.091, 0.034]\\nCentrality -0.020 - -0.029 -\\n[-0.060, 0.019] [-0.079, 0.021]\\nNetwork visibility 0.040 0.027 -\\n[0.002, 0.080] [-0.020, 0.076]\\n#Low-Credible -0.268 0.079 -0.335 -0.037\\n[-0.466, -0.070] [0.047, 0.110] [-0.601, -0.069][-0.074, 0.000]\\n#Uncivil 0.148 - 0.152 -\\n[0.101, 0.196] [0.092, 0.213]\\nParty [Rep] x 0.299 - 0.351 -\\n#Low-Credible [0.103, 0.496] [0.087, 0.614]\\nR20.289 0.382 0.291 0.386\\nCovariate Balance for Matching. We employ 1:1 match-\\ning such that each treated sample is matched to one un-\\ntreated sample. We use Nearest Neighbour matching based\\non Euclidean distance between the deconfounded tweet em-\\nbeddings. We find matches for 9677 (64.0%) uncivil tweets,\\n3957 (73.5%) low-credibility tweets, and 1583 (61.1%) low-\\ncredibility FB posts using a distance cutoff of 0.1 to maxi-\\nmize the covariate overlap. This gives us balanced represen-\\ntations of the observed covariates across the treated and un-\\ntreated samples as shown in Figure 6. We compute the stan-\\ndardized differences for each of the covariates before and\\nafter matching. A score between -0.1-0.1 indicates balance.\\nEffect of Outliers Outliers are common in social net-\\nworks, but their impact on analysis results and conclusions\\ncan vary. In our dataset, outliers may exist in terms of post-\\ning volume and engagement received due to the scale-free\\ndistributions (refer to Fig 12). However, we have taken mea-\\nsures to ensure these outliers do not significantly impact our\\nresults.\\nFor RQ1, we used a non-parametric statistical test, the\\nMann-Whitney U test, to compare distributions, which isrobust to outliers. For RQ2, we used non-linear transforma-\\ntions on study variables to minimize the impact of outliers\\nin the regression analysis.\\nIn RQ3, we matched accounts with similar characteris-\\ntics in the de-confounded embedding space, such as simi-\\nlar posting rates. This process either discarded outliers if no\\nadequate match was found or retained them if an adequate\\nmatch was identified. This matching step ensured the bal-\\nance of the covariates before running the estimation of the\\nConditional Average Treatment Effect (CATE), further re-\\nducing the impact of outliers.\\n1001011021031040.00.20.40.60.81.0ProportionA. T witter posting volume\\nparty\\nRepublican\\nDemocratic\\n101\\n1001011021031040.00.20.40.60.81.0ProportionB. Visibility on T witter\\nparty\\nRepublican\\nDemocratic\\nFigure 12: Scale-free distributions for (A) Posting volumes\\nand (B) Visibility of legislators on Twitter. The distributions\\nare also similar for FB.',\n",
       " 'Constructing the CORD-19 Vaccine Dataset\\nManisha Singh\\nUniversity of Washington\\nmanishas@uw.eduDivy Sharma\\nUniversity of Washington\\ndivy@uw.edu\\nAlonso Ma\\nUniversity of Washington\\namatake@uw.eduBridget Tyree\\nUniversity of Washington\\nbtyree@uw.eduMargaret Mitchell\\nUniversity of Washington\\nmargarmitchell@gmail.com\\nAbstract\\nWe introduce new dataset ‚ÄòCORD-19-Vaccination‚Äô1to cater to scientists specifically\\nlooking into COVID-19 vaccine-related research. This dataset is extracted from\\nCORD-19 dataset [Wang et al., 2020] and augmented with new columns for\\nlanguage detail, author demography, keywords, and topic per paper. Facebook‚Äôs\\nfastText model is used to identify languages [Joulin et al., 2016]. To establish author\\ndemography (author affiliation, lab/institution location, and lab/institution country\\ncolumns) we processed the JSON file for each paper and then further enhanced\\nusing Google‚Äôs search API to determine country values. ‚ÄòYake‚Äô was used to extract\\nkeywords from the title, abstract, and body of each paper and the LDA (Latent\\nDirichlet Allocation) algorithm was used to add topic information [Campos et al.,\\n2020, 2018a,b]. To evaluate the dataset, we demonstrate a question-answering\\ntask like the one used in the CORD-19 Kaggle challenge [Goldbloom et al., 2022].\\nFor further evaluation, sequential sentence classification was performed on each\\npaper‚Äôs abstract using the model from Dernoncourt et al. [2016]. We partially hand-\\nannotated the training dataset and used a pre-trained BERT-PubMed layer. ‚ÄòCORD-\\n19-Vaccination‚Äô contains 30k research papers and can be immensely valuable for\\nNLP research such as text mining, information extraction, and question answering,\\nspecific to the domain of COVID-19 vaccine research.\\n1 Introduction\\nA report released in early 2021 declared, ‚ÄúWorld to spend $157 billion on COVID-19 vaccines\\nthrough 2025‚Äù [Mishra, 2021]. Despite this, there are no datasets that are specific to COVID-19\\nvaccine research. The COVID-19 Open Research Dataset (CORD-19) [Wang et al., 2020] is a corpus\\nof academic papers on coronavirus research. However, the metadata file for the CORD-19 dataset\\n(release version 109) consists of over one million journals, resulting in big data issues and information\\noverload. The overall goal is to create a dataset that was based out of CORD-19 but only includes the\\npapers that are relevant to vaccine research. In this work, we introduce a dataset curated from the\\nCORD-19 dataset and tailored to aid research on COVID-19 vaccines.\\nOur approach utilizes a pipeline of information extraction ,data augmentation , and task implementa-\\ntion:\\nExtraction phase: In this phase, we created a SQLite data pipeline to manage the large volume\\nof the CORD-19 dataset. The language of each paper‚Äôs abstract was determined using Facebook‚Äôs\\n1Our dataset is available at https://github.com/manisha-Singh-UW/CORD-19-VaccinationarXiv:2407.18471v1  [cs.CL]  26 Jul 2024fastText library [Joulin et al., 2016]. Subsequently, using SQLite query we created a subset of the\\nCORD-19 dataset, taking only those papers where the starting ‚Äòpublish time‚Äô was ‚Äò2020‚Äô and either\\nthe ‚ÄòAbstract‚Äô or ‚ÄòTitle‚Äô contained the word ‚Äòvaccine‚Äô or ‚Äòvaccination‚Äô in all the languages present in\\nCORD-19.\\nData augmentation phase: In this phase we added new columns to the dataset. The language ID\\ndetermined from the previous phase was retained. Data on author affiliation was collected from the\\n‚Äòjson parse‚Äô files of the research papers web search of each research paper. Keywords were added\\nusing ‚ÄòYake‚Äô [Campos et al., 2020]. Finally, we implemented ‚ÄòTopic modeling‚Äô where we classified\\nthe dataset into topics based on the ‚ÄòAbstract‚Äô using the LDA model [Dernoncourt et al., 2016].\\nTask implementation phase: We implemented ‚ÄòQuestion and Answering‚Äô and ‚ÄòSequence sentence\\nclassification‚Äô task using the CORD-19-Vaccination dataset.\\nThe implementation of each of these steps is detailed in the sections below. Figure 1 shows a visual\\noverview of the creation of the dataset.\\nFigure 1: CORD-19-Vaccination dataset creation - overview\\nIn what follows, we motivate and describe each phase, highlighting discoveries at each step. We\\nthen demonstrate the utility of the dataset on the tasks of Question and Answering and Sequential\\nSentence Classification.\\n2Extraction phase: user and context information extraction from CORD-19\\nThe CORD-19-Vaccination dataset is extracted from the CORD-19 dataset based on the following\\nfiltering criteria:\\n2Publish time: The CORD-19 metadata.csv has a column publish_time . The extraction filter\\nextracts all data where publish_time is greater than or equal to ‚Äô2020‚Äô.\\nPattern search: ‚Äôvaccine‚Äô/‚Äôvaccination‚Äô in either title orabstract : CORD-19 metadata.csv\\ndoes not indicate the language of a paper. Figure 2 shows the language distribution of CORD-19\\nusing the fastText model. CORD-19-Vaccination dataset is extracted from the CORD-19 dataset. In\\norder to extract papers with the word ‚Äôvaccine/vaccination‚Äô in every language a query for information\\nextraction was customized to search the pattern of ‚Äôvaccine/vaccination‚Äô in every language. This\\nquery was applied on columns title andabstract . The language ID abbreviations are taken from\\nISTD.\\nFigure 2: Language distribution in CORD-19\\nFigure 3: Language distribution in CORD-19-Vaccination\\nPdf_json_files / pmc_json_files: CORD-19 metdata.csv has the columns pdf_json_files and\\npmc_json_files . These columns give the path of the json files in the CORD-19 dataset. All papers\\nselected had the pdf_json_file orpmc_json_files present.\\nAbstract is not null: All papers selected had the abstract column present. Our exploratory data\\nanalysis revealed that almost all standard published papers must follow a particular template where\\nthe abstract must be present. The papers which did not have an abstract were mostly articles that were\\nnot published papers. This improved the quality of our dataset as it only includes research papers.\\nSince several of the ‚ÄòCORD-19-Vaccination‚Äô columns are based on CORD-19, the preprocessing for\\nCORD-19 is detailed in Wang et al. [2020].\\n3 Data augmentation phase: language detail, author demography, keyword\\nand topic\\nTable 1 includes a list of fields added to the CORD-19-Vaccination dataset. The code to generate\\nthese augmented fields is available in the GitHub repo2.\\nTable 1: CORD-19-Vaccination augmented fields\\nLanguage Detail Author Demography Keywords Topic\\nlang_id ,\\nlang_id_confidence ,\\nlang_id_predictionsaff_lab_inst ,\\naff_location ,\\naff_countrykeywords topic ,\\ntopic_index ,\\ntopic_prob\\n2The code for the data augmentation is available at https://github.com/manisha-Singh-UW/\\nCORD-19-Vaccination\\n33.1 Language id\\nLanguage ID is included in the dataset in order to support text demography. The input to the fastText\\nmodel is the text of the ‚Äòabstract‚Äô from each paper and the output are the three fields as shown in\\nTable 2. The fastText model predicts the language as ‚ÄôEnglish‚Äô with a confidence level of ‚Äô0.9167‚Äô.\\nTable 2: Language id - sample data\\nlang_id lang_id_confidence lang_id_predictions\\nen 0.9167 en=0.9167, id=0.0055, fr=0.0043\\nHowever, the fastText model also gives a small confidence level to ‚ÄôIndonesian‚Äô at ‚Äô0.0055‚Äô and\\n‚ÄôFrench‚Äô at ‚Äô0.0043‚Äô. This is likely due to the medical domain including many loan words. In the\\nexample in Table 2, the confidence level of ‚ÄòEnglish‚Äô compared to the other languages is much higher,\\nso Language ID field is set to ‚ÄòEnglish‚Äô. The graph in Figure 3 shows the language distribution of the\\nCORD-19-Vaccination dataset.\\n3.2 Author‚Äôs demography (lab/institution location and country)\\nThe CORD-19 dataset contains authors and a journal name for each paper. However, in order to get\\nmore details regarding the authors‚Äô demography, we augmented the data with authors‚Äô ‚Äòlab/institution\\naffiliation‚Äô, ‚Äòlab/institution location‚Äô and ‚Äòlab/institution country‚Äô. Details on the authors‚Äô demography\\ncan be used to construct a collaboration network to illustrate collaborations or coauthor-ship relations\\namong institutions as in the article Haihua Chen [2022].\\nTable 3: Author detail - sample data\\naff_lab_inst aff_location aff_country\\nUniversity of Maryland\\nSchool of MedicinepostCode=21201; region=MD;\\nsettlement=BaltimoreUSA\\nThe author‚Äôs ‚Äôlab/institution affiliation‚Äô, ‚Äôlab/institution location‚Äô and ‚Äôlab/institution country‚Äô was\\nnot mentioned in the CORD-19 metadata file. However, in order to do descriptive analysis such\\nas number of the papers contributed by each institution, geographical distribution of institutions\\nand collaboration among institutions from different countries/regions we need the institution details\\nrelated to each author of the paper.\\nAdditionally, as the country of affiliation metadata was only available for approximately 63% of the\\nJSON files, further data augmentation was carried out to extract the country of the first author via\\nweb scraping. For this process, titles of papers with missing country data were searched through\\nPython‚Äôs Google search API and the HTML source code of the webpage corresponding to the first\\nquery result was parsed using Selenium and Beautiful Soup. Scraped titles from the search query and\\ntheir linked countries of affiliation were stored and subsequently validated by comparing similarity\\nbetween the original CORD-19 paper title and the scraped title. Entries with a similarity below 0.4\\n(calculated using the Sequence Matcher module from Python‚Äôs diff lib library) were excluded.\\nAuthor demographic: Speaker/Author demographic was mainly assessed via examination of the\\ndistribution of first author‚Äôs countries of affiliation. We initially extracted the country data from the\\nfull text JSON files, achieving coverage of 63% over the total of papers. Through web scraping, we\\nidentified the country of affiliation for an additional group of papers, increasing coverage to 93%.\\n50% of the total papers were concentrated over 7 countries: United States of America, China, India,\\nItaly, United Kingdom, Germany, and Canada, with the USA representing 20% of the dataset. A\\ncomplete map depicting the distribution of number of papers by country of affiliation of the first\\nauthor can be observed in Figure 4. Most notably, apart from the concentration of research in the\\npreviously mentioned countries, a stark lack of representation from the Global South (with the\\nexception of Brazil) was also evident.\\n4Figure 4: Number of papers by first author country of affiliation\\nThe authors‚Äô detail is present in the associated JSON file of each paper, from which institution of\\naffiliation, location and country were extracted. The input is the JSON file and output are the columns\\ncorresponding to each author and paper id.\\n3.3 Keywords from ‚Äôabstract‚Äô, ‚Äôtitle‚Äô and ‚Äôtext body‚Äô\\nTop 20 keywords have been extracted using the Yake library. Keywords from every paper can be used\\nfurther in topic modeling and for keyword search, which gives an idea about the main content of the\\npaper.\\nYake was used because it uses an unsupervised approach, which is corpus-independent, and domain\\nand language independent. Yake follows an unsupervised approach that builds upon features extracted\\nfrom the text, making it applicable to documents written in different languages without the need for\\ndomain-specific knowledge.\\nThe input to the Yake object is the text string, generated from the ‚Äôtitle‚Äô, ‚Äôabstract‚Äô and ‚Äôbody text‚Äô of\\nthe paper. The output is the list of the keywords. One can customize the number of the top key words\\nand n-grams. For our current implementation we have chosen the top-20 keywords and n-grams\\nsize as ‚Äô3‚Äô. The rest of the parameters for Yake are default values. A sample result of the keyword\\nextraction is shown in Table 4.\\nTable 4: Sample data: keywords of a paper\\nKeywords\\nDNA vaccine; archaeosome; DNA; recombinant DNA vaccine;\\npDNA - surface localized archaeosome ; archaeosome vaccines group;\\ncells; DNA vaccine candidate; localized archaeosome;\\nvaccine; archaeosome vaccines; groups;\\nplasmid DNA; gene DNA vaccine; PBS control groups;\\nrecombinant gene; pDNA-encapsulated archaeosomes; gene; mice; control groups\\n3.4 Topic modeling\\nWe further augmented the dataset by implementing a topic modelling algorithm (Latent Dirichlet\\nAllocation). Generation of topic labels has a twofold intention: first, given the variety of possible\\nthemes within the papers (even when filtered to only include vaccine-related documents), it provides\\na comprehensive overview of recurrent subjects and allows for easy inspection of the distribution\\namongst them. Second, it facilitates quick sub-setting of the data to allow potential users to fit more\\nscoped tasks.\\nTraining of the LDA was performed over the complete set of paper abstracts, which were pre-\\nprocessed into lower case; and had stop words, punctuations, and small words (e.g. character length\\nbelow 3) removed. We tested a range of ‚Äúnumber of topics (n)‚Äù parameters (from n=5 to n=14) and\\n5evaluated each model via the Coherence score described by R√∂der et al. [2015], ultimately selecting\\nn=5 as the final parameter due to its higher score and parsimony.\\nTo assign a label to each trained topic, we selected the top 20 words per topic and cross-referenced this\\nlist against the paper titles of the documents with the highest probability of pertaining to a particular\\ntheme. Additional evaluation was performed visually to assess possible topic overlaps by applying\\ndimensionality reduction (through t-SNE) over the topic distribution vectors of each document and\\nplotting them, colored by label. The resulting topic labels and their distribution over the dataset are\\nshown in Table 5.\\nTable 5: Topic distribution in CORD-19-Vaccination dataset\\nTopic % of dataset\\nT1: Vaccine development 20%\\nT2: Vaccination side-effects 14%\\nT3: Vaccination efficacy 16%\\nT4: Methodologies for COVID studies (e.g. simulations) 25%\\nT5: Vaccine uptake (by factors of age, sex, race, etc.) 25%\\n4 Task implementation phase\\nCORD-19-Vaccination dataset contains the metadata of approximately 30k research papers. As\\nthe next step, we evaluated the dataset by performing ‚ÄòQuestion and Answering‚Äô and ‚ÄòSequential\\nSentence Classification‚Äô tasks.\\n4.1 Question and answering task\\nWe designed a task similar to the Goldbloom et al. [2022]‚Äôs Kaggle competition challenge on CORD-\\n19. ‚ÄôCovid-19 vaccine‚Äô Question and Answering system is a domain specific task. In an ideal situation\\nwe need a medical expert to design the questions and evaluate the answers. However, in absence of a\\nmedical expert we designed a simple vaccine specific question. We tried to follow the \"user-based\\napproach\" as per Diekema et al. [2004] to evaluate the answer.\\nThe Question and Answering task consists of three parts: ‚Äôquestion‚Äô, ‚Äôcontext‚Äô, and ‚Äôanswer‚Äô. The\\ninput to the model is a covid-19 vaccine specific question and the context. In this implementation\\nwe are assuming that the question is contained in the context. We needed to keep the context small\\nto implement this model on 30k papers. This is done by selecting the papers similar to the answers,\\nusing ‚ÄôOkapi BM25‚Äô [Wikipedia, 2022]. Okapi BM25 is a ranking function used by search engines\\nto estimate the relevance of the document for a given search query. For each question and context, we\\nare using ‚ÄúHuggingface transformer library‚Äù to predict the answer [Wolf et al., 2020]. We have used\\nthe pretrained QA model ‚Äôbert-large-uncased-whole-word-masking finetuned-squad‚Äô. The solution\\nfor this task was customized for ‚ÄòCORD-19-vaccination‚Äô dataset which is inspired by Besomi [2020]‚Äôs\\nKaggle notebook.\\nFigure 5 is output of the ‚ÄòQuestion and Answering‚Äô task for the question ‚Äòis covid-19 vaccine safe?‚Äô.\\nFigure 5: Question and answering output\\n6Table 6: Answers evaluation: ‚Äôuser-based‚Äô analysis\\nQuestion Papers Citation Viewed Downloads\\nIs Covid-19 vaccine safe? 10.1038/s41577-021-00525-y 111\\n10.1016/j.puhe.2020.05.007 9\\n10.1093/jlb/lsaa024 3 1146 435\\n10.3390/vaccines10020298 627\\n10.1111/jdv.17499 3\\n10.1111/dth.15146 6\\nTable 6 gives the list of the papers as answers for the question ‚ÄôIs Covid-19 vaccine safe? ‚Äô. According\\nto the \"user-based approach\" of evaluation we can say that the papers in the result seem relevant, as\\nmost of the papers were recently published.\\n‚ÄôCORD-19-Vaccination‚Äô dataset is better than ‚ÄôCORD-19‚Äô for vaccination related ‚ÄôQuestion and\\nAnswering‚Äô task due to following reasons: ‚ÄôCORD-19-Vaccination‚Äô was augmented with fields in\\nTable 5. These fields are not present in ‚ÄôCORD-19‚Äô. Researchers using ‚ÄôCORD-19-Vaccination‚Äô can\\nmake use of these augmented fields for better answers.\\nThe column keyword in ‚ÄôCORD-19-Vaccination‚Äô dataset is the list of keywords extracted\\nfrom the body of the text papers using ‚ÄôYake‚Äô, so if we extract the context search using ‚ÄôTi-\\ntle‚Äô/‚ÄôAbstract‚Äô/‚ÄôKeyword‚Äô, the answer should be more accurate.\\n4.2 Sequential sentence classification task\\nText classification is a very important task in Natural Language Processing (NLP) where a label\\nor class is assigned to a text. In the current task, the focus is on the classification of sentences in\\nmedical abstracts. The sentences in the abstracts appear in a sequence therefore this task is called\\n\"Sequential Sentence Classification Task\". This task converts unstructured block-of-text abstracts into\\nstructured abstracts (text organized into semantic headings such as Background, Methods, Results,\\nand Conclusions), making it easy to quickly locate relevant information. This task is based on the\\npaper Dernoncourt et al. [2016]. The output of this task has been provided as a new column named\\nlabeled_abstract in the dataset.\\nThe data for training the model for this task is obtained from the PubMed 200k RCT dataset\\nDernoncourt and Lee [2017] and the CORD-19 dataset itself. 11.58% of the abstracts from the\\nCORD-19 dataset (approximately 117k samples) were found to have abstracts structured with\\nsemantic headings. Similarly, 14.66% of the records in the CORD-19-Vaccine dataset (4294 samples)\\nwere found to have abstracts structured with semantic headings. These records were split into test\\nand validation datasets for model training. A single data sample contains information on target labels,\\nsentence from abstract, and order of sentences, compatible with Dernoncourt et al. [2016]. The\\npubmed_id andcord_uid fields are available as comments and are not inputs to the training model.\\nAs per the guidance of the PubMed 200k RCT paper, numbers from the dataset have been replaced\\nwith the @ sign.\\nThe distribution of various target labels is shown across datasets in Figure 6. It is important to note\\nthat the percentage of OBJECTIVE labels is quite high (at 16.13%) in the CORD-19-Vaccination\\ndataset, while the percentage of CONCLUSION labels is quite low (at 6.21%) compared to PubMed\\nRCT200k and CORD-19 datasets.\\nTask Workflow: The workflow in figure 7 shows the sequence of tasks performed during the\\ntraining and subsequent fine-tuning of the model. This particular workflow was chosen to allow\\ncoarse-grained to fine-grained model training.\\nThe model architecture used for training is based on the Dernoncourt et al. [2016] paper. A pre-\\ntrained and frozen BERT-PubMed layer has been used to improve performance. The original\\ntraining/validation data split of PubMed 200k RCT dataset was used fir the initial round of training.\\n7Figure 6: Distribution of target labels across datasets\\nFigure 7: Task workflow\\nFor fine tuning, a random split of 70-30 was used for stage 1 and split of 50-50 was used for stage 2.\\nThe model training was performed initially with a learning rate of 1e-4, which was reduced to 1e-4\\nfor fine tuning. A system based on Nvidia Tesla P100 GPU was used for training.\\nOutput: Table 7 shows performance metrics of this model on the CORD-19-Vaccine dataset.\\nTable 7: Performance metric on CORD-19-Vaccine\\nAccuracy F1-score Precision Recall\\n0.7618 0.7569 0.7569 0.7618\\nEvaluation: Figures 8 and 9 are the Confusion Matrix plotted using scikit learn. The matrix in\\nFigure 8 shows the raw numbers of label distribution, while the matrix in Figure 9 is normalized on\\nthe ‚Äútrue‚Äù labels.\\nWe can observe from the confusion matrix that the OBJECTIVE label was often confused with\\nBACKGROUND and METHODS. Similarly, METHODS label was often confused with RESULTS.\\nAdditional evaluation was performed by manually reviewing the most wrong predictions. Some\\npatterns that we found in these predictions are short sentences consisting of just a few words were\\nincorrectly predicted, and ungrammatical or ambiguous sentences were misclassified.\\n8Figure 8: Confusion matrix on raw numbers\\n Figure 9: Confusion matrix on normalized\\ntrue labels\\n5 License\\n‚ÄòCORD-19-Vaccination‚Äô dataset is extracted from ‚ÄòCORD-19‚Äô dataset, so ‚ÄòCORD-19-Vaccination‚Äô\\ndataset also follows all the licenses3that are followed by ‚ÄòCORD-19‚Äô.\\n6 Conclusion\\nIn this paper, we are introducing our new dataset ‚ÄòCORD-19-Vaccination‚Äô. This dataset consists of\\napproximately 30k rows of metadata of scientific research papers, specific to the domain of COVID-\\n19 vaccine research, making it the largest known curated resource in this domain. This dataset\\nhas been augmented with valuable details that extends the information present in the CORD-19\\ndataset. The ‚ÄòQuestion and Answering‚Äô and ‚ÄòSequential Sentence Classification‚Äô evaluation results\\nfurther highlights the value of this dataset for various NLP tasks. We hope that the release of this\\ndataset can be immensely valuable to the COVID-19 vaccine-research community and used for NLP\\nresearch such as text mining, information extraction, and question answering, specific to the domain\\nof COVID-19 vaccine research.\\nReferences\\nJonathan Besomi. A qa model to answer them all, 2020. URL https://www.kaggle.com/code/\\njonathanbesomi/a-qa-model-to-answer-them-all/comments .\\nRicardo Campos, V√≠tor Mangaravite, Arian Pasquali, Al√≠pio M√°rio Jorge, C√©lia Nunes, and Adam\\nJatowt. A text feature based automatic keyword extraction method for single documents. In\\nGabriella Pasi, Benjamin Piwowarski, Leif Azzopardi, and Allan Hanbury, editors, Advances in\\nInformation Retrieval , pages 684‚Äì691, Cham, 2018a. Springer International Publishing. ISBN\\n978-3-319-76941-7.\\nRicardo Campos, V√≠tor Mangaravite, Arian Pasquali, Al√≠pio M√°rio Jorge, C√©lia Nunes, and Adam\\nJatowt. Yake! collection-independent automatic keyword extractor. In Gabriella Pasi, Benjamin\\nPiwowarski, Leif Azzopardi, and Allan Hanbury, editors, Advances in Information Retrieval , pages\\n806‚Äì810, Cham, 2018b. Springer International Publishing. ISBN 978-3-319-76941-7.\\n3CORD-19 Dataset License: https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.\\ncom/2020-03-13/COVID.DATA.LIC.AGMT.pdf\\n9Ricardo Campos, V√≠tor Mangaravite, Arian Pasquali, Al√≠pio Jorge, C√©lia Nunes, and Adam Jatowt.\\nYake! keyword extraction from single documents using multiple local features. Information\\nSciences , 509:257‚Äì289, 2020. ISSN 0020-0255. doi: https://doi.org/10.1016/j.ins.2019.09.013.\\nURL https://www.sciencedirect.com/science/article/pii/S0020025519308588 .\\nFranck Dernoncourt and Ji Young Lee. Pubmed 200k RCT: a dataset for sequential sentence\\nclassification in medical abstracts. CoRR , abs/1710.06071, 2017. URL http://arxiv.org/\\nabs/1710.06071 .\\nFranck Dernoncourt, Ji Young Lee, and Peter Szolovits. Neural networks for joint sentence classifica-\\ntion in medical paper abstracts, 2016. URL https://arxiv.org/abs/1612.05251 .\\nAnne Diekema, Ozgur Yilmazel, and Elizabeth Liddy. Evaluation of restricted domain question-\\nanswering systems. Center for Natural Language Processing , 01 2004.\\nAnthony Goldbloom, Allen Institute for AI, Peijen Lin, Paul Mooney, Carissa Schoenick, Sebastian\\nKolmeier, Debrishi, Timo Bozsolik, and Ben Hammer. Covid-19 open research dataset challenge\\n(cord-19), 2022. URL https://www.kaggle.com/datasets/allen-institute-for-ai/\\nCORD-19-research-challenge?datasetId=551982&sortBy=dateCreated .\\nHuyen Nguyen Haihua Chen, Jiangping Chen. Demystifying covid-19 publications: institutions,\\njournals, concepts, and topics, 2022. URL https://jmla.pitt.edu/ojs/jmla/article/\\nview/1141/1342 .\\nISTD. Language studies. URL https://www.science.co.il/language/Codes.php .\\nArmand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, H√©rve J√©gou, and Tomas Mikolov.\\nFasttext.zip: Compressing text classification models, 2016. URL https://arxiv.org/abs/\\n1612.03651 .\\nManas Mishra. World to spend $157 billion on covid-19 vaccines through 2025 -report,\\n2021. URL https://www.reuters.com/business/healthcare-pharmaceuticals/\\nworld-spend-157-billion-covid-19-vaccines-through-2025-report-2021-04-29/ .\\nMichael R√∂der, Andreas Both, and Alexander Hinneburg. Exploring the space of topic coherence\\nmeasures. In Proceedings of the Eighth ACM International Conference on Web Search and Data\\nMining , WSDM ‚Äô15, page 399‚Äì408, New York, NY , USA, 2015. Association for Computing\\nMachinery. ISBN 9781450333177. doi: 10.1145/2684822.2685324. URL https://doi.org/\\n10.1145/2684822.2685324 .\\nLucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar, Russell Reas, Jiangjiang Yang, Doug Burdick,\\nDarrin Eide, Kathryn Funk, Yannis Katsis, Rodney Michael Kinney, Yunyao Li, Ziyang Liu,\\nWilliam Merrill, Paul Mooney, Dewey A. Murdick, Devvret Rishi, Jerry Sheehan, Zhihong Shen,\\nBrandon Stilson, Alex D. Wade, Kuansan Wang, Nancy Xin Ru Wang, Christopher Wilhelm, Boya\\nXie, Douglas M. Raymond, Daniel S. Weld, Oren Etzioni, and Sebastian Kohlmeier. CORD-\\n19: The COVID-19 open research dataset. In Proceedings of the 1st Workshop on NLP for\\nCOVID-19 at ACL 2020 , Online, July 2020. Association for Computational Linguistics. URL\\nhttps://aclanthology.org/2020.nlpcovid19-acl.1 .\\nWikipedia. Okapi bm25, 2022. URL https://en.wikipedia.org/wiki/Okapi_BM25 .\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\\nPierric Cistac, Tim Rault, R√©mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von\\nPlaten, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama\\nDrame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language\\nprocessing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language\\nProcessing: System Demonstrations , pages 38‚Äì45, Online, October 2020. Association for Compu-\\ntational Linguistics. URL https://www.aclweb.org/anthology/2020.emnlp-demos.6 .\\n10',\n",
       " 'arXiv:2408.07322v1  [cs.IT]  14 Aug 2024Encoding and Decoding Algorithms of ANS Variants and\\nEvaluation of Their Average Code Lengths‚àó ‚Ä†\\nHirosuke Yamamoto‚Ä°Ken-ich Iwata¬ß\\nAugust 15, 2024\\nAbstract\\nAsymmetric Numeral Systems (ANS) proposed by Jarek Duda are high-performance distortionless\\ndata compressionschemes that can achieve almost the same co mpression performanceas arithmetic codes\\nwith less arithmetic operations than arithmetic coding. Th e ANS is widely used in various practical\\nsystems like Facebook, Apple, Google, Dropbox, Microsoft, and Pixar, due to their high performance, but\\nmany researchers still lack much knowledge about the ANS. Th is paper thoroughly explains the encoding\\nand decoding algorithms of the ANS, and theoretically analy zes the average code length achievable by\\nthe ANS.\\nIndex terms‚Äî ANS (Asymmetric Numeral Systems), arithmetic code, distor tionless data-compression code,\\naverage code length\\n1 Introduction\\nIn conventional data compression coding like HuÔ¨Äman coding and arithmetic coding, a data sequence\\n/u1D460/u1D447=/u1D4601/u1D4602¬∑¬∑¬∑/u1D460/u1D447is encoded and decoded in the order of /u1D460/u1D461,/u1D461=1,2,¬∑¬∑¬∑,/u1D447[2][3]. But, Jarek Duda proposed\\nAsymmetric Numeral Systems (ANS) to enhance Arithmetic cod ing, such that /u1D460/u1D447is encoded in backward\\norder/u1D460/u1D461,/u1D461=/u1D447,¬∑¬∑¬∑,2,1, while/u1D460/u1D447is decoded in forward order /u1D460/u1D461,/u1D461=1,2,¬∑¬∑¬∑,/u1D447[4]‚Äì[14].\\nSuppose that /u1D450is the codeword obtained by arithmetic coding for a data sequ ence/u1D460/u1D447. Since the arithmetic\\ncode encodes /u1D460/u1D447in forward order of /u1D460/u1D461,/u1D461=1,2,¬∑¬∑¬∑,/u1D447, it is determined by the order of the most signiÔ¨Åcant\\nbit (MSB) to the least signiÔ¨Åcant bit (LSB) of the value of /u1D450. When/u1D460/u1D461is encoded, the arithmetic encoder\\ndoes not know the subsequent sequence /u1D460/u1D447\\n/u1D461+1=/u1D460/u1D461+1¬∑¬∑¬∑/u1D460/u1D447. Therefore, arithmetic codes are encoded using\\nintervals of real numbers (or intervals of integers) includ ing/u1D450so that they can handle any subsequent\\nsequence /u1D460/u1D447\\n/u1D461+1. On the other hand, since the ANS encodes /u1D460/u1D447in backward order /u1D460/u1D461,/u1D461=/u1D447,/u1D447‚àí1,¬∑¬∑¬∑,1, the\\ncodeword /u1D450is determined by the order of the LSB to the MSB. As a result, th e ANS can encode and decode\\n/u1D460/u1D447using a single integer variable. This means that the ANS can a chieve almost the same compression rate\\nas arithmetic codes with less arithmetic operations. Due to this excellent feature, the ANS is utilized by\\nFacebook Zstandard (ZSTD) compressor, Apple LZFSE compres sor, Google, Dropbox, Microsoft, Pixar,\\netc. [14][15][16], and recently, many applied and related p apers have been published [16]‚Äì[34].\\n‚àóThis paper is an English translation version of an invited pa per published in the IEICE Transactions on Fundamentals of\\nElectronics, Communications and Computer Sciences (Japan ese Edition), DOI: 10.14923/transfunj.2024JAI0001, July 11, 2024.\\n‚Ä†This paper is a reworked version of [1] which was presented at the 12th Shannon Theory Workshop (STW2023).\\n‚Ä°The University of Tokyo, hirosuke@ieee.org.\\n¬ßUniversity of Fukui, k-iwata@u-fukui.ac.jp.\\n1However, many people are still unaware of ANS because many pa pers on ANS are only published\\non arXiv.org or as conference papers rather than as journal p apers. Furthermore, the algorithms and\\nperformance analyses in these papers are not written in an ea sy-to-understand manner, and there has been\\nlittle information-theoretical evaluation. Therefore, e ven though people know the name of ANS, many of\\nthem do not know the detailed encoding-decoding algorithms and theoretical compression performance.\\nIn this paper, we provide a detailed and easy-to-understand explanation of the ANS encoding and\\ndecoding algorithms, and present a new information-theore tical evaluation of the average code length that\\nthe ANS can achieve.\\nThere are several variants of ANS. We treat ABS (Asymmetric B inary Systems) in Section 2, rANS\\n(range variant of ANS) in Sections 3 and 4, and tANS (tabled va riant of ANS /one.sup) in Section 5. In each section,\\nwe describe an encoding function, a decoding function, an en coding algorithm, and a decoding algorithm,\\nand demonstrate how these functions and algorithms can be us ed to encode and decode any data sequence\\ncorrectly. Furthermore, we derive a strict information-th eoretic upper bound on the expected value of the\\naverage code length per source symbol, which we call the average code length below for simplicity.\\nIn this paper, we assume that a data sequence /u1D460/u1D447=/u1D4601/u1D4602¬∑¬∑¬∑/u1D460/u1D447,/u1D460/u1D461‚ààS, is generated from an i.i.d. source,\\nwhich takes a value on a Ô¨Ånite discrete alphabet Swith a probability distribution /u1D45D={/u1D45D(/u1D460) |/u1D460‚àà\\nS}. For simplicity, the encoding and decoding algorithms are d escribed assuming that the probability\\ndistribution /u1D45Dand sequence length /u1D447are known. We use the following notations /two.sup. Let lg/u1D44E=log2/u1D44E,\\nand let|A|represent the cardinality of a set A. The entropy of the source is represented by /u1D43B(/u1D45D)=/summationtext.1\\n/u1D460‚ààS/u1D45D(/u1D460)lg(1//u1D45D(/u1D460)), and the relative entropy to a probability distribution /u1D45E={/u1D45E(/u1D460)|/u1D460‚ààS} is denoted\\nby/u1D437(/u1D45D/‚åäar‚åà‚åäl/u1D45E)=/summationtext.1\\n/u1D460‚ààS/u1D45D(/u1D460)lg(/u1D45D(/u1D460)//u1D45E(/u1D460)).\\n2 ABS\\nIn this section, we treat the case of binary source alphabet S={0,1}with/u1D45D1=/u1D45D(1),/u1D45D0=/u1D45D(0)=1‚àí/u1D45D1,\\n0< /u1D45D 1<1. In the encoding and decoding of the Asymmetric Binary Syst ems (ABS) /three.sup, a single integer\\nvariable/u1D465is used. For simplicity, we assume that /u1D465can have any number of digits.\\n2.1 Encoding and decoding procedures of ABS [4]‚Äì[6]\\nA. DeÔ¨Ånition of encoding function\\nEncoding function /u1D465/u1D461‚àí1:=/u1D436(/u1D460/u1D461,/u1D465/u1D461)is deÔ¨Åned by (1) and (2). When we want to specify whether /u1D465/u1D461‚àí1is\\nobtained by /u1D460/u1D461=0 or/u1D460/u1D461=1, we represent it as /u1D465/u1D461‚àí1=/u1D465(/u1D460/u1D461)\\n/u1D461‚àí1.\\n/u1D465/u1D461‚àí1=/u1D465(0)\\n/u1D461‚àí1:=/ceilingleftbigg/u1D465/u1D461+1\\n/u1D45D0/ceilingrightbigg\\n‚àí1 if/u1D460/u1D461=0, (1)\\n/u1D465/u1D461‚àí1=/u1D465(1)\\n/u1D461‚àí1:=/floorleftbigg/u1D465/u1D461\\n/u1D45D1/floorrightbigg\\nif/u1D460/u1D461=1. (2)\\nB. DeÔ¨Ånition of decoding function\\nDecoding function (/u1D460/u1D461,/u1D465/u1D461):=/u1D437(/u1D465/u1D461‚àí1)is deÔ¨Åned by (3)‚Äì(5).\\n/u1D460/u1D461:=‚åà(/u1D465/u1D461‚àí1+1)/u1D45D1‚åâ‚àí‚åà/u1D465/u1D461‚àí1/u1D45D1‚åâ. (3)\\n/one.supIt is also called tabled ANS or table-based ANS.\\n/two.supThe notations used in this paper may be diÔ¨Äerent from the orig inal papers.\\n/three.supIt is also called uABS (uniform ABS) [6].\\n2For/u1D465(/u1D460/u1D461)\\n/u1D461‚àí1:=/u1D465/u1D461‚àí1,\\n/u1D465/u1D461:=/u1D465(0)\\n/u1D461‚àí1‚àí‚åà/u1D465(0)\\n/u1D461‚àí1/u1D45D1‚åâif/u1D460/u1D461=0, (4)\\n/u1D465/u1D461:=‚åà/u1D465(1)\\n/u1D461‚àí1/u1D45D1‚åâ if/u1D460/u1D461=1. (5)\\nC. Encoding algorithm\\na. For a given data sequence /u1D460/u1D447=/u1D4601/u1D4602¬∑¬∑¬∑/u1D460/u1D447, set/u1D465/u1D447‚Üê1.\\nb. Repeat /u1D465/u1D461‚àí1‚Üê/u1D436(/u1D460/u1D461,/u1D465/u1D461)in backward order, /u1D461=/u1D447,¬∑¬∑¬∑,2,1.\\nc. The codeword of /u1D460/u1D447is given by /u1D4650.\\nD. Decoding algorithm\\na. Set a codeword /u1D4650.\\nb. Repeat(/u1D460/u1D461,/u1D465/u1D461)‚Üê/u1D437(/u1D465/u1D461‚àí1)in forward order, /u1D461=1,2,¬∑¬∑¬∑,/u1D447.\\nc./u1D460/u1D447=/u1D4601/u1D4602¬∑¬∑¬∑/u1D460/u1D447is the decoded sequence.\\nWe prove that /u1D437(/u1D465/u1D461‚àí1)deÔ¨Åned by (3)‚Äì(5) is the inverse function of /u1D436(/u1D460/u1D461,/u1D465/u1D461)deÔ¨Åned by (1)‚Äì(2). We Ô¨Årst\\ndeÔ¨Åne ÀÜ/u1D45F(0)\\n/u1D461and ÀÜ/u1D45F(1)\\n/u1D461by\\nÀÜ/u1D45F(0)\\n/u1D461=/u1D465(0)\\n/u1D461‚àí1+1‚àí/u1D465/u1D461+1\\n/u1D45D0, (6)\\nÀÜ/u1D45F(1)\\n/u1D461=/u1D465/u1D461\\n/u1D45D1‚àí/u1D465(1)\\n/u1D461‚àí1. (7)\\nFrom (1) and (2), they satisfy 0 ‚â§ÀÜ/u1D45F(0)\\n/u1D461,ÀÜ/u1D45F(1)\\n/u1D461<1. We next deÔ¨Åne /u1D45F(0)\\n/u1D461and/u1D45F(1)\\n/u1D461by/u1D45F(0)\\n/u1D461=1‚àí/u1D45D0(1‚àíÀÜ/u1D45F(0)\\n/u1D461)and\\n/u1D45F(1)\\n/u1D461=/u1D45D1ÀÜ/u1D45F(1)\\n/u1D461. Then, from relations 0 < /u1D45D 1=1‚àí/u1D45D0‚â§1‚àí/u1D45D0(1‚àíÀÜ/u1D45F(0)\\n/u1D461)<1 and 0‚â§/u1D45D1ÀÜ/u1D45F(1)\\n/u1D461< /u1D45D 1<1, they\\nsatisfy\\n0‚â§/u1D45F(1)\\n/u1D461< /u1D45D 1‚â§/u1D45F(0)\\n/u1D461<1. (8)\\nFrom (6), we have /u1D465/u1D461=/u1D465(0)\\n/u1D461‚àí1‚àí[/u1D465(0)\\n/u1D461‚àí1/u1D45D1+{1‚àí/u1D45D0(1‚àíÀÜ/u1D45F(0)\\n/u1D461)}]=/u1D465(0)\\n/u1D461‚àí1‚àí[/u1D465(0)\\n/u1D461‚àí1/u1D45D1+/u1D45F(0)\\n/u1D461]. Since/u1D465/u1D461and/u1D465(0)\\n/u1D461‚àí1are\\nintegers,[/u1D465(0)\\n/u1D461‚àí1/u1D45D1+/u1D45F(0)\\n/u1D461]must be an integer. Noting that /u1D45F(0)\\n/u1D461satisÔ¨Åes (8), we obtain /u1D465(0)\\n/u1D461‚àí1/u1D45D1+/u1D45F(0)\\n/u1D461=‚åà/u1D465(0)\\n/u1D461‚àí1/u1D45D1‚åâ.\\nHence (4) holds. On the other hand, from (7), we have /u1D465/u1D461=/u1D465(1)\\n/u1D461‚àí1/u1D45D1+/u1D45D1ÀÜ/u1D45F(1)\\n/u1D461=/u1D465(1)\\n/u1D461‚àí1/u1D45D1+/u1D45F(1)\\n/u1D461. Since/u1D465/u1D461is an\\ninteger and /u1D45F(1)\\n/u1D461satisÔ¨Åes (8), we obtain /u1D465(1)\\n/u1D461‚àí1/u1D45D1+/u1D45F(1)\\n/u1D461=‚åà/u1D465(1)\\n/u1D461‚àí1/u1D45D1‚åâ. Hence, (5) holds.\\nFrom the above consideration, we have for any /u1D460/u1D461‚àà{0,1}that\\n‚åà/u1D465(/u1D460/u1D461)\\n/u1D461‚àí1/u1D45D1‚åâ=/u1D465(/u1D460/u1D461)\\n/u1D461‚àí1/u1D45D1+/u1D45F(/u1D460/u1D461)\\n/u1D461, (9)\\nand (3) can be derived as follows.\\n‚åà(/u1D465(/u1D460/u1D461)\\n/u1D461‚àí1+1)/u1D45D1‚åâ‚àí‚åà/u1D465(/u1D460/u1D461)\\n/u1D461‚àí1/u1D45D1‚åâ=‚åà(/u1D465(/u1D460/u1D461)\\n/u1D461‚àí1+1)/u1D45D1‚àí‚åà/u1D465(/u1D460/u1D461)\\n/u1D461‚àí1/u1D45D1‚åâ‚åâ\\n=‚åà(/u1D465(/u1D460/u1D461)\\n/u1D461‚àí1+1)/u1D45D1‚àí(/u1D465(/u1D460/u1D461)\\n/u1D461‚àí1/u1D45D1+/u1D45F(/u1D460/u1D461)\\n/u1D461)‚åâ\\n=‚åà/u1D45D1‚àí/u1D45F(/u1D460/u1D461)\\n/u1D461‚åâ\\n=/u1D460/u1D461, (10)\\nwhere the 2nd and 4th equalities hold from (9) and (8), respec tively.\\n3Remark 1 We can use (11) and (12), instead of (1) and (2), and (13)‚Äì(15) , instead of (3)‚Äì(5), as the encoding\\nfunction/u1D436(/u1D460/u1D461,/u1D465/u1D461)and the decoding function /u1D437(/u1D465/u1D461‚àí1)[5][6].\\n/u1D465/u1D461‚àí1=/u1D465(0)\\n/u1D461‚àí1:=/floorleftbigg/u1D465/u1D461\\n/u1D45D0/floorrightbigg\\nif/u1D460/u1D461=0, (11)\\n/u1D465/u1D461‚àí1=/u1D465(1)\\n/u1D461‚àí1:=/ceilingleftbigg/u1D465/u1D461+1\\n/u1D45D1/ceilingrightbigg\\n‚àí1 if /u1D460/u1D461=1, (12)\\n/u1D460/u1D461:=‚åä(/u1D465/u1D461‚àí1+1)/u1D45D1‚åã‚àí‚åä/u1D465/u1D461‚àí1/u1D45D1‚åã, (13)\\n/u1D465/u1D461:=/u1D465(0)\\n/u1D461‚àí1‚àí‚åä/u1D465(0)\\n/u1D461‚àí1/u1D45D1‚åã if/u1D460/u1D461=0, (14)\\n/u1D465/u1D461:=‚åä/u1D465(1)\\n/u1D461‚àí1/u1D45D1‚åã if/u1D460/u1D461=1. (15)\\n2.2 Average code length of ABS\\nSubstituting (4) and (5) into (9), we obtain\\n/u1D465(0)\\n/u1D461‚àí1\\n/u1D465/u1D461=1\\n/u1D45D0/parenleftBigg\\n1+/u1D45F(0)\\n/u1D461\\n/u1D465/u1D461/parenrightBigg\\n, (16)\\n/u1D465(1)\\n/u1D461‚àí1\\n/u1D465/u1D461=1\\n/u1D45D1/parenleftBigg\\n1‚àí/u1D45F(1)\\n/u1D461\\n/u1D465/u1D461/parenrightBigg\\n. (17)\\nLet/u1D4650(/u1D460/u1D447)represent the codeword /u1D4650of a data sequence /u1D460/u1D447. Since/u1D465/u1D447=1, the bit length /four.supof/u1D4650(/u1D460/u1D447)\\nis given by lg /u1D4650(/u1D460/u1D447)=/summationtext.1/u1D447\\n/u1D461=1lg(/u1D465(/u1D460/u1D461)\\n/u1D461‚àí1//u1D465/u1D461). Therefore, lg(/u1D465(/u1D460/u1D461)\\n/u1D461‚àí1//u1D465/u1D461)represents the increase of code length\\ncaused by encoding /u1D460/u1D461. We note from (8), (16), and (17) that when /u1D465/u1D461is suÔ¨Éciently large, the relations\\nlg(/u1D465(0)\\n/u1D461‚àí1//u1D465/u1D461)‚âà‚àí lg/u1D45D0and lg(/u1D465(1)\\n/u1D461‚àí1//u1D465/u1D461)‚âà‚àí lg/u1D45D1holds with very good accuracy.\\nWe consider the case /five.supof 0< /u1D45D 1<1/2< /u1D45D 0<1. We now deÔ¨Åne /u1D702=min{1//u1D45D0,1/(2/u1D45D1)}>1.\\nThen, from (16), we have /u1D465(0)\\n/u1D461‚àí1> /u1D465/u1D461//u1D45D0‚â•/u1D702/u1D465/u1D461. Furthermore, noting 0 ‚â§/u1D45F(1)\\n/u1D461< /u1D45D 1<1/2 and/u1D465/u1D461‚â•1 in\\n(17), we also have /u1D465(1)\\n/u1D461‚àí1> /u1D465/u1D461/(2/u1D45D1)‚â•/u1D702/u1D465/u1D461. Hence, for any /u1D461,1‚â§/u1D461‚â§/u1D447and any/u1D460‚àà{0,1},/u1D465(/u1D460)\\n/u1D461‚àí1satisÔ¨Åes\\n/u1D465(/u1D460)\\n/u1D461‚àí1> /u1D702/u1D465/u1D461> /u1D702/u1D447‚àí/u1D461+1/u1D465/u1D447=/u1D702/u1D447‚àí/u1D461+1. From this inequality, (8), (16), and (17), lg (/u1D465(/u1D460)\\n/u1D461‚àí1//u1D465/u1D461)is bounded by\\nlg/u1D465(/u1D460)\\n/u1D461‚àí1\\n/u1D465/u1D461<lg1\\n/u1D45D/u1D460/parenleftbigg\\n1+1\\n/u1D465/u1D461/parenrightbigg\\n=lg1\\n/u1D45D/u1D460+lg/parenleftbigg\\n1+1\\n/u1D465/u1D461/parenrightbigg\\n‚â§lg1\\n/u1D45D/u1D460+lg/u1D452\\n/u1D465/u1D461<lg1\\n/u1D45D/u1D460+lg/u1D452\\n/u1D702/u1D447‚àí/u1D461. (18)\\nUsing (18), we can derive an upper bound of the average code le ngth/u1D43Ffor the case of /u1D465/u1D447=1 as follows.\\n/u1D43F=1\\n/u1D447/summationdisplay.1\\n/u1D460/u1D447‚ààS/u1D447/u1D45D(/u1D460/u1D447)lg/u1D4650(/u1D460/u1D447)\\n=1\\n/u1D447/summationdisplay.1\\n/u1D460/u1D447‚ààS/u1D447/u1D45D(/u1D460/u1D447)/u1D447/summationdisplay.1\\n/u1D461=1lg/u1D465(/u1D460/u1D461)\\n/u1D461‚àí1\\n/u1D465/u1D461\\n/four.supExcept for the MSB, the bit length of /u1D4650(/u1D460/u1D447)is given by‚åälg/u1D4650(/u1D460/u1D447)‚åãbits. But, for simplicity, we use real number lg /u1D4650(/u1D460/u1D447)to\\nrepresent the bit length of /u1D4650(/u1D460/u1D447)in this paper.\\n/five.supIt is not necessary to consider the case of /u1D45D0=/u1D45D1=1/2 because we cannot compress /u1D460/u1D447in this case. In the case of /u1D45D0< /u1D45D1,\\nwe use (11) and (12).\\n4=1\\n/u1D447/u1D447/summationdisplay.1\\n/u1D461=1/summationdisplay.1\\n/u1D460/u1D461‚àà{0,1}/u1D45D(/u1D460/u1D461)lg/u1D465(/u1D460/u1D461)\\n/u1D461‚àí1\\n/u1D465/u1D461\\n<1\\n/u1D447/u1D447/summationdisplay.1\\n/u1D461=1/summationdisplay.1\\n/u1D460‚àà{0,1}/u1D45D/u1D460/parenleftbigg\\nlg1\\n/u1D45D/u1D460+lg/u1D452\\n/u1D702/u1D447‚àí/u1D461/parenrightbigg\\n=/summationdisplay.1\\n/u1D460‚àà{0,1}/u1D45D/u1D460lg1\\n/u1D45D/u1D460+lg/u1D452\\n/u1D447/u1D447/summationdisplay.1\\n/u1D461=1/u1D702/u1D461‚àí/u1D447\\n< /u1D43B(/u1D45D)+lg/u1D452\\n/u1D447/u1D702\\n/u1D702‚àí1, (19)\\nwhere/u1D43B(/u1D45D)is the entropy of the source. Since we know /u1D43F‚â•/u1D43B(/u1D45D)from the source coding theorem for\\nvariable length coding [2, Theorem 5.3.1], it holds from (19 ) that/u1D43F‚Üí/u1D43B(/u1D45D)as/u1D447‚Üí‚àû .\\n3 rANS\\nWhile the ABS discussed in the previous section is designed o nly for binary sources, the ANS (Asymmetric\\nNumeral Systems) is devised for general sources. The ANS use s only integer arithmetic operations, although\\nthe ABS uses real numbers /u1D45D0and/u1D45D1. The ANS that uses integer ranges in encoding and decoding, l ike\\narithmetic range coding, is called rANS (range variant of AN S).\\nIn this section, we assume that the integer variable /u1D465can take any large number of bits. However, we\\nwill discuss the case of restricting /u1D465to an appropriate bit size in the next section.\\n3.1 Encoding and decoding procedures of rANS [6]\\nAssume that /u1D441and/u1D441/u1D460,/u1D460‚ààSare integers satisfying /u1D441=/summationtext.1\\n/u1D460‚ààS/u1D441/u1D460and/u1D441/u1D460//u1D441‚âà/u1D45D(/u1D460). For each /u1D460‚ààS, we\\ndeÔ¨Åne/u1D451/u1D460by\\n/u1D451/u1D460=/summationdisplay.1\\nÀÜ/u1D460‚â∫/u1D460/u1D441ÀÜ/u1D460, (20)\\nwhere‚â∫denotes an arbitrarily given total order on S.\\nA. DeÔ¨Ånition of encoding function\\nEncoding function /u1D465/u1D461‚àí1:=/u1D436(/u1D460/u1D461,/u1D465/u1D461)is deÔ¨Åned by /six.sup\\n/u1D465/u1D461‚àí1:=/u1D441/floorleftbigg/u1D465/u1D461\\n/u1D441/u1D460/u1D461/floorrightbigg\\n+/u1D451/u1D460/u1D461+mod(/u1D465/u1D461,/u1D441/u1D460/u1D461). (21)\\nB. DeÔ¨Ånition of decoding function\\nDecoding function (/u1D460/u1D461,/u1D465/u1D461):=/u1D437(/u1D465/u1D461‚àí1)is deÔ¨Åned by (22) and (23).\\n/u1D460/u1D461:=min/braceleftBigg\\n/u1D460: mod(/u1D465/u1D461‚àí1,/u1D441)</summationdisplay.1\\n/u1D456/pr‚åâ‚åã‚åâ‚åà‚åâs‚åâqual/u1D460/u1D441/u1D456/bracerightBigg\\n, (22)\\n/u1D465/u1D461:=/u1D441/u1D460/u1D461/floorleftBig/u1D465/u1D461‚àí1\\n/u1D441/floorrightBig\\n+mod(/u1D465/u1D461‚àí1,/u1D441)‚àí/u1D451/u1D460/u1D461. (23)\\n/six.supIn this paper, residue operation ‚Äú /u1D44Emod/u1D44F‚Äù is represented as ‚Äúmod (/u1D44E,/u1D44F)‚Äù in the same way as Duda‚Äôs paper [6].\\n5Figure 1: Relations used in rANS encoding and decoding.\\nC. Encoding algorithm\\na. For a given data sequence /u1D460/u1D447=/u1D4601/u1D4602¬∑¬∑¬∑/u1D460/u1D447, set/u1D465/u1D447‚Üê1.\\nb. Repeat /u1D465/u1D461‚àí1‚Üê/u1D436(/u1D460/u1D461,/u1D465/u1D461)in reverse order, /u1D461=/u1D447,¬∑¬∑¬∑,2,1.\\nc. The codeword of /u1D460/u1D447is given by /u1D4650.\\nD. Decoding algorithm\\na. Set a codeword /u1D4650.\\nb. Repeat(/u1D460/u1D461,/u1D465/u1D461)‚Üê/u1D437(/u1D465/u1D461‚àí1)in forward order, /u1D461=1,2,¬∑¬∑¬∑,/u1D447.\\nc./u1D460/u1D447=/u1D4601/u1D4602¬∑¬∑¬∑/u1D460/u1D447is the decoded sequence.\\nWe now show that if /u1D465/u1D461‚àí1is given by (21), then /u1D460/u1D461and/u1D465/u1D461can be determined by (22) and (23).\\nFrom (20) and mod (/u1D465/u1D461‚àí1,/u1D441/u1D460/u1D461)< /u1D441/u1D460/u1D461, we obtain\\n/u1D451/u1D460/u1D461+mod(/u1D465/u1D461,/u1D441/u1D460/u1D461)</summationdisplay.1\\n/u1D460‚â∫/u1D460/u1D461/u1D441/u1D460+/u1D441/u1D460/u1D461‚â§/u1D441. (24)\\nHence, the sum of the 2nd and 3rd terms on the right side of (21) is smaller than /u1D441. By dividing both sides\\nof (21) by /u1D441and taking the Ô¨Çoor function, we have\\n/floorleftBig/u1D465/u1D461‚àí1\\n/u1D441/floorrightBig\\n=/floorleftbigg/u1D465/u1D461\\n/u1D441/u1D460/u1D461/floorrightbigg\\n. (25)\\nTherefore, from (21), (24), and (25), the following relatio n holds.\\nmod(/u1D465/u1D461‚àí1,/u1D441)=/u1D465/u1D461‚àí1‚àí/u1D441/floorleftBig/u1D465/u1D461‚àí1\\n/u1D441/floorrightBig\\n=/u1D465/u1D461‚àí1‚àí/u1D441/floorleftbigg/u1D465/u1D461\\n/u1D441/u1D460/u1D461/floorrightbigg\\n=/u1D451/u1D460/u1D461+mod(/u1D465/u1D461,/u1D441/u1D460/u1D461) (26)\\n</summationdisplay.1\\n/u1D460‚â∫/u1D460/u1D461/u1D441/u1D460+/u1D441/u1D460/u1D461. (27)\\nInequality (27) means that /u1D460/u1D461can be decoded by (22). On the other hand, (23) can be derived f rom (25)\\nand (26) as follows.\\n/u1D465/u1D461=/u1D441/u1D460/u1D461/floorleftbigg/u1D465/u1D461\\n/u1D441/u1D460/u1D461/floorrightbigg\\n+mod(/u1D465/u1D461,/u1D441/u1D460/u1D461) (28)\\n=/u1D441/u1D460/u1D461/floorleftBig/u1D465/u1D461‚àí1\\n/u1D441/floorrightBig\\n+mod(/u1D465/u1D461,/u1D441/u1D460/u1D461)\\n=/u1D441/u1D460/u1D461/floorleftBig/u1D465/u1D461‚àí1\\n/u1D441/floorrightBig\\n+mod(/u1D465/u1D461‚àí1,/u1D441)‚àí/u1D451/u1D460/u1D461.\\nFigure 1 shows the relations of /u1D465/u1D461and/u1D465/u1D461‚àí1on the number line, which are given by (21), (25)‚Äì(28).\\n63.2 Average code length of rANS\\nWe evaluate the average code length of rANS based on the sourc e probability distribution /u1D45D={/u1D45D(/u1D460)|/u1D460‚ààS}\\nand the probability distribution /u1D45E={/u1D45E(/u1D460)|/u1D460‚ààS}, which is deÔ¨Åned by /u1D45E(/u1D460)=/u1D441/u1D460//u1D441for/u1D460‚ààS.\\nFrom (25), we have\\n/u1D465/u1D461‚àí1\\n/u1D441‚âà/u1D465/u1D461\\n/u1D441/u1D460/u1D461,\\n/u1D465/u1D461‚àí1\\n/u1D465/u1D461‚âà/u1D441\\n/u1D441/u1D460/u1D461=1\\n/u1D45E(/u1D460/u1D461). (29)\\nWe Ô¨Årst consider the case such that (29) holds with ‚Äú =‚Äù instead of ‚Äú‚âà‚Äù. Let/u1D4650(/u1D460/u1D447)denote the codeword of\\n/u1D460/u1D447=/u1D4601/u1D4602¬∑¬∑¬∑/u1D460/u1D447obtained by applying (21) to /u1D460/u1D461,/u1D461=/u1D447,/u1D447‚àí1,¬∑¬∑¬∑,1, for/u1D465/u1D447=1 in this case. Then, the bit\\nlength of /u1D4650(/u1D460/u1D447)is given by\\nlg/u1D4650(/u1D460/u1D447)=lg/u1D447/productdisplay.1\\n/u1D461=1/u1D465/u1D461‚àí1\\n/u1D465/u1D461+lg/u1D465/u1D447\\n=/u1D447/summationdisplay.1\\n/u1D461=1lg/u1D465/u1D461‚àí1\\n/u1D465/u1D461\\n=/u1D447/summationdisplay.1\\n/u1D461=1lg1\\n/u1D45E(/u1D460/u1D461).\\nHence, the average code length /u1D43Fcan be expressed by\\n/u1D43F=1\\n/u1D447/summationdisplay.1\\n/u1D460/u1D447‚ààS/u1D447/u1D45D(/u1D460/u1D447)lg/u1D4650(/u1D460/u1D447)\\n=1\\n/u1D447/summationdisplay.1\\n/u1D460/u1D447‚ààS/u1D447/u1D45D(/u1D460/u1D447)/u1D447/summationdisplay.1\\n/u1D461=1lg1\\n/u1D45E(/u1D460/u1D461)\\n=1\\n/u1D447/u1D447/summationdisplay.1\\n/u1D461=1/summationdisplay.1\\n/u1D460/u1D461‚ààS/u1D45D(/u1D460/u1D461)lg1\\n/u1D45E(/u1D460/u1D461)\\n=/summationdisplay.1\\n/u1D460‚ààS/u1D45D(/u1D460)lg1\\n/u1D45E(/u1D460)\\n=/u1D43B(/u1D45D)+/u1D437(/u1D45D/‚åäar‚åà‚åäl/u1D45E), (30)\\nwhere/u1D43B(/u1D45D)and/u1D437(/u1D45D/‚åäar‚åà‚åäl/u1D45E)are the source entropy and the relative entropy to /u1D45E, respectively.\\nWe next estimate the degree of approximation of (29). Since i t holds from (25) that\\n/u1D465/u1D461‚àí1\\n/u1D441‚àí1</u1D465/u1D461\\n/u1D441/u1D460/u1D461,/u1D465/u1D461‚àí1\\n/u1D441>/u1D465/u1D461\\n/u1D441/u1D460/u1D461‚àí1,\\n/u1D465/u1D461‚àí1//u1D465/u1D461has the following upper and lower bounds:\\n/u1D441\\n/u1D441/u1D460/u1D461‚àí/u1D441\\n/u1D465/u1D461</u1D465/u1D461‚àí1\\n/u1D465/u1D461</u1D441\\n/u1D441/u1D460/u1D461+/u1D441\\n/u1D465/u1D461. (31)\\nThese bounds mean that (29) is a very good approximation when /u1D465/u1D461satisÔ¨Åes\\n/u1D465/u1D461‚â´/u1D441. (32)\\n7We now evaluate the inÔ¨Çuence of approximation errors on the c ode length. From the right inequality of\\n(31), we have\\nlg/u1D465/u1D461‚àí1\\n/u1D465/u1D461<lg/parenleftbigg/u1D441\\n/u1D441/u1D460/u1D461+/u1D441\\n/u1D465/u1D461/parenrightbigg\\n=lg/u1D441\\n/u1D441/u1D460/u1D461+lg/parenleftbigg\\n1+/u1D441/u1D460/u1D461\\n/u1D465/u1D461/parenrightbigg\\n(33)\\n<lg1\\n/u1D45E(/u1D460/u1D461)+lg/parenleftbigg\\n1+/u1D441\\n/u1D465/u1D461/parenrightbigg\\n<lg1\\n/u1D45E(/u1D460/u1D461)+(lg/u1D452)/u1D441\\n/u1D465/u1D461. (34)\\nThe second term of (34) represents an upper bound on code leng th loss due to the approximation error in\\n(29). Hence the total loss /u1D459loss(/u1D460/u1D447)of the data sequence /u1D460/u1D447is bounded by\\n/u1D459loss(/u1D460/u1D447)<(/u1D441lg/u1D452)/u1D447/summationdisplay.1\\n/u1D461=11\\n/u1D465/u1D461.\\nConsider the case of /u1D465/u1D447=/u1D434, where/u1D434is an integer satisfying /u1D702=/parenleftbigg\\nmin\\n/u1D460‚ààS/u1D441\\n/u1D441/u1D460/parenrightbigg\\n‚àí/u1D441\\n/u1D434>1. Then from the\\nleft inequality of (31) and /u1D434=/u1D465/u1D447< /u1D465/u1D447‚àí1<¬∑¬∑¬∑< /u1D465/u1D461‚àí1, we obtain /u1D465/u1D461‚àí1> /u1D702/u1D447‚àí/u1D461+1/u1D434. In this case, adding the\\nloss lg/u1D434caused by using /u1D465/u1D447=/u1D434instead of /u1D465/u1D447=1, the total loss /u1D459loss(/u1D460/u1D447)has the following upper bound.\\n/u1D459loss(/u1D460/u1D447)<lg/u1D434+(/u1D441lg/u1D452)/u1D447/summationdisplay.1\\n/u1D461=11\\n/u1D702/u1D447‚àí/u1D461/u1D434\\n<lg/u1D434+/u1D441lg/u1D452\\n/u1D434/u1D702\\n/u1D702‚àí1. (35)\\nTherefore, from (30) and (35), the average code length /u1D43Fin the case of /u1D465/u1D447=/u1D434satisÔ¨Åes\\n/u1D43F < /u1D43B(/u1D45D)+/u1D437(/u1D45D/‚åäar‚åà‚åäl/u1D45E)+1\\n/u1D447/parenleftbigg\\nlg/u1D434+/u1D441lg/u1D452\\n/u1D434/u1D702\\n/u1D702‚àí1/parenrightbigg\\n. (36)\\nWe note from (36) that /u1D43F‚Üí/u1D43B(/u1D45D)+/u1D437(/u1D45D/‚åäar‚åà‚åäl/u1D45E)as/u1D447‚Üí‚àû .\\n4 rANS with a Ô¨Ånite digit arithmetic\\nThe rANS treated in the previous section has a practical draw back for large /u1D447because/u1D465/u1D461becomes a very large\\ninteger as /u1D461approaches zero. To overcome this drawback, the stream codi ng is designed so that encoding\\nand decoding can be performed with Ô¨Åxed Ô¨Ånite digits, no matt er how large /u1D447is.\\nIn this section, we explain Townsend‚Äôs encoding and decodin g algorithms [28], which are a stream\\nversion of the rANS, and evaluate its average code length. We assume in this section that /u1D441and/u1D441/u1D460satisfy\\n/u1D441=/summationtext.1\\n/u1D460‚ààS/u1D441/u1D460=2/u1D445for a given integer /u1D445.\\n4.1 Encoding and decoding procedures of rANS using Ô¨Ånite dig it operations [6]\\nFrom (21)‚Äì(23), the encoding and decoding functions are deÔ¨Å ned in the case of /u1D441=2/u1D445as follows.\\n8!\"#$%& !\"#$%&!\"#$%& \\n\\'&#$%& \\'&#$%& \\'&#$%&\\n()*+,-./($.,-./ ()*+,-./($.,-./\\nFigure 2: Relation between Push and Pop used in stream encodi ng and decoding of rANS.\\nA. DeÔ¨Ånition of encoding function\\nEncoding function /u1D465/u1D461‚àí1:=/u1D436(/u1D460/u1D461,/u1D465/u1D461)is deÔ¨Åned by\\n/u1D465/u1D461‚àí1:=2/u1D445/floorleftbigg/u1D465/u1D461\\n/u1D441/u1D460/u1D461/floorrightbigg\\n+/u1D451/u1D460/u1D461+mod(/u1D465/u1D461,/u1D441/u1D460/u1D461). (37)\\nB. DeÔ¨Ånition of decoding function\\nDecoding function (/u1D460/u1D461,/u1D465/u1D461):=/u1D437(/u1D465/u1D461‚àí1)is deÔ¨Åned by\\n/u1D460/u1D461:=min/braceleftBigg\\n/u1D460: mod(/u1D465/u1D461‚àí1,2/u1D445)</summationdisplay.1\\n/u1D456/pr‚åâ‚åã‚åâ‚åà‚åâs‚åâqual/u1D460/u1D441/u1D456/bracerightBigg\\n,\\n/u1D465/u1D461:=/u1D441/u1D460/u1D461/floorleftBig/u1D465/u1D461‚àí1\\n2/u1D445/floorrightBig\\n+mod(/u1D465/u1D461‚àí1,2/u1D445)‚àí/u1D451/u1D460/u1D461.\\nWe now add the following restriction to /u1D465/u1D461‚àí1obtained by (37).\\n2/u1D45F/u1D44E‚àí/u1D45F/u1D44F‚â§/u1D465/u1D461‚àí1<2/u1D45F/u1D44E, (38)\\nwhere/u1D45F/u1D44Eand/u1D45F/u1D44Fare integers satisfying /u1D45F/u1D44E‚àí/u1D45F/u1D44F> /u1D445. Then, from (37) and (38), we have the following\\ninequalities.\\n2/u1D45F/u1D44E‚àí/u1D45F/u1D44F‚â§2/u1D445/floorleftbigg/u1D465/u1D461\\n/u1D441/u1D460/u1D461/floorrightbigg\\n+/u1D451/u1D460/u1D461+mod(/u1D465/u1D461,/u1D441/u1D460/u1D461)<2/u1D45F/u1D44E,\\n2/u1D45F/u1D44E‚àí/u1D45F/u1D44F‚àí/u1D445‚â§/floorleftbigg/u1D465/u1D461\\n/u1D441/u1D460/u1D461/floorrightbigg\\n+/u1D451/u1D460/u1D461+mod(/u1D465/u1D461,/u1D441/u1D460/u1D461)\\n2/u1D445<2/u1D45F/u1D44E‚àí/u1D445,\\n2/u1D45F/u1D44E‚àí/u1D45F/u1D44F‚àí/u1D445‚â§/floorleftbigg/u1D465/u1D461\\n/u1D441/u1D460/u1D461/floorrightbigg\\n<2/u1D45F/u1D44E‚àí/u1D445, (39)\\n2/u1D45F/u1D44E‚àí/u1D45F/u1D44F‚àí/u1D445‚â§/u1D465/u1D461\\n/u1D441/u1D460/u1D461<2/u1D45F/u1D44E‚àí/u1D445,\\n/u1D441/u1D460/u1D4612/u1D45F/u1D44E‚àí/u1D45F/u1D44F‚àí/u1D445‚â§/u1D465/u1D461< /u1D441/u1D460/u1D4612/u1D45F/u1D44E‚àí/u1D445, (40)\\nwhere (39) holds because, from (24), we have (/u1D451/u1D460/u1D461+mod(/u1D465/u1D461,/u1D441/u1D460/u1D461))/2/u1D445<1.\\nIn order for /u1D465/u1D461‚àí1obtained by (37) to satisfy (38), /u1D465/u1D461must satisfy (40). Therefore, if /u1D465/u1D461does not satisfy\\n(40), we remove the lower bits of /u1D465/u1D461after pushing them onto a stack, and then we perform the encod ing of\\n(37). On the other hand, in decoding, /u1D465/u1D461decoded from /u1D465/u1D461‚àí1satisfying (38) satisÔ¨Åes (40). Therefore, by\\npopping the lower bits of /u1D465/u1D461from the stack and adding them to /u1D465/u1D461,/u1D465/u1D461can satisfy 2/u1D45F/u1D44E‚àí/u1D45F/u1D44F‚â§/u1D465/u1D461<2/u1D45F/u1D44E. After\\nÔ¨Ånishing this process, we move on to the step of decoding /u1D460/u1D461+1.\\nFigure 2 shows the relation between Push and Pop operations i n encoding and decoding. Let stack-push (/u1D462,Àú/u1D462)\\nstand for pushing Àú /u1D462into a stack /u1D462, and let stack-pop(/u1D462)represent popping /u1D462topwith an appropriate length\\nfrom the top of the stack /u1D462. Then, the Push and Pop operations are described as follows.\\n9Push operation for /u1D499/u1D495\\nWhile/u1D465/u1D461‚â•/u1D441/u1D460/u1D4612/u1D45F/u1D44E‚àí/u1D445:\\n/u1D462‚Üêstack-push(/u1D462,mod(/u1D465/u1D461,2/u1D45F/u1D44F)), (41)\\n/u1D465/u1D461‚Üê/floorleftBig/u1D465/u1D461\\n2/u1D45F/u1D44F/floorrightBig\\n.\\nPop operation for /u1D499/u1D495\\nWhile/u1D465/u1D461<2/u1D45F/u1D44E‚àí/u1D45F/u1D44F:\\n/u1D462top‚Üêstack-pop(/u1D462), (42)\\n/u1D465/u1D461‚Üê2/u1D45F/u1D44F/u1D465/u1D461+/u1D462top.\\nIn the above Push operation, the number of times that Eq. (41) is applied depends on the values of\\n/u1D460/u1D461and/u1D465/u1D461. If/u1D465/u1D461satisfying 2/u1D45F/u1D44E‚àí/u1D45F/u1D44F‚â§/u1D465/u1D461<2/u1D45F/u1D44Falso satisÔ¨Åes /u1D465/u1D461< /u1D441/u1D460/u1D4612/u1D45F/u1D44E‚àí/u1D445(i.e., if 2‚àí/u1D45F/u1D44F< /u1D45D(/u1D460/u1D461) ‚âà\\n/u1D45E(/u1D460/u1D461)=/u1D441/u1D460/u1D461//u1D441=/u1D441/u1D460/u1D4612‚àí/u1D445and 2/u1D45F/u1D44E‚àí/u1D45F/u1D44F‚â§/u1D465/u1D461< /u1D441/u1D460/u1D4612/u1D45F/u1D44E‚àí/u1D445), Eq. (41) is not executed. On the other hand, if\\n2‚àí/u1D45F/u1D44F> /u1D45D(/u1D460/u1D461)‚âà/u1D45E(/u1D460/u1D461)=/u1D441/u1D460/u1D461//u1D441=/u1D441/u1D460/u1D4612‚àí/u1D445and/u1D441/u1D460/u1D4612/u1D45F/u1D44E‚àí/u1D445‚â§/u1D465/u1D4612‚àí/u1D45F/u1D44F<2/u1D45F/u1D44E‚àí/u1D45F/u1D44F, then Eq. (41) is executed more\\nthan once. We note that in order to satisfy (32), parameters /u1D45F/u1D44E,/u1D45F/u1D44F, and/u1D445must satisfy\\n2/u1D45F/u1D44E‚àí/u1D45F/u1D44F‚â´2/u1D445. (43)\\nCombining Pop-Push operations with encoding and decoding f unctions, we obtain the following encoding\\nand decoding algorithms.\\nC. Encoding algorithm\\na. For a given data sequence /u1D460/u1D447=/u1D4601/u1D4602¬∑¬∑¬∑/u1D460/u1D447, set/u1D465/u1D447‚Üê2/u1D45F/u1D44E‚àí/u1D45F/u1D44F.\\nb. Repeat the following (i) and (ii) in backward order, /u1D461=/u1D447,¬∑¬∑¬∑,2,1.\\n(i) Perform the Push operation on /u1D465/u1D461,\\n(ii)/u1D465/u1D461‚àí1‚Üê/u1D436(/u1D460/u1D461,/u1D465/u1D461).\\nc. The codeword of /u1D460/u1D447is given by /u1D4650and stack /u1D462.\\nD. Decoding algorithm\\na. Set a codeword ( /u1D4650and stack /u1D462).\\nb. Repeat the following (iii) and (iv) in forward order, /u1D461=1,2,¬∑¬∑¬∑,/u1D447.\\n(iii)(/u1D460/u1D461,/u1D465/u1D461)‚Üê/u1D437(/u1D465/u1D461‚àí1),\\n(iv) Perform the Pop operation on /u1D465/u1D461.\\nc./u1D460/u1D447=/u1D4601/u1D4602¬∑¬∑¬∑/u1D460/u1D447is the decoded sequence.\\nRemark 2 If a similar method is applied to the ABS in Section 2, it can be encoded and decoded using\\nÔ¨Ånite digit operations for any data sequence.\\n4.2 Average code length of stream rANS\\nWhen we encode /u1D460/u1D461at time/u1D461, the increase in code length is given by /u1D459(/u1D460/u1D461)=lg/u1D465/u1D461‚àí1‚àílg/u1D465/u1D461=lg(/u1D465/u1D461‚àí1//u1D465/u1D461).\\nHence, combining (33) with the left inequality in (40), we ca n derive the following upper bound of /u1D459(/u1D460/u1D461).\\n/u1D459(/u1D460/u1D461)=lg/u1D465/u1D461‚àí1\\n/u1D465/u1D461<lg/u1D441\\n/u1D441/u1D460/u1D461+lg/parenleftbigg\\n1+/u1D441/u1D460/u1D461\\n/u1D465/u1D461/parenrightbigg\\n‚â§lg/u1D441\\n/u1D441/u1D460/u1D461+lg/parenleftbigg\\n1+1\\n2/u1D45F/u1D44E‚àí/u1D45F/u1D44F‚àí/u1D445/parenrightbigg\\n<lg1\\n/u1D45E(/u1D460/u1D461)+lg e\\n2/u1D45F/u1D44E‚àí/u1D45F/u1D44F‚àí/u1D445.\\n10Hence, in the case of /u1D465/u1D447=2/u1D45F/u1D44E‚àí/u1D45F/u1D44F, the total code length /u1D459(/u1D460/u1D447)is given by\\n/u1D459(/u1D460/u1D447)=/u1D447/summationdisplay.1\\n/u1D461=1/u1D459(/u1D460/u1D461)+lg/u1D465/u1D447\\n</u1D447/summationdisplay.1\\n/u1D461=1/parenleftbigg\\nlg1\\n/u1D45E(/u1D460/u1D461)+lg e\\n2/u1D45F/u1D44E‚àí/u1D45F/u1D44F‚àí/u1D445/parenrightbigg\\n+/u1D45F/u1D44E‚àí/u1D45F/u1D44F\\n=/u1D447/summationdisplay.1\\n/u1D461=1lg1\\n/u1D45E(/u1D460/u1D461)+/parenleftbigg/u1D447lg e\\n2/u1D45F/u1D44E‚àí/u1D45F/u1D44F‚àí/u1D445+/u1D45F/u1D44E‚àí/u1D45F/u1D44F/parenrightbigg\\n. (44)\\nIn the same way as (30), the average code length /u1D43F=(1//u1D447)/summationtext.1/u1D45D(/u1D460/u1D447)/u1D459(/u1D460/u1D447)can be bounded from (44) as\\nfollows.\\n/u1D43F < /u1D43B(/u1D45D)+/u1D437(/u1D45D/‚åäar‚åà‚åäl/u1D45E)+lg e\\n2/u1D45F/u1D44E‚àí/u1D45F/u1D44F‚àí/u1D445+/u1D45F/u1D44E‚àí/u1D45F/u1D44F\\n/u1D447, (45)\\nwhere the 3rd and 4th terms on the right side become suÔ¨Écientl y small when we use parameters /u1D45F/u1D44E,/u1D45F/u1D44F,/u1D445\\nsatisfying (43) and /u1D447‚â´/u1D45F/u1D44E‚àí/u1D45F/u1D44F.\\n5 tANS\\nIn this section, we discuss the tabled variant of ANS (tANS), which is sometimes simply referred to as ANS\\n[13][14]. For any|S|‚â• 2, tANS can encode and decode data sequences using only integ er operations like\\nrANS. In the case of the stream rANS, a data sequence /u1D460/u1D447is encoded to ( /u1D4650and stack /u1D462), but a codeword\\nis not determined for each /u1D460/u1D461. On the other hand, the tANS is designed so that the codeword o f/u1D460/u1D461is\\nuniquely determined from /u1D460/u1D461and/u1D465/u1D461. As a result, tANS has the advantage that encoding and decodi ng can\\nbe performed entirely using tables without arithmetic oper ations, as will be described later.\\nIn the same way as the previous section, we assume that /u1D441and/u1D441/u1D460,/u1D460‚ààSsatisfy/u1D441=/summationtext.1\\n/u1D460‚ààS/u1D441/u1D460=2/u1D445for\\na given integer /u1D445. Then the encoding and decoding of tANS are deÔ¨Åned by using se ts of integer statesX,\\nX/u1D460, andY.\\n1.X: The set of states used in encoding and decoding, which is deÔ¨Å ned asX={/u1D441,/u1D441+1,¬∑¬∑¬∑,2/u1D441‚àí1}and\\n/u1D441=|X|.\\n2.X/u1D460: The set of states corresponding to /u1D460‚ààS, which satisÔ¨ÅesX/u1D460‚à©X/u1D460‚Ä≤=‚àÖfor/u1D460‚â†/u1D460‚Ä≤, andX=/uniontext.1\\n/u1D460‚ààSX/u1D460.\\nThen,/u1D441=/summationtext.1\\n/u1D460‚ààS/u1D441/u1D460for/u1D441/u1D460=|X/u1D460|.\\n3.Y/u1D460: The other set of states corresponding to /u1D460‚ààS, which is deÔ¨Åned as Y/u1D460={/u1D441/u1D460,/u1D441/u1D460+1,¬∑¬∑¬∑,2/u1D441/u1D460‚àí1}\\nand/u1D441/u1D460=|Y/u1D460|.\\nSince it holds that /u1D441/u1D460=|X/u1D460|=|Y/u1D460|for each/u1D460‚ààS, we have a one-to-one correspondence between\\n/u1D465‚ààX/u1D460and/u1D466.alt‚ààY/u1D460. Furthermore, since X/u1D460also satisÔ¨Åes the above condition 2, we also have a one-\\nto-one correspondence between a pair (/u1D460,/u1D466.alt),/u1D460‚ààS,/u1D466.alt‚ààY/u1D460, and/u1D465‚ààX. We represent this one-to-one\\ncorrespondence by encoding and decoding functions Àú/u1D436andÀú/u1D437./seven.sup\\nA. DeÔ¨Ånition of encoding function\\nFor each /u1D460‚ààS, encoding function Àú/u1D436(/u1D460,¬∑)is a bijection function Àú/u1D436(/u1D460,¬∑):Y/u1D460‚ÜíX/u1D460. Note that if /u1D460‚ààS\\nand/u1D466.alt‚ààY/u1D460, then/u1D465:=Àú/u1D436(/u1D460,/u1D466.alt)‚ààX/u1D460.\\n/seven.supWe use notation(Àú/u1D436,Àú/u1D437)since these functions have distinct meanings from the encod ing function /u1D465/u1D461‚àí1:=/u1D436(/u1D460/u1D461,/u1D465/u1D461)and the\\ndecoding function(/u1D460/u1D461,/u1D465/u1D461):=/u1D437(/u1D465/u1D461‚àí1)used in previous sections.\\n11B. DeÔ¨Ånition of decoding function\\nFor each/u1D465‚ààX/u1D460, decoding function Àú/u1D437is a bijection function Àú/u1D437:X/u1D460‚Üí{/u1D460}√óY/u1D460. Note that if /u1D465‚ààX/u1D460‚äÇX,\\nthen(/u1D460,/u1D466.alt):=Àú/u1D437(/u1D465)‚àà{/u1D460}√óY/u1D460.\\n5.1 Encoding and decoding procedures of tANS [6][13][14]\\nLet/u1D465/u1D461,/u1D460/u1D461,/u1D44F/u1D461, and/u1D458/u1D461represent the state, data symbol, codeword, and code length at time/u1D461, respectively.\\nThen, encoding is the process of obtaining (/u1D44F/u1D461,/u1D465/u1D461‚àí1)from(/u1D465/u1D461,/u1D460/u1D461), while decoding is the process of obtaining\\n(/u1D460/u1D461,/u1D465/u1D461)from(/u1D465/u1D461‚àí1,/u1D44F/u1D461). In the tANS, these processes are performed via (/u1D466.alt/u1D461‚àí1,/u1D458/u1D461)as shown in the following\\nencoding and decoding algorithms [6][13], where it is assum ed that functions Àú/u1D436andÀú/u1D437are given.\\nC. Encoding algorithm\\na. For a given data sequence /u1D460/u1D447=/u1D4601/u1D4602¬∑¬∑¬∑/u1D460/u1D447, select/u1D465/u1D447‚ààXarbitrarily.\\nb. Repeat the following calculations in backward order, /u1D461=/u1D447,¬∑¬∑¬∑,2,1.\\n/u1D458/u1D461‚Üê/floorleftbigg\\nlg/u1D465/u1D461\\n/u1D441/u1D460/u1D461/floorrightbigg\\n, (46)\\n/u1D44F/u1D461‚Üêmod(/u1D465/u1D461,2/u1D458/u1D461), (47)\\n/u1D466.alt/u1D461‚àí1‚Üê/floorleftBig/u1D465/u1D461\\n2/u1D458/u1D461/floorrightBig\\n, (48)\\n/u1D465/u1D461‚àí1‚ÜêÀú/u1D436(/u1D460/u1D461,/u1D466.alt/u1D461‚àí1). (49)\\nc. The codeword sequence of /u1D460/u1D447is/u1D4650/u1D44F1/u1D44F2¬∑¬∑¬∑/u1D44F/u1D447.\\nD. Decoding algorithm\\na. For a given codeword sequence /u1D4650/u1D44F1/u1D44F2¬∑¬∑¬∑/u1D44F/u1D447, set/u1D4650and/u1D483‚Üê/u1D44F1/u1D44F2¬∑¬∑¬∑/u1D44F/u1D447.\\nb. Repeat the following calculations in forward order, /u1D461=1,2,¬∑¬∑¬∑,/u1D447.\\n(/u1D460/u1D461,/u1D466.alt/u1D461‚àí1)‚Üê Àú/u1D437(/u1D465/u1D461‚àí1),\\n/u1D458/u1D461‚Üê/u1D445‚àí‚åälg/u1D466.alt/u1D461‚àí1‚åã, (50)\\n/u1D44F/u1D461‚Üêthe Ô¨Åtst/u1D458/u1D461bits of/u1D483, (51)\\n/u1D483‚Üêthe sequence obtained by removing /u1D44F/u1D461from/u1D483,\\n/u1D465/u1D461‚Üê2/u1D458/u1D461/u1D466.alt/u1D461‚àí1+/u1D44F/u1D461. (52)\\nc./u1D460/u1D447=/u1D4601/u1D4602¬∑¬∑¬∑/u1D460/u1D447is the decoded sequence.\\nState/u1D4650included in the codeword sequence can be represented with, e .g., lg/u1D441=/u1D445bits if we use the\\nÔ¨Åxed length code. From (47), we note that /u1D458/u1D461stands for the bit length of /u1D44F/u1D461. In decoding, since /u1D458/u1D461can be\\nobtained by (50), we can extract /u1D44F/u1D461from the codeword sequence /u1D483in (51).\\nRemark 3 Encoding and decoding algorithms include the calculation o f‚åälg/u1D44E‚åãfor a positive integer /u1D44E.\\nBut, since‚åälg/u1D44E‚åãis the number of bits of /u1D44Eexcluding the MSB, it can easily be obtained without logarit hm\\ncalculation.\\nRemark 4 Eq. (47) corresponds to the operation of (41). In Eq. (41), th e remainder is taken by 2/u1D45F/u1D44Fregardless\\nof the values of /u1D460/u1D461and/u1D465/u1D461, whereas in Eq. (47), the remainder is taken by 2/u1D458/u1D461, which is determined by the\\nvalues of /u1D460/u1D461and/u1D465/u1D461, so that the code word /u1D44F/u1D461is determined for each /u1D460/u1D461.\\n12We Ô¨Årst show that /u1D466.alt/u1D461‚àí1given by (48) satisÔ¨Åes /u1D466.alt/u1D461‚àí1‚ààY/u1D460/u1D461, i.e.,/u1D441/u1D460/u1D461‚â§/u1D466.alt/u1D461‚àí1<2/u1D441/u1D460/u1D461. From (46), we have\\nthe following inequalities.\\n2/u1D458/u1D461‚â§/u1D465/u1D461\\n/u1D441/u1D460/u1D461<2/u1D458/u1D461+1,\\n/u1D441/u1D460/u1D461‚â§/u1D465/u1D461\\n2/u1D458/u1D461<2/u1D441/u1D460/u1D461,\\n/u1D441/u1D460/u1D461‚â§/u1D466.alt/u1D461‚àí1=/floorleftBig/u1D465/u1D461\\n2/u1D458/u1D461/floorrightBig\\n<2/u1D441/u1D460/u1D461, (53)\\nwhere (53) holds because /u1D441/u1D460/u1D461is an integer. Since (53) means /u1D466.alt/u1D461‚àí1‚ààY/u1D460/u1D461, we can use the encoding function\\nÀú/u1D436(/u1D460/u1D461,/u1D466.alt/u1D461‚àí1)in (49) to obtain /u1D465/u1D461‚àí1=Àú/u1D436(/u1D460/u1D461,/u1D466.alt/u1D461‚àí1)‚ààX/u1D460/u1D461.\\nSince/u1D44F/u1D461and/u1D466.alt/u1D461‚àí1are given by (47) and (48), respectively, in encoding, /u1D465/u1D461can be decoded by (52) in\\ndecoding. Next we show that /u1D458/u1D461can be obtained by (50). From /u1D465/u1D461‚ààX(i.e.,/u1D441‚â§/u1D465/u1D461<2/u1D441), (52), and\\n/u1D441=2/u1D445, we have the following inequalities.\\n2/u1D445‚â§2/u1D458/u1D461/u1D466.alt/u1D461‚àí1+/u1D44F/u1D461<2/u1D445+1,\\n2/u1D445‚àí/u1D458/u1D461‚â§/u1D466.alt/u1D461‚àí1+/u1D44F/u1D461\\n2/u1D458/u1D461<2/u1D445‚àí/u1D458/u1D461+1,\\n2/u1D445‚àí/u1D458/u1D461‚â§/u1D466.alt/u1D461‚àí1<2/u1D445‚àí/u1D458/u1D461+1, (54)\\n/u1D445‚àí/u1D458/u1D461‚â§lg/u1D466.alt/u1D461‚àí1< /u1D445‚àí/u1D458/u1D461+1,\\nwhich means that /u1D458/u1D461is obtained by (50). Note that (54) holds because 2/u1D445‚àí/u1D458/u1D461and/u1D466.alt/u1D461‚àí1are integers and we\\nhave 0‚â§/u1D44F/u1D461/2/u1D458/u1D461<1 from (47).\\nRemark 5 Although we consider the case of /u1D441=2/u1D445in the above, tANS can also be applied to general\\ncases where /u1D441is not a power of 2 [6]. But, in decoding of general cases, /u1D458/u1D461is given by /u1D458/u1D461=‚åàlg(/u1D441//u1D466.alt/u1D461‚àí1)‚åâ\\nor/u1D458/u1D461=‚åàlg(/u1D441//u1D466.alt/u1D461‚àí1)‚åâ‚àí1 instead of (50), and it is necessary to use /u1D458/u1D461that satisÔ¨Åes /u1D441‚â§2/u1D458/u1D461/u1D466.alt/u1D461‚àí1+/u1D44F/u1D461<2/u1D441.\\nRemark 6 If we precalculate Eqs. (46)‚Äì(49) and (50)‚Äì(52) for all case s of/u1D465/u1D461=/u1D465,/u1D441‚â§/u1D465 < 2/u1D441and\\n/u1D460/u1D461=/u1D460,/u1D460‚ààSand store the results as a table, we can perform encoding and d ecoding without performing\\narithmetic calculations each time. For this reason, it is ca lled tANS (tabled-variant of ANS).\\n5.2 Average code length of tANS\\nThe inÔ¨Çuence of the choice of /u1D465/u1D447‚ààXin encoding and the increase in the average code length due to the bit\\nlength/u1D445of/u1D4650contained in the codeword converge to zero as the data sequen ce length /u1D447becomes longer.\\nFor simplicity, in this section we will ignore these and Ô¨Ånd t he average code length in the steady state.\\nLet/u1D444(/u1D465)denote the stationary probability of /u1D465‚ààX, and let/u1D45E(/u1D460)be deÔ¨Åned by /u1D45E(/u1D460)=/u1D441/u1D460//u1D441. We Ô¨Årst\\n13evaluate the average code length /u1D43Fbased on (46) as follows.\\n/u1D43F=/summationdisplay.1\\n/u1D460‚ààS/summationdisplay.1\\n/u1D465‚ààX/u1D45D(/u1D460)/u1D444(/u1D465)/floorleftbigg\\nlg/u1D465\\n/u1D441/u1D460/floorrightbigg\\n‚â§/summationdisplay.1\\n/u1D460‚ààS/summationdisplay.1\\n/u1D465‚ààX/u1D45D(/u1D460)/u1D444(/u1D465)lg/u1D465\\n/u1D441/u1D460\\n=/summationdisplay.1\\n/u1D460‚ààS/summationdisplay.1\\n/u1D465‚ààX/u1D45D(/u1D460)/u1D444(/u1D465)/parenleftbigg\\nlg/u1D441\\n/u1D441/u1D460+lg/u1D465\\n/u1D441/parenrightbigg\\n=/summationdisplay.1\\n/u1D460‚ààS/u1D45D(/u1D460)lg/u1D441\\n/u1D441/u1D460+/summationdisplay.1\\n/u1D465‚ààX/u1D444(/u1D465)lg/u1D465\\n/u1D441(55)\\n‚â§‚àó1/summationdisplay.1\\n/u1D460‚ààS/u1D45D(/u1D460)lg1\\n/u1D45E(/u1D460)+lg/summationtext.1\\n/u1D465‚ààX/u1D444(/u1D465)/u1D465\\n/u1D441\\n=/u1D43B(/u1D45D)+/u1D437(/u1D45D/‚åäar‚åà‚åäl/u1D45E)+lgE[/u1D44B]\\n/u1D441, (56)\\nwhere‚â§‚àó1comes from Jensen‚Äôs inequality for the lg function, and E [/u1D44B]is the expected value of /u1D465in the\\nsteady state. Since /u1D441‚â§/u1D465‚â§2/u1D441‚àí1, a loose upper bound lg (E[/u1D44B]//u1D441)<1 holds.\\nUnlike ABS and rANS treated in previous sections, tANS encod es each/u1D460/u1D461symbol-by-symbol to a\\ncodeword with /u1D458/u1D461bits. Therefore, there is a loss in the average code length co mpared to assigning one\\ncodeword at a time to the entire /u1D460/u1D447=/u1D4601/u1D4602¬∑¬∑¬∑/u1D460/u1D447. Below, we evaluate this loss under the tANS conditions,\\ni.e.,/u1D441‚â§/u1D465/u1D461‚àí1<2/u1D441and/u1D441/u1D460/u1D461‚â§/u1D466.alt/u1D461‚àí1<2/u1D441/u1D460/u1D461.\\nIf we use Àú /u1D465/u1D461‚àí1:=(/u1D441//u1D441/u1D460/u1D461)/u1D466.alt/u1D461‚àí1instead of /u1D465/u1D461‚àí1, we can satisfy\\n/u1D441‚â§Àú/u1D465/u1D461‚àí1=/u1D441\\n/u1D441/u1D460/u1D461/u1D466.alt/u1D461‚àí1<2/u1D441. (57)\\nHowever, Àú /u1D465/u1D461‚àí1is generally not an integer. Therefore, tANS uses a function Àú/u1D436to map/u1D466.alt/u1D461‚àí1to an integer /u1D465/u1D461‚àí1\\nthat satisÔ¨Åes /u1D441‚â§/u1D465/u1D461‚àí1<2/u1D441.\\nCorresponding to the case where /u1D460/u1D447is encoded all at once, we consider the ideal case where real-\\nvalued codeword length is allowed for each encoding of /u1D460/u1D461. The codeword length in this case is given by\\n/u1D459(/u1D460/u1D461)=lg(/u1D465/u1D461‚àí1//u1D466.alt/u1D461‚àí1), as in the case of rANS.\\nIf Àú/u1D465/u1D461‚àí1is used, then, from (57), /u1D459(/u1D460/u1D461)=lg(Àú/u1D465/u1D461‚àí1//u1D466.alt/u1D461‚àí1)=lg(/u1D441//u1D441/u1D460/u1D461)=lg 1//u1D45E(/u1D460/u1D461), which depends only on\\nthe value of /u1D460/u1D461and not on the value of /u1D466.alt/u1D461‚àí1. Therefore, the average code length Àú/u1D43F‚àóof this case is given by\\nÀú/u1D43F‚àó=/summationdisplay.1\\n/u1D460‚ààS/u1D45D(/u1D460)lg/u1D441\\n/u1D441/u1D460\\n=/u1D43B(/u1D45D)+/u1D437(/u1D45D/‚åäar‚åà‚åäl/u1D45E).\\nHowever, when we use /u1D465/u1D461‚àí1=Àú/u1D436(/u1D460,/u1D466.alt/u1D461‚àí1), the value of /u1D465/u1D461‚àí1//u1D466.alt/u1D461‚àí1deviates slightly from /u1D441//u1D441/u1D460/u1D461, so the average\\ncode length becomes longer than Àú/u1D43F‚àó. The real-valued codeword length for /u1D465/u1D461‚àí1=Àú/u1D436(/u1D460/u1D461,/u1D466.alt/u1D461‚àí1)is given by\\n/u1D459(/u1D460/u1D461)=lg(Àú/u1D436(/u1D460/u1D461,/u1D466.alt/u1D461‚àí1)//u1D466.alt/u1D461‚àí1), which depends on both /u1D460/u1D461and/u1D466.alt/u1D461‚àí1, so the average code length /u1D43F‚àócan be\\n14evaluated as follows.\\n/u1D43F‚àó=/summationdisplay.1\\n/u1D460‚ààS/summationdisplay.1\\n/u1D466.alt‚ààY/u1D460/u1D444(Àú/u1D436(/u1D460,/u1D466.alt))lgÀú/u1D436(/u1D460,/u1D466.alt)\\n/u1D466.alt\\n=/summationdisplay.1\\n/u1D460‚ààS/summationdisplay.1\\n/u1D466.alt‚ààY/u1D460/u1D444(Àú/u1D436(/u1D460,/u1D466.alt))/parenleftbigg\\nlg/u1D441\\n/u1D441/u1D460+lgÀú/u1D436(/u1D460,/u1D466.alt)/u1D441/u1D460\\n/u1D441/u1D466.alt/parenrightbigg\\n=‚àó2/summationdisplay.1\\n/u1D460‚ààS/u1D45D(/u1D460)lg/u1D441\\n/u1D441/u1D460+/summationdisplay.1\\n/u1D460‚ààS/summationdisplay.1\\n/u1D466.alt‚ààY/u1D460/u1D444(Àú/u1D436(/u1D460,/u1D466.alt))lgÀú/u1D436(/u1D460,/u1D466.alt)\\n/u1D441‚àí/summationdisplay.1\\n/u1D460‚ààS/summationdisplay.1\\n/u1D466.alt‚ààY/u1D460/u1D444(Àú/u1D436(/u1D460,/u1D466.alt))lg/u1D466.alt\\n/u1D441/u1D460\\n=‚àó3/summationdisplay.1\\n/u1D460‚ààS/u1D45D(/u1D460)lg/u1D441\\n/u1D441/u1D460+/summationdisplay.1\\n/u1D465‚ààX/u1D444(/u1D465)lg/u1D465\\n/u1D441‚àí/summationdisplay.1\\n/u1D460‚ààS/summationdisplay.1\\n/u1D466.alt‚ààY/u1D460/u1D444(Àú/u1D436(/u1D460,/u1D466.alt))lg/u1D466.alt\\n/u1D441/u1D460, (58)\\nwhere the numbered equalities hold because\\n=‚àó2:/summationtext.1\\n/u1D466.alt‚ààY/u1D460/u1D444(Àú/u1D436(/u1D460,/u1D466.alt))=/u1D45D(/u1D460),\\n=‚àó3:(/u1D460,/u1D466.alt)and/u1D465=Àú/u1D436(/u1D460,/u1D466.alt)have a one-to-one correppondence.\\nFrom (55) and (58), the loss of average code length of tANS com pared with /u1D43F‚àóis bounded by\\n/u1D43F‚àí/u1D43F‚àó‚â§/summationdisplay.1\\n/u1D460‚ààS/summationdisplay.1\\n/u1D466.alt‚ààY/u1D460/u1D444(Àú/u1D436(/u1D460,/u1D466.alt))lg/u1D466.alt\\n/u1D441/u1D460\\n‚â§‚àó1/summationdisplay.1\\n/u1D460‚ààS/u1D45D(/u1D460)lg/summationtext.1\\n/u1D466.alt‚ààY/u1D460/u1D444(Àú/u1D436(/u1D460,/u1D466.alt))\\n/u1D45D(/u1D460)/u1D466.alt\\n/u1D441/u1D460\\n=/summationdisplay.1\\n/u1D460‚ààS/u1D45D(/u1D460)lgE[/u1D44C|/u1D460]\\n/u1D441/u1D460, (59)\\nwhere E[/u1D44C|/u1D460]is the expected value of /u1D466.alt/u1D461‚àí1under the condition of /u1D460/u1D461=/u1D460‚ààS. Since we have E[/u1D44C|/u1D460]//u1D441/u1D460<2\\nin (59) from (53), a looser bound is given by /u1D43F‚àí/u1D43F‚àó<1.\\nAlthough (56) is an upper bound for any Àú/u1D436, the average code length /u1D43Fcan be made smaller by choosing\\nÀú/u1D436appropriately. Methods of constructing Àú/u1D436with good performance have been proposed in references such\\nas [13] [14] [27]. It is also shown in [22] [25] [26] that the id eal stationary probability distribution /eight.supof tANS\\nis given by\\n/u1D444‚àó(/u1D465)=lg/u1D465+1\\n/u1D465,\\nand paper [27] shows how to construct Àú/u1D436based on /u1D444‚àó(/u1D465).\\nFurthermore, it is proved in [34] that if /u1D444(/u1D465)satisÔ¨Åes/u1D444(/u1D465)‚â§/u1D444‚àó(/u1D465)+(/u1D6FC//u1D4412)for every /u1D465‚ààXand a\\nconstant/u1D6FC‚â•0, the average code length /u1D43Fis bounded by\\n/u1D43F‚â§/u1D43B(/u1D45D)+/u1D437(/u1D45D/‚åäar‚åà‚åäl/u1D45E)+/u1D6FC\\n/u1D441.\\nIn the case of /u1D45E(/u1D460)=/u1D441//u1D441/u1D460=/u1D45D(/u1D460), a construction method of Àú/u1D436is shown in [26][34] such that /u1D444(/u1D465)satisÔ¨Åes\\n/u1D444(/u1D465)=/u1D444‚àó(/u1D465)+/u1D442(1//u1D4412).\\n/eight.supIf/u1D444‚àó(/u1D465)can be realized, it is optimal.\\n156 Conclusions\\nIn this paper, we explained in detail the encoding and decodi ng algorithms for variants of ANS, and we\\nderived several upper bounds on their average coding length s.\\nWe assumed that source probability distribution /u1D45D={/u1D45D(/u1D460)}is given. But, if /u1D45Dis unknown, it is\\nnecessary to include information on /u1D45Dor{/u1D441/u1D460}in codewords. Alternatively, it is possible to use a frequen cy\\ndistribution like dynamic HuÔ¨Äman code [35], without includ ing information on /u1D45Dor{/u1D441/u1D460}in codewords.\\nWhen decoding /u1D460/u1D461+1using the probability distribution ÀÜ /u1D45D/u1D461={ÀÜ/u1D45D/u1D461(/u1D460)}estimated based on the frequency\\ndistribution of /u1D460/u1D461\\n1=/u1D4601/u1D4602¬∑¬∑¬∑/u1D460/u1D461, the encoding also requires that the probability distribut ion ÀÜ/u1D45D/u1D461,/u1D461=1,2¬∑¬∑¬∑,/u1D447is\\nÔ¨Årst obtained from /u1D460/u1D447, and then encoding is performed using ÀÜ /u1D45D/u1D461in the backward order, that is, /u1D461=/u1D447,¬∑¬∑¬∑,2,1.\\nIn this paper, we have explained that /u1D460/u1D447is encoded in backward order ( /u1D460/u1D461,/u1D461=/u1D447,/u1D447‚àí1,¬∑¬∑¬∑,1) and decoded\\nin forward order ( /u1D460/u1D461,/u1D461=/u1D447,/u1D447‚àí1,¬∑¬∑¬∑,1). But, it is also possible to perform encoding in forward or der and\\ndecoding in backward order.\\nThe ANS is designed to perform encoding and decoding using in teger arithmetic operations, similar to\\nRange code of arithmetic cording. However, even without usi ng arithmetic operations, high-performance\\ncompression is possible by performing encoding and decodin g in reverse order [32]‚Äì[34]. There has also\\nbeen research into using the ANS as a simple cipher or a simple random number generator for cryptography\\n[8]‚Äì[10][12].\\nAcknowledgement\\nThis work was supported by JSPS KAKENHI Grant Number 24K0748 7.\\nReferences\\n[1] H. Yamamoto and K. Iwata, ‚ÄúEncoding and decoding algorit hms of ANS variants and evaluation of\\ntheir compression performance,‚Äù Proc. of 2023 Shannon Theo ry Workshop (STW2023), pp.36‚Äì43,\\nOct. 2023 (in Japanese).\\n[2] T. M. Cover and J. A. Thomas, Elements of Information Theory , 2nd Ed., Willey-Interscience, 2006.\\n[3] D. Salomon and G. Motta, Handbook of data compression , 5th Ed., Springer, 2010.\\n[4] J. Duda, ‚ÄúOptimal encoding on discrete lattice with tran slational invariant constrains using statistical\\nalgorithms,‚Äù arXiv:0710.3861v5, Nov. 2, 2008.\\n[5] J. Duda, ‚ÄúAsymmetric numeral systems,‚Äù arXiv:0902.027 1v5, May 21, 2009.\\n[6] J. Duda, ‚ÄúAsymmetric numeral systems: entropy coding co mbining speed of HuÔ¨Äman coding with\\ncompression rate of arithmetic coding,‚Äù arXiv:1311.2540v 2, Jan. 6, 2014.\\n[7] J. Duda, K. Tahboub, et al., ‚ÄúThe use of asymmetric numera l systems as an accurate replacement of\\nHuÔ¨Äman coding,‚Äù Proc. of 2015 Picture Coding Symposium (PCS 2015), pp. 65‚Äì69, May, 2015.\\n[8] J. Duda and M. Niemic, ‚ÄúLightweight compression with enc ryption based on asymmetric numeral\\nsystems,‚Äù arXiv:1612.04662v1, Dec. 14, 2016.\\n[9] S. Campete, J. Duda, et al., ‚ÄúCompcrypt-lightweight ANS -based compression and encryption,‚Äù IEEE\\nTrans. on Inform. Foren. and Security, vol. 16, pp. 3859‚Äì387 3, July, 2021.\\n16[10] S. Camtepe, J. Duda, et al., ‚ÄúANS‚Äìbased compression and encryption with 128‚Äìbit security,‚Äù Int. Jour-\\nnal of Inform. Security, vol. 21, pp.1051‚Äì1067, July, 2022.\\n[11] J. Duda, ‚ÄúEncoding of probability distributions for as ymmetric numeral systems‚Äù, arXiv:2106.06438v4,\\nJuly 4, 2022.\\n[12] J. Pieprzyk, M. Pawlowski, et al., ‚ÄúPseudorandom bit ge neration with asymmetric numeral systems,‚Äù\\nCryptology ePrint Archive, Paper 2022/005, 2022\\n[13] J. Pieprzyk, J. Duda, et al., ‚ÄúCompression optimality o f asymmetric numeral systems,‚Äù arXiv:2209.\\n02228v1, Sep 6, 2022.\\n[14] J. Pieprzyk, J. Duda, et al., ‚ÄúThe compression optimali ty of asymmetric numeral systems,‚Äù Entropy,\\nvol. 25, no. 4, article no. 672, April, 2023.\\n[15] Wikipedia, ‚ÄúAsymmetric numeral systems,‚Äù https://en .wikipedia.org/wiki/Asymmetric numeral systems.\\n[16] P. A. Hsieh, J.-L. Wu, ‚ÄúA review of the asymmetric numera l system and its applications to digital\\nimages,‚Äù Entropy, vol. 24, no. 3, 375, March, 2022.\\n[17] A. MoÔ¨Äat and M. Petri, ‚ÄúANS-based index compression,‚Äù P roc. of 2017 ACM on Conf. on Inform. and\\nKnowl. Manage. (CIKM‚Äô17), pp. 677-686, Nov., 2017.\\n[18] A. MoÔ¨Äat and M. Petri, ‚ÄúIndex compression using byte-al igned ANS coding and two‚Äìdimensional\\ncontexts,‚Äù Proc. of the 7th ACM Int. Conf. on Web Search and Da ta Mining (WSDN‚Äô18) pp. 405‚Äì413,\\nFeb., 2018.\\n[19] A. MoÔ¨Äat and M. Petri, ‚ÄúLarge-alphabet semi-static ent ropy coding via asymmetric numeral systems,‚Äù\\nACM Trans. on Inform. Systems, vol. 38, Issue 4, pp. 1‚Äì33, Jul y, 2020.\\n[20] H. Fujisaki, ‚ÄúInvariant measure for the subshifts asso ciated with the asymmetric binary systems,‚Äù\\nProc. of 2018 Int. Sym. of Inform. Theory and its Appli. (ISIT A2018), pp. 675‚Äì679, Oct., 2018.\\n[21] H. Fujisaki, ‚ÄúOn topological entropies of the subshift s associated with the stream version of asymmetric\\nbinary systems,‚Äù Proc. of 46th Sym. on Inform. Theory and its Appli. (SITA2023), 5.1.3, pp. 338‚Äì343,\\nNov., 2023.\\n[22] H. Yokoo, ‚ÄúOn the stationary distribution of asymmetri c binary systems,‚Äù Proc. of 2016 IEEE\\nInt. Sym. of Inform. Theory (ISIT2016), pp.11-15, July, 201 6.\\n[23] L. Inoue and H. Yokoo, ‚ÄúEvaluation of a Probability Appr oximation Method for ANS with Multi-ary\\nSources,‚Äù IEICE Tech. Report, IT2018-93, pp. 109‚Äì114, Marc h, 2019 (in Japanese).\\n[24] X. Qi and H. Yokoo, ‚ÄúA new variation of asymmetric numera l systems,‚Äù IEICE Tech. Report, IT2018-\\n94, pp.115‚Äì120, March, 2019.\\n[25] D. Dub ¬¥e and H. Yokoo, ‚ÄúFast construction of almost optimal symbol d istributions for asymmetric\\nnumeral systems,‚Äù Proc. of 2019 IEEE Int. Sym. of Inform. The ory (ISIT2019), pp. 1682‚Äì1686, July,\\n2019.\\n[26] H. Yokoo and D. Dub ¬¥e, ‚ÄúAsymptotic optimality of asymmetric numeral systems,‚Äù Proc. of 42th Sym. on\\nInform. Theory and its Appli. (SITA2019), 4.2.3, pp. 289‚Äì29 4, Nov., 2019.\\n17[27] H. Yokoo, ‚ÄúReconstuction of ANS entropy coders based on their optimality condition,‚Äù Proc. of\\nSymp. on Inform. Theory and its Appli. (SITA2023), 5.1.2, pp . 332‚Äì337, Nov. 2023 (in Japanese).\\n[28] J. Townsend, ‚ÄúA tutorial on the range variant of asymmet ric numeral systems,‚Äù arXiv:2001.09186v3,\\nOct. 7, 2020.\\n[29] I. Blanes, M. Hern ¬¥andez-Cabronero, et al., ‚ÄúRedundancy and optimization of t ANS entropy encoders,‚Äù\\nIEEE Trans. on Multimedia, vo. 23, pp.4341‚Äì4350, Nov., 2021 .\\n[30] T. Strutz, ‚ÄúRescaling of symbol counts for adaptive rAN S coding,‚Äù Proc. of 2023 31th European Signal\\nProcessing Conf. (EUSIPCO2023), pp. 585-589, Sep., 2023.\\n[31] M. Li, Y. Liu, and N. Wang, ‚ÄúA novel ANS coding with low com putational complexity,‚Äù 2023 IEEE/CIC\\nInt. Conf. on Comm. in China (ICCC2023), pp. 1‚Äì6, Sep., 2023.\\n[32] H. Yamamoto and K. Iwata, ‚ÄúAn asymmetric encoding-deco ding scheme for lossless data compression,‚Äù\\nIEICE Tech. Report, IT2023-17, pp.17-22, Aug., 2023 (in Jap anese).\\n[33] H. Yamamoto and K. Iwata, ‚ÄúAn asymmetric encoding-deco ding scheme for lossless data compression,‚Äù\\nProc. of 2024 IEEE Int. Sym. of Information Theory (ISIT2024 ), pp.55‚Äì60, July, 2024.\\n[34] H. Yamamoto and K. Iwata, ‚ÄúAsymptotic optimality of the asymmetric encoding-decoding scheme,‚Äù\\nProc. of 2018 Int. Sym. of Inform. Theory and its Appli. (ISIT A2018), Nov., 2024 (to appear).\\n[35] R. G. Gallager, ‚ÄúVariations on a Theme by HuÔ¨Äman,‚Äù IEEE T rans. on Infor. Theory,vol. IT-24, no. 6,\\npp.668‚Äì674, Nov., 1978.\\n18',\n",
       " 'Decoding Memes: A Comparative Study of Machine\\nLearning Models for Template Identification\\nLevente Murg ¬¥as, Marcell Nagy, Kate Barnes, and Roland Molontay\\nDeptartment of Stochastics, Institute of Mathematics,\\nBudapest University of Technology and Economics\\nMÀùuegyetem rkp. 3., H-1111 Budapest, Hungary.\\nEmail: murgas.levente@edu.bme.hu, marcessz@math.bme.hu, kbarnes@edu.bme.hu, molontay@math.bme.hu\\nAbstract ‚ÄîImage-with-text memes combine text with imagery\\nto achieve comedy, but in today‚Äôs world, they also play a pivotal\\nrole in online communication, influencing politics, marketing,\\nand social norms. A ‚Äúmeme template‚Äù is a preexisting layout\\nor format that is used to create memes. It typically includes\\nspecific visual elements, characters, or scenes with blank spaces\\nor captions that can be customized, allowing users to easily create\\ntheir versions of popular meme templates by adding personal or\\ncontextually relevant content. Despite extensive research on meme\\nvirality, the task of automatically identifying meme templates\\nremains a challenge.\\nThis paper presents a comprehensive comparison and evalua-\\ntion of existing meme template identification methods, including\\nboth established approaches from the literature and novel tech-\\nniques. We introduce a rigorous evaluation framework that not\\nonly assesses the ability of various methods to correctly identify\\nmeme templates but also tests their capacity to reject non-memes\\nwithout false assignments. Our study involves extensive data\\ncollection from sites that provide meme annotations (Imgflip)\\nand various social media platforms (Reddit, X, and Facebook) to\\nensure a diverse and representative dataset. We compare meme\\ntemplate identification methods, highlighting their strengths\\nand limitations. These include supervised and unsupervised\\napproaches, such as convolutional neural networks, distance-\\nbased classification, and density-based clustering. Our analysis\\nhelps researchers and practitioners choose suitable methods and\\npoints to future research directions in this evolving field.\\nI. I NTRODUCTION\\nThe rapid proliferation of online information demands that\\nindividuals be discerning in their consumption and filter out\\nirrelevant content. Content creators employ potent strategies to\\ncapture the attention of the largest audience in this competitive\\nenvironment. Image-with-text memes efficiently simplify com-\\nplex concepts into easily digestible and engaging forms [1].\\nMemes, previously viewed merely as digital amusement, have\\nrecently been found to exert a growing impact on various\\nsocietal domains. Research suggests that they can manipulate\\npolitical dialogues [2], influence marketing [3], and even have\\na far-reaching impact on social norms [4].\\nDue to memes‚Äô societal impact, understanding the factors\\ncontributing to the virality of memes has gained significant\\nacademic attention [5]‚Äì[7]. However, most studies focus on\\nthe success of individual meme instances, disregarding a\\nfundamental element: the meme template. Templates serve\\nas the structural backbone of memes that allow for a widerange of individual variations. Figure 1 shows different meme\\ninstances derived from the same template by overlaying it\\nwith text, images, or a combination of both. Identifying meme\\ntemplates will allow us to more easily track the spread of\\nmemes across the Internet and study the ideas propagated by\\nmemes on a higher level than the particular meme instances.\\nAlthough memes in the digital landscape are not usually\\nlabeled with their templates, several approaches have been\\nproposed for detecting templates. Zannettou et al. introduced\\na processing pipeline to detect and track memes in multiple\\nweb communities, using perceptual hashing, clustering, and\\na specialized distance metric [8]. Dubey et al. used sparse\\nrepresentation to decouple the overlayed text and imagery\\nfrom the template and then extract multimodal features from\\nboth components using deep neural networks trained for image\\nclassification and natural language processing tasks. They were\\nalso the first to introduce the term local context , referring to the\\noverlaid text and images, and the term global context which\\nis the template image itself [9].\\nCourtois & Frissen introduced a three-step method to iden-\\ntify visual similarities in memes, combining automated key\\nfeature matching and network analysis [10]. The authors pre-\\nsented an image preprocessing method that blurs text regions,\\nthereby allowing visual features to be explored independently\\nof the textual information on the image. Theisen et al. in-\\ntroduced the concept of motif mining, which is the process\\nof finding and summarizing remixed image content in large\\ncollections of unlabeled data [11]. They presented a new image\\nfeature strategy that combines global features (such as VGG\\nand pHash) and local features (such as SURF keypoints) to\\ncapture both holistic and local similarities between images.\\nTheir pipeline was evaluated on three meme datasets, includ-\\ning a dataset from Telegram related to the Russo-Ukrainian\\nconflict.\\nTommasini et al. introduced the Internet Meme Knowledge\\nGraph (IMKG), which provides a comprehensive semantic\\nmodel for representing memes, their templates, and associated\\nmetadata [12]. The IMKG offers a structured approach to cap-\\nture the complex relationships between memes, their origins,\\nand their variations. Joshi et al. developed a framework to\\ncontextualize internet memes on social media using knowledge\\ngraphs [13]. Their method leverages structured knowledge toarXiv:2408.08126v1  [cs.CY]  15 Aug 2024Template\\ntext\\nimage\\ncombined\\n+\\nMemesFig. 1. Different realizations of the ‚ÄúDistracted Boyfriend‚Äù template.\\nidentify and analyze memes across platforms. By mapping\\nmemes to a knowledge graph such as IMKG, they demonstrate\\nhow to study meme prevalence, identify popular memes, and\\nprovide rich contextual information on platforms like Reddit\\nand Discord.\\nA common limitation of the mentioned approaches is the\\nfailure to explicitly account for the non-memes and template-\\nless memes that often appear in real-world data. This study\\ncontinues this line of research and takes an important step\\ntoward a more nuanced template-level analysis. We propose\\na rigorous evaluation framework (depicted in Figure 2) that\\nencompasses a wide range of approaches, including both\\nestablished methods from the literature and novel techniques.\\nThis framework incorporates extensive feature engineering and\\nevaluates both supervised and unsupervised methods, such as\\nconvolutional neural networks (CNNs), distance-based classi-\\nfication, and density-based clustering. We train and evaluate\\nmodels on an exhaustive dataset labeled with meme templates\\nsourced from Imgflip, and further test their performance on a\\nsample from a diverse set of 1.5 million unlabeled memes from\\nvarious social media platforms (Reddit, Facebook, X). Our\\nstudy provides a comprehensive comparison of the proposed\\nmethodologies with existing approaches, including those pre-\\nsented recently by Courtois & Frissen [10], Dubey et al. [9],\\nand Zannettou et al. [8]. Through this rigorous evaluation,\\nwe aim to highlight the strengths and limitations of each\\nmethod, offering insights into their prediction performance,\\ntime efficiency, and scalability. Furthermore, we discuss how\\nthis comparative framework can contribute to a better under-\\nstanding of meme template lifetime analysis and illuminate the\\nconnection between the local and global context of memes.\\nThe main contributions of this paper can be summarized as\\nfollows:\\n‚Ä¢We collect two extensive meme datasets: one featuring\\nover 100,000 memes labeled with their template and the\\nother containing roughly 1.5 million unlabeled memes.\\nUpon request, these datasets will be made available to\\nresearchers for further study and replication of our results.\\n‚Ä¢We introduce a rigorous evaluation framework that en-\\ncompasses a wide range of methods, including both\\nestablished approaches from the literature and novel\\ntechniques. To the best of our knowledge, we are the\\nfirst to compare existing methodologies of meme template\\np H a s h  e m b e d d i n g sC L I P  e m b e d d i n g sC N N  f e a t u r e  e x t r a c tI m g  F l i p\\nF e a t u r e  e n g i n e e r i n gL a b e l e d  \\nm e m e s\\nS o c i a l  m e d i a\\u2028\\n i m a g e sM o d e l  t r a i n i n g\\nE v a l u a t i o n\\nU n l a b e l e d  d a t aM L RR N NH D B S C A NC N Np H a s hR N N - F MS MT r a i n - t e s t  s p l i tT r a i n i n g  d a t a T e s t  d a t a\\nR a n d o m  s a m p l eN o n - m e m e sR a w  i m a g e s\\nw i t h  l a b e l s\\n8 0 : 2 0  r a t i o\\n1 , 0 0 0  i m a g e s\\nH u m a n  e v a l u a t i o nM u l t i c l a s s  ( m e m e  t e m p l a t e \\nB i n a r y  ( m e m e / n o n - m e m e )A u t o m a t e d  e v a l u a t i o nF 1 ,  K a p p a ,  M C CL a b e l e d  m e m e sH V S ,  G r a y s c a l e ,  \\nT e x t u r e  f e a t u r e s\\nFig. 2. Overall workflow of our research.\\nclassification.\\n‚Ä¢We emphasize and validate the importance of models\\nbeing able to handle non-memes and templateless memes\\nin addition to templated memes. This is crucial for real-\\nworld social media analysis, as we found that only 25% of\\nthe analyzed social media memes are memes from well-\\nestablished templates. To the best of our knowledge, we\\nare the first to take this into account when evaluating\\nmeme template identification methods.\\nII. D ATA COLLECTION\\nData were collected from various major social media sites\\n(Reddit, X, and Facebook) excluding platforms that might bias\\nthe results because of their political affiliations or specific\\nethnic and national identities. We also collected data from\\na meme annotation website called Imgflip which provides\\nground-truth meme template labels.\\nImgflip is a popular website that allows users to create\\nand share image-with-text and GIF memes. It is widely used\\nfor generating memes, offering a variety of meme templates\\nthat users can customize. The site also provides a platform\\nfor users to engage with and rate each other‚Äôs creations,\\nmaking it a hub for meme enthusiasts. Using web scrapingtechniques [14], we collected an initial 2,277 templates from\\nthe website. However, after closer inspection, we discovered\\nthat many of these templates are duplicated (with slightly\\ndifferent names). To solve this, we embedded the reference\\nimage of each of the templates using the so-called CLIP (Con-\\ntrastive Language‚ÄìImage Pretraining [15]) model, then using\\ncosine similarity search we manually filtered the matched\\ntemplates, leaving us with a final collection of 1,145 unique\\ntemplates. Each meme created on the website, using any\\nof the templates available in Imgflip‚Äôs collection, is also\\ndisplayed in the corresponding template‚Äôs library. Scraping\\nthe accompanying libraries yielded a sufficiently large dataset\\nconsisting of 124,208 memes labeled with their templates. The\\ntemplates have a collection of 109 memes on average (median:\\n111), but the number of memes ranges from 31 to 582 per\\ntemplate.\\nReddit is a popular social media platform and a signifi-\\ncant source of viral content, often referred to as ‚Äú the front\\npage of the internet ‚Äù [16]. We gathered the posts from the\\nr/Memes subreddit, the largest Reddit community (over 26\\nmillion subscribers) dedicated to sharing memes. The memes\\nposted between January 1, 2018, and November 14, 2022,\\nwere collected using the Pushift API [17]‚Äì[19]. A maximum\\nof 1,000 randomly selected posts per day were collected,\\nresulting in a total of 899,522 images.\\nX X X(formerly Twitter) is a micro-blogging platform for users\\nto disseminate short messages, colloquially known as tweets,\\nto a global audience. Using Selenium [20], we collected tweets\\ncontaining either the hashtag ‚Äúdank‚Äù - a word commonly used\\nin association with memes - or ‚Äúmeme‚Äù, published between\\nJanuary 1, 2018, and December 31, 2022, and written in\\nEnglish. We collected 800-850 randomly sampled tweets per\\nday due to X‚Äôs scraping limits. This data collection campaign\\nresulted in a total of 292,129 posts, from which we extracted\\n174,338 images.\\nFacebook is one of the largest and most widely used social\\nmedia platforms in the world, with over three billion active\\nusers in 2023 [21]. Facebook groups provide a digital hub to\\nconnect to other users with common interests including groups\\ndedicated to sharing memes. To identify these groups, we\\nsearched for groups whose names contained the term ‚Äúmeme‚Äù\\nin any variation. This strategy led to the discovery of 300\\ngroups. After eliminating the non-English-speaking groups,\\nwe were left with 50 groups. These were then exhaustively\\nscraped accumulating a total of 537,897 posts, that contained\\n235,880 images.\\nAlthough our data collection process targeted online com-\\nmunities dedicated to sharing memes, it is still possible that\\nother types of images were posted in these communities and\\nhence included in our dataset. These could be ‚Äútemplateless‚Äù\\nmemes - one-time memes or screenshots that did not evolve\\ninto a template - or even non-meme images, such as advertise-\\nments, selfies, or landscape photos with no humorous content.\\nTo address the challenge of distinguishing between memes\\nand non-memes, we also collected a separate dataset of non-\\nmeme images. This ‚ÄúNon-Meme‚Äù dataset consists of an equalblend of photographs from the Flickr30k dataset and image-\\nwith-text images, such as movie posters, screenshots, and\\nadvertisements. We included these image-with-text formats be-\\ncause they are common online and harder to distinguish from\\nmemes than regular photos [22]. This approach is justified\\nby observations made on social media data, which indicate\\nthat templateless social media memes and non-meme social\\nmedia images are more similar to the images in our non-meme\\ndatabase (such as movie posters, screenshots, and advertise-\\nments) than to the templated memes found on ImgFlip.\\nThe number of images and posts collected from the various\\nonline platforms and the Non-Meme dataset is summarized in\\nTable I.\\nTABLE I\\nOVERVIEW OF DATA SOURCES .\\nData Source Type No. Samples No. Templates\\nImgflip Memes 124,208 1,145\\nReddit Mixed 899,522 unlabeled\\nFacebook Mixed 235,880 unlabeled\\nX X X(Twitter) Mixed 174,338 unlabeled\\nScreenshots [23] Non-Memes 42,891 unlabeled\\nAdvertisements [24] Non-Memes 41,462 unlabeled\\nFlickr30k [25] Non-Memes 31,783 unlabeled\\nMovie posters Non-Memes 8,052 unlabeled\\nIII. F EATURE ENGINEERING\\nLike the sculptor shapes clay into a form that enhances its\\nvalue, feature engineering transforms raw data into features\\nthat better represent the underlying problem in predictive mod-\\nels. Building on this foundational understanding, the question\\nof modality naturally arises. Although combining multiple\\ntypes of data (e.g. image, text) may enhance performance in\\nsome cases [9], in most memes the background image provides\\nthe template and the text overlay introduces the local context.\\nFor this reason, we decided to perform template identification\\nbased solely on visual features. As shown in Figure 2, we used\\na variety of models to extract these image-based features.\\nA. Baseline features\\nIn image analysis, ‚Äúsimple‚Äù features serve as fundamental\\ndescriptors that distill complex visual information into a\\nformat more accessible to algorithms, especially those not\\ninherently designed to handle high-dimensional data, such as\\nraw pixel values:\\n‚Ä¢RGB histograms analyze an image‚Äôs color distribution\\nacross red, green, and blue components, generating a\\nfeature vector that reveals its unique color profile and\\ncomposition.\\n‚Ä¢Grayscale histograms condense color data into intensity,\\noffering insights into an image‚Äôs brightness distribution.\\nThis simplification allows analysis of lightness and dark-\\nness without color complexities. OpenCV libraries aid in\\nextracting RGB and grayscale histograms efficiently [26].\\n‚Ä¢Texture features via Local Binary Patterns (LBP):\\nThe LBP algorithm detects intricate textural patternsin images, distinguishing surfaces with differing tex-\\ntures [27]. It compares the intensity of a central pixel with\\nits neighbors, encoding these comparisons into binary\\npatterns that are aggregated into a histogram capturing\\nthe distribution of texture patterns across the image, with\\neach bin representing a unique pattern and its correspond-\\ning frequency.\\nB. Embeddings via feature extraction\\nRepresenting images as vectors in a low-dimensional space\\nmakes it easier to analyze complex data such as memes. One\\nway to derive embeddings is to capture the vectors calculated\\nin the penultimate layer of our best-performing convolutional\\nneural network model trained for template identification. The\\nrationale behind this technique, also known as feature extrac-\\ntion, relies on the observation that neural networks can learn\\nhigher-level features from the original input image.\\nC. Perceptual hashing\\nPerceptual hashing is a well-regarded method in digital\\nmedia to generate a compact, distinct ‚Äúfingerprint‚Äù of images,\\naudio, or video by focusing on their perceptual features, the\\nfeatures that human perception finds significant [28], [29].\\nIn this work, building on the study of Zanettou et al. [8],\\nwe applied perceptual hashing to extract meme features and\\nanalyze visual similarities of memes. Perceptual hashes were\\nderived using the pHash library [30]. The binary feature\\nvectors generated by pHash are of length 64, showcasing the\\naggressive dimension reduction capabilities of the underlying\\nDiscrete Cosine Transform algorithm.\\nD. CLIP embeddings\\nCLIP, which stands for Contrastive Language‚ÄìImage Pre-\\ntraining, is a neural network introduced by OpenAI [15].\\nThis model is designed to generate embeddings for images,\\ntext, and multi-modal data that include both. In this research,\\nwe used the Image encoder of CLIP, which can be used\\nindependently, to create vector representations of the pictures.\\nE. Orientated FAST and Rotated BRIEF (ORB) features\\nThis technique is a machine translation of how humans\\ninterpret visual stimuli: we look at distinctive combinations of\\nshapes and textures that pop out. Similarly, feature extraction\\nis a procedure that analyzes an image for areas of dense\\ninformation where substantial variation occurs. The Oriented\\nFAST algorithm scans each pixel of a grayscale image and\\ncompares the surrounding pixels for their brightness. The\\nsurrounding area is flagged as a key point if it is darker or\\nbrighter. It does so at different image resolutions, making\\nit a scale-invariant approach. The Rotated BRIEF algorithm\\ndescribes the local appearance around each key feature, i.e.,\\nit establishes the pattern that surrounds a key point being\\nrobust for changes in scale, rotation, translation, and illumina-\\ntion [10]. The ORB feature detector is included in the OpenCV\\nPython package [26], [31].IV. M ODEL TRAINING\\nAlthough our use of the labeled data (Imgflip) suggests a\\ncanonical supervised learning approach, specifically a mul-\\nticlass classification, we should also consider why this task\\nmight not strictly fit the classification framework.\\nFirstly, we cannot assume that every image posted on social\\nmedia can be classified into one of the pre-defined classes\\n(templates). Recall that many of these are templateless, or even\\nnon-meme images, therefore, supervised algorithms that were\\ntrained using a closed set of classes would generally struggle\\nwith handling out-of-distribution (OOD) input that does not\\nbelong to any of the defined classes. Secondly, the nature\\nof meme culture, which is dynamic and constantly evolving,\\nintroduces new templates frequently. This means that the set of\\ntemplates is neither fixed nor complete, making it challenging\\nto maintain a comprehensive, up-to-date classification system.\\nAdditionally, memes often blend elements from multiple tem-\\nplates, further complicating straightforward classification.\\nDespite these challenges, we believe that the problem can\\nreasonably be modeled as a large multiclass classification\\nproblem because the majority of memes still adhere to popular,\\nrecognizable templates that can be effectively categorized.\\nWe also believe that the Imgflip dataset, with its more than\\none thousand unique templates, is an adequate collection to\\nrepresent the majority of templates.\\nIn addition to modeling template identification as a classi-\\nfication problem, we also use unsupervised learning methods.\\nBy clustering memes from social media sites, we can assume\\nthat templateless memes are likely to fall outside of the clusters\\n(e.g., they make up the noise points found by density-based\\nclustering algorithms) or they form very small clusters. In this\\ncase, the labeled dataset can be clustered together with the\\nunlabeled datasets to assign templates to clusters. However,\\nin this approach, achieving the appropriate number of clusters\\ncan present a serious challenge. However, this paper analyzes\\nand compares both supervised and unsupervised approaches.\\nA. Supervised classification\\nWe employed a variety of supervised classification methods,\\neach adapted to suit the unique challenges presented by the\\nmeme dataset. All models were trained and hyperparameters\\noptimized by 5-fold stratified cross-validation on the Imgflip\\ndataset, with the optimization objective set to maximize the\\nMatthews Correlation Coefficient (MCC).\\n1) Multinomial Logistic Regression (MLR): This model\\nserves as a simple baseline model. The simplicity of the\\nmodel also applies to the features used (RGB, grayscale,\\ntexture features). Unlike the high-dimensional data processing\\ncapabilities of more advanced models, MLR requires simpler\\nand more elemental descriptors.\\n2) Radius Nearest Neighbors (RNN): We apply the radius-\\nbased version of k-Nearest Neighbors (kNN) as it facilitates\\nthe identification of outliers through a well-defined bound-\\nary threshold. Memes that do not have neighbors within a\\npredefined radius are designated as templateless, essentially\\ncategorizing them as outliers or noise within our dataset. Tofind a suitable radius for the nearest neighbor search, we\\ndecided to set it to the smallest value where none of the\\nlabeled samples were classified as templateless. We tested\\nthe RNN algorithm using two different features and distances\\nto optimize its performance. Initially, we used the binary\\npHash vectors and the Hamming distance, which quantifies\\nthe number of differing bits between two hashes. Furthermore,\\nwe also experimented with features extracted from our best-\\nperforming CNN, utilizing cosine similarity to measure the\\ncloseness of feature vectors.\\n3) Transfer Learning and CNNs: Central to our method-\\nology was the adaptation of pre-trained convolutional neural\\nnetworks (CNNs) for meme template recognition, supported\\nby a comprehensive training framework designed for flexibility\\nacross different architectures within the Pytorch Lightning\\nlibrary. Modifying the classification layer to correspond to\\nthe 1,145 identified meme templates ensured that each model\\nwas adjusted to our dataset. The process involved the initial\\ntraining of the new classification layer, followed by selective\\nfine-tuning of deeper layers.\\nEach model underwent 10 training epochs, with an early\\nstopping mechanism based on cross-entropy loss to pre-\\nvent overfitting. We tested ResNet, EfficientNetV2, and\\nDenseNet [32]‚Äì[34], finding that the latter performed the best.\\nFor a fair comparison between the other examined models\\nand CNNs, we decided to extend our fine-tuned DenseNet\\nmodel with the ability to handle out-of-distribution (OOD)\\ninputs effectively. There exist several different methods to\\nsolve this problem, such as using a confidence threshold or\\nfurther training [35]. After seeing that there is no optimal\\nconfidence threshold that can be applied to the probabilities\\ncomputed in the last layer of the CNN to keep the number of\\nFalse Positives and False Negatives equally low, we decided to\\nuse a two-headed architecture. The first CNN head classifies\\nwhether the input image is a meme of a known template,\\nand the second head identifies the template. The Non-Meme\\ndataset - described in Section II - in combination with the\\nImgflip dataset, served as the input for the training of the first\\nCNN head. We chose to train the already high-performing\\nDenseNet-121 for the meme vs. non-meme classification task,\\nachieving 99.1% average accuracy on the validation set for\\nthis binary classification problem.\\n4) RNN-Feature Matching (RNN-FM) [10]: Courtois &\\nFrissen developed a methodology to analyze internet memes\\nby combining computer vision with network analysis. They\\nused the ORB feature detector with the ORB Brute-Force\\nmatcher algorithm to formally link meme images based on\\nunique combinations of shapes and textures, enabling the\\nidentification of visual similarities. Following this, network\\nanalysis is employed to discern patterns of connectivity and\\ndissemination among the images. To enhance the accuracy\\nof feature matching, their approach includes a pre-processing\\nstep to blur text regions on the images, thereby reducing\\nthe likelihood of false positives related to text features. The\\nauthors also highlighted scalability challenges due to the\\ncomputationally demanding nature of their feature-matchingmethod. Specifically, the preprocessing stage requires inten-\\nsive text detection, and the matching procedure‚Äôs processing\\ntime grows exponentially as the image corpus expands. This\\nis because the features of each image must be compared\\nwith every other image in the dataset, leading to significant\\nincreases in computational load and time as the number of\\nimages increases [10].\\nTo improve the time efficiency of the original method, we\\nhave replaced the ORB brute-force matcher algorithm with\\nthe faster FLANN-based matcher (Fast Library for Approxi-\\nmate Nearest Neighbors; [36]), which the authors have also\\nhighlighted as a potential way to improve performance. Addi-\\ntionally, while representing the outcome of feature matching\\nas a network graph is a reasonable approach for identifying\\nspecific ‚Äùbridge‚Äù features between memes and highlighting\\nhow ‚Äùcross-over‚Äù memes are created [10], for our use-case\\nof identifying meme templates we deemed this step rather\\nirrelevant. This meant that we discarded the last step of\\nthe author‚Äôs procedure and instead, we derived image-to-\\nimage distances, from the feature-to-feature distances using a\\nprocedure similar to how Courtois & Frissen defined a match\\nbetween two images. That is, if there are at least mfeature\\nmatches at a maximum distance of dbetween a pair of images,\\nwe consider them similar and we take the minimum distance\\nbetween their matched features as the derived distance between\\nthe images. Once we constructed a distance matrix of the\\nimages, we then utilized the RNN algorithm, introduced in\\nSection IV-A2, to assign templates to the unlabeled images\\nusing their labeled neighbors. While in the original paper,\\nthe optimal parameter combination found by the authors was\\nd= 27 andm= 4, on our larger, possibly more diverse\\nImgflip dataset the combination that yielded the best results\\nturned out to be d= 27 andm= 20 .\\n5) Sparse matching (SM) [9]: Dubey et al. apply sparse\\nrepresentation (or sparse matching) to identify the meme\\ntemplate of memes. The idea of sparse matching was first\\nintroduced for face identification by Wright et al. [37] and was\\nadopted by Dubey et al. for template detection in 2018 [9].\\nThe sparse representation operates under the assumption that\\nthe training samples lie in a subspace, allowing any test point\\nto be represented as a sparse linear combination of these\\ntraining points. This was used to identify and separate the\\nmeme template (global context) from its overlaid content (local\\ncontext). The implementation of sparse representation includes\\nsteps such as color normalization and downsampling of im-\\nages, followed by L1-minimization to achieve the sparsest\\nsolution of an underdetermined linear system. Following the\\nseparation of the overlays from the templates, the authors\\nutilize deep convolutional neural networks (CNNs) and re-\\ncurrent neural networks (RNNs) to extract features from both\\nthe image and text components. These extracted features are\\nthen concatenated, creating a feature representation to enhance\\nthe understanding of the memetic imagery. We decided to\\nimplement only the sparse matching algorithm described in\\nthe paper, as it is responsible for assigning memes to their\\ntemplates. To be able to reject templateless images, we use thesparsity concentration index (SCI), also described by Wright,\\nwhich measures how concentrated the coefficients are on a\\nsingle class in the data set [37].\\nB. Unsupervised methods\\n1) HDBSCAN: We implement an unsupervised clustering\\napproach using the BERTopic pipeline [38]. In this setup, the\\nmemes go through a sequence of steps to be assigned to tem-\\nplates: calculating embeddings, dimensionality reduction, and\\nclustering. To validate the goodness-of-fit of the clustering and\\nmake it easier to compare with supervised models, we divided\\nthe Imgflip dataset in a supervised fashion. Specifically, 80%\\nof Imgflip memes, along with all the social media memes\\nformed the core of our clustering, and the remaining 20%\\nof Imgflip memes (with ground truth labels) were used for\\nevaluation.\\nFor embedding purposes, we used the CLIP model and\\npHashes. We reduced the dimensionality of these numerical\\nrepresentations with UMAP (Uniform Manifold Approxima-\\ntion and Projection [39]) because this method is particularly\\nadept at preserving both local and global structures within\\na dataset, which is essential for maintaining the character-\\nistics necessary for clustering visually similar memes. For\\nclustering, we used HDBSCAN (Hierarchical DBSCAN), a\\ndensity-based clustering algorithm [40] due to its proficiency\\nin identifying clusters of varying shapes and sizes while\\neffectively managing outliers. This capability is particularly\\npertinent to a dataset that most likely includes instances from a\\nwide array of meme templates and templateless images. What\\nmakes this approach unique is how templates were assigned\\nto clusters: the identified clusters were associated with meme\\ntemplates based on (80% of) the labeled memes from the\\nImgflip dataset. In each cluster, we perform majority voting:\\nthe memes in a cluster are assigned to the template that has\\nthe most samples within the cluster among the Imgflip memes.\\n2) Perceptual hashing method [8]: Zanettou et al. col-\\nlected 160M images from various platforms including Twitter,\\nReddit, 4chan‚Äôs /pol/, and Gab. They used pHashing to gen-\\nerate image fingerprints, facilitating similarity comparisons.\\nThe DBSCAN algorithm was then used to cluster visually\\nsimilar memes. For annotating these clusters, they relied on\\nmetadata from Know Your Meme (KYM), a website dedicated\\nto documenting Internet memes, calculating the Hamming\\ndistance between the pHash of each cluster‚Äôs medoids and\\nKYM images. The clusters were annotated based on the KYM\\nentry that showed the highest match proportion to the medoids,\\nemploying a customized distance metric that integrated both\\nvisual similarity and KYM data.\\nTo replicate the work of Zanettou et al. , we adjusted the\\nevaluation strategy by segmenting the Imgflip dataset into an\\n80-20 ratio, stratified by meme templates. This adjustment\\nallowed us not only to manually inspect their annotation\\nmethod but also to apply automated evaluation which was\\nnot provided in their work. In our implementation, the same\\ndistance threshold ( Œ¥=8) and distance metric (Hamming) wasused that was proposed by Zanettou et al. , given the similarity\\nof our domains.\\nV. R ESULTS\\nIn this section, we present a comprehensive comparison\\nof all approaches described in Section IV, evaluating their\\nperformance in detecting meme templates across two distinct\\ndatasets. Our evaluation metrics, including Matthew‚Äôs Cor-\\nrelation Coefficient (MCC [41]), Cohen‚Äôs Kappa score [42],\\nand the F1 score, were selected for their ability to handle the\\ncomplexities of an extreme multiclass classification problem\\ncharacterized by significantly imbalanced classes.\\nOur evaluation strategy involves two key steps:\\n‚Ä¢Evaluation on Imgflip dataset: We apply all models\\nto the Imgflip test set, classifying memes into one of\\n1,145 templates. First, feature engineering and extraction\\nare performed on memes in the Imgflip data set. Then,\\nthe engineered image features are used to identify meme\\ntemplates, and the model performance is measured by\\ncomparing the detected template with the ground-truth\\nlabels provided by Imgflip.\\n‚Ä¢Evaluation on social media dataset: We perform a\\nmanual evaluation of all models on 1,000 randomly\\nselected images from our social media dataset (Reddit,\\nX, and Facebook).\\nA. Evaluation on Imgflip Dataset\\nTable II illustrates the performance of all the models\\nevaluated in the Imgflip validation set. This dataset consists\\nof 124,208 memes labeled with their templates, representing\\n1,145 unique template classes. It is important to note that this\\ndataset does not include any templateless images, focusing\\nsolely on memes with identifiable templates.\\nTABLE II\\nPERFORMANCE METRICS FOR MODELS ON THE IMGFLIP DATASET .\\nModel Feature MCC Kappa F1\\nMLR baseline features 0.923 0.921 0.923\\nRNN pHash 0.916 0.913 0.943\\nRNN DenseNet emb. 0.931 0.930 0.952\\nRNNFeature\\nMatching [10]0.976 0.975 0.982\\nResNet-18 raw image 0.988 0.988 0.988\\nDenseNet-121 raw image 0.991 0.991 0.991\\n2-headed\\nDenseNet-121raw image 0.992 0.992 0.995\\nEfficientNetV2 raw image 0.985 0.985 0.985\\nUMAP+HDBSCAN pHash 0.815 0.815 0.807\\nUMAP+HDBSCAN CLIP emb. 0.836 0.836 0.822\\nPerceptual Hashing [8] 0.563 0.483 0.622\\nSparse Matching [9] 0.705 0.686 0.709\\nThe 2-headed DenseNet emerged as the most proficient\\nmodel, achieving the highest performance in all metrics. It also\\nclassified the fewest memes as templateless, demonstrating its\\nrobustness in template identification. Among the models from\\nthe literature, Zannettou et al. ‚Äôs perceptual hashing approach\\nshows high performance when considering only recognizedtemplates. However, it labels over half of the memes tem-\\nplateless, significantly impacting its overall performance. The\\nhigh specificity of their distance threshold led to conservative\\nclustering, resulting in many small homogeneous clusters\\n(25,538), far exceeding the expected 1,145 natural clusters\\nbased on Imgflip metadata. The RNN-Feature Matching model\\ninspired by Courtois & Frissen, despite our modifications to\\nimprove efficiency, remained the most time-consuming, taking\\ntwo weeks distributed across four computers. This suggests\\nthat while effective for smaller datasets, it may be impractical\\nfor large-scale meme analysis involving millions of samples.\\nThe sparse matching method by Dubey et al. , while adequate,\\nshowed reduced effectiveness compared to more recent meth-\\nods. This could be due to the greater variability in coefficients\\nintroduced by the local context in memes compared to their\\noriginal application in face recognition.\\nB. Evaluation on Social Media Dataset\\nTo evaluate the performance of the models on real-world\\ndata, we performed a manual evaluation using 1,000 randomly\\nselected images from our social media datasets (Reddit, X,\\nFacebook). It is crucial to note that although these images were\\ndownloaded from meme-oriented sites, they may still contain\\nnon-memes and templateless memes. This diversity reflects\\nthe real-world challenges of meme analysis on social media\\nplatforms. We expect our models to handle this complexity,\\nas the ability to distinguish between memes, non-memes, and\\ntemplateless memes is essential for large-scale social media\\nanalysis.\\nIt is important to note that manually labeling images is\\na time-consuming and costly process, which informed our\\ndecision to limit this evaluation to 1,000 images. For each\\nimage, all examined models have made a prediction, which\\ncould also be templateless. This made it possible to examine\\nthe models‚Äô robustness against templateless memes and non-\\nmeme images. Six annotators with extensive meme domain\\nknowledge assessed the predictions made by each model. The\\nevaluation process used an application where annotators could\\nmark predictions as ‚ÄúCorrect‚Äù or ‚ÄúIncorrect‚Äù (Figure 3). The\\nannotators did not discuss their responses during the evaluation\\nto ensure independence.\\nFig. 3. A screenshot of the evaluation application.The inter-annotator agreement, measured by the Fleiss\\nkappa, was 0.85, indicating ‚Äúsubstantial agreement‚Äù [43]. We\\nused the majority decision of the annotators as the ground\\ntruth for benchmarking the models‚Äô performance.\\nIn our analysis, we recognized that the behavior of the meme\\ntemplate identification models can go wrong in two distinct\\nways: (a) they may fail in the template vs. non-template clas-\\nsification, or (b) they may misidentify the specific templates\\nthemselves. To gain a better understanding of these potential\\nerrors, we evaluated the methods from both perspectives.\\nWe used both binary and multiclass classification ap-\\nproaches in our evaluation. The binary evaluation looked\\nat whether the algorithm correctly classified the image as\\ntemplated (positive class) or templateless, while the multiclass\\nevaluation also looked at whether it successfully labeled the\\nimage as the specific meme template. Our manual labeling\\nrevealed that the dataset consisted of 75% templateless images\\nand 25% templated images, reflecting the diverse content\\nfound on social media platforms. Our multiclass classification\\nassessed the models under three distinct scenarios: perfor-\\nmance on the whole manually labeled 1000-image dataset (All\\nMemes), on images the model identified as templated (Model\\nTemplated), and on images preliminarily labeled as templated\\nby our annotators (True Templated). Table III presents a com-\\nprehensive view of our evaluation results, combining insights\\nfrom both binary and multiclass classification tasks.\\nThe RNN model with pHash embeddings demonstrates the\\nbest overall performance, with the highest F1 score (0.8281),\\nKappa (0.6203), and MCC (0.6460) across all images. This\\nconsistent performance across metrics suggests its robustness\\nin general meme template detection tasks. However, different\\nmodels showed strengths in specific areas. The perceptual\\nhashing approach of Zanettou et al. shows exceptional perfor-\\nmance (F1: 0.9636) when evaluating only on the set of images\\nit identified as templated. This aligns with its high precision\\n(0.9818) in binary classification, indicating that while it rarely\\nclassifies an image templated, whenever it does, it accurately\\nidentifies the template as well. However, this conservative\\napproach results in very few memes being identified as tem-\\nplated, leading to a poor recall of 0.2186, significantly limiting\\nthe method‚Äôs overall effectiveness in identifying meme tem-\\nplates across a diverse dataset. Interestingly, CNN (2-headed\\nDenseNet) performs best on the subset of truly templated im-\\nages (accuracy 0.7166) and shows the highest recall (0.8747)\\nin binary classification. This suggests that CNN excels at dis-\\ntinguishing between different known templates and identifying\\ntemplated memes, even though it struggles more with rejecting\\ntemplateless images (low precision of 0.3345). In contrast, the\\nSparse Matching model consistently underperformed across all\\nscenarios, suggesting that this approach may not be well-suited\\nfor the complexities of meme template detection in diverse\\nsocial media content.RNN combined with Feature matching\\n(RNN-FM) shows strong and consistent performance across\\nall scenarios, ranking second or third in most metrics. This\\nreliability positions it as a solid choice for applications requir-\\ning a balance between identifying templateless images andTABLE III\\nPERFORMANCE METRICS FOR MODELS ON THE SOCIAL MEDIA DATASET .\\nModels FeatureAll Memes Model Templated True Templated Binary Metrics\\nF1 Kappa MCC F1 Kappa MCC F1 Kappa MCC Recall Precision\\nRNN pHash 0,8281 0,6203 0,6460 0,8598 0,8565 0,8622 0,5567 0,5117 0,5822 0,5344 0,8919\\nRNNDenseNet\\nembedding0,7553 0,4001 0,4238 0,6358 0,6228 0,6963 0,3256 0,3127 0,4472 0,3004 0,6935\\nRNNFeature\\nMatching [10]0,8243 0,5684 0,5714 0,5263 0,5190 0,5583 0,6376 0,6050 0,6291 0,7490 0,6424\\n2-headed\\nDenseNet-121raw image 0,5803 0,3019 0,3689 0,2438 0,2590 0,3533 0,7298 0,7145 0,7228 0,8947 0,3245\\nUMAP+\\nHDBSCANpHash 0,7269 0,3526 0,3546 0,3575 0,3562 0,3987 0,3898 0,3859 0,4187 0,6002 0,5231\\nUMAP+\\nHDBSCANCLIP emb. 0,7749 0,4738 0,4746 0,4671 0,4727 0,5238 0,5281 0,5033 0,5500 0,6397 0,5852\\nPerceptual Hashing [8]0,7286 0,3236 0,4341 0,9636 0,9627 0,9634 0,2497 0,2129 0,3441 0,2186 0,9818\\nSparse Matching [9] 0,5079 0,1382 0,1595 0,1314 0,1299 0,1900 0,3295 0,3057 0,3285 0,6599 0,2796\\naccurately classifying templates. However, as noted earlier,\\nthis performance comes at the cost of significant computational\\ntime, which may limit its practicality for large-scale meme\\nanalysis.\\nVI. C ONCLUSION\\nOur study of meme template identification methods has\\nyielded significant insights into the strengths and limitations\\nof various approaches, both established and novel. We have\\nuncovered nuanced performance characteristics of different\\nmodels in various scenarios. The RNN model with pHash em-\\nbeddings emerged as the top performer in real-world scenarios,\\ndemonstrating robust template identification capabilities across\\ndiverse meme content. The 2-headed DenseNet model showed\\nparticular effectiveness in distinguishing between known tem-\\nplates, especially when dealing with pre-filtered, templated\\nmeme datasets. Zannettou et al. ‚Äôs perceptual hashing method\\nexhibited exceptional precision in template identification, al-\\nbeit with lower recall, making it suitable for applications\\nrequiring high confidence in template assignments. However,\\nfor tasks aimed at capturing a broad range of social media\\noccurrences of a given template, this method may not be\\noptimal due to its strict nature, classifying most images as\\ntemplateless. The RNN-Feature Matching model, inspired by\\nCourtois & Frissen‚Äôs work, offered balanced performance but\\nat a significant computational cost, highlighting the trade-offs\\nbetween accuracy and efficiency in large-scale meme analysis.\\nThese findings have far-reaching implications for meme\\nanalysis and related fields. The high-performing models enable\\nmore accurate tracking of meme templates spread across\\nsocial media platforms, facilitating studies on meme evolution\\nand virality. This capability is crucial for discerning memes‚Äô\\ninfluence on public discourse, cultural trends, and online be-\\nhavior. Our framework for large-scale identification of meme\\ntemplates opens up new possibilities for researchers to analyze\\nmeme usage patterns and their reflections on societal trends\\nand attitudes. Mascarenhas et al. highlight the need to scale\\nup qualitative analysis, noting that analyzing the arguments\\nmemes make is more impactful than analyzing particularmeme instances. Classifying groups of memes into templates\\nmoves us towards this goal [44]. Furthermore, conducting a\\nlife cycle analysis of meme templates on social media could\\nidentify similarities or common patterns between template life\\ncycles, such as ‚Äúevergreen‚Äù templates or ‚Äúshooting star‚Äù tem-\\nplates. Investigating how the interplay between a meme‚Äôs local\\nand global context influences its popularity presents another\\nintriguing research direction. The collected metadata (e.g.,\\ntitles, captions, publication dates, content maturity indicators,\\ncomment counts, and scores) offers opportunities for more\\ncomprehensive analyses of meme engagement and spread, as\\nsuggested by Barnes et al. [45].\\nHowever, our study also revealed certain limitations. Our\\nlimited computational resources hindered full optimization\\nand experimentation with these models. Data collection con-\\nstraints, particularly changes to the API of X, impacted\\nthe breadth of our Xdataset, indicating that expanding the\\ndataset would provide a more representative view of the\\ndiverse meme landscape. The challenge of keeping up with\\nthe rapidly evolving landscape of meme templates was ev-\\nident in the performance discrepancy between the Imgflip\\ndataset and real-world social media content. Although the\\nImgflip database is extensive, it does not cover all circulating\\nmeme templates on social media sites, suggesting the need\\nto diversify data sources in future research. We are aware of\\nKnowYourMeme.com, a popular resource often used in the\\nliterature for meme studies. However, our previous attempts to\\nuse this platform revealed significant drawbacks. The galleries\\nfor meme templates, which would serve as the source for\\nlabeled memes, often contain irrelevant and unverified memes\\nand images, making Imgflip a more reliable choice for our\\nstudy despite its limitations.\\nDespite these limitations, the proposed framework provides\\na solid foundation for future research in meme template\\nidentification and analysis. By highlighting the strengths and\\nweaknesses of various approaches, we aim to guide researchers\\nand practitioners in selecting appropriate methods for their\\nspecific use cases. We have made all our code availablein an open GitHub repository,1including implementation\\ndetails, evaluation frameworks, and data processing scripts. We\\nencourage researchers to build on and extend this codebase,\\npromoting collaboration in meme analysis. As meme culture\\nevolves, we believe that accurately identifying and analyzing\\nmeme templates will become increasingly valuable across\\nvarious disciplines.\\nACKNOWLEDGMENT\\nWe thank Talha Sahin for his assistance in the collection\\nof Facebook and Twitter data. The research was supported by\\nthe European Union project RRF 2.3.1-21-2022-00004 within\\nthe AI National Laboratory and Grant Nr. TKP2021-NV A-02.\\nRoland Molontay is also supported by the National Research,\\nDevelopment and Innovation Fund through the OTKA Grant\\nPD-142585 and by the University Research Fellowship Pro-\\ngram (EK ¬®OP).\\nREFERENCES\\n[1] P. Davison, ‚ÄúThe language of internet memes,‚Äù The Social Media Reader ,\\npp. 120‚Äì134, 2012.\\n[2] C. W. Leach and A. M. Allen, ‚ÄúThe social psychology of the Black\\nLives Matter meme and movement,‚Äù Current Directions in Psychological\\nScience , vol. 26, no. 6, pp. 543‚Äì547, 2017.\\n[3] S. Malodia, A. Dhir, A. Bilgihan, P. Sinha, and T. Tikoo, ‚ÄúMeme\\nmarketing: How can marketers drive better engagement using viral\\nmemes?‚Äù Psychology & Marketing , vol. 39, no. 9, pp. 1775‚Äì1801, 2022.\\n[4] L. Shifman, ‚ÄúMemes in a digital world: Reconciling with a conceptual\\ntroublemaker,‚Äù Journal of Computer-Mediated Communication , vol. 18,\\nno. 3, pp. 362‚Äì377, 2013.\\n[5] K. Barnes, T. Riesenmy, M. D. Trinh, E. Lleshi, N. Balogh, and\\nR. Molontay, ‚ÄúDank or not? Analyzing and predicting the popularity\\nof memes on Reddit,‚Äù Applied Network Science , vol. 6, no. 1, pp. 1‚Äì24,\\n2021.\\n[6] C. Ling, I. AbuHilal, J. Blackburn, E. De Cristofaro, S. Zannettou, and\\nG. Stringhini, ‚ÄúDissecting the meme magic: Understanding indicators of\\nvirality in image memes,‚Äù Proceedings of the ACM on Human-Computer\\nInteraction , vol. 5, no. CSCW1, pp. 1‚Äì24, 2021.\\n[7] O. Tsur and A. Rappoport, ‚ÄúDon‚Äôt let me be# misunderstood: Lin-\\nguistically motivated algorithm for predicting the popularity of textual\\nmemes,‚Äù in Proceedings of the International AAAI Conference on Web\\nand Social Media , vol. 9, no. 1, 2015, pp. 426‚Äì435.\\n[8] S. Zannettou, T. Caulfield, J. Blackburn, E. De Cristofaro, M. Sirivianos,\\nG. Stringhini, and G. Suarez-Tangil, ‚ÄúOn the origins of memes by means\\nof fringe web communities,‚Äù in Proceedings of the Internet Measurement\\nConference 2018 , 2018, pp. 188‚Äì202.\\n[9] A. Dubey, E. Moro, M. Cebrian, and I. Rahwan, ‚ÄúMemesequencer:\\nSparse matching for embedding image macros,‚Äù in Proceedings of the\\n2018 World Wide Web Conference , 2018, pp. 1225‚Äì1235.\\n[10] C. Courtois and T. Frissen, ‚ÄúComputer vision and internet meme\\ngenealogy: An evaluation of image feature matching as a technique\\nfor pattern detection,‚Äù Communication Methods and Measures , vol. 17,\\nno. 1, pp. 17‚Äì39, 2023.\\n[11] W. Theisen, D. G. Cedre, Z. Carmichael, D. Moreira, T. Weninger,\\nand W. Scheirer, ‚ÄúMotif mining: Finding and summarizing remixed\\nimage content,‚Äù in Proceedings of the IEEE/CVF Winter Conference\\non Applications of Computer Vision , 2023, pp. 1319‚Äì1328.\\n[12] R. Tommasini, F. Ilievski, and T. Wijesiriwardene, ‚ÄúImkg: The inter-\\nnet meme knowledge graph,‚Äù in European Semantic Web Conference .\\nSpringer, 2023, pp. 354‚Äì371.\\n[13] S. Joshi, F. Ilievski, and L. Luceri, ‚ÄúContextualizing internet memes\\nacross social media platforms,‚Äù in Companion Proceedings of the ACM\\non Web Conference 2024 , 2024, pp. 1831‚Äì1840.\\n[14] L. Richardson, ‚ÄúBeautiful soup documentation,‚Äù April , 2007.\\n1https://github.com/hsdslab/meme-research[15] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\\nG. Sastry, A. Askell, P. Mishkin, J. Clark et al. , ‚ÄúLearning transferable\\nvisual models from natural language supervision,‚Äù in International\\nConference on Machine Learning . PMLR, 2021, pp. 8748‚Äì8763.\\n[16] S. B and R. M, ‚ÄúWe‚Äôve reddit, have you?: what librarians can learn from\\na site full of memes,‚Äù Coll Res Libr News , vol. 10, no. 74, pp. 518‚Äì521,\\n2013.\\n[17] J. Baumgartner, S. Zannettou, B. Keegan, M. Squire, and J. Blackburn,\\n‚ÄúThe pushshift Reddit dataset,‚Äù in Proceedings of the International AAAI\\nConference on Web and Social Media , vol. 14, 2020, pp. 830‚Äì839.\\n[18] B. Boe, ‚ÄúPRAW: The Python Reddit API wrapper,‚Äù\\nhttps://github.com/praw-dev/praw, 2016, accessed: 2022-12-15.\\n[19] M. Podolak, ‚ÄúPmaw: Pushshift multithread API wrapper,‚Äù\\nhttps://github.com/mattpodolak/pmaw, 2021, accessed: 2022-12-15.\\n[20] B. Muthukadan, ‚ÄúSelenium python bindings,‚Äù 2016.\\n[21] S. J. Dixon, ‚ÄúFacebook q2 earnings report (2023),‚Äù\\nhttps://www.statista.com/statistics/264810/number-of-monthly-active-\\nfacebook-users-worldwide/, 2024, accessed: 2024-02-29.\\n[22] V . Sherratt, K. Pimbblet, and N. Dethlefs, ‚ÄúMulti-channel convolutional\\nneural network for precise meme classification,‚Äù in Proceedings of the\\n2023 ACM International Conference on Multimedia Retrieval , 2023, pp.\\n190‚Äì198.\\n[23] B. Deka, Z. Huang, C. Franzen, J. Hibschman, D. Afergan, Y . Li,\\nJ. Nichols, and R. Kumar, ‚ÄúRico: A mobile app dataset for building\\ndata-driven design applications,‚Äù in Proceedings of the 30th Annual ACM\\nSymposium on User Interface Software and Technology , 2017, pp. 845‚Äì\\n854.\\n[24] Z. Hussain, M. Zhang, X. Zhang, K. Ye, C. Thomas, Z. Agha, N. Ong,\\nand A. Kovashka, ‚ÄúAutomatic understanding of image and video adver-\\ntisements,‚Äù in Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition , 2017, pp. 1705‚Äì1715.\\n[25] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo, J. Hocken-\\nmaier, and S. Lazebnik, ‚ÄúFlickr30k entities: Collecting region-to-phrase\\ncorrespondences for richer image-to-sentence models,‚Äù in Proceedings\\nof the IEEE International Conference on Computer Vision , 2015, pp.\\n2641‚Äì2649.\\n[26] G. Bradski, ‚ÄúThe OpenCV Library,‚Äù Dr. Dobb‚Äôs Journal of Software\\nTools , 2000.\\n[27] M. Pietik ¬®ainen, ‚ÄúLocal binary patterns,‚Äù Scholarpedia , vol. 5, no. 3, p.\\n9775, 2010.\\n[28] M. Alkhowaiter, K. Almubarak, and C. Zou, ‚ÄúEvaluating perceptual\\nhashing algorithms in detecting image manipulation over social media\\nplatforms,‚Äù in 2022 IEEE International Conference on Cyber Security\\nand Resilience (CSR) . IEEE, 2022, pp. 149‚Äì156.\\n[29] X.-m. Niu and Y .-h. Jiao, ‚ÄúAn overview of perceptual hashing,‚Äù Acta\\nElectronica Sinica , vol. 36, no. 7, p. 1405, 2008.\\n[30] D. S. Evan Klinger, ‚Äúphash, the open source perceptual hash library,‚Äù\\nhttps://www.phash.org, 2008, accessed: 2024-04-25.\\n[31] E. Rublee, V . Rabaud, K. Konolige, and G. Bradski, ‚ÄúOrb: An efficient\\nalternative to sift or surf,‚Äù in 2011 International Conference on Computer\\nVision . Ieee, 2011, pp. 2564‚Äì2571.\\n[32] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for\\nimage recognition,‚Äù 2016 IEEE Conference on Computer Vision and\\nPattern Recognition (CVPR) , pp. 770‚Äì778, 2015. [Online]. Available:\\nhttps://api.semanticscholar.org/CorpusID:206594692\\n[33] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, ‚ÄúDensely\\nconnected convolutional networks,‚Äù in Proceedings of the IEEE Confer-\\nence on Computer Vision and Pattern Recognition , 2017, pp. 4700‚Äì4708.\\n[34] M. Tan and Q. Le, ‚ÄúEfficientnetv2: Smaller models and faster training,‚Äù\\ninInternational conference on machine learning . PMLR, 2021, pp.\\n10 096‚Äì10 106.\\n[35] J. Yang, K. Zhou, Y . Li, and Z. Liu, ‚ÄúGeneralized out-of-distribution\\ndetection: A survey,‚Äù arXiv preprint arXiv:2110.11334 , 2021.\\n[36] D. A. Suju and H. Jose, ‚ÄúFlann: Fast approximate nearest neighbour\\nsearch algorithm for elucidating human-wildlife conflicts in forest ar-\\neas,‚Äù in 2017 Fourth International Conference on Signal Processing,\\nCommunication and Networking (ICSCN) . IEEE, 2017, pp. 1‚Äì6.\\n[37] J. Wright, A. Y . Yang, A. Ganesh, S. S. Sastry, and Y . Ma, ‚ÄúRobust face\\nrecognition via sparse representation,‚Äù IEEE Transactions on Pattern\\nAnalysis and Machine Intelligence , vol. 31, no. 2, pp. 210‚Äì227, 2008.\\n[38] M. Grootendorst, ‚ÄúBertopic: Neural topic modeling with a class-based\\ntf-idf procedure,‚Äù arXiv preprint arXiv:2203.05794 , 2022.\\n[39] L. McInnes, J. Healy, and J. Melville, ‚ÄúUmap: Uniform manifold\\napproximation and projection for dimension reduction,‚Äù 2020.[40] L. McInnes, J. Healy, and S. Astels, ‚Äúhdbscan: Hierarchical density\\nbased clustering.‚Äù Journal of Open Source Software , vol. 2, no. 11, p.\\n205, 2017.\\n[41] B. Matthews, ‚ÄúComparison of the predicted and observed secondary\\nstructure of t4 phage lysozyme,‚Äù Biochimica et Biophysica Acta\\n(BBA) - Protein Structure , vol. 405, no. 2, pp. 442‚Äì451, 1975.\\n[Online]. Available: https://www.sciencedirect.com/science/article/pii/\\n0005279575901099\\n[42] J. Cohen, ‚ÄúA coefficient of agreement for nominal scales,‚Äù Educational\\nand Psychological Measurement , vol. 20, no. 1, pp. 37‚Äì46, 1960.\\n[43] Wikipedia, ‚ÄúFleiss‚Äô kappa ‚Äî Wikipedia, the free encyclopedia,‚Äù\\nhttp://en.wikipedia.org/w/index.php?title=Fleiss‚Äô%20kappa&oldid=\\n1219080464, 2024, [Online; accessed 16-July-2024].\\n[44] M. Mascarenhas, D. A. Friedman, and R. J. Cordes, ‚ÄúBridging gaps\\nin image meme research: A multidisciplinary paradigm for scaling up\\nqualitative analyses,‚Äù Journal of the Association for Information Science\\nand Technology , 2024.\\n[45] K. Barnes, P. Juh ¬¥asz, M. Nagy, and R. Molontay, ‚ÄúTopicality boosts\\npopularity: a comparative analysis of NYT articles and Reddit memes,‚Äù\\nSocial Network Analysis and Mining , vol. 14, no. 1, p. 119, 2024.',\n",
       " 'PQV-Mobile: A Combined Pruning and Quantization Toolkit to\\nOptimize Vision Transformers for Mobile Applications\\nKshitij Bhardwaj1\\nAbstract\\nWhile Vision Transformers (ViTs) are extremely\\neffective at computer vision tasks and are replac-\\ning convolutional neural networks as the new\\nstate-of-the-art, they are complex and memory-\\nintensive models. In order to effectively run these\\nmodels on resource-constrained mobile/edge sys-\\ntems, there is a need to not only compress these\\nmodels but also to optimize them and convert\\nthem into deployment-friendly formats. To this\\nend, this paper presents a combined pruning and\\nquantization tool, called PQV-Mobile, to optimize\\nvision transformers for mobile applications. The\\ntool is able to support different types of struc-\\ntured pruning based on magnitude importance,\\nTaylor importance, and Hessian importance. It\\nalso supports quantization from FP32 to FP16 and\\nint8, targeting different mobile hardware back-\\nends. We demonstrate the capabilities of our tool\\nand show important latency-memory-accuracy\\ntrade-offs for different amounts of pruning and\\nint8 quantization with Facebook Data Efficient\\nImage Transformer (DeiT) models. Our results\\nshow that even pruning a DeiT model by 9.375%\\nand quantizing it to int8 from FP32 followed by\\noptimizing for mobile applications, we find a la-\\ntency reduction by 7.18√ówith a small accuracy\\nloss of 2.24%. The tool is open source.1\\n1. Introduction\\nVision Transformers (ViTs) (Kolesnikov et al.) have recently\\nemerged as a competitive alternative to Convolutional Neu-\\nral Networks (CNNs) that are currently state-of-the-art in\\ndifferent image recognition computer vision tasks. ViTs\\nhave shown to outperform the CNNs by almost 4√óin terms\\n1Lawrence Livermore National Lab. Correspondence to: Kshi-\\ntij Bhardwaj <Bhardwaj2@llnl.gov >.\\nAccepted in International Conference on Machine Learning, ES-\\nFOMO Workshop , Honolulu, Hawaii, USA. 2024. Copyright 2024\\nby the author(s).\\n1https://github.com/kshitij11/PQV-Mobileof computational efficiency and accuracy (VISO). Addition-\\nally, ViTs have been shown to be more robust than CNNs\\nand can be easily trained on smaller datasets.\\nWhile ViTs are extremely effective at computer vision tasks,\\nthey are complex and memory-intensive models. For exam-\\nple, Facebook‚Äôs Data Efficient Image Transformers (DeiT)\\ntake 331MB memory and are therefore not suitable for\\nresource-constrained edge systems such as for mobile ap-\\nplications. Previous research has focused on pruning (Fang\\net al., 2023) and quantizing (Li & Gu, 2023) ViTs, but\\nmostly separately and they do not target mobile applications\\nwhere deployment of such models is challenging and re-\\nquires converting these models to mobile hardware friendly\\nlightweight and optimized formats.\\nThis paper presents a combined pruning and quantization\\ntool, called PQV-Mobile, to optimize vision transformers\\nfor mobile applications. The tool is able to support dif-\\nferent types of structured pruning based on magnitude im-\\nportance, Taylor importance, and Hessian importance. It\\nalso supports quantization from FP32 to FP16 and int8,\\ntailored towards several hardware backends, such as x86,\\nFBGEMM (Facebook General Matrix Multiplication (Face-\\nbook FBGEMM)), QNNPACK (Quantized Neural Network\\nPackage (QNNPACK)), and ONEDNN (Intel‚Äôs ONEDNN).\\nThe pruned and quantized models are optimized for mobile\\napplications and converted to mobile-friendly lightweight\\nformats. We demonstrate the capabilities of our tool and\\nshow important latency-memory-accuracy trade-offs for dif-\\nferent amounts of pruning, int8 quantization, and hardware\\nbackends with two types of Facebook DeiT models.\\nOur results show that even pruning a DeiT model by 9.375%\\nand quantizing it to int8 from FP32, we find a latency reduc-\\ntion by 7.18√ówith a small accuracy loss of 2.24%. All of\\nour compared models are optimized for mobile applications\\nand converted into deployment friendly lightweight formats.\\n2. PQV-Mobile Tool\\nFigure 1 shows our PQV-Mobile Tool flow. It supports\\ndifferent kinds of post-training pruning strategies such as\\nL1, Taylor, etc. We found that the pruned model shows a\\nmajor accuracy drop and therefore needs to be finetuned.\\n1arXiv:2408.08437v1  [cs.CV]  15 Aug 2024PQV-Mobile: A Combined Pruning and Quantization Toolkit to Optimize Vision Transformers for Mobile Applications\\nFigure 1. PQV-Mobile tool flow\\nThe pruned and finetuned model is then input to a quanti-\\nzation engine, which can be tailored towards various hard-\\nware backends, and optimized for mobile applications. Our\\nresults showed that there is a small accuracy drop after\\nquantization and therefore we did not perform any further\\nfinetuning of the quantized model. Currently, our tool can\\nhandle any of the HuggingFace ViTs from the TIMM library\\n(Pytorch Image Models) (TIMM). This section describes\\nthese steps in more details.\\n2.1. Pruning method\\nPQV-Mobile supports several pruning strategies, corre-\\nsponding to structured pruning. In structured pruning,\\na block is removed, which can be a neuron in a fully-\\nconnected layer, a channel of filter in a convolutional layer,\\nor a self-attention head in a Transformer. An alternative\\napproach is unstructured pruning (also called magnitude\\npruning) where some of the parameters or weights with\\nsmaller values are converted to zeroes. PQV-Mobile targets\\nstructured pruning methods as they do not rely on specific\\nAI accelerators or software to reduce memory consumption\\nand computational costs, thereby finding a wider domain of\\napplications in practice (Fang et al., 2023).\\nIn structural pruning, a ‚ÄòGroup‚Äô is defined as the minimal\\nunit that can be removed. Many of these groups consistof multiple layers which can be interdependent and need\\nto be pruned together so as to maintain the integrity of the\\nresulting pruned networks. We follow the approach of (Fang\\net al., 2023) that uses a dependency graph to model these\\ndependencies and find the right groupings for parameter\\npruning. Similar to (Fang et al., 2023), PQV-Mobile accepts\\na group (i.e., an Attention block of a ViT with Linear layers)\\nas inputs, and returns a 1-D tensor with the same length as\\nthe number of channels. All groups must be pruned simulta-\\nneously and thus their importance should be accumulated\\nacross channel groups. PQV-Mobile supports the following\\ngroupings:\\nMagnitude importance based grouping. In this case, L1-\\nor L2-norm regularization term is applied to the loss func-\\ntion which penalizes non-zero parameters. If the value\\nof a connection is less than a threshold, the connection is\\ndropped (Anwar et al., 2017).\\nTaylor importance based grouping. The importance is\\ncalculated as the squared change in loss induced by remov-\\ning a specific filter from the network. This importance is\\napproximated with a Taylor expansion which allows for\\nfaster computation from parameter gradients, even for larger\\nnetworks (Molchanov et al., 2019).\\nHessian importance based grouping. In this method the\\nimportance is computed using a fast second-order metric\\nto find insensitive parameters in a model. In particular, the\\naverage Hessian trace is used to weight the magnitude of the\\nparameters; parameters with large second-order sensitivity\\nremain unpruned, and those with relatively small sensitivity\\nare pruned (Yu et al., 2022).\\n2.2. Quantization method\\nPQV-Mobile currently supports post-training quantization\\nof both weights and activations from FP32 to FP16 and int8.\\nWe plan to extend this to int4 as future work. The follow-\\ning steps are performed for quantization using quantization\\nlibraries of Pytorch:\\n‚Ä¢Quantize models for a specific backend: We first cre-\\nate a quantization engine based on a backend. The\\nsupported backends are: x86, FBGEMM (Facebook\\nGeneral Matrix Multiplication (Facebook FBGEMM)),\\nQNNPACK (Quantized Neural Network Package (QN-\\nNPACK)), and ONEDNN (Intel‚Äôs ONEDNN). The gen-\\nerated engine is then used to quantize the model using\\neither static or dynamic quantization (Pytorch Quanti-\\nzation).\\n‚Ä¢Convert Pytorch models to Torchscript format: Python\\nmodels are inefficient to run during deployment. There-\\nfore, we export the Pytorch models to production en-\\nvironments through Torchscript (Pytorch Torchscript),\\n2PQV-Mobile: A Combined Pruning and Quantization Toolkit to Optimize Vision Transformers for Mobile Applications\\nFigure 2. Latency and memory results for deit base patch16 model\\nwith varying pruning and quantization. All models are scripted,\\nmobile optimized, and converted to Pytorch Lite format.\\nwhich is an easy way to create serializable and optimiz-\\nable models.\\n‚Ä¢Use Pytorch‚Äôs mobile optimizer to optimize the quan-\\ntized and scripted model (Pytorch Mobile Optimizer)\\nfor mobile applications.\\n‚Ä¢Use Pytorch‚Äôs Lite Interpreter to create a deployable\\nand light version of the mobile optimized model (Py-\\ntorch Lite Interpreter).\\n3. Experimental Results\\nIn this section, we demonstrate the effectiveness of PQV-\\nMobile to prune and quantize Facebook‚Äôs Data Efficient Im-\\nage Transformers (DeiT). We evaluate the models‚Äô latency-\\nmemory-accuracy trade-offs. The structured pruning im-\\nportance used in this study is based on Taylor‚Äôs expansion\\n(as shown later, it performs the best in terms of accuracy).\\nThe pruned model is then finetuned for 60 epochs (using\\ndistributed training on 4 GPUs) at a learning rate of 0.00015\\nwith a batch size of 64. The finetuned pruned models are\\nthen quantized for the x86 backend engine and then con-\\nverted to optimized and Lite format. We use the ImageNet\\ndataset (IMAGENET) for image classification tasks and run\\nthe models on Intel Xeon ES2695 at 2.3 GHz. Pytorch-2.0.0\\nis used in all our experiments. Please note that our experi-\\nments are only meant to demonstrate the capabilities of our\\ntool and not to achieve the state-of-the-art accuracy. All\\ncomparisons are performed on scripted, mobile optimized,\\nand Pytorch Lite format models.\\nFigure 2 shows latency/image and memory for\\ndeitbase patch16 model after varying degrees of\\npruning and quantizing the model from FP32 to int8.\\nQuantizing the original dense model to int8 leads to 6.47√ó\\nlower latency as the Pytorch Lite interpreter is more\\neffective with quantized models than FP32 models. Pruning\\nthe quantized model by 9.375% leads to further 9.8%\\nlower latency (an overall 7.14√ólatency reduction over the\\noriginal dense/FP32 Lite model). Increasing the amount\\nof pruning to 25% and 50% shows further improvements\\nin latency by 27.9% and 45.4%, respectively. The tool canalso be used to perform a detailed profiling of the latency of\\nthe mobile optimized and Lite model as shown in Figure 3,\\nwhere we can identify the bottleneck based on the time\\nspent on the various operations (as depicted by the Name of\\nthe process). In terms of accuracy, we found it to be similar\\nacross the different backends.\\nWhile pruning the model to 50% leads to significant im-\\nprovements in latency and memory, Figure 4 shows that the\\naccuracy degradation is considerable. Pruning the original\\nmodel by 9.375% shows 1.25% lower accuracy. Further\\nquantizing this model leads to an additional 0.99% loss in\\naccuracy. While 25% pruned int8 model achieved a speedup\\nof 27.9% over 9.375% pruned int8 model, its accuracy loss\\nis 3.41%. These results demonstrate the importance of per-\\nforming latency-memory-accuracy trade-offs which can be\\nseamlessly performed using our PQV-Mobile tool.\\nFigure 5 shows how the different types of structured pruning\\ngroupings affect accuracy and motivates why we chose Tay-\\nlor pruning for all of our experiments. We prune the dense\\nmodel by 9.375%, finetune it, and also quantize the fine-\\ntuned pruned model to int8 for this experiment. Although\\nthere is a very small change in accuracy between Taylor,\\nL1-norm, and Hessian-based pruning, Taylor outperforms\\nthe other methods.\\nWe further compare the latency and accuracy of\\npruning and quantizing deit base patch16 model with\\ndeit3 medium patch16 model (Figure 6). The original\\ndense FP32 accuracies for the two models are 80.79% and\\n82.19%, respectively. The latter is a smaller model with\\n38.85M parameters compared to 86.56M parameters in the\\nformer. As evident, the models show similar accuracy when\\npruned to the same levels (9.375% or 25%) at int8 quantiza-\\ntion. However, deit3 medium patch16 model shows latency\\nimprovements by 18.65% and 13.55%, respectively over\\ndeitbase patch16 model.\\nFinally, as shown in Figure 7, we also evaluate the latency\\nfor different int8 quantization hardware backends. We use\\nthe 9.375% pruned deit3 medium patch16 model for this\\nexperiment. We find that x86 and FBGEMM backends to be\\nthe best with FBGEMM slightly outperforming x86. These\\nresults are expected as we are running on an x86 machine\\nwith Advanced Vector Extensions (A VX) enabled, which\\nare used for fast path executions for both x86 and FBGEMM\\nbackends.\\n4. Conclusion and Future Work\\nThis paper presents a combined pruning and quantization\\ntool, called PQV-Mobile, to optimize vision transformers\\nfor mobile applications. The tool is able to support different\\ntypes of structured pruning based on magnitude importance,\\nTaylor importance, and Hessian importance. It also supports\\n3PQV-Mobile: A Combined Pruning and Quantization Toolkit to Optimize Vision Transformers for Mobile Applications\\nFigure 3. Latency profile for deit3 medium patch16 int8 model at 9.375% pruning level for x86 backend. The model is scripted, mobile\\noptimized, and converted to Pytorch Lite format.\\nFigure 4. Accuracy results for deit base patch16 model with vary-\\ning pruning and quantization. All models are scripted, mobile\\noptimized, and converted to Pytorch Lite format.\\nFigure 5. Accuracy results for deit base patch16 model with differ-\\nent structured pruning groupings (for both FP32 and int8 models).\\nAll models are scripted, mobile optimized, and converted to Py-\\ntorch Lite format.\\nquantization from FP32 to FP16 and int8, targeting different\\nmobile hardware backends. We demonstrate the capabilities\\nof our tool and show important latency-memory-accuracy\\ntrade-offs for different amounts of pruning and int8 quanti-\\nzation with two types of Facebook DeiT models.\\nAs future work, we plan to extend PQV-Mobile to int4\\nquantization. Additionally, we will extend this tool to target\\nlarge language models as well.\\n5. Acknowledgements\\nThis work was performed under the auspices of the U.S.\\nDepartment of Energy by LLNL under contract DE-AC52-\\nFigure 6. Latency and accuracy results for deit base patch16\\nmodel vs. deit3 medium patch16 model with varying pruning\\nlevels. All models are scripted, mobile optimized, and converted\\nto Pytorch Lite format.\\nFigure 7. Latency results for deit3 medium patch16 int8 model\\nat 9.375% pruning level for different hardware backends. All\\nmodels are scripted, mobile optimized, and converted to Pytorch\\nLite format.\\n07NA27344 (LLNL-CONF-865054).\\nReferences\\nAnwar, S., Hwang, K., and Sung, W. Structured pruning\\nof deep convolutional neural networks. ACM Journal on\\nEmerging Technologies in Computing Systems (JETC) ,\\n13(3):1‚Äì18, 2017.\\n4PQV-Mobile: A Combined Pruning and Quantization Toolkit to Optimize Vision Transformers for Mobile Applications\\nFacebook FBGEMM. FBGEMM and FBGEMM GPU Doc-\\numentation Homepage. https://pytorch.org/\\nFBGEMM/ .\\nFang, G., Ma, X., Song, M., Mi, M. B., and Wang, X. Dep-\\ngraph: Towards any structural pruning. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition , pp. 16091‚Äì16101, 2023.\\nIMAGENET. IMAGENET. https://www.\\nimage-net.org/ .\\nIntel‚Äôs ONEDNN. oneAPI Deep Neural Network Library.\\nhttps://github.com/oneapi-src/oneDNN .\\nKolesnikov, A., Dosovitskiy, A., Weissenborn, D., Heigold,\\nG., Uszkoreit, J., Beyer, L., Minderer, M., Dehghani, M.,\\nHoulsby, N., Gelly, S., et al. An image is worth 16 √ó16\\nwords: Transformers for image recognition at scale. arxiv\\n2021. arXiv preprint arXiv:2010.11929 .\\nLi, Z. and Gu, Q. I-vit: integer-only quantization for effi-\\ncient vision transformer inference. In Proceedings of the\\nIEEE/CVF International Conference on Computer Vision ,\\npp. 17065‚Äì17075, 2023.\\nMolchanov, P., Mallya, A., Tyree, S., Frosio, I., and Kautz,\\nJ. Importance estimation for neural network pruning. In\\nProceedings of the IEEE/CVF conference on computer\\nvision and pattern recognition , pp. 11264‚Äì11272, 2019.\\nPytorch Lite Interpreter. Introduce lite inter-\\npreter workflow in Android and iOS. https:\\n//pytorch.org/tutorials/prototype/\\nlite_interpreter.html .\\nPytorch Mobile Optimizer. torch.utils.mobile optimizer.\\nhttps://pytorch.org/docs/stable/\\nmobile_optimizer.html .\\nPytorch Quantization. Quantization. https://\\npytorch.org/docs/stable/quantization.\\nhtml .\\nPytorch Torchscript. Torchscript. https://pytorch.\\norg/docs/stable/jit.html .\\nQNNPACK. QNNPACK. https://github.com/\\npytorch/QNNPACK .\\nTIMM. Pytorch Image Models. https://github.\\ncom/huggingface/pytorch-image-models .\\nVISO. Vision Transformers (ViT) in\\nImage Recognition ‚Äì 2024 Guide.\\nhttps://viso.ai/deep-learning/\\nvision-transformer-vit/ .Yu, S., Yao, Z., Gholami, A., Dong, Z., Kim, S., Mahoney,\\nM. W., and Keutzer, K. Hessian-aware pruning and op-\\ntimal neural implant. In Proceedings of the IEEE/CVF\\nWinter Conference on Applications of Computer Vision ,\\npp. 3880‚Äì3891, 2022.\\n5',\n",
       " 'BNSENTMIX: A Diverse Bengali-English Code-Mixed Dataset\\nfor Sentiment Analysis\\nSadia Alam, Md Farhan Ishmam, Navid Hasin Alvee,\\nMd Shahnewaz Siddique, Abu Raihan Mostofa Kamal, Md Azam Hossain\\nDepartment of Computer Science and Engineering, Islamic University of Technology\\n{sadiaalam,farhanishmam,navidhasin,shahnewaz,raihan.kamal,azam}@iut-dhaka.edu\\nAbstract\\nThe widespread availability of code-mixed\\ndata in digital spaces can provide valuable in-\\nsights into low-resource languages like Bengali,\\nwhich have limited annotated corpora. Sen-\\ntiment analysis, a pivotal text classification\\ntask, has been explored across multiple lan-\\nguages, yet code-mixed Bengali remains under-\\nrepresented with no large-scale, diverse bench-\\nmark. Code-mixed text is particularly challeng-\\ning as it requires the understanding of multi-\\nple languages and their interaction in the same\\ntext. We address this limitation by introduc-\\ningBNSENTMIX, a sentiment analysis dataset\\non code-mixed Bengali comprising 20,000 sam-\\nples with 4sentiment labels, sourced from Face-\\nbook, YouTube, and e-commerce sites. By ag-\\ngregating multiple sources, we ensure linguistic\\ndiversity reflecting realistic code-mixed scenar-\\nios. We implement a novel automated text fil-\\ntering pipeline using fine-tuned language mod-\\nels to detect code-mixed samples and expand\\ncode-mixed text corpora. We further propose\\nbaselines using machine learning, neural net-\\nworks, and transformer-based language models.\\nThe availability of a diverse dataset is a critical\\nstep towards democratizing NLP and ultimately\\ncontributing to a better understanding of code-\\nmixed languages.\\n1 Introduction\\nIn the rapidly evolving digital landscape, code-\\nmixing has become increasingly prevalent, par-\\nticularly in multilingual societies. Code-mixing\\nis the phenomenon of alternating between two or\\nmore languages within a single conversation or\\nsentence (Thara and Poornachandran, 2018). Code-\\nmixing can occur in various forms, including intra-\\nsentential switching, where words from different\\nlanguages appear within the same sentence, and\\nintra-word switching, where elements from other\\nlanguages combine to form a single word (Ste-\\nfanich et al., 2019; Litcofsky and Van Hell, 2017).\\n Bengali -English : Movie tar \\n first half  bhalo laage nai.\\n Translation: I did not like  \\n the movie \\'s first half .\\nMixed NeutralNegative Positive\\n Bengali -English : Street food \\n amar onek bhalo lage.\\n Translation: I really love \\n street food . \\n Bengali -English : Bahirer  \\n weather  ektu rainy .\\n Translation: The weather  \\n outside is a bit rainy . Bengali -English : Video tar  \\n content  bhalo, gaan kharap.\\n Translation: The video has \\n good content but bad music .Figure 1: Examples of the four sentiment labels from\\nour code-mixed Bengali-English dataset BNSENTMIX\\nand the corresponding English translations. Red rep-\\nresents English words, blue represents Bengali words\\nwritten in English alphabets, and cyan represents im-\\nplicit words in the code-mixed text.\\nIntra-sentential switching is more frequently ob-\\nserved in colloquial settings. One significant yet\\nunderstudied domain of code-switching is Bengali-\\nEnglish code-mixed text.\\nWe consider Fig. 1 where the sentences are ex-\\namples of Bengali-English intra-sentential switch-\\ning. Intra-word switching is observed in the nega-\\ntive sentiment example. Here, Movietar is consid-\\nered a single word, whereas the Bengali sub-word\\ntar indicates possession. We also observe several\\nwords in the transliterated text that are not explic-\\nitly written in the code-mixed text. These implicitly\\ndefined words increase the challenges in processing\\ncode-mixed Bengali-English texts.\\nWith over 250 million native speakers globally,\\nBengali is the seventh most spoken language in the\\nworld but remains a low-resource language in terms\\nof research. While typing texts, Bengali speakers\\noften use Bengali-English code-mixed terms to ex-\\npress their thoughts in writing. Despite the preva-\\n1arXiv:2408.08964v3  [cs.CL]  10 Dec 2024Dataset #Samples #SL #DS Filtering #Baselines PA\\nHindi (Joshi et al., 2016) 3.9k 3 1 Manual 10 ‚úì\\nBengali (Mandal et al., 2018) 5k 3 1 Manual 5 ‚úó\\nTamil (Chakravarthi et al., 2020b) 15.7k 5 1 langdetect 10 ‚úì\\nMalayalam (Chakravarthi et al., 2020a) 6.7k 5 1 langdetect 10 ‚úì\\nPersian (Sabri et al., 2021) 3.6k 3 1 Keywords search 3 ‚úì\\nSwiss (Pustulka-Hunt et al., 2018) 963 3 1 Manual 7 ‚úó\\nBnSentMix (Ours) 20k 4 3 mBERT 14 ‚úì\\nTable 1: Comparison of the number of samples, #SL: Sentiment Labels, #DS: Data Sources, filtering method,\\nnumber of baselines, and PA: Public Availability of various code-mixed (with English) sentiment analysis datasets.\\nlence of code-mixed text on social media platforms,\\ne-commerce sites, and other digital spaces, there\\nremains a notable scarcity of resources to analyze\\nand process such data.\\nSentiment analysis, the computational study of\\npeople‚Äôs opinions, sentiments, emotions, and atti-\\ntudes expressed in written language, plays a crit-\\nical role in various applications, including social\\nmedia monitoring, customer feedback, market re-\\nsearch, and public opinion analysis (Wankhade\\net al., 2022). While substantial progress has been\\nmade in monolingual sentiment analysis (Medhat\\net al., 2014; Birjali et al., 2021), the complexi-\\nties introduced by code-mixed texts present unique\\nchallenges that current models struggle to address\\n(Barman et al., 2014). This is particularly true for\\nBengali-English code-mixed texts (Chanda et al.,\\n2016), which have not received adequate attention\\nin existing research.\\nTable 1 highlights the limitations of Bengali-\\nEnglish code-mixed sentiment analysis datasets\\ncompared to other Indic-English code-mixed\\ndatasets. The only available Bengali dataset (Man-\\ndal et al., 2018) is limited to 5ksamples, 3sen-\\ntiment labels, a single data source, 5baselines,\\nand is not publicly available. The existing lan-\\nguage detection tools also have severe limitations\\nin filtering code-mixed Bengali-English. Tools\\nlike langdetect1and Bengali phonetic parser2\\ndesigned for general language identification and\\ncode-mixed Bengali identification struggled with\\nthe spelling nuances of code-mixed text.\\nAddressing these challenges, our contribution\\ncan be summarized:\\n‚Ä¢We present, BNSENTMIX, a novel Bengali-\\nEnglish code-mixed dataset comprising\\n1https://pypi.org/project/langdetect/\\n2https://github.com/porimol/bnbphoneticparser20,000 samples and 4 sentiment labels for\\nsentiment analysis. Data has been curated\\nfrom YouTube, Facebook, and e-commerce\\nplatforms to encapsulate a broad spectrum of\\ncontexts and topics.\\n‚Ä¢Following the intricacies of code-mixed test,\\nvisualized in Fig. 1, we propose a novel au-\\ntomated code-mixed text detection pipeline\\nusing fine-tuned language models, reaching\\nan accuracy of 94.56%.\\n‚Ä¢We establish 11 baselines including classical\\nmachine learning, neural network, and pre-\\ntrained transformer-based models, with BERT\\nachieving accuracy and F1 score of 69.5% and\\n68.8% respectively.\\n2 Related Work\\n2.1 Code-Mixing\\nCode-mixed data can be the source of several text\\nclassification tasks (Thara and Poornachandran,\\n2018) with sentiment analysis (Mahadzir et al.,\\n2021) being one of the most popular ones. Other\\nnatural language processing tasks (NLP) on code-\\nmixed data include hate speech detection (Sreelak-\\nshmi et al., 2020), translation (Gautam et al., 2021),\\npart of speech tagging (Vyas et al., 2014), emotion\\nclassification (Ameer et al., 2022), language iden-\\ntification (Mandal and Singh, 2018), and speech\\nsynthesis (Sitaram and Black, 2016). Researchers\\nalso incorporate training data augmentation (Gupta\\net al., 2021; Rizvi et al., 2021) and code-mix word\\nembeddings (Pratapa et al., 2018) to process code-\\nmixed texts.\\n2.2 Sentiment Analysis\\nThe significance of sentiment analysis has grown\\nwith the rise of social media, prompting extensive\\n2Data\\nScrapingData\\nFilteringData\\nCleaning\\nData Pre-processing Data Annotation\\nAnnotators\\nData SourceBnSentMix\\nDatasetFigure 2: Dataset creation pipeline of the B NSENTMIXdataset.\\nresearch on monolingual corpora. Studies explored\\nvarious languages, including English (Hu and Liu,\\n2004; Wiebe et al., 2005; Jiang et al., 2019), Rus-\\nsian (Rogers et al., 2018), German (Cieliebak et al.,\\n2017), Norwegian (M√¶hlum et al., 2019), several\\nIndian languages (Agrawal and Awekar, 2018; Rani\\net al., 2020), and Bengali (Fahim, 2023; Kabir et al.,\\n2023). Multilingual sentiment analysis (Dashtipour\\net al., 2016; Pustulka-Hunt et al., 2018) gained pop-\\nularity with the recent advancements in multilin-\\ngual language models (Devlin et al., 2019; Conneau\\net al., 2020).\\n2.3 Code-Mixing in Bengali\\nBengali is often code-mixed with English (Chanda\\net al., 2016) and Hindi (Raihan et al., 2023). In\\nBengali-English code-mixing, English tokens are\\ncommonly used alongside romanized or translit-\\nerated Bengali (Shibli et al., 2023; Fahim et al.,\\n2024), which is often back-transliterated before\\nprocessing (Haider et al., 2024). Sentiment anal-\\nysis on code-mixed Bengali has limited studies,\\neither using small private datasets (Mandal et al.,\\n2018) or performed in a multilingual setting (Pa-\\ntra et al., 2018). Data augmentation techniques\\nhave also been explored to enhance code-mixed\\nsentiment analysis datasets in Bengali (Tareq et al.,\\n2023). Emotion detection, a task similar to senti-\\nment analysis, has also been studied in the context\\nof code-mixed Bengali (Raihan et al., 2024).\\n3 B NSENTMIXDataset\\nThe BNSENTMIXdata has been collected from\\nmultiple data sources to reflect realistic code-mixed\\ntexts commonly found in digital spaces. We labeled\\nthe data using four distinct sentiments: the com-\\nmonly used positive, negative, and neutral senti-\\nments, as well as a mixed sentiment. As illustrated\\nin Fig. 1, the mixed sentiment represents instances\\nwhere both positive and negative sentiments are\\nconveyed within different parts of the text. We de-\\ncided to include the mixed label because the associ-\\nated sentences are frequently observed in everydaytexts and cannot be correctly classified under the\\ntraditional sentiment labels.\\nE-Commerce \\n9.0%\\nYouTube\\n18.0%\\nFacebook\\n73.0%\\nFigure 3: Composition of data sources of the BNSENT-\\nMIXdataset.\\n3.1 Data Sourcing\\nWe collected extensive user-generated content from\\nYouTube comments, Facebook comments, and e-\\ncommerce site reviews. These data sources were\\nchosen for their high engagement rates and diverse\\nlinguistic input. YouTube comments were scraped\\nusing the YouTube API. We used Facepager3to\\nextract comments from public Facebook posts,\\npages, and groups. Selenium4was employed to\\nmimic human browsing behavior on e-commerce\\nsites to scrape product reviews. We amassed over\\n3 million samples of user-generated content, form-\\ning the foundation for our dataset and subsequent\\nanalysis. Fig. 3 illustrates the composition of the\\naforementioned data sources.\\n3.2 Data Cleaning\\nWe discard samples with four words or less and\\nsamples containing external URLs. Redundant\\nwhitespaces, special characters, and non-ASCII\\ncharacters including emojis and emoticons are also\\nremoved. Consequent sequences of punctuation\\nsymbols are reduced to single instances. The En-\\nglish words are downcased unless they appear at\\n3https://github.com/strohne/Facepager\\n4https://selenium-python.readthedocs.io/\\n3the beginning of the sentence. However, we did not\\ncorrect any form of typing or grammatical errors in\\nour dataset to ensure the trained model is robust for\\npractical scenarios. The data cleaning procedure\\nhas been formally described in Algo. 1.\\nAlgorithm 1 Clean Text\\nRequire: text‚ÜêInput text\\nEnsure: text‚ÜêPreprocessed text\\n1:text‚Üêtext.lower (){Convert to lowercase}\\n2:text‚ÜêRemove all special characters except\\n\"?\", \",\", \"!\", and \".\"\\n3:text‚ÜêReduce consecutive sequences of\\npunctuations to a single instance\\n4:text‚ÜêRemove all non-ASCII characters\\n5:text‚ÜêRemove extra white spaces\\n6:text‚ÜêCapitalize the first letter after each\\nperiod (.)\\n7:return text\\n3.3 Data Filtering\\nWe construct a novel Bengali-English code-mix de-\\ntection dataset and fine-tune pre-trained language\\nmodels to automatically filter code-mixed Bengali-\\nEnglish. Detecting these texts can pose significant\\nchallenges: (i) rule-based methods struggle with\\nintra-word switching (ii) romanized Bengali or En-\\nglish samples may be incorrectly classified as code-\\nmixed text by automated methods, and (iii) samples\\nfrom a third language often bypass the filtering pro-\\ncess. Our approach addresses these challenges by\\nincorporating pre-trained language models, which\\nexcel in intricate text detection settings. Algo. 2\\noutlines the data filtering pipeline.\\n3.3.1 Code-mix Detection Dataset\\nThe fine-tuning dataset for code-mixed Bengali-\\nEnglish detection comprises 3data sources. We in-\\ncorporate the Dakshina dataset (Roark et al., 2020)\\nwhich has a rich collection of Southeast Asian\\nlanguages, including many Bengali-English code-\\nmixed sentences. Secondly, we utilized a Kaggle\\nEnglish dataset5consisting of a wide range of En-\\nglish words and extended with a third source Man-\\ndal and Singh (2018). By integrating these diverse\\nsources, we curated a comprehensive dataset of\\n100kwords, ensuring a balanced mix of Bengali,\\nEnglish, and code-mixed Bengali-English words.\\nTo maintain the linguistic purity of code-mixed\\n5https://www.kaggle.com/datasets/rtatman/english-word-\\nfrequencyAlgorithm 2 Detect Code-mixed Bengali\\nRequire: S‚ÜêList of sentences\\nRequire: model ‚ÜêPre-trained mBERT model\\nRequire: tokenizer ‚ÜêPre-trained mBERT tok-\\nenizer\\nEnsure: pred‚ÜêPredicted class label (0 or 1)\\n1:b_count‚Üê0\\n2:w_count‚Üê0\\n3:foreachsent inSdo\\n4:words ‚Üêsplit(sent )\\n5: foreachwinwords do\\n6: w‚Üêpreprocess( w)\\n7: ifwis empty then\\n8: continue\\n9: end if\\n10: w_count‚Üêw_count + 1\\n11: inputs ‚Üêtokenize( w)\\n12: outputs ‚Üêmodel( inputs )\\n13: pred _class‚Üêargmax( outputs )\\n14: ifpred _class == 1 then\\n15: b_count‚Üêb_count + 1\\n16: end if\\n17: end for\\n18:end for\\n19:ifw_count < 4then\\n20: return 0\\n21:end if\\n22:b_percent ‚Üêb_count/w _count\\n23:ifb_percent ‚â•0.3then\\n24: return 1\\n25:else\\n26: return 0\\n27:end if\\nBengali-English, we exclude sentences containing\\nwords that are neither English nor Bengali, e.g.\\nHindi words.\\n3.3.2 Code-mix Detection Results\\nWe evaluate 3pre-trained models ‚Äì the multilingual\\nmodels, mBERT (Devlin et al., 2019) and XLM-\\nRoBERTa (Conneau et al., 2020), and the Bengali-\\nEnglish model BanglishBERT (Bhattacharjee et al.,\\n2022). Table 2 reveals mBERT showing substan-\\ntially higher accuracy and F1 score in code-mixed\\nBengali-English detection. We argue that the pre-\\ntrained multilingual capabilities of mBERT effec-\\ntively handled the nuances of code-mixed Bengali-\\nEnglish text.\\n4Model Acc(%) F1 Score\\nXLM-RoBERTa 89.60 0.8985\\nBanglishBERT 90.56 0.8961\\nmBERT 94.56 0.9403\\nTable 2: Comparison of the accuracy and F1 score of\\nthe code-mixed Bengali-English detection methods.\\nMixed\\n9.2%\\nNeutral\\n37.2%Positive\\n27.9%\\nNegative\\n25.8%\\nFigure 4: Distribution of sentiment labels in the\\nBNSENTMIXdataset.\\n3.4 Data Annotation\\nEach sample in our dataset has been annotated\\ntwice by two different annotators to ensure gen-\\neralized sentiment is conveyed. In cases where the\\ntwo independent annotations did not match, a third\\nannotator would break the tie. To perform data\\nannotation, we recruited 64annotators who had\\nbeen provided hourly monetary compensation. The\\ndata annotators have at least a high-school degree\\n(equivalent to Grade 12 education) and are familiar\\nwith social media and digital spaces. The annota-\\ntors were asked to re-label the same 250samples\\nto measure inter-annotator agreement. We mea-\\nsured the agreement score using Cohen‚Äôs Kappa\\nŒ∫= 0.86, indicating substantial agreement.\\n3.5 Dataset Statistics\\nFig. 4 visualizes the label composition of the anno-\\ntated dataset. An overview of the key statistics of\\nthe annotated dataset is shown in table-3. We split\\nthe dataset into [70 : 15 : 15] training, validation,\\nand test splits i.e. 14,000, 3,000, and 3,000 samples\\nrespectively.4 Methodology and Experimental Setup\\n4.1 Baseline Models\\nWe evaluate 11baselines encompassing traditional\\nmachine learning models, recurrent neural network\\nvariants, and transformer-based pre-trained lan-\\nguage models, observed in table 4. All the pre-\\ntrained models were fine-tuned on our dataset.\\n4.2 Evaluation Metrics\\nWe use classification accuracy and F1-score for\\nmodel evaluation ‚Äì both well-known metrics for\\ntext classification (Hossin and Sulaiman, 2015).\\nStatistic Value\\nMean Character Length 62.77\\nMax Character Length 1985\\nMin Character Length 14\\nMean Word Count 11.65\\nMax Word Count 368\\nMin Word Count 4\\nUnique Word Count 37734\\nUnique Sentence Count 20000\\nTable 3: Key statistics of the B NSENTMIXdataset.\\n4.3 Implementation Details\\nThe models were trained on NVIDIA Tesla P100\\nGPUs with 16GB of memory. We followed the\\nHuggingface implementation (Wolf et al., 2019) for\\nthe pre-trained language models. All the models\\nutilized Adam Optimizer (Kingma and Ba, 2014)\\nwith a training batch size of 32. The training config-\\nuration used most of the default hyperparameters.\\nLogistic Regression, RNN, and LSTM models used\\nthe learning rate of 1E‚àí5while the BERT-family\\nlanguage models used the learning rate of 1.5E‚àí6.\\nThe training time for each epoch varied from 8to\\n13minutes.\\n5 Results and Analysis\\n5.1 Performance Evaluation\\nTable 4 highlights the performance of the 11base-\\nlines with BERT achieving the best performance\\nin terms of both accuracy and F1 score. We now\\nanalyze the category-wise model performance.\\n5.1.1 Machine Learning (ML) Models\\nThe ML models provide simple baselines and\\nachieve considerably high accuracy, with the Sup-\\n5ModelValidation Test\\nAcc Precision Recall F1 Acc Precision Recall F1\\nMachine Learning Models\\nLogistic Regression 0.668 0.656 0.668 0.662 0.667 0.614 0.667 0.639\\nRandom Forest 0.672 0.661 0.672 0.666 0.648 0.635 0.648 0.641\\nSVM 0.694 0.676 0.694 0.685 0.660 0.637 0.660 0.648\\nRecurrent Neural Network Variants\\nRNN 0.406 0.308 0.406 0.350 0.401 0.352 0.401 0.375\\nLSTM 0.678 0.670 0.678 0.674 0.670 0.657 0.670 0.663\\nMultilingual Language Models\\nXLM-RoBERTa 0.726 0.709 0.726 0.717 0.698 0.642 0.698 0.669\\nmBERT 0.726 0.713 0.726 0.719 0.694 0.675 0.694 0.684\\nBangla Language Models\\nBanglaBERT 0.721 0.668 0.721 0.693 0.698 0.642 0.698 0.669\\nBanglishBERT 0.694 0.715 0.694 0.704 0.686 0.653 0.686 0.669\\nEnglish Language Models\\nDistilBERT 0.701 0.694 0.701 0.697 0.672 0.665 0.672 0.668\\nBERT 0.727 0.710 0.724 0.717 0.695 0.683 0.694 0.688\\nTable 4: Performance of the proposed baselines based on accuracy, precision, recall, and F1 score.\\nport Vector Machine (SVM) (Vapnik, 1995) achiev-\\ning accuracy and F1 score on par with larger\\ntransformer-based models like BanglishBERT. The\\nother two ML baselines Logistic Regression (Cox,\\n1958) and Random Forest (Breiman, 2001) achieve\\nsatisfactory performance with relatively simpler ar-\\nchitectures. These ML baselines can be effective\\nin resource-constrained scenarios.\\n5.1.2 Recurrent Neural Networks (RNNs)\\nRNN (Hopfield, 1982) underperformed compared\\nto the other baselines. On the contrary, the per-\\nformance of Long Short-Term Memory (LSTM)\\nmodels (Hochreiter and Schmidhuber, 1997) was\\nsignificantly higher in terms of both accuracy and\\nF1 score. We argue that the long-term textual de-\\npendencies and the impact of vanishing and explod-\\ning gradients limited the performance of the RNN\\nmodels.\\n5.1.3 Transformer-based Models\\nThe best performance is achieved by the BERT\\nmodel (Devlin et al., 2019) pre-trained on an En-\\nglish corpus. The BERT model is closely followed\\nby the multilingual models XLM-RoBERTa (Con-\\nneau et al., 2020) and mBERT (Devlin et al., 2019).\\nWe hypothesize that the low proportion of Bengali\\ntext in the multilingual pre-training corpus does not\\n0.00.51.01.5\\n2 4 6 8 10 12 14RNN\\nLSTM\\nBERT\\nBanglaBERT\\nXLM-\\nRoBERTa\\nmBERT\\nBanglishBERT\\nDistilBERTFigure 5: Comparison of epoch-wise training loss of the\\nestablished baselines.\\nprovide any significant advantage in code-mixed\\nBengali classification tasks.\\nIn contrast, English pre-trained models like\\nBERT exhibit better understanding of the linguistic\\nintricacies of English words used in code-mixed\\nBengali, thereby producing better performance\\nthan other multilingual and Bengali models. Sim-\\nilarly, the Bengali language models BanglaBERT\\n(Bhattacharjee et al., 2022) and BanglishBERT\\n(Bhattacharjee et al., 2022) are trained on Bengali\\nand Bengali-English corpora respectively. Code-\\nmixed Bengali uses English tokens and hence, the\\npre-training on Bengali tokens does not provide\\nany significant advantage. The lighter version of\\nBERT, DistilBERT (Sanh et al., 2019) produces\\n6comparable but slightly worse results.\\n5.2 Training Loss Analysis\\nFigure 5 illustrates the training loss across 15\\nepochs for the baselines. We observe that all mod-\\nels converge before reaching the 15thepoch. The\\nonly exception is the LSTM model which shows a\\nslight indication of being benefited by additional\\ntraining epochs. Excluding DistilBERT, the other\\nBERT family models converged relatively faster in\\nthe earlier epochs. For most models, training for\\n5-8epochs is appropriate to prevent overfitting.\\n6 Conclusion\\nWe introduce BNSENTMIX, a novel sentiment\\nanalysis dataset tailored for code-mixed Bengali-\\nEnglish. Our work opens several potential research\\navenues for code-mixed Bengali. Researchers can\\nexplore other tasks, such as hate speech, offensive\\nlanguage, and abusive content detection on code-\\nmixed data. Our work addresses a significant gap\\nfor low-resource languages and sets a new stan-\\ndard for sentiment analysis in code-mixed Bengali-\\nEnglish.\\nData Availability\\nOur dataset will be publicly available under the\\nCreative Commons Attribution 4.0 International\\n(CC BY 4.0). Any form of private data or personal\\nidentification information has been removed from\\nthe dataset to prevent privacy violations. We have\\nensured that the redistribution of social media data\\nis consistent with the policies of the corresponding\\nplatforms.\\nLimitations\\nThe label distribution of BNSENTMIXdataset is\\nslightly imbalanced with only 9.2% samples la-\\nbeled as mixed sentiment which can affect the per-\\nformance of the model in classifying mixed sen-\\ntiments. We also acknowledge that the sentiment\\nof the annotator can be a source of bias during\\ndata annotation, though each data sample has been\\nannotated twice by two different annotators, and\\nannotation conflicts have been resolved by a third\\nannotator.\\nEthical Statement\\nThe hired data annotators were compensated sig-\\nnificantly higher than the region‚Äôs minimum wage.\\nEach annotator was only given around 630datasamples with no time restrictions. This ensured\\nthat the annotator did not overwork during data\\nannotation. Annotator sentiment is subject to long\\nworking hours and can affect sentiment labeling.\\nTo prevent this, we mandated five-minute breaks\\nafter every twenty-minute interval and provided\\nrefreshments upon request.\\nAcknowledgements\\nOur work is supported by the Islamic University\\nof Technology Research Seed Grants (IUT RSG)\\n(Ref: REASP/IUT-RSG/2022/OL/07/012). We sin-\\ncerely appreciate Mohammed Saidul Islam and Md\\nMezbaur Rahman for guidance and Nejd Khadija\\nfor proofreading our work.\\nReferences\\nShivam Agrawal and Amit Awekar. 2018. No more\\nbeating about the bush: A step towards idiom han-\\ndling for indian language NLP. In Proceedings of\\nthe Eleventh International Conference on Language\\nResources and Evaluation (LREC 2018) , Miyazaki,\\nJapan. European Language Resources Association\\n(ELRA).\\nIqra Ameer, Grigori Sidorov, Helena Gomez-Adorno,\\nand Rao Muhammad Adeel Nawab. 2022. Multi-\\nlabel emotion classification on code-mixed text: Data\\nand methods. IEEE Access , 10:8779‚Äì8789.\\nUtsab Barman, Amitava Das, Joachim Wagner, and Jen-\\nnifer Foster. 2014. Code mixing: A challenge for\\nlanguage identification in the language of social me-\\ndia. In Proceedings of the first workshop on compu-\\ntational approaches to code switching , pages 13‚Äì23.\\nAbhik Bhattacharjee, Tahmid Hasan, Wasi Ahmad,\\nKazi Samin Mubasshir, Md Saiful Islam, Anindya\\nIqbal, M. Sohel Rahman, and Rifat Shahriyar.\\n2022. BanglaBERT: Language model pretraining\\nand benchmarks for low-resource language under-\\nstanding evaluation in Bangla. In Findings of the\\nAssociation for Computational Linguistics: NAACL\\n2022 , pages 1318‚Äì1327, Seattle, United States. Asso-\\nciation for Computational Linguistics.\\nMarouane Birjali, Mohammed Kasri, and Abderrahim\\nBeni-Hssane. 2021. A comprehensive survey on sen-\\ntiment analysis: Approaches, challenges and trends.\\nKnowledge-Based Systems , 226:107134.\\nLeo Breiman. 2001. Random forests. Machine Learn-\\ning, 45(1):5‚Äì32.\\nBharathi Raja Chakravarthi, Navya Jose, Shardul\\nSuryawanshi, Elizabeth Sherly, and John Philip Mc-\\nCrae. 2020a. A sentiment analysis dataset for code-\\nmixed Malayalam-English. In Proceedings of the 1st\\nJoint Workshop on Spoken Language Technologies\\n7for Under-resourced languages (SLTU) and Collab-\\noration and Computing for Under-Resourced Lan-\\nguages (CCURL) , pages 177‚Äì184, Marseille, France.\\nEuropean Language Resources association.\\nBharathi Raja Chakravarthi, Vigneshwaran Muralidaran,\\nRuba Priyadharshini, and John Philip McCrae. 2020b.\\nCorpus creation for sentiment analysis in code-mixed\\nTamil-English text. In Proceedings of the 1st Joint\\nWorkshop on Spoken Language Technologies for\\nUnder-resourced languages (SLTU) and Collabora-\\ntion and Computing for Under-Resourced Languages\\n(CCURL) , pages 202‚Äì210, Marseille, France. Euro-\\npean Language Resources association.\\nArunavha Chanda, Dipankar Das, and Chandan Mazum-\\ndar. 2016. Unraveling the english-bengali code-\\nmixing phenomenon. In Proceedings of the sec-\\nond workshop on computational approaches to code\\nswitching , pages 80‚Äì89.\\nMark Cieliebak, Jan Milan Deriu, Dominic Egger, and\\nFatih Uzdilli. 2017. A twitter corpus and benchmark\\nresources for german sentiment analysis. In Proceed-\\nings of the Fifth International Workshop on Natural\\nLanguage Processing for Social Media , pages 45‚Äì\\n51, Valencia, Spain. Association for Computational\\nLinguistics.\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\\nVishrav Chaudhary, Guillaume Wenzek, Francisco\\nGuzm√°n, Edouard Grave, Myle Ott, Luke Zettle-\\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\\ncross-lingual representation learning at scale. In Pro-\\nceedings of the 58th Annual Meeting of the Asso-\\nciation for Computational Linguistics , pages 8440‚Äì\\n8451, Online. Association for Computational Lin-\\nguistics.\\nDavid R Cox. 1958. The regression analysis of binary\\nsequences. Journal of the Royal Statistical Society\\nSeries B: Statistical Methodology , 20(2):215‚Äì232.\\nKia Dashtipour, Soujanya Poria, Amir Hussain, Erik\\nCambria, Ahmad YA Hawalah, Alexander Gelbukh,\\nand Qiang Zhou. 2016. Multilingual sentiment anal-\\nysis: state of the art and independent comparison of\\ntechniques. Cognitive computation , 8:757‚Äì771.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of the 2019 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies, Volume 1 (Long and Short Papers) , pages\\n4171‚Äì4186, Minneapolis, Minnesota. Association for\\nComputational Linguistics.\\nMd Fahim. 2023. Aambela at BLP-2023 task 2: Enhanc-\\ning BanglaBERT performance for Bangla sentiment\\nanalysis task with in task pretraining and adversar-\\nial weight perturbation. In Proceedings of the First\\nWorkshop on Bangla Language Processing (BLP-\\n2023) , pages 317‚Äì323, Singapore. Association for\\nComputational Linguistics.Md Fahim, Fariha Tanjim Shifat, Fabiha Haider,\\nDeeparghya Dutta Barua, MD Sakib Ul Rahman\\nSourove, Md Farhan Ishmam, and Md Farhad Alam\\nBhuiyan. 2024. BanglaTLit: A benchmark dataset\\nfor back-transliteration of Romanized Bangla. In\\nFindings of the Association for Computational Lin-\\nguistics: EMNLP 2024 , pages 14656‚Äì14672, Miami,\\nFlorida, USA. Association for Computational Lin-\\nguistics.\\nDevansh Gautam, Prashant Kodali, Kshitij Gupta, An-\\nmol Goel, Manish Shrivastava, and Ponnurangam\\nKumaraguru. 2021. Comet: Towards code-mixed\\ntranslation using parallel monolingual sentences. In\\nProceedings of the Fifth Workshop on Computational\\nApproaches to Linguistic Code-Switching , pages 47‚Äì\\n55.\\nAbhirut Gupta, Aditya Vavre, and Sunita Sarawagi.\\n2021. Training data augmentation for code-mixed\\ntranslation. In Proceedings of the 2021 Conference\\nof the North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies , pages 5760‚Äì5766.\\nFabiha Haider, Fariha Tanjim Shifat, Md Farhan Ish-\\nmam, Deeparghya Dutta Barua, Md Sakib Ul Rah-\\nman Sourove, Md Fahim, and Md Farhad Alam.\\n2024. Banth: A multi-label hate speech detection\\ndataset for transliterated bangla. arXiv preprint\\narXiv:2410.13281 .\\nSepp Hochreiter and J√ºrgen Schmidhuber. 1997. Long\\nshort-term memory. Neural computation , 9(8):1735‚Äì\\n1780.\\nJohn J Hopfield. 1982. Neural networks and physi-\\ncal systems with emergent collective computational\\nabilities. Proceedings of the national academy of\\nsciences , 79(8):2554‚Äì2558.\\nMohammad Hossin and Md Nasir Sulaiman. 2015. A\\nreview on evaluation metrics for data classification\\nevaluations. International journal of data mining &\\nknowledge management process , 5(2):1.\\nMinqing Hu and Bing Liu. 2004. Mining and sum-\\nmarizing customer reviews. In Proceedings of the\\nTenth ACM SIGKDD International Conference on\\nKnowledge Discovery and Data Mining , KDD ‚Äô04,\\npages 168‚Äì177, New York, NY , USA. Association\\nfor Computing Machinery.\\nQingqing Jiang, Lei Chen, Rui Xu, Xiao Ao, and Min\\nYang. 2019. A challenge dataset and effective models\\nfor aspect-based sentiment analysis. In Proceedings\\nof the 2019 Conference on Empirical Methods in Nat-\\nural Language Processing and the 9th International\\nJoint Conference on Natural Language Processing\\n(EMNLP-IJCNLP) , pages 6279‚Äì6284, Hong Kong,\\nChina. Association for Computational Linguistics.\\nAditya Joshi, Ameya Prabhu, Manish Shrivastava, and\\nVasudeva Varma. 2016. Towards sub-word level\\ncompositions for sentiment analysis of Hindi-English\\ncode mixed text. In Proceedings of COLING 2016,\\n8the 26th International Conference on Computational\\nLinguistics: Technical Papers , pages 2482‚Äì2491, Os-\\naka, Japan. The COLING 2016 Organizing Commit-\\ntee.\\nMohsinul Kabir, Obayed Bin Mahfuz, Syed Rifat\\nRaiyan, Hasan Mahmud, and Md Kamrul Hasan.\\n2023. BanglaBook: A large-scale Bangla dataset\\nfor sentiment analysis from book reviews. In Find-\\nings of the Association for Computational Linguis-\\ntics: ACL 2023 , pages 1237‚Äì1247, Toronto, Canada.\\nAssociation for Computational Linguistics.\\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\\nmethod for stochastic optimization. arXiv preprint\\narXiv:1412.6980 .\\nKaitlyn A Litcofsky and Janet G Van Hell. 2017.\\nSwitching direction affects switching costs: Be-\\nhavioral, erp and time-frequency analyses of intra-\\nsentential codeswitching. Neuropsychologia , 97:112‚Äì\\n139.\\nNurul Husna Mahadzir et al. 2021. Sentiment analy-\\nsis of code-mixed text: a review. Turkish Journal\\nof Computer and Mathematics Education (TURCO-\\nMAT) , 12(3):2469‚Äì2478.\\nSoumil Mandal, Sainik Kumar Mahata, and Dipankar\\nDas. 2018. Preparing bengali-english code-mixed\\ncorpus for sentiment analysis of indian languages.\\nArXiv , abs/1803.04000.\\nSoumil Mandal and Anil Kumar Singh. 2018. Language\\nidentification in code-mixed data using multichan-\\nnel neural networks and context capture. Preprint ,\\narXiv:1808.07118.\\nWalaa Medhat, Ahmed Hassan, and Hoda Korashy.\\n2014. Sentiment analysis algorithms and applica-\\ntions: A survey. Ain Shams engineering journal ,\\n5(4):1093‚Äì1113.\\nStian M√¶hlum, John Barnes, Lilja √òvrelid, and Erik\\nVelldal. 2019. Annotating evaluative sentences for\\nsentiment analysis: a dataset for norwegian. In Pro-\\nceedings of the 22nd Nordic Conference on Compu-\\ntational Linguistics , pages 121‚Äì130, Turku, Finland.\\nLinkoping University Electronic Press.\\nBraja Gopal Patra, Dipankar Das, and Amitava Das.\\n2018. Sentiment analysis of code-mixed indian lan-\\nguages: An overview of sail_code-mixed shared\\ntask@ icon-2017. arXiv preprint arXiv:1803.06745 .\\nAdithya Pratapa, Monojit Choudhury, and Sunayana\\nSitaram. 2018. Word embeddings for code-mixed\\nlanguage processing. In Proceedings of the 2018\\nconference on empirical methods in natural language\\nprocessing , pages 3067‚Äì3072.\\nEla Pustulka-Hunt, Thomas Hanne, Eliane Blumer, and\\nManuel Frieder. 2018. Multilingual sentiment anal-\\nysis for a swiss gig. In 2018 6th International Sym-\\nposium on Computational and Business Intelligence\\n(ISCBI) , pages 94‚Äì98. IEEE.Md Nishat Raihan, Dhiman Goswami, Antara Mahmud,\\nAntonios Anastasopoulos, and Marcos Zampieri.\\n2023. Sentmix-3l: A novel code-mixed test dataset in\\nbangla-english-hindi for sentiment analysis. In Pro-\\nceedings of the First Workshop in South East Asian\\nLanguage Processing , pages 79‚Äì84.\\nNishat Raihan, Dhiman Goswami, Antara Mahmud,\\nAntonios Anastasopoulos, and Marcos Zampieri.\\n2024. Emomix-3l: A code-mixed dataset for bangla-\\nenglish-hindi emotion detection. arXiv preprint\\narXiv:2405.06922 .\\nPoonam Rani, Suryakanth Suryawanshi, Koustav\\nGoswami, B. R. Chakravarthi, Tommaso Fransen,\\nand John P. McCrae. 2020. A comparative study of\\ndifferent state-of-the-art hate speech detection meth-\\nods for hindi-english code-mixed data. In Proceed-\\nings of the Second Workshop on Trolling, Aggression\\nand Cyberbullying , Marseille, France. European Lan-\\nguage Resources Association (ELRA).\\nMohd Sanad Zaki Rizvi, Anirudh Srinivasan, Tanuja\\nGanu, Monojit Choudhury, and Sunayana Sitaram.\\n2021. Gcm: A toolkit for generating synthetic code-\\nmixed text. In Proceedings of the 16th Conference of\\nthe European Chapter of the Association for Compu-\\ntational Linguistics: System Demonstrations , pages\\n205‚Äì211.\\nBrian Roark, Lawrence Wolf-Sonkin, Christo Kirov,\\nSabrina J. Mielke, Cibu Johny, I¬∏ sin Demir¬∏ sahin, and\\nKeith Hall. 2020. Processing South Asian languages\\nwritten in the Latin script: the Dakshina dataset. In\\nProceedings of The 12th Language Resources and\\nEvaluation Conference (LREC) , pages 2413‚Äì2423.\\nAnna Rogers, Aleksei Romanov, Anna Rumshisky, Svit-\\nlana V olkova, Maksim Gronas, and Alexander Gribov.\\n2018. Rusentiment: An enriched sentiment analysis\\ndataset for social media in russian. In Proceedings of\\nthe 27th International Conference on Computational\\nLinguistics , pages 755‚Äì763, Santa Fe, New Mexico,\\nUSA. Association for Computational Linguistics.\\nNazanin Sabri, Ali Edalat, and Behnam Bahrak. 2021.\\nSentiment analysis of persian-english code-mixed\\ntexts. In 2021 26th International Computer Confer-\\nence, Computer Society of Iran (CSICC) , pages 1‚Äì4.\\nIEEE.\\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\\nThomas Wolf. 2019. Distilbert, a distilled version\\nof bert: smaller, faster, cheaper and lighter. arXiv\\npreprint arXiv:1910.01108 .\\nGM Shahariar Shibli, Md Tanvir Rouf Shawon,\\nAnik Hassan Nibir, Md Zabed Miandad, and\\nNibir Chandra Mandal. 2023. Automatic back\\ntransliteration of romanized bengali (banglish) to ben-\\ngali. Iran Journal of Computer Science , 6(1):69‚Äì80.\\nSunayana Sitaram and Alan W Black. 2016. Speech\\nsynthesis of code-mixed text. In Proceedings of\\nthe Tenth International Conference on Language\\nResources and Evaluation (LREC‚Äô16) , pages 3422‚Äì\\n3428.\\n9K Sreelakshmi, B Premjith, and KP Soman. 2020. De-\\ntection of hate speech text in hindi-english code-\\nmixed data. Procedia Computer Science , 171:737‚Äì\\n744.\\nSara Stefanich, Jennifer Cabrelli, Dustin Hilderman,\\nand John Archibald. 2019. The morphophonology\\nof intraword codeswitching: Representation and pro-\\ncessing. Frontiers in Communication , 4:54.\\nMohammad Tareq, Md Fokhrul Islam, Swakshar Deb,\\nSejuti Rahman, and Abdullah Al Mahmud. 2023.\\nData-augmentation for bangla-english code-mixed\\nsentiment analysis: Enhancing cross linguistic con-\\ntextual understanding. IEEE Access , 11:51657‚Äì\\n51671.\\nS Thara and Prabaharan Poornachandran. 2018. Code-\\nmixing: A brief survey. In 2018 International con-\\nference on advances in computing, communications\\nand informatics (ICACCI) , pages 2382‚Äì2388. IEEE.\\nVladimir N. Vapnik. 1995. The nature of statistical\\nlearning theory . Springer-Verlag New York, Inc.\\nYogarshi Vyas, Spandana Gella, Jatin Sharma, Kalika\\nBali, and Monojit Choudhury. 2014. Pos tagging\\nof english-hindi code-mixed social media content.\\nInProceedings of the 2014 conference on empirical\\nmethods in natural language processing (EMNLP) ,\\npages 974‚Äì979.\\nMayur Wankhade, Annavarapu Chandra Sekhara Rao,\\nand Chaitanya Kulkarni. 2022. A survey on senti-\\nment analysis methods, applications, and challenges.\\nArtificial Intelligence Review , 55(7):5731‚Äì5780.\\nJanyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.\\nAnnotating expressions of opinions and emotions\\nin language. Language Resources and Evaluation ,\\n39(2):165‚Äì210.\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\\nChaumond, Clement Delangue, Anthony Moi, Pier-\\nric Cistac, Tim Rault, R√©mi Louf, Morgan Funtowicz,\\net al. 2019. Huggingface‚Äôs transformers: State-of-\\nthe-art natural language processing. arXiv preprint\\narXiv:1910.03771 .\\n10',\n",
       " 'A modified Ricci flow on arbitrary weighted graph\\nJicheng Ma1, Yunyan Yang\\x0c1\\n1School of Mathematics, Renmin University of China, Beijing, 100872, China\\nAbstract\\nIn this paper, we propose a modified Ricci flow, as well as a quasi-normalized Ricci\\nflow, on arbitrary weighted graph. Each of these two flows has a unique global solution.\\nIn particular, these global existence and uniqueness results do not require an exit condi-\\ntion proposed by Bai et al in a recent work [2]. As applications, these two Ricci flows\\nare applied to community detection for complex networks, including Karate Club, Amer-\\nican football games, Facebook, as well as artificial networks. In our algorithms, unlike\\nin [5, 15], there is no need to perform surgery at every iteration, only one surgery needs\\nto be performed after the last iteration. From three commonly used criteria for evaluating\\ncommunity detection algorithms, ARI, NMI and Q, we conclude that our algorithms outper-\\nform existing algorithms, including Ollivier‚Äôs Ricci flow [5], normalized Ollivier‚Äôs Ricci flow\\nand normalized Lin-Lu-Yau‚Äôs Ricci flow [15]. The codes for our algorithms are available at\\nhttps: //github.com /mjc191812 /Modified-Ricci-Flow.\\nKeywords: weighted graph; Ricci curvature; Ricci flow; community detection\\n2020 MSC: 05C21; 05C85; 35R02; 68Q06\\n1. Introduction\\nRicci flow was first introduced by Hamilton [12] in 1982. It was originally designed to\\ndeform the Riemannian metric on a smooth Riemannian manifold ( M,g0), and was explicitly\\nexpressed as an evolution equation\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3‚àÇtg=‚àí2Ric\\ng(0)=g0,\\nwhere Ric stands for the Ricci curvature on Riemannian manifold ( M,g(t)). The power of this\\nflow has already been known to the world. For example, it was used by Brendle-Schoen [4]\\nto prove the di fferential sphere theorem, and also used by Perelman [26] to solve the Poincar ¬¥e\\nconjecture.\\nIntuitively, Ricci flow is valuable for understanding the evolution and community struc-\\nture of networks. One can think of a network as a discretization of a high-dimensional man-\\nifold, similar to a 3-manifold, and the communities in the network as components in the\\ngeometric decomposition of the 3-manifold. Perelman‚Äôs work [26] has shown that Ricci flow\\ncan predict the geometric components of a 3-manifold, suggesting that a discrete Ricci flow\\non a network should be able to detect the community structure. Analogous to the work of\\n\\x0ccorresponding author\\nEmail addresses: 2019202433@ruc.edu.cn (Jicheng Ma), yunyanyang@ruc.edu.cn (Yunyan Yang)\\nPreprint submitted to *** August 20, 2024arXiv:2408.09435v1  [math.AP]  18 Aug 2024Hamilton and Perelman on Ricci flow, the number of iterations and the threshold value for\\nsurgery in the flow process may depend on individual networks.\\nIndeed, as expected, Ricci flow performs well in complex networks. In 2019, based on\\nOllivier‚Äôs Ricci flow [23, 24] and a surgery procedure, Ni et al [5] provided an outstanding\\ncommunity detection method. Later, the same problem was re-solved by Lai et al [15] by\\nusing a normalized Ricci flow, which is based on Lin-Lu-Yau‚Äôs Ricci curvature [19] and a\\nstar coupling Ricci curvature of Bai et al [1]. Let‚Äôs say a few more words about complex\\nnetworks, which are commonly used to represent connections between elements in various\\nfields, including social networks [30], biochemistry (such as protein-protein [3] networks,\\nmetabolic networks, and gene networks), and computer science [6, 28]. It‚Äôs widely known\\nthat many real-world networks exhibit community structures, where nodes within the same\\ncommunity are closely connected, while nodes from di fferent communities are sparsely con-\\nnected. Recognizing these community structures is essential for identifying key functional\\ncomponents and supporting processes on networks, such as disease spread, information dis-\\nsemination, and behavioral patterns. Lots of algorithms [7, 10, 11, 18, 21, 25, 31] have been\\ndeveloped to detect and separate communities. Most of these algorithms focus on identify-\\ning dense clusters in a graph, using randomized approaches like label propagation or random\\nwalks, optimizing centrality measures such as betweenness centrality, or considering mea-\\nsures like modularity. In contrast to these methods, [5, 15, 23, 24] suggest discrete Ricci\\nflows on weighted graphs, which o ffers broader applicability and greater solution stability.\\nMore precisely, in [23], Ollivier suggested using the following equation as Ricci flow\\nwith continuous time parameter t: for each edge e‚ààE, the weight we(t) satisfies the ordinary\\ndifferential system\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3d\\ndtwe(t)=‚àíŒ∫e(t)we(t)\\nwe(0)=w0,e,(1.1)\\nwhere w0,edenotes the initial weight and Œ∫e(t) stands for Ollivier‚Äôs Ricci curvature of the\\nedge eon a connected weighted graph ( V,E,w(t)). For a discrete Ricci flow, in each iteration,\\nthe process generates a time dependent family of weighted graph ( V,E,w(t)) such that each\\nweight we(t) changes proportional to Œ∫e(t) at time t. In [5], slightly di fferent from the discrete\\nversion of (1.1), Ni et al used the iteration system\\nw(i+1)\\ne=œÅ(i)\\ne‚àísŒ∫(i)\\neœÅ(i)\\ne (1.2)\\nin their algorithm, where w(i)\\ne,Œ∫(i)\\neandœÅ(i)\\nedenote the weight, Ollivier‚Äôs Ricci curvature and\\nthe distance, respectively, of the edge eat the ith iteration, and sis a step size. Here, if one\\nrepresents e=xy, then the distance is represented by\\nœÅ(i)\\ne=inf\\nŒ≥‚ààŒìeX\\nœÑ‚ààŒ≥w(i)\\nœÑ,\\nwhere Œìeis a set of all paths connecting vertices xandy. It is conceivable that if the volume\\nof a weighted graph tends to zero along the Ricci flow, then the community detection e ffect of\\nthe network would not be satisfactory. To overcome this shortcoming, Lai et al [15] proposed\\na normalized discrete Ricci flow, the ith process of which says\\nw(i+1)\\ne=w(i)\\ne‚àísŒ∫(i)\\new(i)\\ne+sw(i)\\neX\\nœÑ‚ààEŒ∫(i)\\nœÑw(i)\\nœÑ, (1.3)\\nwhere all the symbols have the same meaning as those of (1.2) except for Lin-Lu-Yau‚Äôs Ricci\\ncurvatureŒ∫(i)\\ne[19]. Clearly, the continuous version of (1.3) is\\nd\\ndtwe(t)=‚àíŒ∫e(t)we(t)+we(t)X\\nœÑ‚ààEŒ∫œÑwœÑ. (1.4)\\n2Both approaches of [5, 15] have successfully detected communities for various complex net-\\nworks including Karate Club graph, American football games, Facebook ego network, etc.\\nThough [5] and [15] are greatly successful in applications, there is no theoretical support.\\nIn order to supplement this aspect, very recently, Bai et al [2] considered long time existence\\nand convergence of Ricci flow (1.1) and normalized Ricci flow (1.4) on weighted graphs\\n(V,E,w(t)). According to [2], Lin-Lu-Yau‚Äôs Ricci curvature [19] is an appropriate choice for\\nus when considering the intrinsic metric or curvature on graph. Moreover, they proved that\\nunder an exit condition (once there exist some e‚ààEand some t‚àà[0,+‚àû) satisfying\\nwe(t)>œÅ e(t), (1.5)\\nthe edge eis removed), the normalized Ricci flow (1.4) has a unique global solution on\\nt‚àà[0,+‚àû). Similar as above, if e=xy, thenœÅe(t), the distance between xandyat the time t,\\nis defined by\\nœÅe(t)=inf\\nŒ≥X\\nœÑ‚ààŒ≥wœÑ(t),\\nwhereŒ≥is taken over all paths connecting xandy. For the Ricci flow (1.1) with Ollivier‚Äôs\\nRicci curvature replaced by Lin-Lu-Yau‚Äôs Ricci curvature, the exit condition can still ensure\\nthe global existence and uniqueness of the solution. As an example, if a graph is a finite path,\\nthen the exit condition holds, and thus both the Ricci flow and the normalized Ricci flow have\\nunique global solutions. However, for a general connected weighted graph, the exit condi-\\ntion would not hold. This is exactly one reason why Lai et al [15] did surgery at each iteration.\\nThe purpose of this paper is to modify systems (1.1) and (1.4) so that solutions exist for\\nany initial weight and all time t‚àà[0,+‚àû). Our first result reads\\nTheorem 1.1. Let(V,E,w)be a connected weighted graph, where E ={e1,e2,¬∑¬∑¬∑,em}de-\\nnotes the set of all edges, w=(we1,we2,¬∑¬∑¬∑,wem), w eiis the weight of the edge e i, i=\\n1,2,¬∑¬∑¬∑,m. We denote for each e i=xiyi, the distance between x iand y iby\\nœÅei=min\\nŒ≥‚ààŒìeiX\\nek‚ààŒ≥wek,\\nwhere Œìeiis a set of all paths connecting x iand y i, i=1,2,¬∑¬∑¬∑,m. Then for any initial weight\\nw0=(w0,1,w0,2,¬∑¬∑¬∑,w0,m)‚ààRm\\n+, the Ricci flow\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3d\\ndtwei(t)=‚àíŒ∫e(t)œÅei(t)\\nwei(0)=w0,i\\ni=1,2,¬∑¬∑¬∑,m(1.6)\\nhas a unique solution w(t)=(we1(t),we2(t),¬∑¬∑¬∑,wem(t))for all t‚àà[0,+‚àû).\\nConsider a quasi-normalized Ricci flow\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3d\\ndtwei(t)=‚àíŒ∫ei(t)œÅei(t)+P\\nœÑ‚ààEŒ∫œÑ(t)œÅœÑ(t)P\\nh‚ààEwhœÅei(t)\\nwei(0)=w0,i,i=1,2,¬∑¬∑¬∑,m.(1.7)\\nIfP\\nh‚ààEwh, the denominator of the second term on the right side of (1.7), is replaced byP\\nh‚ààEœÅh, then (1.7) is a normalization of (1.6). In this sense, we call (1.7) a quasi-normalization\\nof (1.6). Regarding (1.7), we have the following:\\nTheorem 1.2. Let(V,E,w)be a connected weighted graph, where E ={e1,e2,¬∑¬∑¬∑,em}de-\\nnotes the set of all edges, w=(we1,we2,¬∑¬∑¬∑,wem), w eiis the weight of the edge e i, i=\\n1,2,¬∑¬∑¬∑,m. Then for any initial weight w0=(w0,1,w0,2,¬∑¬∑¬∑,w0,m)‚ààRm\\n+, the quasi-normalized\\nRicci flow (1.7) has a unique solution w(t)=(we1(t),we2(t),¬∑¬∑¬∑,wem(t))for all t‚àà[0,+‚àû).\\n3Both the proof of Theorem 1.1 and Theorem 1.2 rely on a key observation, namely\\n‚àí2 max\\nœÑ‚ààEwœÑ(t)‚â§Œ∫e(t)œÅe(t)‚â§2we(t) (1.8)\\nfor all e‚ààEand all t>0, which will be seen in Section 3 below. Compared with Bai et al‚Äôs\\nresult [2], we removed the exit condition (1.5).\\nLetTmax(w0) be the maximum time such that (1.1) has a solution on the time interval\\n[0,Tmax(w0)). Clearly (1.8) does not exclude the possibility that up to a subsequence,\\nŒ∫e(t)we(t)‚Üí‚àû ast‚ÜíTmax(w0). (1.9)\\nNote that the possibility of (1.9) is the other reason why Lai et al [15] did surgery at each\\niteration. Actually, in [15], approximately 5 percent of edges are removed in each iteration.\\nIn theory, this could lead to inaccurate community detection in some cases.\\nAlso, we apply modified Ricci flow and quasi-normalized Ricci flow in Theorems 1.1 and\\n1.2 to detect communities for complex networks including Karate Club, American football\\ngames, Facebook ego network, as well as artificial networks. Unlike [5, 15], in our algo-\\nrithms, we don‚Äôt need to do surgery at each iteration; instead, we just need to delete some\\nedges with heavy weights after the final iteration. The advantage of our algorithms is that the\\nflow process does not require the detection of the connected components of the graph. Our\\nexperimental results are as impressive as those of [5, 15], especially with better modularity.\\nAlgorithm 1 demonstrates the process of applying a discrete version of the modified Ricci\\nflow (1.6) to community detection. Here Œ∫i\\nedenotes Lin-Lu-Yau‚Äôs Ricci curvature, which is\\ncalculated via a star coupling Ricci curvature as Lai et al did in [15].\\nAlgorithm 1 Community detection using discrete version of (1.6)\\nInput: an undirected network G=(V,E); maximum iteration T; step size s.\\nOutput: community detection results of G\\nfori=1,¬∑¬∑¬∑,Tdo\\ncompute the Ricci curvature Œ∫i\\ne;\\nwi+1\\ne=wi\\ne‚àís√ó\\x10\\nŒ∫i\\ne√óœÅi\\ne\\x11\\n;\\nend\\ncutoff‚Üêwmax;\\nwhile cutoff>wmindo\\nfore‚ààEdo\\nifwe>cutoffthen\\nremove the edge e;\\nend\\nend\\ncutoff‚Üêcutoff‚àí0.01;\\ncalculate the Modularity, ARI and NMI of G;\\nend\\nThe remaining part of this paper will be organized as follows: In Section 2, we prove that\\non a connected weighted graph, both the distance function and the Ricci curvature function\\nare Lipschitz continuous with respect to the weights; In Section 3, we prove Theorems 1.1 and\\n1.2, by combining the results in Section 2, several conclusions in [1, 2], the key observation\\n(1.8) and some ordinary di fferential system theory; As applications of Theorems 1.1 and 1.2,\\nexperimental process, results and analysis are provided in Section 4.\\n42. The Lipschitz continuity\\nLet ( V,E,w) be a weighted graph, where Vis the set of all vertices, E={e1,e2,¬∑¬∑¬∑,em}\\nis the set of all edges, ei=xiyiis the ith edge for each 1 ‚â§i‚â§m, and w=(we1,we2,¬∑¬∑¬∑,wem)\\nis the weight on E. On this weighted graph, the distance between two vertices u,v‚ààVis\\ngiven by\\nd(u,v)=inf\\nŒ≥X\\ne‚ààŒ≥we, (2.1)\\nwhere the infimum takes over all paths Œ≥connecting uandv. LetŒ∫(u,v) be Lin-Lu-Yau‚Äôs\\nRicci curvature [19], namely\\nŒ∫(u,v)=lim\\nŒ±‚Üí11\\n1‚àíŒ±\"\\n1‚àíW(¬µŒ±\\nu,¬µŒ±\\nv)\\nd(u,v)#\\n, (2.2)\\nwhere d(u,v) is defined as in (2.1), Œ±‚àà[0,1],W(¬µŒ±\\nu,¬µŒ±\\nv) denotes the usual transportation\\ndistance between two probability measures\\n¬µŒ±\\nu(z)=\\uf8f1\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3Œ± ifz=u\\n(1‚àíŒ±)wzuP\\ny‚àºuwyuifz‚àºu\\n0 if otherwise ,¬µŒ±\\nv(z)=\\uf8f1\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3Œ± ifz=v\\n(1‚àíŒ±)wzvP\\ny‚àºvwyvifz‚àºv\\n0 if otherwise .\\nTo simplify notations, we often write œÅe=d(x,y) andŒ∫e=Œ∫(x,y) ife=xy‚ààE. Set\\nRm\\n+={w=(w1,w2,¬∑¬∑¬∑,wm)‚ààRm:wi>0,i=1,2,¬∑¬∑¬∑,m}.\\nNow, for any fixed two vertices u,v‚ààV, we have two functions (still denoted by d(u,v),\\nŒ∫(u,v)), which map w‚ààRm\\n+tod(u,v)‚àà[0,+‚àû) andŒ∫(u,v)‚ààRrespectively.\\nWe first have the Lipschitz continuity of d(u,v) with respect to the weight w. In particular,\\nwe have the following:\\nLemma 2.1. For any w,ew‚ààRm\\n+, if d and ed are two distance functions determined by wand\\newrespectively, then for any two fixed vertices u ,v‚ààV, there holds\\n|d(u,v)‚àíed(u,v)|‚â§‚àöm|w‚àíew|,\\nwhere|w‚àíew|denotes the usual Euclidean norm of w‚àíew‚ààRm.\\nProof. Ifd(u,v)‚â•ed(u,v), then by definition of ed(u,v), there exists a path Œ≥0connecting\\nvertices uandvsuch that\\ned(u,v)=X\\ne‚ààŒ≥0ewe,\\nwhich leads to\\n0‚â§d(u,v)‚àíed(u,v)‚â§X\\ne‚ààŒ≥0we‚àíX\\ne‚ààŒ≥0ewe‚â§X\\ne‚ààŒ≥0|we‚àíewe|‚â§‚àöm|w‚àíew|,\\nwhere mis the number of the edges in E. Ifd(u,v)‚â§ed(u,v), there exists a path Œ≥1connecting\\nvertices uandvsuch that\\nd(u,v)=X\\ne‚ààŒ≥1we,\\nwhich leads to\\n0‚â§ed(u,v)‚àíd(u,v)‚â§X\\ne‚ààŒ≥1ewe‚àíX\\ne‚ààŒ≥1we‚â§X\\ne‚ààŒ≥1|we‚àíewe|‚â§‚àöm|w‚àíew|.\\n5Thus we get the desired result. ‚ñ°\\nWe next prove the Lipschitz continuity of Œ∫ewith respect to the weight was below.\\nLemma 2.2. For any edge e =xy‚ààE, as a function of w=(we1,we2,¬∑¬∑¬∑,wem), the Ricci\\ncurvatureŒ∫e(see (2.2) above) is locally Lipschitz continuous in Rm\\n+.\\nProof. Letw=(we1,¬∑¬∑¬∑,wem) andew=(ewe1,¬∑¬∑¬∑,ewem) be two vectors in Rm\\n+. Fixing the edge\\ne=xy‚ààE, we have that Œ∫eandeŒ∫eare Ricci curvatures determined by wandewrespectively.\\nClearly, we may assume\\nŒõ‚àí1‚â§wei‚â§Œõ,Œõ‚àí1‚â§ewei‚â§Œõ,|wei‚àíewei|‚â§Œ¥,i=1,2,¬∑¬∑¬∑,m, (2.3)\\nwhere ŒõandŒ¥are two positive constants. According to ([20], Theorem 2.1; [2], Remark 1),\\none has\\nŒ∫e= inf\\nf‚ààLip 1,‚àáyxf=1‚àáxy‚àÜf,eŒ∫e= inf\\nf‚ààgLip 1,e‚àáyxf=1e‚àáxye‚àÜf,\\nwhere\\nLip 1 ={f:V‚ÜíR:|f(u)‚àíf(v)|‚â§d(u,v),‚àÄu,v‚ààV},\\ngLip 1 =n\\nf:V‚ÜíR:|f(u)‚àíf(v)|‚â§ed(u,v),‚àÄu,v‚ààVo\\n,\\n‚àáxyf=f(x)‚àíf(y)\\nd(x,y),e‚àáyxf=f(x)‚àíf(y)\\ned(x,y),\\n‚àÜf(x)=1P\\nu‚àºxwxuX\\nv‚àºxwxv(f(v)‚àíf(x))\\nand\\ne‚àÜf(x)=1P\\nu‚àºxewxuX\\nv‚àºxewxv(f(v)‚àíf(x)).\\nWith no loss of generality, we assume d(x,y)‚â§ed(x,y). Hereafter we distinguish two\\ncases to proceed.\\nCase 1.Œ∫e‚â§eŒ∫e.\\nA direct method of variation implies that Œ∫eis achieved by a function f1‚ààLip 1. In\\nparticular‚àáyxf1=1,Œ∫e=‚àáxy‚àÜf1. In view of Lemma 2.1, we find a constant C1depending\\nonly on Œõandmsatisfying\\n1‚àíC1|w‚àíew|‚â§d(u,v)\\ned(u,v)‚â§1+C1|w‚àíew|,‚àÄu,v‚ààV.\\nAs a consequence\\n|f1(u)‚àíf1(v)|‚â§(1+C1|w‚àíew|)ed(u,v),‚àÄu,v‚ààV.\\nWe may assume|w‚àíew|<1/C1. Define a function f‚àó\\n1=(1‚àíC1|w‚àíew|)f1. It then follows\\nthat f‚àó\\n1‚ààgLip 1 and\\nf‚àó\\n1(y)‚àíf‚àó\\n1(x)‚â§(1‚àíC2\\n1|w‚àíew|2)ed(x,y)<ed(x,y).\\nThanks to ([2], Lemma 2), we find a function ef1‚ààgLip 1 such that\\nef1(y)‚àíef1(x)=ed(x,y)\\n6and for all u‚ààV,\\n|ef1(u)‚àíf‚àó\\n1(u)|‚â§ed(x,y)‚àí(f‚àó\\n1(y)‚àíf‚àó\\n1(x)). (2.4)\\nTherefore\\neŒ∫e= inf\\nf‚ààgLip 1,e‚àáyxf=1e‚àáxye‚àÜf‚â§e‚àÜef1(x)‚àíe‚àÜef1(y)\\ned(x,y),\\nand whence\\neŒ∫e‚àíŒ∫e= inf\\nf‚ààgLip 1,e‚àáyxf=1e‚àáxye‚àÜf‚àí‚àÜf1(x)‚àí‚àÜf1(y)\\nd(x,y)\\n‚â§e‚àÜef1(x)‚àíe‚àÜef1(y)\\ned(x,y)‚àí‚àÜf1(x)‚àí‚àÜf1(y)\\nd(x,y)\\n‚â§\\x0c\\x0c\\x0c\\x0c\\x0c\\x0ce‚àÜef1(x)\\ned(x,y)‚àí‚àÜf1(x)\\nd(x,y)\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c+\\x0c\\x0c\\x0c\\x0c\\x0c\\x0ce‚àÜef1(y)\\ned(x,y)‚àí‚àÜf1(y)\\nd(x,y)\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c. (2.5)\\nIn view of (2.4), we have for all u‚ààV,\\nef1(u)‚àíef1(x)=ef1(u)‚àíf‚àó\\n1(u)+f‚àó\\n1(u)‚àíf‚àó\\n1(x)+f‚àó\\n1(x)‚àíef1(x)\\n‚â§f‚àó\\n1(u)‚àíf‚àó\\n1(x)+2(ed(x,y)‚àíf‚àó\\n1(y)+f‚àó\\n1(x)) (2.6)\\nand\\nef1(u)‚àíef1(x)‚â•f‚àó\\n1(u)‚àíf‚àó\\n1(x)‚àí2(ed(x,y)‚àíf‚àó\\n1(y)+f‚àó\\n1(x)). (2.7)\\nIt follows from (2.6) that\\ne‚àÜef1(x)\\ned(x,y)‚àí‚àÜf1(x)\\nd(x,y)=X\\nu‚àºx\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8edewxu(ef1(u)‚àíef1(x))\\ned(x,y)P\\nz‚àºxewxz‚àíwxu(f1(u)‚àíf1(x))\\nd(x,y)P\\nz‚àºxwxz\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\n‚â§X\\nu‚àºx\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8edewxu(f‚àó\\n1(u)‚àíf‚àó\\n1(x))\\ned(x,y)P\\nz‚àºxewxz‚àíwxu(f1(u)‚àíf1(x))\\nd(x,y)P\\nz‚àºxwxz\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\n+2\\ned(x,y)(ed(x,y)‚àíf‚àó\\n1(y)+f‚àó\\n1(x)). (2.8)\\nWhile (2.7) gives\\n‚àÜf1(x)\\nd(x,y)‚àíe‚àÜef1(x)\\ned(x,y)‚â§X\\nu‚àºx\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8edwxu(f1(u)‚àíf1(x))\\nd(x,y)P\\nz‚àºxwxz‚àíewxu(f‚àó\\n1(u)‚àíf‚àó\\n1(x))\\ned(x,y)P\\nz‚àºxewxz\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\n+2\\ned(x,y)(ed(x,y)‚àíf‚àó\\n1(y)+f‚àó\\n1(x)). (2.9)\\nCombining (2.8), (2.9), ‚àáyxf1=1 and the definition of f‚àó\\n1, we obtain\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\x0ce‚àÜef1(x)\\ned(x,y)‚àí‚àÜf1(x)\\nd(x,y)\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c‚â§X\\nu‚àºx\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c(1‚àíC1|w‚àíew|)ewxu\\ned(x,y)P\\nz‚àºxewxz‚àíwxu\\nd(x,y)P\\nz‚àºxwxz\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c|f1(u)‚àíf1(x)|\\n+2\\ned(x,y)(ed(x,y)‚àí(1+C1|w‚àíew|)d(x,y))\\n=X\\nu‚àºx|ewxud(x,y)P\\nz‚àºxwxz‚àíwxued(x,y))P\\nz‚àºxewxz|\\ned(x,y)d(x,y)(P\\nz‚àºxewxz)(P\\nz‚àºxwxz)|f1(u)‚àíf1(x)|\\n+2\\ned(x,y)(ed(x,y)‚àíd(x,y)+C1|w‚àíew|d(x,y)).\\n7This together with (2.3) and Lemma 2.1 leads to\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\x0ce‚àÜef1(x)\\ned(x,y)‚àí‚àÜf1(x)\\nd(x,y)\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c‚â§C|w‚àíew| (2.10)\\nfor some constant Cdepending only on Œõandm.\\nAlso we derive from (2.4) that\\nef1(u)‚àíef1(y)‚â§f‚àó\\n1(u)‚àíf‚àó\\n1(y)+2(ed(x,y)‚àíf‚àó\\n1(y)+f‚àó\\n1(x))\\nand\\nef1(u)‚àíef1(y)‚â•f1(u)‚àíf1(y)‚àí2(ed(x,y)‚àíf‚àó\\n1(y)+f‚àó\\n1(x))\\nfor all u‚ààV. Using the same method as proving (2.10), we get\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\x0ce‚àÜef1(y)\\ned(x,y)‚àí‚àÜf1(y)\\nd(x,y)\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c‚â§C|w‚àíew|, (2.11)\\nwhere Cis a constant depending only on Œõandm. Inserting (2.10) and (2.11) into (2.5), we\\nfind some constant Cdepending only on Œõandmsuch that\\neŒ∫e‚àíŒ∫e‚â§C|w‚àíew|. (2.12)\\nCase 2.Œ∫e>eŒ∫e.\\nA direct method of variation implies that eŒ∫eis attained by some function eg2‚ààgLip 1\\nsatisfying e‚àáyxeg2=1 andeŒ∫e=e‚àáxye‚àÜeg2. Setef2(u)=eg2(u)‚àíeg2(x) for all u‚ààV. Clearly\\nef2‚ààgLip 1,e‚àáyxef2=1 andeŒ∫e=e‚àáxye‚àÜef2. Hence there exists a constant C‚àódepending only on\\nŒõandmsuch that for all u‚ààV,\\n|ef2(u)|=|ef2(u)‚àíef2(x)|‚â§ed(u,x)‚â§C‚àó, (2.13)\\nsinceef2(x)=0. Define\\nf‚àó\\n2(u)=\\uf8f1\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f4\\uf8f3d(x,y)\\ned(x,y)ef2(u) if u‚àà{x,y}\\n1\\n1+A|w‚àíew|ef2(u) if u‚ààV\\\\{x,y},(2.14)\\nwhere Ais a large positive number to be determined later. Keeping in mind e‚àáyxef2=1, we\\nnotice that\\nf‚àó\\n2(y)‚àíf‚àó\\n2(x)=d(x,y)\\ned(x,y)(ef2(y)‚àíef2(x))=d(x,y), (2.15)\\nand that for all u,v‚ààV\\\\{x,y},\\n|f‚àó\\n2(u)‚àíf‚àó\\n2(v)|=1\\n1+A|w‚àíew||ef2(u)‚àíef2(v)|‚â§1+C1|w‚àíew|\\n1+A|w‚àíew|d(u,v), (2.16)\\n|f‚àó\\n2(u)‚àíf‚àó\\n2(x)|=1\\n1+A|w‚àíew||ef2(u)‚àíef2(x)|‚â§1+C2|w‚àíew|\\n1+A|w‚àíew|d(u,x), (2.17)\\n|f‚àó\\n2(u)‚àíf‚àó\\n2(y)|=\\x0c\\x0c\\x0c\\x0c\\x0c\\x0cef2(u)‚àíef2(x)\\n1+A|w‚àíew|‚àíd(x,y)\\ned(x,y)(ef2(y)‚àíef2(x))\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\n‚â§\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8edd(x,y)\\ned(x,y)‚àí1\\n1+A|w‚àíew|\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8ef2(x)\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c+\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8edd(x,y)\\ned(x,y)‚àí1\\n1+A|w‚àíew|\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8ef2(y)\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c+|ef2(u)‚àíef2(y)|\\n1+A|w‚àíew|\\n‚â§O(|w‚àíew|)+1+C2|w‚àíew|\\n1+A|w‚àíew|d(u,y). (2.18)\\n8If we first fix Œõ>1, then choose a su fficiently large A>1, finally take a su fficiently small\\nŒ¥ > 0, then we conclude from (2.13)-(2.18) that f‚àó\\n2‚ààLip 1 and‚àáyxf‚àó\\n2=1. For such f‚àó\\n2,\\nin view of ([2], Lemma 3), ‚àÄa,0<a<d(x,y), there exists a function f2‚ààLip 1 such that\\nf2(y)‚àíf2(x)=d(x,y) and|f‚àó\\n2(z)‚àíf2(z)|‚â§d(x,y)‚àíafor all z‚ààV, which together with\\nf‚àó\\n2(x)=0 implies for all u‚ààV,\\nf‚àó\\n2(u)‚àí2(d(x,y)‚àía)‚â§f2(u)‚àíf2(x)‚â§f‚àó\\n2(u)+2(d(x,y)‚àía).\\nNow we choose a=d(x,y)‚àí|w‚àíew|. If we take Œ¥ <Œõ‚àí1, then 0<a<d(x,y). As a\\nconsequence, there holds\\nf‚àó\\n2(u)‚àí2|w‚àíew|‚â§f2(u)‚àíf2(x)‚â§f‚àó\\n2(u)+2|w‚àíew|. (2.19)\\nNote that\\nŒ∫e‚àíeŒ∫e= inf\\nf‚ààLip 1,‚àáyxf=1‚àáxy‚àÜf‚àíe‚àáxye‚àÜef2\\n‚â§ ‚àá xy‚àÜf2‚àíe‚àáxye‚àÜef2\\n=‚àÜf2(x)‚àí‚àÜf2(y)\\nd(x,y)‚àíe‚àÜef2(x)‚àíe‚àÜef2(y)\\ned(x,y)\\n‚â§\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c‚àÜf2(x)\\nd(x,y)‚àíe‚àÜef2(x)\\ned(x,y)\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c+\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c‚àÜf2(y)\\nd(x,y)‚àíe‚àÜef2(y)\\ned(x,y)\\x0c\\x0c\\x0c\\x0c\\x0c\\x0c. (2.20)\\nOn one hand, we calculate\\n‚àÜf2(x)\\nd(x,y)‚àíe‚àÜef2(x)\\ned(x,y)=X\\nu‚àºxwxu(f2(u)‚àíf2(x))\\nd(x,y)P\\nz‚àºxwxz‚àíX\\nu‚àºxewxuef2(u)\\ned(x,y)P\\nz‚àºxewxz\\n=X\\nu‚àºx\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8edwxuf‚àó\\n2(u)\\nd(x,y)P\\nz‚àºxwxz‚àíewxuef2(u)\\ned(x,y)P\\nz‚àºxewxz\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8+O(|w‚àíew|)\\n=X\\nu‚àºx\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8edwxu\\nd(x,y)P\\nz‚àºxwxz‚àíewxu\\ned(x,y)P\\nz‚àºxewxz\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8ef2(u)\\n+X\\nz‚àºxwxu(f‚àó\\n2(u)‚àíef2(u))\\nd(x,y)P\\nz‚àºxwxz+O(|w‚àíew|)\\n=O(|w‚àíew|),\\nsince (2.19) gives f2(u)‚àíf2(x)=f‚àó\\n2(u)+O(|w‚àíew|), (2.14) leads to f‚àó\\n2(u)‚àíef2(u)=O(|w‚àíew|),\\n(2.13) implies ef2(u)=O(1), while (2.3) and Lemma 2.1 yields ed(x,y)/d(x,y)=1+O(|w‚àíew|).\\nOn the other hand, the last term of the inequality (2.20) is also O(|w‚àíew|) in the same way.\\nTherefore\\nŒ∫e‚àíeŒ∫e‚â§C|w‚àíew|\\nfor some constant Cdepending only on Œõ,mandŒ¥.\\nCombining Case 1 and Case 2, we complete the proof of the lemma. ‚ñ°\\n3. Long time existence\\nIn this section, concerning the global existence and uniqueness of solutions to the mod-\\nified Ricci flow and quasi-normalized Ricci flow on an arbitrary weighted graph, we prove\\nTheorems 1.1 and 1.2.\\n9Proof of Theorem 1.1. We divide the proof into two steps.\\nStep 1. Short time existence.\\nSetwi=wei,i=1,2,¬∑¬∑¬∑,m. Given any initial value w0=(w0,1,w0,2,¬∑¬∑¬∑,w0,m)‚ààRm\\n+,\\nthe modified Ricci flow (1.6) is exactly the ordinary di fferential system\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3d\\ndtw=f(w)\\nw(0)=w0,(3.1)\\nwhere w=(w1,w2,¬∑¬∑¬∑,wm)‚ààRm\\n+andfis a vector-valued function defined by\\nf:Rm\\n+‚ÜíRm\\nw7‚Üí (Œ∫e1œÅe1,¬∑¬∑¬∑,Œ∫emœÅem).\\nBy Lemmas 2.1 and 2.2, we know that all œÅei,Œ∫ei,i=1,2,¬∑¬∑¬∑,m, are locally Lipschitz con-\\ntinuous in Rm\\n+, and so is the vector-valued function f(w). According to the ODE theory, for\\nexample ([29], Section 6.1.1, page 250), there exists a constant T0>0 such that the system\\n(3.1) has a unique solution w(t)‚ààRm\\n+fort‚àà[0,T0].\\nStep 2. Long time existence.\\nDefine\\nT‚àó=sup{T: (3.1) has a unique solution on [0 ,T]}.\\nIfT‚àó<+‚àû, then (3.1) has a unique solution w(t)=(we1(t),we2(t),¬∑¬∑¬∑,wem(t)) on the time\\ninterval [0,T‚àó); moreover, according to an extension theorem of ordinary di fferential system\\n([29], Section 6.1.1, page 250), there must hold w(t)‚Üí‚àÇRm\\n+or|w(t)|‚Üí+‚àûast‚ÜíT‚àó. As\\na consequence, we have either\\nlim inf\\nt‚ÜíT‚àómin{we1(t),we2(t),¬∑¬∑¬∑,wem(t)}=0, (3.2)\\nor\\nlim sup\\nt‚ÜíT‚àómax{we1(t),we2(t),¬∑¬∑¬∑,wem(t)}= +‚àû. (3.3)\\nDenoteœï(t)=min{we1(t),we2(t),¬∑¬∑¬∑,wem(t)}andŒ¶(t)=max{we1(t),we2(t),¬∑¬∑¬∑,wem(t)}. Thanks\\nto ([2], Lemma 1) and [1], there holds for all e‚ààE,\\n‚àí2Œ¶(t)\\nœÅe(t)‚â§Œ∫e(t)‚â§2,\\nand thus\\n‚àí2œÅe(t)‚â§‚àíŒ∫e(t)œÅe(t)‚â§2Œ¶(t). (3.4)\\nThe power of (3.4) is evident. On one hand, since œÅe(t)‚â§we(t), the left side of (3.4) gives\\nw‚Ä≤\\ne(t)=‚àíŒ∫e(t)œÅe(t)‚â•‚àí2œÅe(t)‚â•‚àí2we(t),\\nwhich implies we(t)‚â•we(0)e‚àí2tfor all t‚àà[0,T‚àó). Henceœï(t)‚â•œï(0)e‚àí2T‚àófor all t‚àà[0,T‚àó),\\ncontradicting (3.2). On the other hand, the right side of (3.4) leads to\\nd\\ndtX\\ne‚ààEwe(t)‚â§2mŒ¶(t)‚â§2mX\\ne‚ààEwe(t),‚àÄt‚àà[0,T‚àó].\\nIt follows thatX\\ne‚ààEwe(t)‚â§\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8edX\\ne‚ààEwe(0)\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8e2mT‚àó,‚àÄt‚àà[0,T‚àó],\\n10which together with Œ¶(t)‚â§P\\ne‚ààEwe(t) contradicts (3.3). Therefore T‚àó= +‚àûand the proof\\nof the theorem is completed. ‚ñ°\\nProof of Theorem 1.2. Suppose that the Ricci curvature Œ∫eiand the distance œÅeiare deter-\\nmined by the weight w=(we1,we2,¬∑¬∑¬∑,wem). Let g:Rm\\n+‚ÜíRmbe a vector-valued function\\nwritten as g(w)=(g1(w),g2(w),¬∑¬∑¬∑,gm(w)), where\\ngi(w)=‚àíŒ∫eiœÅei+P\\nœÑ‚ààEŒ∫œÑœÅœÑP\\nh‚ààEwhœÅei,i=1,2,¬∑¬∑¬∑,m. (3.5)\\nFrom Lemmas 2.1 and 2.2, it follows that gis locally Lipschitz continuous in Rm\\n+, i.e. for any\\nbounded domain ‚Ñ¶‚äÇRm\\n+with‚Ñ¶‚äÇRm\\n+, there would exist a constant Cdepending only on ‚Ñ¶\\nsuch that\\n|g(w)‚àíg(ew)|‚â§C|w‚àíew|,‚àÄw,ew‚àà‚Ñ¶.\\nHence the existence and uniqueness theorem ([29], Section 6.1.1, page 250) gives some T1>\\n0 such that the flow (1.7) has a unique solution w(t) on the time interval [0 ,T1]. Set\\nT‚àó=sup{T: (1.7) has a unique solution on [0 ,T]}.\\nIfT‚àó<+‚àû, then (1.7) has a unique solution w(t)=(we1(t),we2(t),¬∑¬∑¬∑,wem(t)) for all t‚àà\\n[0,T‚àó); meanwhile, an extension theorem ([29], Section 6.1.1, page 250) leads to\\nlim inf\\nt‚ÜíT‚àómin{we1(t),we2(t),¬∑¬∑¬∑,wem(t)}=0 (3.6)\\nor\\nlim sup\\nt‚ÜíT‚àómax{we1(t),we2(t),¬∑¬∑¬∑,wem(t)}= +‚àû. (3.7)\\nWriteœï(t)=min{we1(t),we2(t),¬∑¬∑¬∑,wem(t)}andŒ¶(t)=max{we1(t),we2(t),¬∑¬∑¬∑,wem(t)}. We\\nknow from ([2], Lemma 1) and [1] that for all 1 ‚â§i‚â§m,\\n‚àí2Œ¶(t)\\nœÅei(t)‚â§Œ∫ei(t)‚â§2.\\nSince 0<œÅ ei(t)‚â§wei(t) and Œ¶(t)‚â§Pm\\ni=1wei(t) for all t‚àà[0,T‚àó), it follows that\\n‚àí2œÅei(t)‚â§‚àíŒ∫ei(t)œÅei(t)‚â§2Œ¶(t) (3.8)\\nand\\n‚àí2mœÅei‚â§P\\nœÑ‚ààEŒ∫œÑœÅœÑP\\nh‚ààEwhœÅei‚â§2œÅei,‚àÄ1‚â§i‚â§m. (3.9)\\nNoticing that wis a solution of (1.7), we have by (3.8) and (3.9),\\nd\\ndtmX\\ni=1wei=‚àímX\\ni=1Œ∫eiœÅei+P\\nœÑŒ∫œÑœÅœÑP\\nhwhmX\\ni=1œÅei‚â§2(m+1)mX\\ni=1wei.\\nThis leads to\\nŒ¶(t)‚â§mX\\ni=1wei(t)‚â§e2(m+1)T‚àómX\\ni=1w0,i\\nfor all t‚àà[0,T‚àó). This concludes (3.7) can not be true.\\nOn the other hand, it follows from (3.8) and (3.9) that\\nw‚Ä≤\\nei=gi(w)‚â•‚àí2(m+1)weii=1,2,¬∑¬∑¬∑,m.\\nHence\\nwei(t)‚â•e‚àí2(m+1)tw0,i,i=1,2,¬∑¬∑¬∑,m.\\nAs a consequence\\nœï(t)‚â•e‚àí2(m+1)T‚àóœï(0),\\nwhich excludes the possibility of (3.6). Therefore T‚àó= +‚àû, as we desired. ‚ñ°\\n114. Evaluation\\nIn this section, we apply modified Ricci flow (1.6) and quasi-normalized Ricci flow (1.7)\\nto community detection. To simplify notations, in the subsequent experiments, these two\\nRicci flows are written as Rho and RhoN respectively. We have listed the algorithm for Rho\\nin the introduction part, and now give the algorithm for RhoN as below.\\nAlgorithm 2 Community detection using the discrete version of RhoN\\nInput: an undirected network G=(V,E); maximum iteration T; step size s.\\nOutput: community detection results of G\\nfori=1,¬∑¬∑¬∑,Tdo\\ncompute the Ricci curvature Œ∫i\\ne;\\nsum r‚ÜêP\\ne‚ààEwi\\ne;\\nsumŒ∫‚ÜêP\\ne‚ààEœÅi\\ne√óŒ∫i\\ne;\\nwi+1\\ne=wi\\ne‚àís√ó\\x10\\nŒ∫i\\ne√óœÅi\\ne+œÅi\\ne√ósumŒ∫/sum r\\x11\\n;\\nend\\ncutoff‚Üêwmax;\\nwhile cutoff>wmindo\\nfore‚ààEdo\\nifwe>cutoffthen\\nremove the edge e;\\nend\\nend\\ncutoff‚Üêcutoff‚àí0.01;\\ncalculate the Modularity, ARI and NMI of G;\\nend\\nAccording to [1, 15], in Algorithms 1 and 2, the Ricci curvature is calculated by\\nŒ∫e=1\\nœÅesup\\nBX\\nu,v‚ààVB(u,v)d(u,v),\\nwhere Bis taken over all star couplings defined as follows. Let e=xy,¬µ0\\nxand¬µ0\\nybe two\\nprobability measures as in Section 2. A coupling Bbetween¬µ0\\nxand¬µ0\\nyis a star coupling if\\nthe following four properties are satisfied:\\n(i)B(x,y)>0,B(u,v)‚â§0 for all u,xorv,y;\\n(ii)P\\nu,v‚ààVB(u,v)=0;\\n(iii)P\\nv‚ààVB(u,v)=‚àí¬µ0\\nx(u) for all u,x;\\n(iv)P\\nu‚ààVB(u,v)=‚àí¬µ0\\ny(v) for all v,y.\\nThe main complexity of our algorithms comes from finding the shortest path in the graph\\nand solving a linear programming problem. The run times for these tasks are O(|E|log|V|)\\nandO(|E|D3) respectively, where Dis the average degree of the network, |E|is the number\\nof all edges and|V|is the number of all vertices of the network. Despite the network‚Äôs sparse\\nconnectivity, where |D|‚â™| E|,O(|E|D3) often exceeds O(|E|log|V|) in most cases. Therefore,\\nthe computational complexity of our approach is O(|E|D3).\\nHereafter, we first introduce commonly used criteria for evaluating community detection\\nalgorithms. Then, we perform experiments on several real-world networks (such as Karate\\n[32], Football [11], and Facebook [17]) as well as artificial networks to assess the algorithms.\\nTo conduct ablation comparison, Rho (1.6) and RhoN (1.7) are tested. In the experiments,\\nwe compare three traditional machine learning methods that are not based on neural net-\\nworks. These methods are the Girvan Newman algorithm based on edge betweenness [11],\\n12the Greedy Modularity algorithm based on modularity maximization [7, 27], and the Label\\nPropagation algorithm based on stochastic methods [8]. We also use three di fferent models\\nbased on Ricci curvature for comparison: unnormalized discrete Ricci flow model with Ol-\\nlivier Ricci curvature (DORF) [5], normalized discrete Ricci flow model with Ollivier Ricci\\ncurvature (NDORF), and normalized discrete Ricci flow model with star coupling Ricci cur-\\nvature (NDSRF) [15].\\n4.1. Criteria\\nThe Adjusted Rand Index (ARI) [13] and Normalized Mutual Information (NMI) [9]\\nare two commonly used metrics for evaluating the similarity between two partitions. When\\none of these partitions represents the true labels of network communities, we can assess the\\ndetection accuracy of algorithms by comparing the ARI or NMI values between the ground\\ntruth and the predicted labels. Unlike ARI and NMI, Modularity [7, 22], denoted as Q, is used\\nin community detection to evaluate partitions by calculating the proportion of edges within\\ncommunities without requiring the true partition. It assesses a partition based on the principle\\nthat edges are dense within communities and sparse between them.\\nThe calculation methods for ARI and NMI each have their strengths. ARI evaluates the\\nconsistency between two partitions by counting the number of correctly classified sample\\npairs, discounting the impact of random partitions. NMI, grounded in information theory,\\nmeasures the amount of shared information between two partitions to assess their similarity.\\nEach metric has its advantages in practical applications. For example, ARI performs well\\nwhen dealing with communities of varying sizes, while NMI remains stable across di fferent\\nnumbers of communities. Modularity Q, by assessing the proportion of within-community\\nedges, provides a quality measure that is independent of known labels, making it widely\\napplicable in real-world scenarios.\\nLet{U1,U2,..., UI}and{V1,V2,..., VJ}be two partitions of the set Sofnvertices\\n(nodes). Let ni j=|Ui‚à©Vj|denote the number of vertices in Ui‚à©Vj, while aiandbjrepresent\\nthe numbers of vertices in UiandVj, respectively. Then the explicit expressions of the above\\nmentioned three criteria are listed below.\\n‚Ä¢Adjusted Rand Index\\nARI=PI\\ni=1PJ\\nj=1\\x10ni j\\n2\\x11\\n‚àí\\x10PI\\ni=1\\x10ai\\n2\\x11PJ\\nj=1\\x10bj\\n2\\x11\\x0e\\x10n\\n2\\x11\\x11\\n1\\n2\\x10PI\\ni=1\\x10ai\\n2\\x11\\n+PJ\\nj=1\\x10bj\\n2\\x11\\x11\\n‚àí\\x10PI\\ni=1\\x10ai\\n2\\x11PJ\\nj=1\\x10bj\\n2\\x11\\x0e\\x10n\\n2\\x11\\x11,\\nwhere\\x10n\\n2\\x11\\nis the number of di fferent pairs from nvertices, while symbols\\x10ni j\\n2\\x11\\n,\\x10ai\\n2\\x11\\nand\\x10bj\\n2\\x11\\nhave the same meaning. ARI ranges from ‚àí1 to 1, with larger values indicating\\nhigher concordance between the two partitions.\\n‚Ä¢Normalized Mutual Information\\nNMI =‚àí2PI\\ni=1PJ\\nj=1ni jlog\\x12\\nni jn\\naibj\\x13\\nPI\\ni=1ailog\\x10ai\\nn\\x11\\n+PJ\\nj=1bjlog\\x10bj\\nn\\x11.\\nNMI ranges from 0 to 1, with higher values indicating greater similarity between the\\npartitions.\\n‚Ä¢Modularity\\nQ=NX\\nk=1\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8edLk\\n|E|‚àíŒ≥ Dk\\n2|E|!2\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8,\\n13where Nrepresents the number of communities, Lkis the number of connections within\\nthekth community, Dkis the total degree of vertices in the kth community, and Œ≥is a\\nresolution parameter, with a default value of 1. The value of Qranges from‚àí0.5 to 1,\\nwith values closer to 1 indicating a stronger community structure and better division\\nquality.\\n4.2. Results\\nIn this subsection, we outline the model networks and real-world datasets that were used\\nto assess the accuracy of our community detection method. For the model network, we eval-\\nuate a standard and widely used stochastic block model (SBM) [14] that provides community\\nlabels. As for real-world datasets, we select three di fferent community graphs that come with\\nground-truth community labels.\\n4.2.1. Real world datasets\\nBasic information for real-world networks is listed in Table1.\\nTable 1: Summary of real-world network characteristics\\nnetworks vertexes edges #Class AvgDeg density Diameter\\nKarate 34 78 2 4.59 0.139 5\\nFootball 115 613 12 10.66 0.094 4\\nFacebook 775 14006 18 36.15 0.047 8\\nThe Karate dataset [32] consists of a karate club network with 34 members and 78 edges.\\nThe vertices represent members, and the edges represent the connections between members.\\nThe actual community structure of the network comprises members from two karate clubs.\\nFor the football dataset [11], it is based on the American college football league, which\\ncomprises 115 teams (vertices) and 613 matches (edges). The vertices correspond to the\\nteams, while the edges represent the matches between the teams.\\nThe Facebook network [17] is a real dataset from the Stanford Network Analysis Project,\\nrepresenting interaction networks from an online social networking site. In the interaction\\nnetwork, the benchmark community ground truth is organized by well-defined themes such\\nas interests and a ffiliations.\\nThe three real-world datasets represent network data at di fferent scales. According to\\nTable 1, the square of the average degree (denoted as D) exceeds log|V|. As a result, the\\ncomputational complexity is O\\x10\\n|E|D3\\x11\\n. The results from our algorithm 2, RhoN (1.7), for\\nvarious real-world datasets are presented in sequential order in Figures 1, 2, and 3, corre-\\nsponding to Karate, Football, and Facebook, respectively. The experimental results of our\\nalgorithm 1, Rho (1.6), and other existing algorithms (Girvan-Newman [11], Greedy Modu-\\nlarity [7, 27], Label Propagation [8], DORF [5], NDORF [1] and NDSRF [15]) are presented\\nin Table 2.\\nSince the explanations of Figures 1, 2 and 3 are completely analogous, we only explain\\nFigure 2 for the Football network, and leave the explanations for Figures 1 and 3 to interested\\nreaders.\\n‚Ä¢Figure 2(a) - Before quasi-normalized Ricci flow:\\nHistogram of Ricci Curvatures: The distribution is heavily skewed towards negative\\nvalues, with the bulk of edges showing curvatures from -235 to 0. A pronounced\\npeak near zero indicates minimal contribution from many edges to the total curvature,\\nwhereas a tail extending to -235 highlights edges with significantly negative curvature.\\n14(a)\\n(b)\\n(c)\\nFigure 1: Karate (a) is the histograms of Ricci Curvatures and Edge Weights before discrete Ricci Flow. (b) is\\nthe histograms of Ricci Curvatures and Edge Weights after discrete Ricci Flow. (c) is the evaluation metrics after\\nsurgery.\\n15(a)\\n(b)\\n(c)\\nFigure 2: Football (a) is the histograms of Ricci Curvatures and Edge Weights before discrete Ricci Flow. (b) is\\nthe histograms of Ricci Curvatures and Edge Weights after discrete Ricci Flow. (c) is the evaluation metrics after\\nsurgery.\\n16(a)\\n(b)\\n(c)\\nFigure 3: Facebook (a) is the histograms of Ricci Curvatures and Edge Weights before discrete Ricci Flow. (b) is\\nthe histograms of Ricci Curvatures and Edge Weights after discrete Ricci Flow. (c) is the evaluation metrics after\\nsurgery.\\n17Histogram of Edge weights: There is a notable concentration of edge weights at\\nthe lower end, primarily at weight 1, suggesting uniformity across the graph‚Äôs edges\\ninitially.\\n‚Ä¢Figure 2(b) - After quasi-normalized Ricci flow:\\nHistogram of Ricci Curvatures: The curvature distribution becomes more balanced\\nand less negatively skewed post-flow, predominantly ranging between -10 and 0, in-\\ndicative of a normalizing e ffect of the Ricci flow on the graph‚Äôs curvatures.\\nHistogram of Edge weights: The edge weights now span a broader range from 0 to\\n3.3, indicating di fferentiation in edge weights as a result of the Ricci flow process.\\n‚Ä¢Figure 2(c) - Evaluation Metrics Post-Procedure:\\nThe graph below shows Modularity, Adjusted Rand Index, and NMI plotted against\\nedge weight cuto ffs. Initially, all these indicators are zero because only a few edges are\\nremoved, and there is minimal community structure. However, the indicators gradually\\nincrease, reach a peak, and then stabilize as the cutoffvalue increases. When the cuto ff\\napproaches wmin, most of the edges are deleted, leading to a rapid drop in these indi-\\ncators, essentially reaching zero. This indicates that the Ricci flow has optimized the\\ngraph structure up to this point, improving its functionality for community detection.\\nNow we compare the performance of our algorithms with other six algorithms (Girvan-\\nNewman [11], Greedy Modularity [7, 27], Label Propagation [8], DORF [5], NDORF [1] and\\nNDSRF [15]) across three real-world network datasets using di fferent metrics. The results in\\nTable 2 demonstrate the e ffectiveness of our algorithm in detecting communities in real-world\\nscenarios.\\nTable 2: Performance of various algorithms on real-world networks\\nMethods\\\\Networks Karate club Football Facebook\\nARI NMI Q ARI NMI Q ARI NMI Q\\nGirvan Newman 0.77 0.73 0.48 0.14 0.36 0.50 0.03 0.16 0.01\\nGreedy Modularity 0.57 0.56 0.58 0.47 0.70 0.82 0.49 0.68 0.55\\nLabel Propagation 0.38 0.36 0.54 0.75 0.87 0.90 0.39 0.65 0.51\\nDORF 0.59 0.57 0.69 0.93 0.94 0.91 0.67 0.73 0.68\\nNDORF 0.59 0.57 0.69 0.93 0.94 0.91 0.68 0.73 0.68\\nNDSRF 0.59 0.57 0.68 0.93 0.94 0.91 0.68 0.73 0.68\\nRho 0.77 0.68 0.82 0.89 0.92 0.90 0.64 0.72 0.63\\nRhoN 0.77 0.68 0.84 0.89 0.93 0.92 0.69 0.72 0.93\\nAs shown in Table 2, in the Karate network, the Rho, RhoN algorithms are slightly less\\neffective than the Girvan Newman algorithm, but only in the NMI metric with a di fference of\\n0.05. When compared to other algorithms, RhoN stands out in the ARI and Q metrics. No-\\ntably, RhoN achieves a significantly higher modularity score, averaging 0.3 points higher than\\nthe Girvan-Newman, Greedy Modularity, and Label Propagation algorithms. It also surpasses\\nthe DORF, NDORF, and NDSRF algorithms by an average of 0.15 points in modularity.\\nIn medium-sized networks such as Football and larger networks like Facebook, the Ricci\\nflow-based algorithms, Rho and RhoN, surpass the three traditional non-neural network-\\nbased machine learning methods across all three metrics. In our findings, the Q-values are\\nsignificantly superior, averaging 0.4 points higher than traditional non-neural network-based\\nmachine learning methods and 0.25 points higher than Ricci flow-based algorithms. These\\n18algorithms e ffectively capture the community topology of the network and demonstrate su-\\nperior community detection capabilities regardless of network scale.\\nIf for any three nodes x,yandz, there always holds d(x,y)‚â§d(x,z)+d(y,z), where d\\nis the distance function defined by (2.1), then the network is called satisfying the triangle\\ninequality condition. Compared with other geometry-based discrete Ricci flow algorithms,\\nsuch as DORF, NDORF, and NDSRF, our algorithms are particularly well-suited for networks\\nwithout the triangle inequality condition.\\n4.2.2. Artificial datasets\\nStochastic Block Model (SBM) [14] is a model to generate networks with blocks (com-\\nmunities) randomly. A network generated by SBM is consist with nnodes, which can be\\ndivided into kblocks, hence there are ninodes in the ith block Bi. The edges in the graph\\nwill be assigned by a probability matrix Pk√ók=(pi j), where pi jpresents the edge density from\\nBitoBj.pintraare usually used as the probability of edges connecting two nodes in the same\\nblock, while pinterfor nodes in di fferent blocks. Namely, pintra:=pii,pinter:=pi j,i,j, for\\ni,j=1,..., k. For networks with implicit community structures, it is necessary that pintrais\\ngreater than pinter, indicating that there are denser connections within the same community\\nand sparser connections between di fferent communities. The upcoming experiments will\\nmaintain this implicit community structure.\\nThe synthetic datasets consist of two sets of SBM benchmark networks with di fferent pa-\\nrameters, D1 and D2. The specific parameters and settings are detailed in Table 3, with any\\nunspecified parameters set to the default values of the SBM tool. To strike a balance between\\ncomputational speed and intra-community edge density, pintrais fixed at 0.15. The D1 net-\\nwork is designed to study the e ffects of varying pintervalues on the accuracy of the algorithm.\\nThe mixing parameter pinterindicates the strength of connections between communities; the\\nlarger the value, the denser the connections between nodes of di fferent communities, result-\\ning in more compact communities, a less clear community structure, and a more challenging\\ntask for the algorithm to correctly identify the communities.\\nThe D2 network, on the other hand, focuses on studying the e ffects of di fferent network\\nsizes on the accuracy of the algorithm. A larger size value corresponds to a larger network\\nscale.\\nTable 3: Parameter settings for artificial data\\nParameter Description D1 D2\\nsize Number of nodes 500 500-2000\\nk #Communities 2 2-8\\npintra pii 0.15 0.15\\npinter pi j 0.01-0.10 0.05\\nseed Random seed 0 0\\ndirected Whether the edges are directed False False\\nselfloops Whether self-loops are allowed False False\\nIn a series of synthetic datasets, we demonstrate the scalability and e ffectiveness of our\\nalgorithms based on Rho and RhoN respectively, by comparing its overall performance with\\nthe comparison algorithms Girvan Newman [11], Greedy Modularity [7, 27], and Label Prop-\\nagation [8], under di fferent metrics. Since we find that DORF, NDORF, and NDSRF have\\nsimilar performance to Rho and RhoN, we only compare our algorithms with Girvan New-\\nman, Greedy Modularity and Label Propagation. The experimental results are shown in the\\nfigures below. To minimize the impact of randomness, each data point in the figure represents\\nan average of 10 runs.\\n19(a)\\n(b)\\n(c)\\nFigure 4: Comparing di fferent models on SBM networks (varying pinter).\\n20(a)\\n(b)\\n(c)\\nFigure 5: Comparing di fferent models on SBM networks (varying pinter).\\n21As the pintervalue increases, the pinter/pintravalue gradually increases, making the com-\\nmunity structure less apparent and the experiments more challenging. In Figure 4(a), it is\\nevident that Rho and RhoN show a consistent modularity about 0.8 across all pintervalues,\\nindicating a stable and robust detection of community structures even as inter-community\\nconnections increase. Greedy Modularity also exhibits stability, but with Modularity values\\naround 0.35, which are lower than those of Rho and RhoN. Conversely, Label Propagation\\nand Girvan-Newman show a significant decrease from 0.46 to 0 respectively in Modularity\\nvalues as the experimental complexity increases.\\nIn Figures 4(b) and 4(c), when p=0.01, the experiment presents minimal challenges, and\\nthe community structure is quite clear. In this scenario, both Rho and RhoN achieve ARI and\\nNMI values of 1.0, signifying that they accurately identify the network‚Äôs community struc-\\nture. As the experimental di fficulty increases, all algorithms show a decrease in performance,\\nyet Rho and RhoN consistently maintain the highest ARI and NMI scores across all levels.\\nIn general, Rho and RhoN excel compared to other algorithms across all three metrics,\\nparticularly in situations where the community structure is less distinct, indicating superior\\nrobustness.\\nIn Figure 5(a), Greedy Modularity and Label Propagation both initially have high modu-\\nlarity at k =2, with values around 0.14 and 0.28 respectively. However, Label Propagation\\nexperiences a sharp decline to approximately 0 at k =3, while Greedy Modularity stabilizes\\naround 0 for larger k. On the other hand, Girvan-Newman starts at 0.4 modularity at k =2\\nand maintains this level as k increases.\\nIn Figure 5(b), ARI initially starts at approximately 0.27 for k=2 but steadily decreases\\naskincreases, eventually nearing 0.05 for higher kvalues when using the Greedy Modularity\\nalgorithm. This trend indicates that Greedy Modularity becomes progressively less e ffective\\nat accurately identifying community structures as the number of communities grows. The\\nGirvan-Newman algorithm consistently exhibits poor performance, with ARI values remain-\\ning close to 0 across all kvalues, signifying its ine ffectiveness in community detection of\\nthe Stochastic Block Model. Similarly, Label Propagation demonstrates a sharp decline in\\nARI after k=2, stabilizing near 0, which suggests its inadequacy in handling an increasing\\nnumber of communities. In contrast, the Rho and RhoN algorithms maintain high and sta-\\nble ARI values around 0.5, reflecting their robust and consistent ability to detect community\\nstructures across varying numbers of communities.\\nSimilarly, Figure 5(c) reveals that NMI decreases with increasing kvalues in the Greedy\\nModularity algorithm, mirroring the trend observed in ARI and highlighting a decline in per-\\nformance as the number of communities rises. The Girvan-Newman algorithm consistently\\nshows low NMI values across all klevels, with only slightly improving from k=2 tok=3\\nbefore stabilizing at low values. Like its ARI performance, Label Propagation exhibits poor\\nNMI scores, approaching 0 as kincreases, further demonstrating its limitations. However, the\\nRho and RhoN algorithms continue to display relatively high and stable NMI values, ranging\\nfrom 0.4 to 0.5, underscoring their consistent performance in detecting communities across\\ndifferent kvalues.\\nTo summarize, though Greedy Modularity and Label Propagation algorithms are e ffective\\nfor networks with a limited number of communities, their performance tends to decline as\\nthe number of communities increases. On the other hand, the Girvan-Newman algorithm\\nconsistently underperforms across various metrics. In contrast, both algorithms based on Rho\\nand RhoN excel, achieving the highest scores in Modularity, ARI, and NMI, and they also\\nexhibit remarkable stability across networks with di fferent community sizes. This suggests\\nour algorithms are not only robust but also highly capable of accurately detecting community\\nstructures in networks with diverse scales and community distributions.\\n225. Conclusion\\nWe have developed two algorithms, which are based on a modified Ricci flow and a quasi-\\nnormalized Ricci flow, for weighted graph community detection. Our method significantly\\nimproves accuracy and stability compared to other widely used methods, namely, Girvan\\nNewman [11], Greedy Modularity [7, 27], and Label Propagation [8]. It outperforms others\\nin Modularity, ARI, and NMI metrics, showcasing its e ffectiveness for real-world applica-\\ntions. By cutting edges with large weights only at the final step, our method achieves stable,\\nreliable results, demonstrating robustness, especially in challenging scenarios where commu-\\nnity structures are less clear. Future work will aim to optimize these methods further and\\napply them to larger, more complex networks to enhance performance and applicability in\\ndiverse contexts.\\nAcknowledgements The authors appreciate Lai Xin for her patient and detailed explanation\\non this topic, especially on her doctoral dissertation [16]. They also thank Liu Shuang for\\nsharing her knowledge about Ricci curvature on connected weighted graph. This research\\nwas partly supported by Public Computing Cloud, Renmin University of China.\\nData availability All data needed are available freely. One can find the codes of our algo-\\nrithms at https: //github.com /mjc191812 /Modified-Ricci-Flow.\\nDeclarations\\nConflict of interest The authors declared no potential conflicts of interest with respect to the\\nresearch, authorship, and publication of this article.\\nEthics approval The research does not involve humans and /or animals. The authors declare\\nthat there are no ethics issues to be approved or disclosed.\\nReferences\\n[1] S. Bai, A. Huang, L. Lu, S. T. Yau, On the sum of ricci-curvatures for weighted graphs, Pure Appl. Math.\\nQuart. 17 (2021) 1599-1617.\\n[2] S. Bai, Y . Lin, L. Lu, Z. Wang, S. T. Yau, Ollivier ricci-flow on weighted graphs, arXiv: 2010.01802v8, 2024.\\n[3] S. S. Bhowmick, B. S. Seah, Clustering and summarizing protein-protein interaction networks: a survey, IEEE\\nTrans. Knowl. Data Eng. 28 (2015) 638-658.\\n[4] S. Brendle, R. Schoen, Manifolds with 1 /4-pinched curvature are space forms, J. Amer. Math. Soc. 22 (2009)\\n287-307.\\n[5] C. C. Ni, Y . Y . Lin, F. Luo, J. Gao, Community detection on networks with ricci flow, Sci. Rep. 9 (2019) 9984.\\n[6] C. C. Ni, Y . Y . Lin, F. Luo, J. Gao, X. Gu, E. Saucan, Ricci curvature of the internet topology, 2015 IEEE\\nConf. Comput. Commun. INFOCOM (2015) 2758-2766.\\n[7] A. Clauset, M. Newman, C. Moore, Finding community structure in very large networks, Phys. Rev. E 70\\n(2004) 066111.\\n[8] G. Cordasco, L. Gargano, Community detection via semi-synchronous label propagation algorithms, BASNA\\n2010 (2010) 1-8.\\n[9] L. Danon, A. D ¬¥ƒ±az-Guilera, J. Duch, A. Arenas, Comparing community structure identification, J. Stat. Mech.\\nTheory Exp. 2005 (2005) P09008.\\n[10] S. Fortunato, Community detection in graphs, Phys. Rep. 486 (2010) 75-174.\\n[11] M. Girvan, M. E. J. Newman, Community structure in social and biological networks, Proc. Natl. Acad. Sci.\\n99 (2002) 7821-7826.\\n[12] R. Hamilton, Three-manifolds with positive ricci curvature, J. Di ffer. Geom. 17 (1982) 255-306.\\n[13] L. Hubert, P. Arabie, Comparing partitions, J. Classif. 2 (1985) 193-218.\\n[14] B. Karrer, M. Newman, Stochastic blockmodels and community structure in networks, Phys. Rev. E Stat.\\nNonlin. Soft Matter Phys. 83 (2011) 016107.\\n[15] X. Lai, S. Bai, Y . Lin, Normalized discrete Ricci flow used in community detection, Phys. A 597 (2022)\\n127251.\\n23[16] X. Lai, The applications of discrete Ricci curvature and Ricci flow in graph data analysis, Doctoral dissertation,\\nRenmin University of China, 2023.\\n[17] J. Leskovec, SNAP datasets: Stanford large network dataset collection, http: //snap.stanford.edu /data, 2014.\\n[18] J. Leskovec, K. J. Lang, M. Mahoney, Empirical comparison of algorithms for network community detection,\\nProc. 19th Int. Conf. World Wide Web 2010 (2010) 631-640.\\n[19] Y . Lin, L. Lu, S. T. Yau, Ricci curvature of graphs, Tohoku Math. J. 63 (2011) 605-627.\\n[20] F. M ¬®unch, R. K. Wojciechowski, Ollivier ricci curvature for general graph laplacians: heat equation, laplacian\\ncomparison, non-expansion and diameter bounds, Adv. Math. 356 (2019) 11.\\n[21] M. Newman, Modularity and community structure in networks, Proc. Natl. Acad. Sci. 103 (2006) 8577-8582.\\n[22] M. Newman, Networks: an introduction, Oxford Univ. Press, 2010.\\n[23] Y . Ollivier, Ricci curvature of markov chains on metric spaces, J. Funct. Anal. 256 (2009) 810-864.\\n[24] Y . Ollivier, Ricci curvature of metric spaces, C. R. Math. 345 (2007) 643-646.\\n[25] L. Peel, D. Larremore, A. Clauset, The ground truth about metadata and community detection in networks,\\nSci. Adv. 3 (2016) 08.\\n[26] G. Perelman, The entropy formula for the ricci flow and its geometric applications, arXiv: math /0211159,\\n2002.\\n[27] J. Reichardt, S. Bornholdt, Statistical mechanics of community detection, Phys. Rev. E 74 (2006) 016110.\\n[28] S. Tauro, C. Palmer, G. Siganos, et al., A simple conceptual model for the internet topology, GLOBECOM‚Äô01\\nIEEE Global Telecommun. Conf. 3 (2001) 1667-1671.\\n[29] G. Wang, Z. Zhou, S. Zhu, S. Wang, Ordinary di fferential equations (in Chinese), Higher Education Press,\\n2006.\\n[30] S. Wasserman, K. Faust, Social network analysis: methods and applications, Cambridge University Press,\\n1994.\\n[31] Z. Yang, R. Algesheimer, C. Tessone, A comparative analysis of community detection algorithms on artificial\\nnetworks, Sci. Rep. 6 (2016) 30750.\\n[32] W. Zachary, An information flow model for conflict and fission in small groups, J. Anthropol. Res. 33 (1977)\\n452-473.\\n24',\n",
       " 'SMART-TBI: Design and Evaluation of the Social Media\\nAccessibility and Rehabilitation Toolkit for Users with Traumatic\\nBrain Injury\\nYaxin Hu*\\nDepartment of Computer Sciences\\nUniversity of Wisconsin‚ÄìMadison\\nyaxin.hu@wisc.eduHajin Lim*\\nDepartment of Communication\\nSeoul National University\\nhajin@snu.ac.krLisa Kakonge\\nSchool of Rehabilitation Science\\nMcMaster University\\nkakongel@mcmaster.ca\\nJade T. Mitchell\\nDept. of Hearing & Speech Sciences\\nVanderbilt University Medical Center\\njade.t.mitchell@vanderbilt.eduHailey L. Johnson\\nDepartment of Computer Sciences\\nUniversity of Wisconsin‚ÄìMadison\\nhljohnson22@wisc.eduLyn S. Turkstra\\nSchool of Rehabilitation Science\\nMcMaster University\\nturkstrl@mcmaster.ca\\nMelissa C. Duff\\nDept. of Hearing & Speech Sciences\\nVanderbilt University Medical Center\\nmelissa.c.duff@vanderbilt.eduCatalina L. Toma\\nDepartment of Communication Arts\\nUniversity of Wisconsin‚ÄìMadison\\nctoma@wisc.eduBilge Mutlu\\nDepartment of Computer Sciences\\nUniversity of Wisconsin‚ÄìMadison\\nbilge@cs.wisc.edu\\nABSTRACT\\nTraumatic brain injury (TBI) can cause a range of cognitive and com-\\nmunication challenges that negatively affect social participation\\nin both face-to-face interactions and computer-mediated commu-\\nnication. In particular, individuals with TBI report barriers that\\nlimit access to participation on social media platforms. To improve\\naccess to and use of social media for users with TBI, we introduce\\nthe Social Media Accessibility and Rehabilitation Toolkit ( SMART-\\nTBI). The toolkit includes five aids (Writing Aid, Interpretation Aid,\\nFilter Mode, Focus Mode, and Facebook Customization) designed to\\naddress the cognitive and communicative needs of individuals with\\nTBI. We asked eight users with moderate-severe TBI and five TBI\\nrehabilitation experts to evaluate each aid. Our findings revealed\\npotential benefits of aids and areas for improvement, including the\\nneed for psychological safety, privacy control, and balancing busi-\\nness and accessibility needs; and overall mixed reactions among\\nthe participants to AI-based aids.\\nCCS CONCEPTS\\n‚Ä¢Social and professional topics ‚ÜíPeople with disabilities ;‚Ä¢\\nHuman-centered computing ‚ÜíAccessibility technologies .\\nKEYWORDS\\nTraumatic Brain Injury (TBI), Accessibility, Social Media, Facebook\\nPermission to make digital or hard copies of part or all of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for third-party components of this work must be honored.\\nFor all other uses, contact the owner/author(s).\\nASSETS ‚Äô24, October 27‚Äì30, 2024, St. John‚Äôs, NL, Canada\\n¬©2024 Copyright held by the owner/author(s).\\nACM ISBN 979-8-4007-0677-6/24/10.\\nhttps://doi.org/10.1145/3663548.3675641ACM Reference Format:\\nYaxin Hu*, Hajin Lim*, Lisa Kakonge, Jade T. Mitchell, Hailey L. Johnson,\\nLyn S. Turkstra, Melissa C. Duff, Catalina L. Toma, and Bilge Mutlu. 2024.\\nSMART-TBI: Design and Evaluation of the Social Media Accessibility and\\nRehabilitation Toolkit for Users with Traumatic Brain Injury. In The 26th\\nInternational ACM SIGACCESS Conference on Computers and Accessibility\\n(ASSETS ‚Äô24), October 27‚Äì30, 2024, St. John‚Äôs, NL, Canada. ACM, New York,\\nNY, USA, 20 pages. https://doi.org/10.1145/3663548.3675641\\nFigure 1: In this paper, we present the Social Media Accessibil-\\nity and Rehabilitation Toolkit (SMART-TBI) that consists of\\nfive aids designed to serve as communication and cognitive\\nsupport for individuals with TBI when using social media\\nplatforms. Eight users with TBI and five TBI rehabilitation\\nexperts evaluated our toolkit. The evaluation of these aids\\nshowed the usefulness of the aids as well as revealed usability\\nchallenges, informing our next steps in building accessible\\nsocial media platforms for users with cognitive and commu-\\nnication challenges.arXiv:2408.09683v1  [cs.HC]  19 Aug 2024ASSETS ‚Äô24, October 27‚Äì30, 2024, St. John‚Äôs, NL, Canada Hu and Lim, et al.\\n1 INTRODUCTION\\nTraumatic brain injury (TBI) is a significant public health concern,\\naffecting approximately 69 million individuals every year world-\\nwide [ 30,66]. TBI refers to damage caused to the brain as a result\\nof an external force, and typically occurs through falls, car acci-\\ndents, sports injuries, or assaults [ 38]. TBI can vary in severity from\\nmild to severe, limiting an individual‚Äôs functioning and leading to\\nchronic cognitive, physical, and emotional impairments. Among\\nthese, cognitive and communication challenges are particularly\\ndebilitating, often interfering with an individual‚Äôs ability to engage\\nin everyday activities and social interactions [49].\\nAdults with TBI often report social isolation [ 53] and friendship\\nloss [ 55] after injury. They may experience physical and cogni-\\ntive limitations that make in-person social interactions challenging\\n[42,44,64]. Thus, individuals with TBI could especially benefit from\\nthe social connection opportunities provided by computer-based\\ncommunication technologies. Computer-mediated communication\\n(CMC) is the use of social media, texting, or email to communi-\\ncate with others, and is ubiquitous in today‚Äôs society [ 47]. Social\\nmedia platforms have revolutionized how people establish social\\nconnections, collaborate, participate in social events, and obtain\\ninformation in daily life [ 34,39,41,45]. Previous literature showed\\nthat social media use can enhance users‚Äô social capital [ 34], con-\\ntribute to friendship maintenance [ 58], and stimulate social sharing\\n[23,24], all of which could benefit adults with TBI. There is evidence\\nthat people with TBI use social media platforms such as Facebook\\nand Twitter as frequently as those without a brain injury [ 14] and\\nmay even prefer these online interactions to face-to-face commu-\\nnication [ 62]. Yet, the benefits of social media may not be fully\\naccessible to these individuals due to their cognitive and communi-\\ncation impairments [ 62]. These impairments can include cognitive\\noverload, which makes processing information more challenging,\\nand difficulties interpreting social cues, which are crucial for effec-\\ntive online communication [ 12,14,15,35,63]. While prior research\\nin this area has provided critical information about social media\\nusage among individuals with TBI [ 12,15,18,52] and identified\\ntheir challenges and needs [ 2,16,17], there is limited research on\\nhow to overcome those challenges and make social media accessible\\nfor individuals with TBI [12, 56].\\nTo address this gap, we designed and built SMART-TBI (Social\\nMedia Accessibility and Rehabilitation Toolkit for Traumatic\\nBrain Injury) , a suite of digital accessibility aids that aim to support\\nadults with TBI-related cognitive and communication challenges\\nso they can successfully use social media platforms. Our choice\\nof accessibility aids was based on our prior collaborative research\\nwith adults with TBI, in which users envisioned social media ac-\\ncessibility supports [ 2,48,68]. SMART-TBI consists of five types of\\naids: Writing Aid, Interpretation Aid, Filter Mode, Focus Mode, and\\nFacebook Customization. The toolkit was designed using Facebook\\nbecause it was the most actively used social media platform among\\nindividuals with TBI [ 52], and can be easily integrated into a user‚Äôs\\ncurrent social media practices.\\nWe evaluated each aid in the toolkit with both users with TBI\\nand rehabilitation experts specializing in TBI. During the user eval-\\nuation, we asked eight Facebook users with TBI to perform a series\\nof tasks on the Facebook platform, both with and without using theaids. They also completed questionnaires to assess the usability and\\nintention to use each aid and participated in interviews to provide\\nfeedback on potential improvements. Subsequently, we presented\\nthe SMART-TBI to five TBI experts and solicited their feedback\\non each aid with a questionnaire derived from the W3C Cognitive\\nAccessibility Guidelines, which outline requirements and recom-\\nmendations for making web content more accessible to people with\\ncognitive and learning disabilities [26].\\nOur findings revealed the potential benefits of the toolkit in\\naddressing diverse cognitive and communication challenges that in-\\ndividuals with TBI may encounter on social media platforms, while\\nalso indicating areas for improvement for each aid. In particular,\\nthe results highlighted SMART-TBI‚Äôs potential to enhance social\\ncommunications across various aspects, including self-presentation,\\norganized use of social media, and distraction reduction. The re-\\nsults also shed light on design implications for future accessible\\nsocial media design, emphasizing the need to promote psychological\\nsafety and privacy control, balance business profits with accessibil-\\nity needs, and address mixed reactions to AI-based aids for toolkit\\nadoption among individuals with diverse TBI needs.\\nOur contributions are as follows:\\n‚Ä¢Toolkit design and development : We designed and imple-\\nmented the SMART-TBI that could be easily integrated into\\nFacebook platforms.\\n‚Ä¢Insights for accessibility toolkit for individuals with\\nTBI: We evaluated the SMART-TBI with both users with\\nTBI and TBI rehabilitation experts. Our findings highlighted\\nboth positive feedback and areas of improvement for each\\naid, offering design insights for the development and imple-\\nmentation of future accessible social media platforms for\\nindividuals with TBI.\\n2 RELATED WORK\\n2.1 Cognitive and Social Communication\\nChallenges of Individuals with TBI\\nIndividuals with TBI face a myriad of chronic cognitive, commu-\\nnication, and social cognitive challenges [ 54,57,61]. Cognitive\\nchallenges from TBI may include difficulties in reasoning, attention\\nand concentration, problem-solving skills, memory, and executive\\nfunctions [ 57]. In particular, impairments in executive functions,\\nsuch as inhibitory control ( i.e., the ability to manage one‚Äôs atten-\\ntion, thoughts, and behaviors to perform a necessary task) and\\nworking memory ( i.e., holding information in mind and mentally\\nmanipulating it), can result in diminished focus and attention [ 31].\\nIn particular, cognitive-communication difficulties refer to chal-\\nlenges in communication related to language comprehension and\\nproduction [ 49]. Individuals with cognitive-communication chal-\\nlenges may struggle with speaking, word finding, understanding\\nlanguage, or expressing their thoughts effectively. For example,\\nan individual with cognitive-communication difficulty might miss\\nkey details in written correspondences or repeat information [32].\\nThese difficulties can lead to frustration and social isolation, making\\nit harder for individuals to engage in meaningful interactions with\\nothers [60].\\nSocial communication, which relies on social cognition and lan-\\nguage skills to engage in meaningful conversations across variousSMART-TBI ASSETS ‚Äô24, October 27‚Äì30, 2024, St. John‚Äôs, NL, Canada\\nsocial settings, is often impaired in individuals with TBI [ 36,54,59].\\nSocial cognition is crucial for interpreting social cues and communi-\\ncating effectively [ 19,57] involving recognizing emotions, predict-\\ning behaviors, and understanding others‚Äô intentions, encompassing\\ncomponents like the theory of mind and empathy [19].\\nAdditionally, many individuals with TBI struggle with behav-\\nioral self-regulation, including emotional modulation and impulse\\ncontrol [ 51]. These difficulties together could lead to reduced social\\nparticipation and lower life satisfaction [27].\\n2.2 Social Media Use and Individuals with TBI\\nIndividuals with traumatic brain injuries could benefit from so-\\ncial media platforms in mitigating social isolation. For instance,\\nsocial media may lessen the cognitive, communication, and social\\ndemands of face-to-face interactions by providing more time to\\nprocess information, formulate responses, and engage at their own\\npace without the immediate pressure of real-time conversation [ 15].\\nSocial media can also help individuals with TBI connect with others\\nwho have similar lived experiences and exchange social support\\n[15,52]. Promisingly, prior research showed that individuals with\\nTBI maintain social media accounts at similar rates as healthy indi-\\nviduals [ 52,63] and are highly interested in using social media for\\nvarious purposes, including social connection [13, 15, 52].\\nNevertheless, individuals with TBI may also encounter various\\nchallenges when using social media due to their cognitive ( e.g.,\\nattention, memory) and cognitive-communication ( e.g., process-\\ning written information) impairments. One significant challenge is\\nnavigating the varied interfaces and features of social media plat-\\nforms [ 48]. The complexity of these interfaces can be overwhelm-\\ning for individuals with TBI, and the abundance of information\\ncan be difficult to process [ 13]. Due to changes in cognitive func-\\ntion, individuals with TBI may experience difficulties in expressing\\nthemselves online, leading to reduced confidence in communica-\\ntion [ 13,48]. Additionally, individuals with TBI may experience a\\ndecreased ability to understand others‚Äô sentences or to read texts,\\nwhich can further compound their social media challenges [ 37,48].\\nFor example, an individual with TBI with impaired social cognition\\nmay misinterpret a friend‚Äôs sad post and comment with laughing\\nemojis, failing to recognize the emotional context of the message\\n[25]. These challenges highlight the importance of providing suffi-\\ncient resources to support individuals with TBI in navigating and\\ntaking advantage of social media platforms.\\n2.3 Accessibility Support for Social Media\\nAlthough Internet usage is common among individuals with TBI,\\nthere are still notable technological and access barriers in compar-\\nison to the general population [ 5], limiting their ability to fully\\ntake advantage of the social benefits of social media platforms [ 52].\\nTherefore, accessibility support for social media is crucial for indi-\\nviduals with TBI to access and utilize these platforms effectively. To\\nimprove social media accessibility for people with traumatic brain\\ninjuries, a recent study by Lim et al . [48] adopted a participatory\\ndesign approach to gather insights on technological tools that could\\nimprove the social media experience of users with TBI. Brunner\\nand colleagues Brunner et al . [16] developed an online training\\ncourse as part of the ‚ÄúSocial Brain Toolkit‚Äù to support people withacquired brain injury to learn about social media use. Furthermore,\\nZhao et al . [68] proposed the design of four social media support\\naids to address challenges of sensory overload, memory loss, social\\ncommunication, and lack of confidence in using social media faced\\nby users with TBI.\\nResearch on individuals with cognitive and physical disabilities\\nhas revealed benefits and challenges of social media use similar to\\nthose experienced by people with TBI [ 6,7,21]. Common challenges\\ninclude cognitive challenges, limited digital literacy, communica-\\ntion barriers, and the complexity of online interactions [ 3,21]. To\\naddress accessibility issues that stem from these challenges, re-\\nsearchers have developed solutions such as ‚ÄúEndeavor Connect,‚Äù a\\ncognitively accessible Facebook interface designed for young adults\\nwith intellectual disabilities [29].\\nAs such, this work builds on a rich body of previous research on\\nsocial media accessibility support. We designed and implemented\\nSMART-TBI as web browser extensions that work on the web ver-\\nsion of Facebook, providing users with essential support for social\\nmedia use in their everyday lives. The SMART-TBI can be easily\\ninstalled, offering a practical solution to the challenges faced by\\nindividuals with TBI when using social media platforms. While\\ncurrently focused on Facebook, our approach has the potential to\\nextend across various social media platforms.\\n3 SYSTEM DESIGN - ACCESSIBILITY SOCIAL\\nMEDIA TOOLKIT FOR TBI USERS\\nWe designed and built a social media toolkit to meet the accessi-\\nbility needs and challenges of TBI users that had been identified\\nin prior literature [ 16,29,48,68]. We categorized the major chal-\\nlenges in social media use by individuals with TBI into two types:\\ncommunication challenges and cognitive challenges.\\nCommunication challenges included challenges related to impair-\\nments in social cognition, language comprehension, and language\\nproduction. These challenges might lead users with TBI to mis-\\ninterpret the tone or intent of a written post and take sarcasm\\nor humor literally, write messages that readers would consider\\ninappropriate to the context, or overshare personal information.\\nCognitive challenges in using social media are mostly related to\\nimpairments in executive functions, leading to challenges in main-\\ntaining focus, planning, and managing information overload. As\\na result, users with TBI might struggle to organize their thoughts\\ncoherently in a post, leading to fragmented or confusing content.\\nSimilarly, challenges with planning could lead to impulsive posting\\nwithout considering the consequences or the appropriateness of\\nthe content for a public platform. Information overload could have\\nusers with TBI become overwhelmed by details and irrelevant in-\\nformation surrounding posts‚Äîincluding text in sidebars‚Äîand give\\nup on reading or posting content.\\nIn addressing these challenges, we identified several design goals\\nthat we realized in five types of aids as shown in Figure 2. In partic-\\nular, to address the communication challenges, we designed aids\\nto help users comprehend social media content more accurately,\\nenhance message construction, and minimize the creation of inap-\\npropriate content ( e.g., offensive posts) [ 48]. We identified three\\ndesign goals: (1) minimizing inappropriate language use; (2) sup-\\nporting sentiment comprehension; and (3) improving grammar andASSETS ‚Äô24, October 27‚Äì30, 2024, St. John‚Äôs, NL, Canada Hu and Lim, et al.\\nFigure 2: Design process for the SMART-TBI. Our designs were motivated by the social media challenges and needs by users\\nwith TBI identified in the prior work. Focusing on communication and cognitive challenges, we proposed design goals to\\novercome these challenges and generate the design of the SMART-TBI. Left: a series of challenges of social media use faced by\\nindividuals with TBI; Middle: design goals to overcome these accessibility challenges; Right: five aids to provide communication\\nsupport and cognitive support for social media use.\\nspelling. To address cognitive challenges, aids must assist users\\nwith TBI in managing information, facilitate navigation through\\nsocial media features, and minimize distractions while viewing con-\\ntent [ 48]. Accordingly, we established four design goals to address\\nthese cognitive challenges: (4) simplifying social media features;\\n(5) personalizing social media features; (6) simplifying newsfeed\\ncontent, and (7) and personalizing newsfeed content.\\nBased on these design goals, we developed five accessibility\\naids to assist individuals with TBI in using social media (Figure 3).\\nTwo aids were designed to provide communication support: (1) the\\nWriting Aid , which enabled users to perform four writing checks\\nbefore posting on their Facebook feed, and (2) the Interpretation Aid ,\\nwhich helped users interpret social cues (focusing on sentiment\\nand emotion) within Facebook posts. Three aids were designed to\\nprovide cognitive support: (3) the Filter Mode , which allowed usersto customize their Facebook feed; (4) the Focus Mode , which declut-\\ntered the Facebook news feed; and (5) the Facebook Customization ,\\ndesigned to customize the Facebook layout. All five aids were im-\\nplemented as Google Chrome extensions for use within the web\\nversion of Facebook. We detail the design and implementation of\\neach aid in the following section. Demonstrations and implementa-\\ntion of each aid are provided in the GitHub repository1.\\n1https://smart-tbi.github.io/index.html\\nhttps://github.com/smart-tbi/smart-tbiSMART-TBI ASSETS ‚Äô24, October 27‚Äì30, 2024, St. John‚Äôs, NL, Canada\\n3.1 Communication Support\\n3.1.1 Writing Aid. The goal of the Writing Aid was to help users\\nwith TBI compose postings that convey their intentions and mean-\\ning effectively and in socially appropriate ways by giving them\\nfeedback on various aspects of their post writing. In particular, the\\nWriting Aid performed four types of writing checks, including (1)\\npotential spelling or grammatical errors in their post; (2) potential\\ntoxicity within the language of the post; (3) the tone ( e.g., positive,\\nneutral, negative) and emotion type ( e.g., happy, sad) of the post;\\nand (4) the privacy settings of the post ( e.g., public versus private).\\nThe detection of grammar errors, sentiment, and toxicity in the\\nposts was achieved through external application programming in-\\nterfaces (APIs) ( i.e., Textgears API1for grammar check, IBM Watson\\nNLP API2for sentiment and emotion detection, and perspective\\nAPI for toxicity detection3).\\nThe Writing Aid interface starts to appear on the screen after\\nusers begin writing a post. Once they write a draft of their post, the\\naid guides users through the four writing checks, allowing them to\\nrecheck each step after any updates are made. After completing all\\nfour checks, the aid provides a full summary, including all writing-\\ncheck results, followed by an opportunity for the user to review the\\nchanges to their post prior to posting the final draft. The accuracy\\nof sentiment and emotion analysis was reported between 73%‚Äì85%\\n[1,20], and the AUC-ROC scores [ 8] of the toxicity detection was\\nreported between 0.97‚Äì0.99.4While the spell-checking function-\\nality in our Writing Aid powered by the Textgears API may not\\ndiffer fundamentally from default browser spellcheckers, our goal\\nwas to provide a centralized, step-by-step approach to address the\\nvarious considerations involved in writing posts. For individuals\\nwith TBI, navigating multiple aspects of writing, such as basic spell\\nchecking, toxicity and tone management, and privacy settings, can\\nbe overwhelming and prone to errors when distributed across dif-\\nferent tools. Therefore, we consolidated these considerations into a\\nsingle, guided process within the Writing Aid.\\n3.1.2 Interpretation Aid. The Interpretation Aid was designed to\\nhelp users understand the meanings and sentiments other users\\nintend to convey in their Facebook posts. This aid used the same\\nexternal API as the Writing Aid (IBM Watson NLP API5) to extract\\nthe emotion and sentiment of individual Facebook posts. Addition-\\nally, the alt texts of post images in the posts were extracted and\\nshown to display the image details.\\nWhile individuals with TBI are scrolling through their Facebook\\nfeed with this aid, an ‚ÄúAnalyze‚Äù button appears alongside every\\npost. If the user clicks the button, the Interpretation Aid interface\\nappears beside the post that summarizes the sentiment analysis of\\nthe post, including the tone and emotion type. It also shows users\\nthe types of media used within the post ( i.e., images, videos, links)\\nand displays the alt text of images.\\n1https://textgears.com/\\n2https://www.ibm.com/products/natural-language-understanding\\n3https://perspectiveapi.com/\\n4https://developers.perspectiveapi.com/s/about-the-api-model-cards\\n5https://www.ibm.com/products/natural-language-understanding3.2 Cognitive Support\\n3.2.1 Filter Mode. The Filter Mode aid was designed to help users\\ncustomize their Facebook feed so that they only see preferred posts.\\nThe goal of this aid was to create a curated feed tailored to the\\nuser‚Äôs interests, filtering out undesired or distressing content.\\nOnce users activate the Filter Mode, a gray options bar appears at\\nthe bottom of the Facebook screen. This bar contains four filtering\\noptions for users to choose from. The \"Newsfeed Order\" drop-down\\noption allows users to view their Facebook newsfeed either in the\\ndefault algorithmic order or chronologically based on the time of\\nposting. The \"Source\" option lets users select the source of posts,\\nsuch as personal posts, group posts, or page posts. The \"Contains\"\\noption enables users to display posts that contain images, videos, or\\nlinks. The ‚ÄúSentiment‚Äù option allows users to filter their newsfeed\\nto show only positive posts. Finally, there is an option to reset all\\nprevious filter choices.\\n3.2.2 Focus Mode. The Focus Mode aid was designed to declutter\\nthe Facebook interface and help users limit their information intake\\nby only focusing on the newsfeed. We had an initial design of the\\naid and we updated design based on the usability test and user\\nfeedback from the user evaluation. The initial design eliminated\\nabstraction by showing only one post in the newsfeed at a time.\\nWhen activated, it creates a screen overlay that includes a single\\npost, an option to interact with the post via the ‚ÄúLike‚Äù button, and\\nan option to view the next post in the user‚Äôs feed. In the updated\\ndesign, the user can see the full newsfeed list and the remainder of\\nthe Facebook interface is blurred in the background to minimize\\npotential distractions.\\n3.2.3 Facebook Customization. The Facebook Customization aid\\nwas designed to streamline the visual interface, minimize clutter,\\nand optimize the navigation experience of the social media plat-\\nforms. It enabled users to toggle Facebook screen elements on and\\noff, including elements on the left menu of the homepage (menu\\nbar), the right section of the homepage (contact information), and\\nthe stories feed at the top of the website.\\n4 SYSTEM EVALUATION\\nTo assess the SMART-TBI‚Äôs potential usefulness and gather feedback\\nfor improvements, we conducted two system evaluation studies. In\\nthe first study, we recruited participants with TBI, and asked them\\nto perform tasks on Facebook, with and without the aids from the\\nSMART-TBI, and then provide feedback on each aid. For the second\\nstudy, we recruited rehabilitation professionals who were experts\\nin TBI, each of whom evaluated each aid. All study materials and\\nprotocols were administrated and approved by the University of\\nWisconsin-Madison Institutional Review Board (IRB).\\n4.1 Study 1: Feedback from users with TBI\\n4.1.1 Participants. Participants were eight adults with moderate-\\nto-severe TBI (3 women, 5 men; ùëÄ=32.88years,ùëÜùê∑=9.60). All\\nparticipants were from the continental US, native speakers of North\\nAmerican English, and recruited from a major hospital system\\nregistry [ 33]. Inclusion criteria consisted of: (1) self-identification\\nof English as a primary language; (2) no self-reported history of\\nmedical or neurological conditions, including brain diseases orASSETS ‚Äô24, October 27‚Äì30, 2024, St. John‚Äôs, NL, Canada Hu and Lim, et al.\\nFigure 3: An overview of the SMART-TBI. We developed two communication support aids and three cognitive support aids to\\nassist the social media use for individuals with TBI. Communication support aids are The Writing Aid andThe Interpretation\\nAid, and cognitive support aids are Filter Mode ,Focus Mode andFacebook Customization .SMART-TBI ASSETS ‚Äô24, October 27‚Äì30, 2024, St. John‚Äôs, NL, Canada\\nFigure 4: Procedure for Study 1: Feedback from users with TBI.\\npremorbid language or learning disabilities affecting cognition; (3)\\npossession of an active Facebook account; (4) knowledge of their\\nFacebook log-in information; and (5) regular usage of Facebook.\\nExclusion criteria consisted of: (1) age under 18 years or over 55\\nyears; and (2) an injury date less than six months from testing\\nfor participants with TBI. Participants older than 55 years were\\nexcluded to avoid the potential influence of cognitive changes and\\ncomorbid conditions associated with aging. Participants under 18\\nyears were excluded to minimize cohort effects, as adolescents were\\nlikely to use other social media platforms.\\nMedical records and intake interviews verified that the partic-\\nipants met the Mayo Classification System criteria for moderate-\\nsevere TBI [ 50]. Barin injuries were classified as moderate-severe if\\nat least one of the following criteria were met: (1) Glasgow Coma\\nScale (GCS) <13 within 24 hours of acute care admission; (2) pos-\\nitive neuroimaging findings (acute CT findings or lesions visible\\non a chronic MRI); (3) loss of consciousness (LOC) >30 minutes; or\\n(4) post-traumatic amnesia PTA >24 hours. Participants with TBI\\nwere all in the chronic phase of injury (>6 months post-injury), and\\nthe average time post-injury was 68 months ( ùëÜùê∑=76.22). Partici-\\npant demographic details, injury characteristics, and information\\non the presence of long-term cognitive deficits are presented in\\nTable 1. At the end of the study, each participant received $20 USD\\nas compensation for their participation.\\n4.1.2 Study Procedure. The study involving participants with TBI\\nwas conducted in a private lab space. Upon arrival, participants com-\\npleted consent and payment forms in REDCap (Research Electronic\\nData Capture), a secure, web-based software platform designed\\nto support data capture for research studies [ 40]. The lab space\\nwas equipped with an HDR video camera, a participant laptop, andan experimenter laptop. The participant laptops were set up with\\nFacebook open within a Chrome browser and a shared Google Doc\\nfor writing tasks. Each participant‚Äôs screen was shared in Zoom\\nso that the experimenter and other research team members could\\nmonitor and record the session. After the consent process, partici-\\npants were asked to sign into their personal Facebook account on\\nthe provided laptop. We then guided the following four stages in\\nthe study session (Figure 4).\\nStage 1 - Reflect on Facebook Usage. In Stage 1, participants were\\nasked to browse and use Facebook for ten minutes as usual without\\nusing aids to gather insights on their general Facebook experience.\\nWhile using Facebook for ten minutes, an experimenter asked the\\nparticipant questions to encourage reflection on their Facebook\\nexperience ( e.g., ‚ÄúCan you tell me your favorite part of Facebook‚Äù).\\nStage 2 - Social Media Tasks without Using the Aids. Within stage\\ntwo, participants were asked to perform the following three tasks\\non Facebook without using any aids.\\nInterpretation Task: The first task involved interpreting three\\nposts on the Facebook account, specifically created for this study\\n(Appendix A.2). After participants viewed each post, we asked open-\\nended questions to participants, such as ‚ÄúWhat emotions do you\\nthink of in this post?‚Äù They were also asked to rate the sentiment of\\neach post on a five-point Likert scale ( i.e., Very Negative, Negative,\\nNeutral, Positive, and Very Positive). Following completion of these\\ntasks, participants rated their confidence in their judgments and\\nease in performing these interpretation tasks by answering the\\ninterpretation task questions in Table 3.\\nFocus Task: Participants were instructed to browse and scroll\\nthrough their Facebook newsfeed for two minutes. Following that,\\nthe experimenter prompted them to recall and describe the postsASSETS ‚Äô24, October 27‚Äì30, 2024, St. John‚Äôs, NL, Canada Hu and Lim, et al.\\nTable 1: Demographic, injury, and Facebook usage information for participants with TBI.\\nID Age Edu Etiology of\\nTBITSO Race (Ethnic-\\nity)Sex Years\\non\\nFBFB usage\\npatternCognitive and Communicative Challenges post-TBI\\nP1 32 18 Ped vs. auto 68 White (Not His-\\npanic)F 16 Multiple\\ntimes a weekShort-term or long-term memory loss; trouble concentrating or paying\\nattention; Difficulty with language or speech production and thought\\nprocessing; difficulty organizing or problem-solving; impulsiveness and\\nlack of inhibition\\nP2 54 16 MVA 227 White (Not His-\\npanic)M 14 Multiple\\ntimes a dayShort-term or long-term memory loss\\nP3 36 16 Ped vs. auto 130 Black or African\\nAmerican (Not\\nHispanic)M 16 Multiple\\ntimes a dayShort-term or long-term memory loss; trouble concentrating or paying\\nattention; difficulty with language or speech production and thought\\nprocessing\\nP4 29 18 MVA 61 White (Not His-\\npanic)F 15 Daily Short-term or long-term memory loss; trouble concentrating or paying\\nattention; difficulty with language or speech production and thought\\nprocessing\\nP5 28 12 MVA 20 White (Hispanic) M 14 Multiple\\ntimes a dayShort-term or long-term memory loss; impaired judgment and percep-\\ntion; trouble concentrating or paying attention; difficulty with language\\nor speech production and thought processing; difficulty organizing or\\nproblem-solving\\nP6 26 12 MVA 13 White (Not His-\\npanic)M 13 Multiple\\ntimes a dayShort-term or long-term memory loss; impaired judgment and percep-\\ntion; trouble concentrating or paying attention; difficulty with language\\nor speech production and thought processing; difficulty organizing or\\nproblem-solving\\nP7 35 12 MVA 12 White (Not His-\\npanic)M 17 Multiple\\ntimes a dayNone reported\\nP8 23 12 MVA 13 Black or African\\nAmerican (Not\\nHispanic)F 7 Multiple\\ntimes a dayShort-term or long-term memory loss; difficulty with language or speech\\nproduction and thought processing\\nID = participant ID number. Education (edu) reflects years of highest degree obtained. MVA = motor vehicle accident. MCC includes both motorcycle and snowmobile\\naccidents. Ped vs. auto = participant was hit by a car while walking or running. Time since onset (TSO) is presented in months. F = female. M = male.\\nthey just viewed, using a few sentences ( e.g., ‚ÄúCan you recall the\\nposts you just browsed‚Äù). Participants were then asked to provide\\nfeedback on the task by answering Focus task questions in Table 3.\\nFilter Task: In the third task, participants were asked to browse\\ntheir Facebook newsfeed and inform the experimenter whenever\\nthey found an interesting post. Then, they were asked to refresh\\nthe page again and find a post from a friend2. Following this, par-\\nticipants were asked to provide feedback on the task by answering\\nFilter task questions in Table 3.\\nAfter completing the three tasks described above, the participants\\nwere asked to identify their top five most frequently used menus in\\nthe left panel of the Facebook web browser. The total duration of\\nstage 2 ranged from 20 to 30 minutes for each participant.\\nStage 3 - Social Media Tasks Using the SMART-TBI aids. During\\nstage 3, participants first repeated the same Interpretation Task, Fo-\\ncus Task, and Filter Task as in stage 2 using the aids: Interpretation\\nMode, Focus Mode, and Filter Mode, respectively. Before starting\\neach task, the experimenter introduced the aid that would be used\\nfor the task and demonstrated how to use it step by step. Then the\\nuser had a trial session using the aid until they felt comfortable and\\n2Based on our observations, Facebook always prioritizes unseen content at the top.\\nThus, we asked participants to refresh the newsfeed page before and between tasks.familiar with the tool. After the participant finished the trial session,\\nthe experimenter instructed the participant to proceed with the\\ntasks. Additionally, participants completed a Writing Task where\\nthe experimenter asked the participant to write three hypothet-\\nical posts on provided topics (Appendix A.1). Additionally, they\\nwere asked to use the Writing Aid to improve their writing. After\\nwriting each post, the participant was asked a series of questions\\nabout their experience in the writing task (see Table 5). Lastly, par-\\nticipants worked on the Facebook Customization Task that asked\\nthem to customize the layout of their Facebook main page using\\nthe Facebook Customization aid.\\nAfter each task, participants evaluated each aid with the System\\nUsability Scale (SUS) [ 11] and answered open-ended questions ( e.g.,\\n‚ÄúHow was your experience with the tool?‚Äù) to reflect on their im-\\npressions on each aid. After completing all five tasks in stage 3, the\\nparticipants were instructed to rank the five aids in order of most\\nhelpful to least helpful. The total duration of stage 3 ranged from\\n45 to 60 minutes.\\nStage 4 - Questionnaires: Demographic Information and Facebook\\nUsage. Stage 4 consisted of participants answering demographic\\nquestions ( e.g., gender, age, race, education, and employment status)SMART-TBI ASSETS ‚Äô24, October 27‚Äì30, 2024, St. John‚Äôs, NL, Canada\\nand questions related to their TBI ( e.g., time since injury, cause of in-\\njury, and challenges since injury). This stage ended with surveying\\nparticipants about their usual Facebook usage ( e.g., usage amount,\\nreasons for usage, changes to social media usage after their TBI).\\nThe study session for each participant lasted between 90 minutes\\nto two hours.\\n4.2 Study 2: Feedback from TBI Experts\\nThe SMART-TBI was envisioned not only as a standalone tool but\\nalso as a potential asset in therapy settings. We foresee its use in\\ntraining people with TBI to utilize social media platforms effectively\\nby TBI experts. Consequently, we sought TBI experts‚Äô views on\\nhow the SMART-TBI might impact current therapy practices in\\ncommunication and cognitive rehabilitation support.\\n4.2.1 Recruiting TBI Experts. We recruited a convenience sample\\nof rehabilitation professionals with expertise in brain injury, all\\nknown to the research team members. The rehabilitation profes-\\nsionals (ùëõ=5) were all speech-language pathologists who provided\\nrehabilitation support to either a pediatric or adult population of\\nindividuals with TBI, within community or rehabilitation center\\nsettings. The rehabilitation professionals had an average clinical\\nexperience of ùëÄ=11.6years (ùëÜùê∑=10.83), ranging from 3 to 30\\nyears (See Table 2).\\n4.2.2 Study Procedure. In the expert evaluation study, TBI reha-\\nbilitation professionals were invited to join a remote session via\\na Zoom link provided by the research team. A brief description\\nof the purpose of this study was followed by instructions on the\\nstudy procedures and the recording of sessions. After signing the\\nconsent form, we present participants with the W3C‚Äôs Cognitive\\nAccessibility Guidelines [ 26] and ask them to evaluate the current\\nweb version of Facebook based on the guideline. Under each ob-\\njective in the guideline, there is a list of the design requirements\\nwith checkboxes. For example, the objective five ‚ÄúHelp users focus‚Äù\\nhas four design requirements: (1) ‚ÄúLimit interruptions‚Äù; (2) ‚ÄúMake\\nshort critical paths‚Äù; (3) ‚ÄúAvoid too much content‚Äù; and (4) ‚ÄúProvide\\ninformation so a user can complete and prepare for a task.‚Äù Partici-\\npants checked out the boxes of the design requirement items if they\\nthought Facebook fulfilled the corresponding design requirement.\\nNext, we asked participants to share their web browser screens\\nand helped them install the five aids on their Chrome browsers. We\\nthen introduced each aid and demonstrated how to use it. Follow-\\ning that, participants used the aid to complete a task the aid was\\ndesigned for. For example, they used ‚ÄúFocus Mode‚Äù to browse the\\nnews feed and ‚ÄúWriting Aid‚Äù to compose a post. Following this, par-\\nticipants evaluated each aid with the W3C‚Äôs Cognitive Accessibility\\nGuidelines [ 26]. After evaluating each aid, participants filled out\\nthe questions regarding their experiences with TBI and provided\\ndemographic information. The study procedures lasted between 30\\nminutes to 1 hour, and participants received $50 USD in the form\\nof gift cards as compensation.\\n4.3 Data Analysis\\nWe employed a mixed-methods approach to collect and analyze\\nfive distinct sets of data described in Section 4: (1) interviews with\\nTBI users from Study 1; (2) task performance data for TBI usersfrom Study 1; (3) survey responses from TBI users from Study 1; (4)\\ninterviews with TBI experts from Study 2; and (5) survey responses\\nfrom TBI experts from Study 2.\\n4.3.1 Quantitative Analysis.\\nUser Evaluation: Aid Ranking and SUS. Participants were asked to\\nrank each aid from one to five and evaluate each aid with the System\\nUsability Scale (SUS). To accommodate the cognitive challenges\\nassociated with participants‚Äô TBI, we employed a simplified three-\\npoint Likert scale (Disagree, Neutral, and Agree) for each statement\\nin the SUS.\\nUser Evaluation: Tasks Feedback and Task Performance Measures.\\nWe developed questionnaires with a three-point Likert scale (1‚Äì3;\\n1 = Disagree, 2 = Neutral, 3 = Agree) for each aid (Table 3). For the\\nwriting task, we utilized a seven-item questionnaire and derived\\ntwo scales that measured the perceived quality of the written post\\n(items 1, 7; Cronbach‚Äôs ùõº=0.62) and how well the message is\\nreceived by other people (items 3, 5; Cronbach‚Äôs ùõº=0.61). For the\\nInterpretation task, we used a three-item questionnaire to evaluate\\nthe confidence level, the clarity of the post, and the ease of the\\ntask. For the Focus task and Filter task, we developed a five-item\\nquestionnaire and derived two scales that measured the perceived\\nease of the task (items 1, 2, 5; Cronbach‚Äôs ùõº=0.86) and success of\\nthe task (items 3, 4; Cronbach‚Äôs ùõº=0.94). We hypothesized that\\nthe user would perceive the writing task to have higher quality\\nusing the Writing aid than without the aid; the user would have\\na higher level of confidence in the Interpretation task using the\\nInterpretation aid than without the aid; the user would perceive\\nthe task to be easier and more successful using the Focus Mode and\\nFilter Mode than without the aids.\\nWe also collected and analyzed task performance data for the\\nFocus Mode and Filter Mode aids, and we evaluated their effec-\\ntiveness pre- and post-use of the aids. Specifically, we collected\\nthe number of posts recalled by the participant after viewing the\\nnewsfeed for two minutes before and after applying Focus Mode\\nto the newsfeed. We hypothesized that the user would recall more\\nposts using the Focus Mode than without the aid. We also counted\\nthe time the user found an interesting post and the time to locate a\\npost from their friend before and after they applied the Filter Mode\\nto customize the feed. We hypothesized that the user would spend\\nless time locating the posts using the Filter Mode than without the\\naid.\\nExpert Evaluation: W3C Survey Result. W3C‚Äôs Cognitive Acces-\\nsibility Guidelines contain eight objectives and each objective has\\na list of design requirements necessary to meet the objective. We\\ncustomized the W3C questionnaires for each aid and removed ir-\\nrelevant items. For example, ‚ÄúObjective 8: Support adaptation and\\npersonalization.‚Äù is not applied to the Writing Aid and the Inter-\\npretation Aid because these two aids were designed to provide\\ncommunication support rather than improve the personalization\\nof social media use. Therefore, Objective 8 was removed when\\nparticipants evaluated these two aids.\\nIn analyzing the expert questionnaires for the guidelines for\\nFacebook in general, as well as for the five aids, we calculated the\\npercentage of design patterns met for each objective. For example,\\nto determine if the ‚ÄúFocus mode‚Äù met the objective ‚ÄúHelp usersASSETS ‚Äô24, October 27‚Äì30, 2024, St. John‚Äôs, NL, Canada Hu and Lim, et al.\\nTable 2: Demographic Information of Traumatic Brain Injury (TBI) Expert Participants\\nExpert\\nIDTBI Experi-\\nenceNumber of\\nYearsAge Description Race Education\\nEP1 Daily 30 55-64 years old Female White or Cau-\\ncasianGraduate or professional degree (MA, MS, MBA,\\nPhD, JD, MD, DDS, etc.)\\nEP2 Daily 12 35-44 years old Female White or Cau-\\ncasianGraduate or professional degree (MA, MS, MBA,\\nPhD, JD, MD, DDS, etc.)\\nEP3 4-6 times a\\nweek8 25-34 years old Female Asian Graduate or professional degree (MA, MS, MBA,\\nPhD, JD, MD, DDS, etc.)\\nEP4 Once a week 5 35-44 years old Female White or Cau-\\ncasianGraduate or professional degree (MA, MS, MBA,\\nPhD, JD, MD, DDS, etc.)\\nEP5 Once a week 3 25-34 years old Female White or Cau-\\ncasianGraduate or professional degree (MA, MS, MBA,\\nPhD, JD, MD, DDS, etc.)\\nExpert ID = ID number assigned to the expert. TBI Experience = How often do you interact with people with TBI or develop/design technologies for people with TBI\\nor other individuals with cognitive challenges? Number of Years = How many years have you worked with people with TBI? Age= How old are you? Description\\n= How do you describe yourself? - Selected Choice. Race = Choose one or more races that you consider yourself to be. Education = What is the highest level of\\neducation you have completed?\\nTable 3: Questionnaire for user‚Äôs social media task feedback\\nWriting task\\n1. The message is very clear.\\n2. The message says what I mean to say.\\n3. The message will be well-received by others.\\n4. The message will receive many likes and comments.\\n5. The message will not offend other people.\\n6. Others will understand this message.\\n7. This message is well-written.\\nInterpretation task\\n1. I am confident in my interpretation of these posts.\\n2. What the writers intend to say is clear to me.\\n3. These posts were easy to understand.\\nFilter task & Focus task\\n1. The task I just did is simple.\\n2. The task I just did is mentally demanding.\\n3. I feel there is time pressure.\\n4. I did well on the task.\\n5. I feel frustrated with the task.\\nfocus,‚Äù we scored it as 50% when one participant answered ‚ÄúYes‚Äù\\nto two design patterns out of four. We also transcribed and open-\\ncoded the comments that expert participants provided while using\\neach of our five aids.\\n4.3.2 Qualitative Analysis.\\nInterviews with Users with TBI. We recorded the full study ses-\\nsions with users with TBI. Audio files were first transcribed with\\nan automatic transcription tool6, and then one researcher from\\nthe team verified the transcriptions and corrected errors. This re-\\nsearcher further segmented the transcriptions according to the\\nstudy procedures, differentiating between responses related to ques-\\ntionnaire items and answers given during the experimenter‚Äôs inter-\\nview questions.\\n6https://otter.aiWe analyzed the transcriptions using thematic analysis [ 9,10].\\nTwo coders first independently open-coded three data samples\\n(more than 10% of the data) at the sentence level and then merged\\ntheir codes to develop the initial codebook. Any disagreements dur-\\ning this phase were resolved through discussion. The same coders\\ncontinued to process the remaining data individually, updating\\nthe codebook as new codes emerged. The final codebook included\\ncategories detailing participants‚Äô social media usage patterns, the\\nchallenges they faced using social media due to TBI, and their feed-\\nback on each aid. Given we have specific goals to understand the\\nusefulness and usability for each aid in the toolkit, we follow the\\ndeductive approach [ 9,10] to generate themes focusing on particu-\\nlar aspects, i.e., the perceived usefulness, usability challenges, and\\nsuggested new functions for each aid.\\nInterviews with TBI experts. We had both online and in-person\\nstudy sessions with TBI expert participants. The online study ses-\\nsion was hosted through teleconferencing technology (Zoom), and\\nwe recorded the full study sessions. In one in-person study session,\\nthe experimenter experienced technical issues with audio recorders,\\nand the experimenter took field notes for the participant‚Äôs response.\\nThe interview data were transcribed using an automatic transcrip-\\ntion tool, and one researcher verified the accuracy of the transcripts.\\nWe followed the same approach of deductive thematic analysis as\\nthe analysis for interviews with TBI users as described above.\\n5 RESULTS\\nThis section presents the findings from our evaluation of the SMART-\\nTBI involving eight users with moderate-severe TBI and five TBI\\nrehabilitation experts. Both qualitative and quantitative results\\nhighlighted the strengths of each aid and the areas that require\\nimprovements.\\n5.1 Quantitative Results\\n5.1.1 User Evaluation: Aid Ranking and SUS. Our participants in\\nStudy 1 (users with TBI) ranked the five aids according to their\\npreference after the study session (Table 4). On average, the WritingSMART-TBI ASSETS ‚Äô24, October 27‚Äì30, 2024, St. John‚Äôs, NL, Canada\\nTable 4: User Ranking for Each Aid\\nP1 P2 P3 P4 P5 P6 P7 P8\\nWriting 4 1 1 1 1 3 4 4\\nInterpretation 5 4 3 2 3 2 3 3\\nFocus 1 3 5 5 4 5 5 1\\nFilter 3 2 4 3 5 4 2 2\\nFacebook Customization 2 5 2 4 2 1 1 5\\nAid ranked the highest, followed by Facebook Customization in\\nsecond place. The Interpretation Aid and Filter Mode had the same\\nranking score in third place, while Focus Mode ranked the lowest.\\nThe preference towards the aids was also reflected in the SUS score\\nreported in Appendix C.1. In response to the statement, ‚ÄúI think\\nthat I would like to use the system frequently,‚Äù 50% of participants\\nagreed to the Writing Aid and 83.3% for Facebook Customization.\\nIn contrast, only 25% of the participants selected ‚ÄúAgree‚Äù for the\\nInterpretation Aid, Focus Mode, and Filter Mode. These evaluations\\npointed to the usability challenges faced by participants and are\\nreported in detail in ¬ß5.2.\\n5.1.2 User Evaluation: Task Feedback and Task Performance. One-\\ntailed paired samples t-tests were used for the evaluation of task\\nfeedback and task performance by TBI participants. In terms of the\\ntask feedback, participants perceived that Focus Task were signifi-\\ncantly easier without the aid than with the aid [ ùë°(7)=2.645,ùëù=\\n0.02], suggesting the potential usability challenge of the Focus Mode.\\nWe did not find statistically significant differences in participants‚Äô\\nfeedback on the Interpretation and Filter tasks before and after\\nusing the aid. The results are presented in Table 5.\\nIn addition, we did not find statistically significant differences\\nbetween participants‚Äô task performance with and without using the\\naid, including the number of the posts viewed within two minutes\\nbefore (ùëÄ=1.4,ùëÜùëáùê∑=1.1) and after ( ùëÄ=0.9,ùëÜùëáùê∑=0.9) using\\nthe Focus Mode [ ùë°(7)=2.65,ùëù=0.98]; the time spent to find an\\ninteresting post before ( ùëÄ=21.75ùë†ùëíùëêùëúùëõùëëùë† ,ùëÜùëáùê∑ =16.6ùë†ùëíùëêùëúùëõùëëùë† )\\nand after (ùëÄ=27.4ùë†ùëíùëêùëúùëõùëëùë† ,ùëÜùëáùê∑=16.3ùë†ùëíùëêùëúùëõùëëùë† ) using the Filter\\nMode [ùë°(7)=‚àí0.55,ùëù=0.70]; Time spent to find a post from a\\nfriend before ( ùëÄ=24.6ùë†ùëíùëêùëúùëõùëëùë† ,ùëÜùëáùê∑ =30.4ùë†ùëíùëêùëúùëõùëëùë† ) and after\\n(ùëÄ=27.8ùë†ùëíùëêùëúùëõùëëùë† ,ùëÜùëáùê∑ =22.0ùë†ùëíùëêùëúùëõùëëùë† ) using the Filter Mode\\n[ùë°(7)=‚àí0.18,ùëù=0.57]. Therefore, all hypotheses in ¬ß4.3.1 were\\nnot supported. We report participants‚Äô feedback for each aid in more\\ndetail, including the perceived usefulness and usability challenges\\nin ¬ß5.2 to inform the areas of improvement and design implications\\nfor the accessibility toolkit.\\n5.1.3 Expert Evaluation: W3C Survey Result. The results for expert\\nevaluation of Facebook and each aid based on the W3C Cognitive\\nAccessibility Guidelines are presented in Table 6. Overall, the ex-\\nperts‚Äô evaluation of Facebook reported low scores, with only 26% of\\nthe requirements being fulfilled among all the objectives. Experts\\nevaluated each aid‚Äôs function and design and reported relatively\\nhigh scores for the Writing Aid and Facebook Customization. Specif-\\nically, more than 70% of the criteria under the overall objectives\\nfor Writing Aid (71.9%) and Facebook Customization (73.8%) weremet. The Interpretation Aid, Focus Mode, and Filter Mode fulfilled\\n60.0%, 60.0%, and 67.2% of the overall objectives, respectively.\\n5.2 Feedback for Each Aid in the SMART-TBI\\nBased on the usability challenges indicated in ¬ß5.1, participants\\nfurther provided feedback on how to address these challenges and\\nimprove each aid in the SMART-TBI. This section highlights the\\nfindings from the qualitative feedback followed by key design in-\\nsights from TBI users and experts respectively. For each aid, we\\nfirst present participants‚Äô overall attitudes, drawing on their re-\\nsponses to the interview question, ‚ÄúHow do you like the aid?‚Äù , and\\ntheir ratings from the SUS statement, ‚ÄúI think I would like to use\\nthe system frequently. ‚Äù This is followed by detailed comments from\\nparticipants about their experiences focusing on the perceived use-\\nfulness of the aid, usability challenges, and suggested new features.\\nThe findings are summarized in Table 7.\\n5.2.1 Feedback for Writing Aid .\\nUser Feedback. During the study, participants used the Writing\\nAid for spelling and grammar checks for the writing tasks. All\\nparticipants found the aid useful, and four out of eight participants\\nagreed that they would like to use the aid frequently (P1, 5, 7, 8).\\nThe aid was reported to support message construction and spell\\nchecks (P6, P8), facilitate sentiment analysis (P5, P7), and enhance\\nprivacy settings (P7). One participant noted it provided ‚Äúa different\\nperspective on how others might view it‚Äù (P6), thus indicating that\\nthe design objectives of the Writing Aid could be attained to some\\ndegree. Additionally, two participants emphasized the need for self-\\npresentation in using social media (P1, P5) and thought that the\\naid could ‚Äúhelp you change your story‚Äù (P5) and confirm that ‚ÄúIs this\\ncharacter (myself) polished enough to be on someone‚Äôs network‚Äù (P1).\\nMeanwhile, TBI user participants identified two major usability\\nchallenges: inaccurate grammar and sentiment analysis of the con-\\ntent (P3, P4, P7) and the inconvenience of repeating writing checks\\nafter modifying posts (P2, P7, P8). For example, P4 experienced\\ninaccurate grammar suggestions that incorrectly flagged slang she\\nwrote in her posts, such as ‚Äúgonna‚Äù and‚ÄúIma .‚Äù P7 suggested a po-\\ntential design improvement, proposing a single button to restart\\nthe sequence of checks after making edits. He stated, ‚ÄúI would add\\na button at the end to go back to the start...in case you want to redo\\nsomething and maybe recheck a specific section before you post it. ‚Äù\\n(P7). Additionally, P2 desired more learning support, stating that,\\n‚Äúit was frustrating at first‚Äù (P2), and wanted the aid to provide more\\n‚Äúspecific‚Äù instructions for improving the content.\\nExpert Feedback. Four out of five expert participants (EP1‚Äì3,\\nEP5) provided positive feedback on the overall functionality of the\\naid, highlighting its capability to reduce communication errors and\\nenhance writing clarity (EP1, EP2) and provide useful perspectives\\nto tweak the written post (EP3, EP5). Further, EP5 emphasized the\\nneed to use the writing aid to support social communication; she\\ncommented: ‚ÄúSome of the big issues is kind of that impulsivity and\\nnot being able to kind of check and correct their own errors and stuff\\nwhen posting...First of all, does it make sense, what it‚Äôs saying, but also\\nkind of how that might come across to other people, as well .‚Äù (EP5).ASSETS ‚Äô24, October 27‚Äì30, 2024, St. John‚Äôs, NL, Canada Hu and Lim, et al.\\nTable 5: Statistics for User Social Media Task Feedback. Participants provided ratings from 1‚Äì3 (1 = Disagree, 2 = Neutral, 3 =\\nAgree) in answering the questions from Table 3.\\nAid ScaleBefore using the aid After using the Aid Statistics\\nM Std M Std t DF p\\nWriting TaskHow the message will be received\\nby others2.79 0.39 2.77 0.51 0.44 23 0.67\\nPost is well-written 2.85 0.27 2.85 0.38 0 23 0.5\\nInterpretation TaskI am confident in my interpretation\\nof these posts.2.72 0.70 3.00 0.00 -1 6 0.18\\nWhat the writers intend to say is\\nclear to me.2.71 0.70 2.71 0.70 NA 6 NA\\nThese posts were easy to under-\\nstand.2.71 0.70 2.86 0.35 -1 6 0.18\\nFocus TaskEase of the task 2.96 0.11 2.71 0.42 2.645 7 0.02*\\nPerformance of the task 2.75 0.66 2.94 0.17 -1 7 0.82\\nFilter TaskEase of the task 2.96 0.11 2,54 0.8 1.33 7 0.11\\nPerformance of the task 2.69 0.66 2.69 0.66 0 7 0.5\\nTable 6: W3C evaluation results from TBI experts study\\nAid Name Objective 1 Objective 2 Objective 3 Objective 4 Objective 5 Objective 6 Objective 7 Objective 8 Overall\\nFacebook 20% 20% 60% 28% 25% 20% 20% 15% 26%\\nWriting Aid 77% 80% 72% 64% 70% 80% 60% NA 71.9%\\nInterpretation Aid 74% 80% 80% 46% 60% 20% NA NA 60%\\nFocus Mode 66% 80% 28% 26% 100% 60% 10% 67% 60%\\nFilter Mode 57% 60% 88% 48% 70% 80% 20% 73% 67.2%\\nFacebook Customization 74% 65% 68% 56% 100% 80% 10% 93% 73.8%\\nObjective 1: Help Users Understand What Things are and How to Use Them; Objective 2: Help Users Find What They Need; Objective 3: Use Clear and Understandable\\nContent; Objective 4: Help Users Avoid Mistakes and Know How to Correct Them; Objective 5: Help Users Focus; Objective 6: Ensure Processes Do Not Rely on\\nMemory; Objective 7: Provide Help and Support; Objective 8: Support Adaptation and Personalization\\nExperts (EP1‚Äì5) also pointed out usability challenges of the aid\\nbased on the W3C guideline and suggested improvements, focus-\\ning on TBI users‚Äô cognitive and sensory needs. These suggested\\nimprovements included increasing font sizes (EP1); using simple\\nand clear sentences for instructions (EP1‚Äì3); defining keywords\\nused in the aid ( e.g.,‚Äútoxicity‚Äù ) (EP3); and providing more detailed\\nsuggestions or automatically implement the suggestions after the\\ngrammar checks (EP1, 3). EP2 suggested rewording the aid‚Äôs in-\\nstruction in a ‚Äúmore direct or simple‚Äù way because of the diverse\\nliteracy levels of the patients she had worked with. EP2 shared, ‚ÄúA\\nlot of my patients that I work with who use social media a lot of the\\ntime, they come from backgrounds. Some of them don‚Äôt have a lot of\\neducation. ‚Äù (EP2).\\nFurthermore, experts (EP2, EP5) pointed out that oversharing\\npersonal information often happens for TBI users and suggested\\nthat the Writing Aid could ‚Äúadd in that extra level of security‚Äù (EP5)\\nto ensure the user‚Äôs safety, such as sending out alerts when over-\\nsharing activity was detected (EP5). EP5 also suggested using verbal\\ninput to overcome the challenge of word-finding.\\n5.2.2 Feedback for Interpretation Aid .User Feedback. The Interpretation Aid was found to support\\ncomprehension of posts and images (P2, P6, P8), particularly for\\nlonger messages (P2), and two participants agreed that they would\\nlike to use it frequently (P1, P8). For example, P8 reported that\\n‚ÄúRight after the accident, I had a really hard time understanding what\\npeople meant with their words...This [aid] would have been really\\ncomforting to me. ‚Äù P6 appreciated that the aid provided a different\\nperspective of the post, stating, ‚Äúit‚Äôs nice to see how other people,\\nsame as earlier, like, could interpret what you‚Äôre saying. ‚Äù\\nThe major usability challenge reported by participants for the\\nInterpretation Aid was disagreement with the results of sentiment\\nanalysis for the content (P2, P3, P7, P8). For example, P2 disagreed\\nwith the classification of the emotion type ‚Äújoy‚Äù for the post in the\\nfirst interpretation task. P8 thought a post to be ‚Äúpositive‚Äù while the\\naid predicted it to be ‚Äúnegative .‚Äù Additionally, P2 noted confusion\\nabout the image information and thought the description of its\\nemotion should be shortened.\\nNotably, two participants (P1, 5) suggested that the aid provide\\nmore detail and explain the reason behind the interpretation results.\\nFor example, P1 commented: ‚ÄúI want them to emphasize more, like,\\nyou know, why are they negative, why are they positive?‚Äù (P1).SMART-TBI ASSETS ‚Äô24, October 27‚Äì30, 2024, St. John‚Äôs, NL, Canada\\nTable 7: Qualitative Findings: User feedback and expert feedback on each aid\\nAid Name Would like to\\nuse the aid fre-\\nquently (User)Perceived Usefulness Usability Challenges and Sugges-\\ntionsSuggested new functions\\nWriting Aid Agree (50%)\\nNeutral (25%)\\nDisagree (25%)Helped with message construction\\nand spell checks (P6, 8; EP2)\\nSupport sentiment check (P5, 7;\\nEP1, 4)\\nSupport self-presentation (P1, 5)\\nProvide different interpretation\\nperspective (P6; EP3, 5)\\nPrivacy check (P7)\\nLearning grammar (P2)\\nSummarize the post (P7)Be able to go back and redo the checks\\nafter modification (P2, 7, 8; EP3, 4)\\nFurther support the content correction\\n(P2; EP1, 3)\\nNeed learning support (P2; EP2)\\nMis-detection (P3, 4, 7; EP4)\\nImprove the instruction and wording\\n(EP1‚Äì3)\\nThe font is small (EP1)Automatically correct or rephrase the\\nmessages (P3, 5, 8)\\nProvide alternative word recommenda-\\ntions (P2, 5)\\nPrivacy control and prevent overshar-\\ning (EP2, 5)\\nSupport clinical practice (EP3)\\nUse speech input (EP5)\\nInterpretation\\nAidAgree (25%)\\nNeutral (0%)\\nDisagree (75%)Support comprehension (P2, 6, 8)\\nProvide different interpretation\\nperspective (P6; EP5)\\nSimplified the content (EP5)\\nSupport social communication\\n(EP1, 4)Inaccurate interpretation results (P2, 3,\\n7, 8; EP1, 2, 5)\\nImprove image description (P2; EP3)\\nClarify the content being analyzed (EP1,\\n3, 4)Explain the reason behind the interpre-\\ntation (P1, 5)\\nSupport more content types (P2, 8, 5)\\nReport hate speech (EP3)\\nFocus Mode Agree (25%)\\nNeutral (0%)\\nDisagree (75%)Easier to distinguish ads and posts\\n(P8)\\nHelp to read in more details (P8)\\nHelp to focus (EP1, 4, 5)\\nSimplify the page (EP2, 3)Need learning support (P1; EP3)\\nIncrease font size and photo size (P5, 8)\\nPost navigation challenge (P3, 4, 5, 8)Apply to other pages (EP5)\\nPrivacy control and prevent overshar-\\ning (EP5)\\nFilter Mode Agree (25%)\\nNeutral (12.5%)\\nDisagree (62.5%)Help to filter out ads (P8)\\nHelp with organized ways of using\\nthe social media (P8; EP3)\\nEnsure psychological safety (EP3,\\n5)\\nNarrowing down content (EP3)\\nSorting is helpful (EP4, 5)Lost track of seen posts (P8; EP2)\\nDid not filter properly (P3, 4, 7; EP4, 5)\\nImprove labels (EP1, 2)\\nThe font is small (EP3)\\nAdd user instructions (EP2)Provide additional filtering options (P2,\\n5, 8)\\nFacebook\\nCustomiza-\\ntionAgree (83.3%)\\nNeutral (0%)\\nDisagree (16.6%)Customize the newsfeed (P3, 7, 8;\\nEP1, 3, 4, 5)\\nMake the user more focused (P5;\\nEP1)Need learning support (P7)\\nImprove labels (EP1, 2)Provide verbal interaction (P4)\\nFilter out more specific posts: certain\\ngroups, more left side options (P5, 7)\\nClean up ads (EP5)\\nExpert Feedback. Three expert participants (EP1, EP4, EP5) re-\\nacted positively to the Interpretation Aid. They appreciated that the\\naid could support social-emotional communication (EP1, EP4) and\\nsummarize the posts for individuals with TBI (EP5). For example,\\nEP1 described how the aid can help users react to other people‚Äôs\\nsocial media posts, stating, ‚ÄúThe aid would help them to know what\\nthe post is sort of about and whether to ‚Äòlike‚Äô it or not. Maybe even\\ndetermine whether it‚Äôs a like or a ‚Äòlove‚Äô. Like if they were to learn\\nthat the greener it is the more chance the person is looking for a ‚Äòlove‚Äô,\\nversus a yellow one the person is looking for a ‚Äôlike‚Äù‚Äô (EP1). Similar to\\nthe TBI users, experts also identified usability challenges for the aid,\\nsuch as the need to improve the accuracy of interpretation results\\n(EP1, EP2, EP5) and clarify the analyzed content (EP1, EP4).5.2.3 Feedback for Focus Mode .We evaluated both versions\\nof the Focus Mode as described in ¬ß3.2.2. The initial design was\\nevaluated by our TBI user participants and the updated design was\\nevaluated by our TBI expert participants.\\nUser Feedback. Three participants (P1, P3, P8) reacted positively\\ntowards the Focus Mode aid, and two participants reported that they\\nwould like to use it frequently (P1, P8). For example, P8 described\\nthe aid ‚Äúamazingly helpful‚Äù (P8) and shared, ‚ÄúI read more captions\\nthan I would have if I was scrolling the newsfeed. They‚Äôre bigger and\\neasier to see. ‚Äù\\nOn the other hand, five participants (P2, P4‚Äì7) held negative\\nviews towards the aid, finding it ‚Äúunnecessary‚Äù (P2) and ‚Äúcumber-\\nsome‚Äù (P5). Two major usability challenges reported by multipleASSETS ‚Äô24, October 27‚Äì30, 2024, St. John‚Äôs, NL, Canada Hu and Lim, et al.\\nparticipants were the post navigation for viewing the full content\\n(P3‚Äì5, P8) and the unresponsiveness of the buttons (P2, P6‚Äì8). P5\\ndescribed how the buttons for post navigation were inaccessible\\nfor users with motor impairments, stating ‚ÄúIf you have, you know,\\nin nimble fingers, and you can‚Äôt really do that, or if you‚Äôre old and\\ndon‚Äôt know how to do it‚Äù (P5). In addition to the above challenges,\\ntwo participants (P5, P8) requested to increase the size of the post\\ncreator‚Äôs profile photo and name texts.\\nExpert Feedback. All TBI expert participants provided positive\\nfeedback for the Focus Mode, highlighting its ability to enhance the\\nusability of social media by streamlining the individual‚Äôs feed (EP1‚Äì\\n5) and the simplicity of use (EP1). For example, EP4 commented:\\n‚ÄúWow. Okay, this is the best one. Wow, I can really focus on the posts. ‚Äù\\n(EP4). Similarly, EP5 mentioned that ‚ÄúYou can literally just focus\\non scrolling through your newsfeed. Yeah. This is cool. ‚Äù (EP5). The\\nmajor usability challenge reported by TBI experts participants (EP3,\\nEP4) is the interactivity of the blurred area. EP4 described how she\\naccidentally clicked in the blurred area and went to an unexpected\\npage: ‚ÄúThe blur, I accidentally clicked and they went to a sponsor\\npage. ‚Äù (EP4).\\n5.2.4 Feedback for Filter Mode .\\nUser Feedback. Four participants (P1, P4, P6, P8) held positive\\nviews towards the aid, two participants (P2) held neutral views, and\\nthree (P3, P5, P7) reacted negatively to the aid. Two participants\\n(P1, P8) agreed that they would like to use the aid frequently. In\\nparticular, P8 found it helpful that advertisements and posts were\\ndistinguished after applying the aid, stating that ‚ÄúI think the ad-\\nvertisements being filtered away is something that makes it clear\\nthat I‚Äôm looking at people‚Äôs posts. ‚Äù (P8). However, three participants\\nexperienced usability challenges that the filtering was not accu-\\nrate and did not sync up as experienced by three participants (P3,\\nP4, P7). As P7 commented: ‚ÄúI would say it‚Äôs not very accurate. It‚Äôs\\nnot personal to me. ‚Äù In addition, the filtering caused challenges for\\ntracking viewed posts because of the posts hiding and reordering,\\nas what P8 experienced refreshing the page: ‚ÄúOne that I was just on\\nis now gone‚Äù (P8) .\\nParticipants (P2, P5, P8) suggested that the Filter Mode could\\nhave provided more options and combined multiple filters to achieve\\nmore customized results. For example, P2 compared the filter with\\nExcel, and desired to have a wider range of filters such as ‚ÄúQuotes‚Äù\\nand‚ÄúMeme .‚Äù\\nExpert Feedback. Filter Mode received positive feedback for its\\nsorting feature and feed personalization (EP3‚Äì5) as well as its ease\\nof use (EP3). As EP3 noted, ‚ÄúIt‚Äôs just helpful to be able to narrow down,\\nlike, the type of content that you want, what‚Äôs most meaningful to\\nthem‚Äù (EP3). However, some expert participants pointed out several\\nusability challenges and provided suggestions for improvement,\\nsuch as improving the accuracy of filtering results (EP2), making\\nfilter labels clearer (EP1, EP2), and increasing font size (EP3).\\n5.2.5 Feedback for Facebook Customization .\\nUser Feedback. Seven participants (P1‚Äì3, P5‚Äì8) held positive\\nviews toward the aid, and one participant (P4) was neutral. Three\\nparticipants (P3, P7, P8) appreciated the ability to control the news-\\nfeed with the tool and mentioned that the aid could make them‚Äúmore focused‚Äù (P5) and hide information that they ‚Äúdon‚Äôt really need‚Äù\\n(P3), such as advertisements. On the other hand, one participant\\n(P4) found this customization was not necessary because their goal\\nof using social media was to ‚Äúcheck in and look around‚Äù (P4), thus\\nsimplifying the information source would have a side effect for\\nthem to explore different content. P7 also wished that there were\\nmore options for the left-side filtering for him to ‚Äúpersonalize it a\\nlittle bit further‚Äù (P7). P7 also desired more guidance on how to use\\nthe aid.\\nExpert Feedback. Similar to the benefits mentioned by users with\\nTBI, experts (EP1, EP3‚Äì5) noted that the tool simplified the newsfeed\\npage and could help avoid distractions for users with TBI. EP1 noted,\\n‚ÄúFor me as a user, I love this. I never looked at the stuff [extra fields\\non Newsfeed] before but I love it not being there. Look how much\\nnicer that is to look at. I can ignore it with my own mind but I love\\nnot seeing it. ‚Äù Similarly, EP3 commented, ‚ÄúThey have control over\\nwhat they‚Äôre seeing. This is very helpful‚Äù (EP3). However, experts\\n(EP1, 2) mentioned the current design could be further improved\\nby clarifying the wording of the options (EP2) and using icons to\\nsupport the user‚Äôs understanding of the control (EP1).\\n6 DISCUSSION\\nOverall, our findings reported feedback for our accessibility toolkit.\\nUsability considerations reported by experts and users echoed the\\neight objectives from the W3C cognitive guidelines [ 26], focusing\\non ease of use, prevention of errors, design consistency, clarity of\\ninformation and personalization of the social media platform. Our\\nfindings highlighted the needs of building accessibility features in\\ntechnologies to support cognitive and communication challenges\\nfor people with TBI. Communication support features in our sys-\\ntem, such as sentiment and toxicity detection, writing support and\\npost interpretation, can be applied to other types of online activi-\\nties, such as sending instant messages, group communications in\\nonline communities, and reading online articles. Cognitive support\\nfeatures can be applied to other online platforms to simplify the\\nwebsite layouts, highlighting the needed features and providing\\nspecific step-by-step instructions to accomplish the tasks for indi-\\nviduals with TBI who are facing cognitive challenges. Our findings\\nprovided actionable steps for us to implement usability improve-\\nments and further iterate the system in preparation of extensive\\nfield testing.\\n6.1 Design Implications\\nOur toolkit focused on providing communication support and cog-\\nnitive support in social media use for individuals with TBI. Based\\non our findings, we drew the following five design implications for\\ndesigning accessible social media: ensuring psychological safety,\\nprivacy control and protection, trade off between the business prof-\\nits and user experience, mixed feedback in AI-tools, tool adoption\\nfor diverse TBI needs.\\nEnsure Psychological Safety .Individuals with TBI can expe-\\nrience a variety of psychological challenges such as mood swings,\\ndepression, anxiety and agitation [ 43]. Our user participants (P1,\\nP2, P3) reported discomfort in seeing arguments or violent content\\non social media. Our expert participants (EP3, EP5) emphasizedSMART-TBI ASSETS ‚Äô24, October 27‚Äì30, 2024, St. John‚Äôs, NL, Canada\\nthe importance of protecting TBI users‚Äô psychological safety and\\nthat the aid should give control to the content they are seeing. Our\\ntoolkit employed the use of the ‚Äú positive ‚Äù filter to avoid disturbing\\ncontent in the newsfeed, and future design could further explore\\ndesign solutions to increase the accessibility of content browsing\\nand social communication for populations with psychological chal-\\nlenges. For example, the aid could predict the psychological safety\\nlevel of a page that the user is going to visit for the user so they\\ncan decide if they want to continue or not. The aid can also provide\\nsupport for the user to disclose their negative feelings if the user\\nsees the content causing emotional swings. One of our TBI user\\nparticipants (P2) mentioned that she would talk to her partner if she\\nexperienced emotional swing after seeing discomforting content\\non social media. Similarly, the aid could create a virtual character\\nfor the user to chat to and disclose their negative feelings if they\\ndo not have other people to talk to in the moment.\\nPrivacy Control and Protection .Expert participants cared for\\nsafety in using social media platforms and reported the possible\\noversharing activities by individuals with TBI in private messages\\nand social media posts. This can be difficult to detect by individuals\\nwith TBI because they can be unaware of their behaviors or risks of\\noversharing on social media [ 18], especially for children and ado-\\nlescents with brain injury. With more functional features provided\\non social media such as Market Place , it is easy for users to talk to\\nstrangers, which can be risky for TBI users. To prevent oversharing\\nsocial communications, the user needs to always pay attention to\\nwho they are talking to and what they are sharing with. First, the\\naid can alert the user if they are talking to strangers and ask for\\nconfirmation before they message out any personal or sensitive\\ninformation. Second, the aid could help the user monitor if there\\nare any potential risks in the ongoing conversation and provide\\nresources and clear instructions on how to handle dangerous situa-\\ntions. Social communication can happen in a variety of forms on\\nsocial media such as private messaging, group chat, post comments\\nand sharing. Therefore, privacy protection should be supported\\nacross social media features.\\nTrade-offs between Business Profits and Accessibility Needs .\\nAdvertisements on social media were reported as the main chal-\\nlenge by our participants with TBI (six out of eight). Advertisements\\nin the news feed can add to TBI users‚Äô cognitive load, cause difficul-\\nties for them to main focus, and have them lost in the navigation.\\nMoreover, individuals with TBI often have short-term or long-term\\nmemory loss and can forget people in their friend list, thus causing\\nchallenges for the users to differentiate advertisements and posts\\nfrom their social circle. It is acknowledged that advertisements\\nare important for the company to make profits, however, they are\\nalso the major barriers for individuals with TBI to use social media\\nplatforms. Social media platforms could change the layout of the\\nwebpage to separate advertisements from the posts from the user‚Äôs\\nsocial circle. As suggested by our participant (P8), there could be a\\nspecific area on the webpage dedicated to advertisements to reduce\\ndistraction. In addition, social media platforms could offer different\\nmodes of advertisements and allow users to choose the one that\\nmeets their needs. One mode could be the existing approach that\\nadvertisements are integrated into the newsfeed. An alternative\\nmode could be showing advertisements at certain times such aswhen opening a new webpage, or always displaying advertisements\\nin a centralized area to make the social media site more accessible\\nfor users with cognitive impairments.\\nMixed Feedback for AI-based Tools for People with TBI .One\\ninteresting finding was participants‚Äô perceptions of using AI tools\\nfor communication support, in particular the use of sentiment anal-\\nysis and toxicity detection in our toolkit. We demonstrated the\\npotential of using these tools in supporting message constructions\\nand interpretations during social communications for individu-\\nals with TBI. Nevertheless, we acknowledged the potential risks of\\nadopting AI tools for people with TBI. The major concern expressed\\nby our participants is the inaccurate results from the interpretation.\\nWe employed an off-the-shelf commercial AI tool for the analysis,\\nhowever, an accuracy level of 80% in the analysis caused much\\nconfusion for our participants as observed in the study. The in-\\naccurate or biased results [ 65] in AI-based tools can be risky for\\npeople with TBI because they can have difficulty understanding\\nunderlying messages and differentiating the biased views from the\\nAI tool, which can shape the way they think in the long term. In\\naddition, AI tools are becoming more pervasive and available to\\nthe public and an increasing number of rehabilitation features are\\nbuilt on top of them [ 4]. The pervasiveness and overconfidence\\nof AI tools can reduce people‚Äôs initiatives and willingness to seek\\nprofessional rehabilitation support provided by TBI experts and\\nnegatively affect their recovery process.\\nTool Adoption with Diverse TBI Needs .Individuals with TBI\\ncan have diverse accessibility needs for using social media plat-\\nforms. Through our study, participants with TBI reported specific\\ndesired features of the aids based on their personal needs, such as\\npersonalizing the Filter Mode with a ‚ÄúMeme‚Äù filter and using the\\nWriting Aid to learn grammar (P2). Some participants preferred\\nsimplification of the interface using our cognitive support aids,\\nwhile some preferred more complicated page layouts and richer\\ninteractions. Due to sensory challenges, participants have different\\npreferences for the UI elements such as font and picture size.\\nThe accessibility toolkit design should provide customized op-\\ntions for the user to choose their desired features. Moreover, the\\ntoolkit should provide adaptive learning support for TBI users for\\nbetter tool adaption. The learning support should provide clear\\nstep-by-step guidance for the use and should be always available\\nwhen needed by the user due to the memory challenges commonly\\nfaced by people with TBI.\\n6.2 Limitations & Future Work\\nOur work has several limitations in the capabilities of the aids as\\nwell as the conclusions we draw from our data.\\nFirst, our aids relied on third-party APIs for text and sentiment\\nanalysis, which occasionally provided inaccurate interpretations.\\nSignificant errors in interpretation can impair user trust. Users with\\nTBI might be particularly prone to the negative effects of errors due\\nto their social communication challenges. Moreover, Hutchinson\\net al. [46] revealed that Perspective API used in the toolkit exhibited\\nsocial biases towards disability-related terms and associated these\\nterms with toxicity and negativity. Future work should incorporateASSETS ‚Äô24, October 27‚Äì30, 2024, St. John‚Äôs, NL, Canada Hu and Lim, et al.\\nbias mitigation techniques ( e.g., Cheng et al . [22] ) into the toolkit\\nto prevent the harms from the socially biased analysis.\\nSecond, our aids are still proof-of-concept prototypes and can be\\nimproved through integrating more advanced techniques. For exam-\\nple, Interpretation Aid can utilize tools for alt-text auto-generation\\n[28,67] to improve its image description. In addition, usability is-\\nsues of our prototypes can limit the effectiveness of the aids and\\nhave a negative effect on user perceptions.\\nThird, our evaluation took place in a laboratory environment\\nfor a short period of time, which offers little insight into long-term\\nuse and adoption patterns. In our future work, we plan to conduct\\nan in-the-wild study with an extended period of time to reflect on\\nuser‚Äôs actual use of the toolkit in their social media platforms.\\nFinally, due to the difficulty of recruiting users with TBI as well\\nas TBI rehabilitation experts, our study included a relatively small\\nsample size, which prevented meaningful quantitative data analysis.\\nGiven the limited number of participants in our user evaluation\\n(ùëÅ=8), our primary goal is to understand the potential and initial\\nuser experiences through a qualitative approach. Data from our\\nquantitative measures (¬ß5.1.2) did not offer significant insights due\\nto the potential lack of representativeness. However, the rich quali-\\ntative findings will guide the refinement of the toolkit for a more\\ncomprehensive and longer-term field study, where quantitative\\nmeasures can be incorporated to assess the toolkit‚Äôs effectiveness\\nover an extended period.\\n7 CONCLUSION\\nIn this paper, we present the Social Media Accessibility and Reha-\\nbilitation Toolkit (SMART-TBI) , which we built to support social\\ncommunication and newsfeed browsing of social media users with\\nTBI. The toolkit includes two communication support aids and\\nthree cognitive support aids. We evaluated the toolkit with eight\\nsocial media users with TBI and five TBI experts. Our findings high-\\nlighted the effective features in the toolkit and pointed to areas of\\nimprovement for each aid. Based on our findings, we generated\\ndesign implications to improve the accessibility of social media\\nuse by people with TBI, considering the psychological and privacy\\nsafety, the trade-offs between business profits and accessibility\\nneeds, mixed feedback for AI-based tools and tool adoption with\\ndiverse TBI needs. These design implications can inform the build-\\ning of accessible social media platforms for users with cognitive\\nand communication difficulties.\\nACKNOWLEDGMENTS\\nThis work was funded by the National Institutes of Health (NIH\\nR01-HD071089-06A1). Figure 1 modified an image by freepik for\\nits design. We would like to thank our participants for their time\\nand participation in this research study.\\nREFERENCES\\n[1]Bilal Abu-Salih, Mohammad Alhabashneh, Dengya Zhu, Albara Awajan, Yazan\\nAlshamaileh, Bashar Al-Shboul, and Mohammad Alshraideh. 2023. Emotion\\ndetection of social data: APIs comparative study. Heliyon 9, 5 (2023).\\n[2]Reihaneh Ahmadi, Hajin Lim, Bilge Mutlu, Melissa Duff, Catalina Toma, and Lyn\\nTurkstra. 2022. Facebook Experiences of Users With Traumatic Brain Injury: A\\nThink-Aloud Study. JMIR Rehabilitation and Assistive Technologies 9, 4 (2022),\\ne39984.\\n[3]Kristin Alfredsson √Ögren, Anette Kjellberg, and Helena Hemmingsson. 2020.\\nAccess to and use of the Internet among adolescents and young adults with intel-\\nlectual disabilities in everyday settings. Journal of Intellectual & DevelopmentalDisability 45, 1 (2020), 89‚Äì98.\\n[4]Mashael Alsobhi, Harpreet Singh Sachdev, Mohamed Faisal Chevidikunnan, Reem\\nBasuodan, Dhanesh Kumar KU, and Fayaz Khan. 2022. Facilitators and barriers\\nof artificial intelligence applications in rehabilitation: a mixed-method approach.\\nInternational Journal of Environmental Research and Public Health 19, 23 (2022),\\n15919.\\n[5]Christina Baker-Sparr, Tessa Hart, Thomas Bergquist, Jennifer Bogner, Laura\\nDreer, Shannon Juengst, David Mellick, Therese M O‚ÄôNeil-Pirozzi, Angelle M\\nSander, and Gale G Whiteneck. 2018. Internet and social media use after traumatic\\nbrain injury: a traumatic brain injury model systems study. The Journal of head\\ntrauma rehabilitation 33, 1 (2018), E9.\\n[6]Anthony Bassey, Nnaemeka Meribe, Emmanuel Bassey, and Caroline Ellison.\\n2023. Perceptions and experience of social media use among adults with physical\\ndisability in Nigeria: attention to social interaction. Disability & society 38, 7\\n(2023), 1146‚Äì1163.\\n[7]Antonia Baumgartner, Tobias Rohrbach, and Philomen Sch√∂nhagen. 2023. ‚ÄòIf the\\nphone were broken, I‚Äôd be screwed‚Äô: media use of people with disabilities in the\\ndigital era. Disability & Society 38, 1 (2023), 73‚Äì97.\\n[8]Andrew P Bradley. 1997. The use of the area under the ROC curve in the\\nevaluation of machine learning algorithms. Pattern recognition 30, 7 (1997),\\n1145‚Äì1159.\\n[9]Virginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology.\\nQualitative research in psychology 3, 2 (2006), 77‚Äì101.\\n[10] Virginia Braun and Victoria Clarke. 2022. Thematic analysis: a practical guide .\\nSAGE Publications Ltd.\\n[11] John Brooke et al .1996. SUS-A quick and dirty usability scale. Usability evaluation\\nin industry 189, 194 (1996), 4‚Äì7.\\n[12] Melissa Brunner, Bronwyn Hemsley, Stuart Palmer, Stephen Dann, and Leanne\\nTogher. 2015. Review of the literature on the use of social media by people\\nwith traumatic brain injury (TBI). Disability and rehabilitation 37, 17 (2015),\\n1511‚Äì1521.\\n[13] M Brunner, S Palmer, L Togher, S Dann, and B Hemsley. 2019. Content Analysis\\nof Tweets by People with Traumatic Brain Injury (TBI): Implications for Rehabil-\\nitation and Social Media Goals. In Proceedings of the 52nd Hawaii International\\nConference on System Sciences 2019 (HICSS-52) . Scholar Space at the University of\\nHawaii at Manoa.\\n[14] Melissa Brunner, Stuart Palmer, Leanne Togher, Stephen Dann, and Bronwyn\\nHemsley. 2020. ‚ÄúIf I knew what I was doing on Twitter then I would use it more‚Äù:\\nTwitter experiences and networks of people with traumatic brain injury (TBI).\\nBrain Impairment 21, 1 (2020), 1‚Äì18.\\n[15] Melissa Brunner, Stuart Palmer, Leanne Togher, and Bronwyn Hemsley. 2019.\\n‚ÄòI kind of figured it out‚Äô: the views and experiences of people with traumatic\\nbrain injury (TBI) in using social media‚Äîself-determination for participation and\\ninclusion online. International Journal of Language & Communication Disorders\\n54, 2 (2019), 221‚Äì233.\\n[16] Melissa Brunner, Rachael Rietdijk, Petra Avramovic, Emma Power, Melissa Miao,\\nNick Rushworth, Liza MacLean, Anne-Maree Brookes, and Leanne Togher. 2023.\\nDeveloping social-ABI-lity: an online course to support safe use of social media\\nfor connection after acquired brain injury. American journal of speech-language\\npathology 32, 2S (2023), 924‚Äì940.\\n[17] Melissa Brunner, Rachael Rietdijk, and Leanne Togher. 2022. Training resources\\ntargeting social media skills to inform rehabilitation for people who have an\\nacquired brain injury: Scoping review. Journal of Medical Internet Research 24, 4\\n(2022), e35595.\\n[18] Melissa Brunner, Leanne Togher, Stuart Palmer, Stephen Dann, and Bronwyn\\nHemsley. 2021. Rehabilitation professionals‚Äô views on social media use in trau-\\nmatic brain injury rehabilitation: gatekeepers to participation. Disability and\\nRehabilitation 43, 14 (2021), 1955‚Äì1964.\\n[19] Lindsey J Byom and Lyn Turkstra. 2012. Effects of social cognitive demand on the-\\nory of mind in conversations of adults with traumatic brain injury. International\\nJournal of Language & Communication Disorders 47, 3 (2012), 310‚Äì321.\\n[20] Arthur Carvalho, Adam Levitt, Seth Levitt, Edward Khaddam, and John Benamati.\\n2019. Off-the-shelf artificial intelligence technologies for sentiment and emotion\\nanalysis: a tutorial on using IBM natural language processing. Communications\\nof the Association for Information Systems 44, 1 (2019), 43.\\n[21] Sue Caton and Melanie Chapman. 2016. The use of social media and people with\\nintellectual disability: A systematic review and thematic analysis. Journal of\\nintellectual and developmental disability 41, 2 (2016), 125‚Äì139.\\n[22] Lu Cheng, Ahmadreza Mosallanezhad, Yasin N Silva, Deborah L Hall, and Huan\\nLiu. 2022. Bias mitigation for toxicity detection via sequential decisions. In\\nProceedings of the 45th International ACM SIGIR Conference on Research and\\nDevelopment in Information Retrieval . 1750‚Äì1760.\\n[23] Mina Choi and Catalina L Toma. 2014. Social sharing through interpersonal media:\\nPatterns and effects on emotional well-being. Computers in Human Behavior 36\\n(2014), 530‚Äì541.\\n[24] Mina Choi and Catalina L Toma. 2021. Understanding Mechanisms of Media Use\\nfor the Social Sharing of Emotion. Journal of Media Psychology (2021).SMART-TBI ASSETS ‚Äô24, October 27‚Äì30, 2024, St. John‚Äôs, NL, Canada\\n[25] Sharice Clough, Emily Morrow, Bilge Mutlu, Lyn Turkstra, and Melissa C Duff.\\n2023. Emotion recognition of faces and emoji in individuals with moderate-severe\\ntraumatic brain injury. Brain injury 37, 7 (2023), 596‚Äì610.\\n[26] World Wide Web Consortium. 2022. All Supplemental Guidance | WAI | W3C: Cog-\\nnitive Accessibility Guidance. https://www.w3.org/WAI/WCAG2/supplemental/\\n#cognitiveaccessibilityguidance\\n[27] Cynthia Dahlberg, Lenore Hawley, Clare Morey, Jody Newman, Christopher P\\nCusick, and Cynthia Harrison-Felix. 2006. Social communication skills in persons\\nwith post-acute traumatic brain injury: Three perspectives. Brain injury 20, 4\\n(2006), 425‚Äì435.\\n[28] Maitraye Das, Alexander J. Fiannaca, Meredith Ringel Morris, Shaun K. Kane,\\nand Cynthia L. Bennett. 2024. From Provenance to Aberrations: Image Creator\\nand Screen Reader User Perspectives on Alt Text for AI-Generated Images. In\\nProceedings of the CHI Conference on Human Factors in Computing Systems (Hon-\\nolulu, HI, USA) (CHI ‚Äô24) . Association for Computing Machinery, New York, NY,\\nUSA, Article 900, 21 pages. https://doi.org/10.1145/3613904.3642325\\n[29] Daniel K Davies, Steven E Stock, Larry R King, R Brian Brown, Michael L\\nWehmeyer, and Karrie A Shogren. 2015. An interface to support independent use\\nof Facebook by people with intellectual disability. Intellectual and Developmental\\nDisabilities 53, 1 (2015), 30‚Äì41.\\n[30] Michael C Dewan, Abbas Rattani, Saksham Gupta, Ronnie E Baticulon, Ya-Ching\\nHung, Maria Punchak, Amit Agrawal, Amos O Adeleye, Mark G Shrime, Andr√©s M\\nRubiano, et al .2018. Estimating the global incidence of traumatic brain injury.\\nJournal of neurosurgery 130, 4 (2018), 1080‚Äì1097.\\n[31] Adele Diamond. 2013. Executive functions. Annual review of psychology 64 (2013),\\n135‚Äì168.\\n[32] Carly Dinnes, Karen Hux, Morgan Holmen, Alaina Martens, and Megan Smith.\\n2018. Writing changes and perceptions after traumatic brain injury:‚ÄúOh, by the\\nway, I can‚Äôt write‚Äù. American journal of speech-language pathology 27, 4 (2018),\\n1523‚Äì1538.\\n[33] Melissa C Duff, Emily L Morrow, Malcolm Edwards, Ryan McCurdy, Sharice\\nClough, Nirav Patel, Kimberly Walsh, and Natalie V Covington. 2022. The value\\nof patient registries to advance basic and translational research in the area of\\ntraumatic brain injury. Frontiers in behavioral neuroscience 16 (2022), 846919.\\n[34] Nicole B Ellison and Jessica Vitak. 2015. Social network site affordances and\\ntheir relationship to social capital processes. The handbook of the psychology of\\ncommunication technology (2015), 203‚Äì227.\\n[35] Jessica L Feuston, Charlotte G Marshall-Fricker, and Anne Marie Piper. 2017. The\\nsocial lives of individuals with traumatic brain injury. In Proceedings of the 2017\\nCHI Conference on Human Factors in Computing Systems . 182‚Äì194.\\n[36] Emma Finch, Anna Copley, Petrea Cornwell, and Crystal Kelly. 2016. Systematic\\nreview of behavioral interventions targeting social communication difficulties\\nafter traumatic brain injury. Archives of Physical Medicine and Rehabilitation 97,\\n8 (2016), 1352‚Äì1365.\\n[37] Margaret A Flynn, Arianna Rigon, Rachel Kornfield, Bilge Mutlu, Melissa C Duff,\\nand Lyn S Turkstra. 2019. Characterizing computer-mediated communication,\\nfriendship, and social participation in adults with traumatic brain injury. Brain\\ninjury 33, 8 (2019), 1097‚Äì1104.\\n[38] Centers for Disease Control and Prevention. 2022. Get the Facts About TBI:\\nCenters for Disease Control and Prevention (CDC). https://www.cdc.gov/\\ntraumaticbraininjury/get_the_facts.html\\n[39] Homero Gil de Z√∫√±iga, Nakwon Jung, and Sebasti√°n Valenzuela. 2012. Social\\nmedia use for news and individuals‚Äô social capital, civic engagement and political\\nparticipation. Journal of computer-mediated communication 17, 3 (2012), 319‚Äì336.\\n[40] Paul A Harris, Robert Taylor, Robert Thielke, Jonathon Payne, Nathaniel Gonzalez,\\nand Jose G Conde. 2009. Research electronic data capture (REDCap)‚Äîa metadata-\\ndriven methodology and workflow process for providing translational research\\ninformatics support. Journal of biomedical informatics 42, 2 (2009), 377‚Äì381.\\n[41] Susan C Herring. 2002. Computer-mediated communication on the internet.\\nAnnual Review of Information Science and Technology 36, 1 (2002), 109‚Äì168.\\n[42] Dan Hoofien, Assaf Gilboa, Eli Vakil, and Peter J Donovick. 2001. Traumatic brain\\ninjury (TBI) 10? 20 years later: a comprehensive outcome study of psychiatric\\nsymptomatology, cognitive abilities and psychosocial functioning. Brain injury\\n15, 3 (2001), 189‚Äì209.\\n[43] Jonathon R Howlett, Lindsay D Nelson, and Murray B Stein. 2022. Mental health\\nconsequences of traumatic brain injury. Biological psychiatry 91, 5 (2022), 413‚Äì\\n420.\\n[44] Yaxin Hu, Hajin Lim, Hailey L Johnson, Josephine M O‚ÄôShaughnessy, Lisa\\nKakonge, Lyn Turkstra, Melissa Duff, Catalina Toma, and Bilge Mutlu. 2023.\\nInvestigating Day-to-day Experiences with Conversational Agents by Users with\\nTraumatic Brain Injury. In Proceedings of the 25th International ACM SIGACCESS\\nConference on Computers and Accessibility . 1‚Äì15.\\n[45] Yaxin Hu, Yuxiao Qu, Adam Maus, and Bilge Mutlu. 2022. Polite or direct?\\nConversation design of a smart display for older adults based on politeness\\ntheory. In Proceedings of the 2022 CHI Conference on Human Factors in Computing\\nSystems . 1‚Äì15.\\n[46] Ben Hutchinson, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster, Yu\\nZhong, and Stephen Denuyl. 2020. Social Biases in NLP Models as Barriers for Per-\\nsons with Disabilities. In Proceedings of the 58th Annual Meeting of the Associationfor Computational Linguistics , Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel\\nTetreault (Eds.). Association for Computational Linguistics, Online, 5491‚Äì5501.\\nhttps://doi.org/10.18653/v1/2020.acl-main.487\\n[47] Andreas M Kaplan and Michael Haenlein. 2010. Users of the world, unite! The\\nchallenges and opportunities of Social Media. Business horizons 53, 1 (2010),\\n59‚Äì68.\\n[48] Hajin Lim, Lisa Kakonge, Yaxin Hu, Lyn Turkstra, Melissa Duff, Catalina Toma,\\nand Bilge Mutlu. 2023. So, I Can Feel Normal: Participatory Design for Accessible\\nSocial Media Sites for Individuals with Traumatic Brain Injury. In Proceedings of\\nthe 2023 CHI Conference on Human Factors in Computing Systems . 1‚Äì19.\\n[49] Sheila MacDonald. 2017. Introducing the model of cognitive-communication\\ncompetence: A model to guide evidence-based communication interventions\\nafter brain injury. Brain injury 31, 13-14 (2017), 1760‚Äì1780.\\n[50] James F Malec, Allen W Brown, Cynthia L Leibson, Julie Testa Flaada, Jayawant N\\nMandrekar, Nancy N Diehl, and Patricia K Perkins. 2007. The mayo classification\\nsystem for traumatic brain injury severity. Journal of neurotrauma 24, 9 (2007),\\n1417‚Äì1424.\\n[51] Skye McDonald and Helen Genova. 2021. The effect of severe traumatic brain\\ninjury on social cognition, emotion regulation, and mood. Handbook of clinical\\nneurology 183 (2021), 235‚Äì260.\\n[52] Emily L Morrow, Fangyun Zhao, Lyn Turkstra, Catalina Toma, Bilge Mutlu, and\\nMelissa C Duff. 2021. Computer-mediated communication in adults with and\\nwithout moderate-to-severe traumatic brain injury: survey of social media use.\\nJMIR rehabilitation and assistive technologies 8, 3 (2021), e26586.\\n[53] Debjani Mukherjee, Judy Panko Reis, and Wendy Heller. 2003. Women living with\\ntraumatic brain injury: Social isolation, emotional functioning and implications\\nfor psychotherapy. Women & Therapy 26, 1-2 (2003), 3‚Äì26.\\n[54] Academy of Neurologic Communication Disorders Traumatic Brain Injury Writ-\\ning Committee, Lindsey Byom, Therese M O‚ÄôNeil-Pirozzi, Rik Lemoncello, Sheila\\nMacDonald, Peter Meulenbroek, Bryan Ness, and McKay Moore Sohlberg. 2020.\\nSocial communication following adult traumatic brain injury: A scoping review\\nof theoretical models. American Journal of Speech-Language Pathology 29, 3\\n(2020), 1735‚Äì1748.\\n[55] Christian E Salas, Martin Casassus, Leanne Rowlands, Steve Pimm, and\\nDesmond AJ Flanagan. 2018. ‚ÄúRelating through sameness‚Äù: a qualitative study of\\nfriendship and social isolation in chronic traumatic brain injury. Neuropsycho-\\nlogical rehabilitation 28, 7 (2018), 1161‚Äì1178.\\n[56] Carmit-Noa Shpigelman and Carol J Gill. 2014. Facebook use by persons with\\ndisabilities. Journal of Computer-Mediated Communication 19, 3 (2014), 610‚Äì624.\\n[57] McKay Moore Sohlberg, Justine Hamilton, and Lyn S Turkstra. 2022. Transforming\\ncognitive rehabilitation: effective instructional methods . Guilford Publications.\\n[58] Susan Sprecher, Diane Felmlee, Jeffrey E Stokes, and Brandon McDaniel. 2019.\\nSocial networks and relationship maintenance. In Relationship maintenance:\\nTheory, process, and context . Cambridge University Press, 152‚Äì177.\\n[59] Margaret A Struchen, Monique R Pappadis, Angelle M Sander, Christina S Bur-\\nrows, and Katherine A Myszka. 2011. Examining the contribution of social\\ncommunication abilities and affective/behavioral functioning to social integra-\\ntion outcomes for adults with traumatic brain injury. The Journal of head trauma\\nrehabilitation 26, 1 (2011), 30‚Äì42.\\n[60] Leanne Togher, Jacinta Douglas, Lyn S Turkstra, Penny Welch-West, Shannon\\nJanzen, Amber Harnett, Mary Kennedy, Ailene Kua, Eleni Patsakos, Jennie Pons-\\nford, et al .2023. INCOG 2.0 guidelines for cognitive rehabilitation following\\ntraumatic brain injury, part IV: cognitive-communication and social cognition\\ndisorders. The Journal of head trauma rehabilitation 38, 1 (2023), 65‚Äì82.\\n[61] Leanne Togher, Skye McDonald, Carl A Coelho, and Lindsey Byom. 2013. Cogni-\\ntive communication disability following TBI: Examining discourse, pragmatics,\\nbehaviour and executive function. In Social and Communication Disorders Fol-\\nlowing Traumatic Brain Injury . Psychology Press, 89‚Äì118.\\n[62] Catalina L Toma, Juwon Hwang, Lisa Kakonge, Emily L Morrow, Lyn S Turkstra,\\nBilge Mutlu, and Melissa C Duff. 2024. Does facebook use provide social benefits\\nto adults with traumatic brain injury? Cyberpsychology, Behavior, and Social\\nNetworking 27, 3 (2024), 214‚Äì220.\\n[63] Theodore Tsaousides, Yuka Matsuzawa, and Matthew Lebowitz. 2011. Familiarity\\nand prevalence of Facebook use for social networking among individuals with\\ntraumatic brain injury. Brain injury 25, 12 (2011), 1155‚Äì1162.\\n[64] Lyn S Turkstra, W Huw Williams, James Tonks, and Ian Frampton. 2008. Measur-\\ning social cognition in adolescents: Implications for students with TBI returning\\nto school. NeuroRehabilitation 23, 6 (2008), 501‚Äì509.\\n[65] Pranav Narayanan Venkit, Mukund Srinath, and Shomir Wilson. 2023. Automated\\nableism: An exploration of explicit disability biases in sentiment and toxicity\\nanalysis models. arXiv preprint arXiv:2307.09209 (2023).\\n[66] Kwankaew Wongchareon, Hilaire J Thompson, Pamela H Mitchell, Jason Barber,\\nand Nancy Temkin. 2020. IMPACT and CRASH prognostic models for traumatic\\nbrain injury: external validation in a South-American cohort. Injury prevention\\n26, 6 (2020), 546‚Äì554.\\n[67] Shaomei Wu, Jeffrey Wieland, Omid Farivar, and Julie Schiller. 2017. Automatic\\nAlt-text: Computer-generated Image Descriptions for Blind Users on a SocialASSETS ‚Äô24, October 27‚Äì30, 2024, St. John‚Äôs, NL, Canada Hu and Lim, et al.\\nNetwork Service. In Proceedings of the 2017 ACM Conference on Computer Sup-\\nported Cooperative Work and Social Computing (Portland, Oregon, USA) (CSCW\\n‚Äô17). Association for Computing Machinery, New York, NY, USA, 1180‚Äì1192.\\nhttps://doi.org/10.1145/2998181.2998364[68] Fangyun Zhao, Hajin Lim, Emily L Morrow, Lyn S Turkstra, Melissa C Duff,\\nand Bilge Mutlu. 2022. Designing evidence-based support aids for social media\\naccess for individuals with moderate-severe traumatic brain injury: A preliminary\\nacceptability study. Frontiers in digital health 4 (2022), 991814.SMART-TBI ASSETS ‚Äô24, October 27‚Äì30, 2024, St. John‚Äôs, NL, Canada\\nA SOCIAL MEDIA TASKS\\nA.1 Writing Task\\nBelow are the three writing tasks that the participant completed\\nwith and without using the Writing Aid.\\n(1)If you are going to share a movie on Facebook, how would\\nyou write about it?\\n(2)If you are going to recommend a new restaurant on Facebook,\\nhow would you write about it?\\n(3)If you are going to write a post about the city you are living\\non Facebook, how would you write about it?\\nQuestions asked to the participant after they wrote each post:\\n(1) What emotions do you think of in the post? (open-ended)\\n(2)How negative or how positive do you think of this post?\\n(open-ended)\\n(3)Please choose from Very Negative, Negative, Neutral, Posi-\\ntive and Very Positive.\\nA.2 Interpretation Task\\nThe following are the three posts that participants were asked to\\nread and interpret with and without using the Interpretation Aid.\\n(1)Steamy day, but at least we have nature near where I work.\\nTook a walk near the lakeshore and saw some muskrats\\nswimming in the water. Took some photos and then went\\nback to my office. My daily routine of living by the lake.\\n(2)I can‚Äôt believe squirrels ate the electrical wires in my car.\\nToyota coats the wires with a soy product to be \"eco-friendly\",\\nand the squirrels ate it. It‚Äôs going to cost me $10,000 to replace.\\nSquirrels are just rats with bushy tails and the city should\\nban them!\\n(3)The week is half over and I still have so much to do! I attended\\na few meetings and then worked by the water the rest of the\\nday. It is a beautiful day out today and working near nature\\nhelps me focus on everything I need to complete.\\nQuestions asked to the participant to interpretation each post:\\n(1) What emotions do you think of in the post? (open-ended)\\n(2)How negative or how positive do you think of this post?\\n(open-ended)\\n(3)Please choose from Very Negative, Negative, Neutral, Posi-\\ntive and Very Positive.B QUESTIONNAIRE\\nB.1 TBI Background\\nWhat was the main cause of your brain injury?\\n‚Ä¢Motor vehicle crashes involving occupants or pedestrians\\n‚Ä¢Sports and recreation injuries (e.g. sports concussions, bicy-\\ncling injuries)\\n‚Ä¢Assaults and violence (e.g. domestic violence, abuse, gunshot\\nwounds/firearm injuries)\\n‚Ä¢Shaken Baby Syndrome- Abusive Head Trauma (AHT) or\\ninflicted Traumatic Brain Injury (iTBI)\\n‚Ä¢Blunt trauma- struck by or against an object\\n‚Ä¢Penetrating or open head wounds (e.g. lacerations)\\n‚Ä¢Explosive blasts (e.g. Improvised Explosive Devices)\\nWhat kinds of challenges have you experienced after you ac-\\nquired a brain injury? (You can select multiple answers)\\n‚Ä¢Short-term or long-term memory loss\\n‚Ä¢Impaired judgment and perception\\n‚Ä¢Trouble concentrating or paying attention\\n‚Ä¢Difficulty with language or speech production and thought\\nprocessing\\n‚Ä¢Spatial disorientation\\n‚Ä¢Difficulty organizing or problem solving\\n‚Ä¢Sensory loss or impairment (vision, hearing, etc.)\\n‚Ä¢Headaches or migraines\\n‚Ä¢Decreased motor abilities\\n‚Ä¢Depression\\n‚Ä¢Anxiety, restlessness, agitation, frustration, impatience\\n‚Ä¢Lack of motivation\\n‚Ä¢Reduced level of self-esteem\\n‚Ä¢Mood swings\\n‚Ä¢Impulsiveness and lack of inhibition\\n‚Ä¢Personality changes\\n‚Ä¢Emotional flatness and passivity\\n‚Ä¢Other\\n‚Ä¢Prefer not to answer\\nC USER EVALUATION RESULT\\nC.1 SUS scoreASSETS ‚Äô24, October 27‚Äì30, 2024, St. John‚Äôs, NL, Canada Hu and Lim, et al.\\nTable 8: TBI user participants‚Äô SUS feedback on each aid. D: Disagree , N:Neutral , A:Agree . Data represent the percentage of\\nusers choosing the option. E.g., For the statement ‚ÄúI think that I would like to use this system frequently,‚Äù 0.25 under D in\\nWriting aid indicates that 25% of participants chose Disagree for this statement; 0.25 under N indicates that 25% of participants\\nchose Neutral for this statement; and 0.5 under A indicates that 50% of participants chose Agree for this statement.\\nSUS result Writing Aid Interpretation Aid Focus Mode Filter Mode Facebook Customization\\nPercentage of participants choosing\\nDisagree ,Neutral , and AgreeD N A D N A D N A D N A D N A\\nI think that I would like to use this\\nsystem frequently.0.25 0.25 0.50 0.75 0.00 0.25 0.75 0.00 0.25 0.63 0.13 0.25 0.17 0.00 0.83\\nI found the system unnecessarily\\ncomplex.0.88 0.13 0.00 0.88 0.13 0.00 0.50 0.25 0.25 0.88 0.00 0.13 0.83 0.00 0.17\\nI thought the system was easy to use. 0.13 0.13 0.75 0.00 0.00 1.00 0.00 0.25 0.75 0.00 0.13 0.88 0.00 0.00 1.00\\nI think that I would need the support\\nof a technical person to be able to use\\nthis system.1.00 0.00 0.00 1.00 0.00 0.00 0.88 0.00 0.13 1.00 0.00 0.00 1.00 0.00 0.00\\nI found the various functions in this\\nsystem were well integrated.0.00 0.13 0.88 0.00 0.13 0.88 0.25 0.25 0.50 0.25 0.00 0.75 0.33 0.00 0.67\\nI thought there was too much incon-\\nsistency in this system.1.00 0.00 0.00 0.75 0.00 0.25 0.50 0.00 0.50 0.50 0.00 0.50 0.83 0.00 0.17\\nI would imagine that most people\\nwould learn to use this system very\\nquickly.0.00 0.00 1.00 0.00 0.00 1.00 0.25 0.13 0.63 0.00 0.13 0.88 0.00 0.00 1.00\\nI found the system very cumbersome\\nto use.0.63 0.00 0.38 0.75 0.00 0.25 0.50 0.25 0.25 0.63 0.00 0.38 0.67 0.00 0.33\\nI felt very confident using the sys-\\ntem.0.00 0.00 1.00 0.13 0.00 0.88 0.38 0.00 0.63 0.13 0.13 0.75 0.00 0.00 1.00\\nI needed to learn a lot of things be-\\nfore I could get going with this sys-\\ntem.1.00 0.00 0.00 0.88 0.00 0.13 0.75 0.00 0.25 0.88 0.00 0.13 0.83 0.00 0.17',\n",
       " 'State surveillance in the digital age: Factors associated with citizens‚Äô attitudes\\ntowards trust registers\\nKATJA TURHA, University of Maribor, Slovenia\\nSIMON VRHOVEC, University of Maribor, Slovenia\\nIGOR BERNIK, University of Maribor, Slovenia\\nThis paper investigates factors related to the acceptance of trust registers (e.g., the Chinese Social Credit System ‚Äì SCS) in Western\\nsettings. To avoid a negative connotation, we first define the concept of trust register which encompasses surveillance systems in other\\nsettings beyond China, such as FICO in the US. Then, we explore which factors are associated with people‚Äôs attitude towards trust\\nregisters leaning on the technology acceptance and privacy concern theories. A cross-sectional survey among Slovenian Facebook and\\nInstagram users ( ùëÅ=147) was conducted. Covariance-based structural equation modeling (CB-SEM) was used to test the hypothesized\\nassociations between the studied constructs. Results indicate that attitude towards trust register is directly associated with perceived\\ngeneral usefulness of the trust register. Additionally, perceived general usefulness is associated with perceived usefulness of the trust\\nregister for ensuring national security and fighting crime, its ease of use, and privacy concern regarding data collection. As one of the\\nfirst studies investigating attitude towards trust registers in a Western country, it provides pioneering insights into factors that may\\nbe relevant in case such registers would be implemented in a Western context, and provides some practical implications regarding\\nmessaging for would-be implementers of such systems.\\nAdditional Key Words and Phrases: data surveillance, dataveillance, netizens, cybernetic citizen, security, internet society, technology\\nacceptance model, TAM, privacy paradox, online privacy\\n1 INTRODUCTION\\nThe Social Credit System (SCS) developed in China is a widely discussed topic in the global community [ 52]. Human\\nrights advocates, government officials, and scholars of various specializations have taken an increasing interest in this\\nphenomenon [ 1,4,26,33,35,39,42,52]. However, despite a high public profile, this problem is scientifically still poorly\\nunderstood [52].\\nThe emerging SCS in China stands out as an initiative to radically transform society and the economy in the country\\n[4], with a desire to take even greater control of its population and other entities. The system is envisioned to rate\\nindividuals, businesses, social organizations, and government agencies based on their level of trustworthiness , and\\naims to be administered through various systems of punishments and rewards [ 26,32]. One of the more controversial\\nfeatures of this system is that it requires large amounts of personal data and information on each individual to function,\\nwhich is collected from a variety of sources, such as financial, criminal, and government records, as well as various\\ndata from registry and school offices [ 5,30,33]. Notably, it also tracks subjects‚Äô activities online by including various\\ndata from digital sources [ 5,33]. Digital data includes information collected on the internet, such as a person‚Äôs online\\nsearch history, shopping preferences, and social media interactions [ 33], control of which represents an intrusion into\\nan individual‚Äôs privacy [ 31]. In the future, the system could also include video system information obtained with facial\\nrecognition technology which is already widespread in China [ 5] coupled with techniques, for example, for image\\nsuperresolution (i.e., the process of enlarging and enhancing low-resolution images) [ 28] and age estimation [ 20],\\napplications of artificial intelligence [ 62], integration with VoIP mass surveillance systems [ 37], or data obtained from\\nsmart systems, such as smart transportation [ 6]. The development of the SCS itself is still ongoing so it cannot exactly\\nAuthors‚Äô addresses: Katja Turha, katja.turha@student.um.si, University of Maribor, Maribor, Slovenia; Simon Vrhovec, simon.vrhovec@um.si, University\\nof Maribor, Maribor, Slovenia; Igor Bernik, igor.bernik@um.si, University of Maribor, Maribor, Slovenia.\\n1arXiv:2408.09725v1  [cs.CY]  19 Aug 20242 Turha et al.\\nbe described as a single system [ 26,52,55]. For the time being, it is only an attempt to bring together different national,\\nprovincial, and municipal testing systems which focus on entirely different policies and issues [ 5,58]. This means that\\nthey do not have the same goal therefore the system cannot yet be unified. However, what all different SCS initiatives\\nhave in common is that they all seek to control through the establishment of a distributed state surveillance system fit\\nfor the current digital age and facilitate the rise of the digital society and cybernetic citizenship [21, 41, 50].\\nSuch systems of control are not specific only for China, though [ 58]. Different systems of control exist in other\\ncountries, such as Germany and the US, too, although that does not mean that they are socially recognized as such\\nas well since they focus on the financial aspect [ 61]. Schufa in Germany and EDGAR in the US are two examples of\\nnationwide public platforms disclosing corporate information on listed companies [ 27]. Among the best-known systems\\nfor individuals is the US FICO which indicates the creditworthiness of a particular individual [ 24] and is used by 75\\npercent of lenders in the US [ 22]. It was developed around 1950 [ 16] and officially came into use in 1989 [ 24]. It is a\\nsystem that evaluates whether or not an individual is creditworthy based on a credit score [ 3,22,24]. According to this\\nsystem, the higher the score, the higher the level of creditworthiness [ 7]. The system uses five different factors [ 8,14],\\nbut the exact formula for calculating an individual‚Äôs creditworthiness is not publicly known [ 3,14,22]. Banks use such\\nsystems to protect themselves from untrustworthy individuals making it easier for them to increase their profits [ 47]. A\\ngood way to predict an individual‚Äôs actions is to evaluate their past [ 14,16]. The credit score is based completely on the\\ninformation contained in the credit report [ 22] so the information obtained and processed must be correct. Numerous\\ncases have shown that errors still occur in the processing of data (e.g., mistaken social security number, bankruptcy\\nerror) [ 7,22] with individuals being severely affected as the errors impact their creditworthiness [ 3]. To have a credit\\nscore, an individual must have at least one credit card or one loan, and if an individual does not have them, banks do\\nnot have enough information to calculate their credit score [ 17]. A 2016 survey found that approximately 45 million\\npeople in the US do not have a credit score [ 7]. The consequences of a poor or undefined credit score significantly\\nimpact an individual‚Äôs life and functioning in society, as they consequently have limited access to financial assistance\\n[7,17]. Even when borrowing money, they are charged the highest possible interest rates which has a direct impact on\\ntheir lives and their ability to save and improve their living situation [7, 17].\\nBoth the Chinese SCS and comparable systems in other countries measure the level of trust of an individual and thus\\ninfluence the individual‚Äôs social functioning. To the best of our knowledge, the general concept of such surveillance\\nsystems has not been defined before. For the purpose of our study, we define the trust register as an official register that\\ncan be introduced at the state level to monitor, assess and regulate the financial, social, moral and political behavior\\nof natural and legal persons through a system of penalties and rewards. Trust registers provide various benefits to\\ntrusted people (e.g., tax breaks, easier access to loans and housing, cheaper public transport, shorter waiting times\\nfor health-related services, the possibility of renting a car without providing a security deposit) and through various\\npenalties (e.g., tougher access to loans, limited access to public services, prohibition to perform state jobs, harder access\\nto education) to encourage untrustworthy people to improve [ 56]. Trust registers draw information from a variety of\\ntraditional (e.g., financial, criminal, and state records, registry and school office data) and digital sources (e.g., online\\nsearch history, online shopping history, social media activities) [ 30]. The data is accumulated into trust registers and\\nprocessed automatically [ 61]. The use of trust registers may be possible through a mobile application that allows\\nindividuals and organizations to see the level of trustworthiness of others, for example, online stores and potential\\ncustomers. Everyone may quickly find out whether or how he wants to establish contact or do business with natural\\nand legal persons whom he does not know yet or has no experience with [57].Factors associated with citizens‚Äô attitudes towards trust registers 3\\nA few studies have explored public opinion on SCS in China by examining citizens‚Äô attitudes toward the system\\nand their privacy concerns [ 1,26,39,42]. Published literature suggests that SCS receives high levels of support among\\nChinese citizens which could be attributed to the lack of knowledge about the system [ 60]. Studies also indicate that\\nsupport is correlated with one‚Äôs generalized fear [ 63]. On the organizational level, the use of SCS has also been associated\\nwith innovation [ 64]. However, there is a general lack of research that would investigate individuals‚Äô attitudes towards\\nsuch surveillance systems outside of China. We found a single study, conducted in China, that indicates that public\\nsupport for SCS may be lower when exposed to Western framing albeit only when individuals are informed about the\\nmonitoring of social behavior by SCS [ 60]. Therefore, it might be safe to assume that such surveillance systems may\\nnot be as well-received in Western countries even though comparable systems are already in place there. To the best of\\nour knowledge, there are no studies that would explore the factors associated with attitudes, adoption or rejection of\\ntrust registers outside of China.\\nThis paper aims to address the above presented gaps in our understanding of what shapes one‚Äôs attitude toward\\ntrust registers. This study makes four key contributions. First, this study defines the concept of trust register as an\\numbrella term for surveillance systems. Second, by considering trust registers as a technological innovation leveraged\\nby a state for surveillance, our study leans on the technology acceptance theory to explore factors associated with\\nattitude towards trust registers contributing to both state data surveillance and technology acceptance theory. Third, it\\nis among the first to study how online privacy concerns are related to attitude towards trust registers contributing to\\nthe privacy concerns literature. Fourth, this study provides some insights into acceptance factors for implementations\\nof trust registers with broadened scopes in Western countries.\\n2 RESEARCH MODEL\\nThe study aims to investigate the main factors that would positively or negatively influence attitude toward trust\\nregisters. For this research, we have built a research model based on the hypotheses we have put forward. The model\\nshows us the factors that we assume will have the most significant relations with attitude towards trust registers. The\\nresearch model is presented in Figure 1.\\nHow individuals would accept the use of new technology can be tested using the Technology Acceptance Model\\n(TAM) [ 12]. The original TAM is based on two main predictors (i.e., perceived usefulness and perceived ease of use) of\\nattitude towards use of new technology [ 12]. Perceived usefulness measures the extent to which a person believes that\\nusing a new technology will increase their performance and gain benefits [ 53]. Perceived ease of use however helps\\nus to determine the extent to which a person believes that they would be able to use the new technology effortlessly\\nand that it would be easy to use [ 12]. In other words, the more benefits the user gets and the easier new technology is\\nto use, the more likely individuals will support it and be willing to use it [ 2]. Based on the original TAM model, we\\ntherefore formulate the following hypotheses:\\nH1: Perceived usefulness of the trust register is positively associated with attitude towards trust register.\\nH2a: Ease of use of the trust register is positively associated with attitude towards trust register.\\nH2b: Ease of use of the trust register is positively associated with perceived usefulness of the trust register.\\nPerceived usefulness can be measured by various indicators, such as speed of achievement, increased productivity,\\nefficiency, etc. [ 59]. In the context of trust registers, there are two key benefits that are emphasized by their implementers,\\nnamely ensuring national security and fighting crime. For the purpose of our study, we assume these two dimensions4 Turha et al.\\nFig. 1. Research model.\\nof perceived usefulness shape the perceived general usefulness of trust registers. Based on these considerations, we\\ndevelop the following set of hypotheses:\\nH3a:Perceived usefulness of the trust register for ensuring national security is positively associated with perceived\\ngeneral usefulness of the trust register.\\nH3b:Perceived usefulness of the trust register for fighting crime is positively associated with perceived general\\nusefulness of the trust register.\\nInformation privacy pertains to an individual‚Äôs capacity to personally manage the information concerning their\\nidentity [ 48]. When discussing measuring privacy concerns, we can divide them into five different dimensions [ 18,\\n40,46,51]. First, concern regarding data collection is connected with the collection and storage of personal data in a\\ncertain system. Second, concern regarding secondary use of data is about the use of stored data in a certain system for\\na different purpose than it was collected for. Third, concern regarding improper access is related to data stored in a\\ncertain system being accessed by unauthorized persons. Fourth, concern regarding errors is linked to errors in collected\\ndata stored in a certain system. Fifth, concern regarding control is connected with (the lack of) control that individuals\\nhave over their collected data stored in a certain system. People who worry more about their privacy may therefore\\nfeel mentally burdened when they need to share their personal data with trust registers decreasing their motivation to\\nengage with such systems [ 13,18]. However, such feelings have a negative impact on perceived usefulness [ 18], and\\nindividuals will be less likely to use technology that lowers their expectations of privacy [ 15]. Based on the privacy\\nliterature, we thus pose the following set of hypotheses:\\nH4a:Privacy concern about data collection is negatively associated with perceived usefulness of the trust register.Factors associated with citizens‚Äô attitudes towards trust registers 5\\nH4b:Privacy concern about secondary use of data is negatively associated with perceived usefulness of the trust\\nregister.\\nH4c:Privacy concern about improper access to data is negatively associated with perceived usefulness of the\\ntrust register.\\nH4d: Privacy concern about data errors is negatively associated with perceived usefulness of the trust register.\\nH4e: Privacy concern about data control is negatively associated with perceived usefulness of the trust register.\\nTrust may equal power and can therefore be valuable in many interactions. Establishing trust is a long process\\nas it develops over a long period of time, but can be lost in an instant [ 49]. Trust is a psychological state in which\\nan individual is willing to acknowledge, accept, or show their vulnerability to a particular individual or the public\\nat large because they expect positive intentions from the other party [ 10]. Trust also has a strong influence on the\\nperformance of governments [ 9]. Studies show that in modern democracies citizens‚Äô distrust of their government can\\nhave a negative impact on its performance [ 38]. Our trust in institutions is conditioned by our expectations, knowledge\\nof their functioning and intentions, and the competence of the individuals who work within them [ 9]. The greater\\nthe trust in government, the more willing citizens are to engage with it [ 49]. Citizen engagement with government is\\nnecessary because government institutions and agencies need personal data to operate, which they collect and process\\n[38] to ensure the proper functioning of government systems such as e-government, healthcare, etc. Providing personal\\ndata to government organizations is therefore absolutely necessary in many cases [ 10]. So it makes it all the more\\nimportant that government and government organizations handle sensitive information correctly and use the data\\nonly for the purposes for which it is collected [ 38]. Individuals who do not trust the government and its performance\\nare skeptical about participating [ 10] in different surveillance systems, including trust registers. Based on this, we\\ndeveloped the final hypothesis:\\nH5: Trust in government is negatively associated with attitude towards trust register.\\nThe research model therefore draws from the technology acceptance, privacy concern and trust theories, and tries to\\ndraw synergies in explaining how attitudes towards trust registers are formed.\\n3 METHODS\\n3.1 Research design\\nOur study used a cross-sectional research design to investigate how the Slovenian population would accept a trust\\nregister (i.e., a surveillance system comparable to the SCS). In order to identify the factors related to attitude towards a\\ntrust register, we conducted the survey among Slovenian Facebook and Instagram users.\\n3.2 Ethical considerations\\nApproval from the Institutional Review Board for this study was not required according to the legislation of the Republic\\nof Slovenia and internal acts of the University of Maribor.\\n3.3 Measures\\nThe questionnaire measured 12 theoretical constructs shown in Table 1. We measured the following constructs: attitude\\ntowards trust register, perceived usefulness (overall, national security, crime), ease of use, privacy concern (collection,\\nerrors, secondary use, improper access, control) and trust in government. For the purposes of our research, we have took\\nor adapted previously validated construct items to the context of our study. We took the items for trust in government6 Turha et al.\\nfrom [ 25]. Items for all privacy concern constructs were adapted from [ 23]. Items for perceived usefulness were adapted\\nfrom [ 34]. Items for perceived usefulness (national security), perceived usefulness (crime) and ease of use were adapted\\nfrom [ 54]. Items for attitude towards trust register were adapted from [ 45]. Items were measured using Likert and\\nbipolar scales. Items for attitude towards the trust register were measured with a 5-point bipolar scale. Items for all\\nprivacy concern constructs were measured by using a 5-point Likert scale from 1 ( strongly disagree ) to 5 ( strongly agree ).\\nThe remaining items were measured with a 7-point Likert scale from 1 ( strongly disagree ) to 7 ( strongly agree ). The\\nsurvey was conducted in the Slovenian language.\\nTable 1. Theoretical construct definitions.\\nTheoretical construct Operational definition\\nAttitude towards trust register An individual‚Äôs positive versus negative evaluations of the trust register.\\nPerceived usefulness The perceived general usefulness of the trust register.\\nPerceived usefulness (national security) The perceived usefulness of the trust register for ensuring national security.\\nPerceived usefulness (crime) The perceived usefulness of the trust register for fighting crime.\\nEase of use The perceived ease of use of the trust register.\\nPrivacy concern (collection) The extent of concerns regarding the collection of personal information by the trust register.\\nPrivacy concern (errors) The extent of concerns regarding errors in personal information stored in the trust register.\\nPrivacy concern (secondary use) The extent of concerns regarding secondary use of personal information stored in the trust register.\\nPrivacy concern (improper access) The extent of concerns regarding improper access to personal information stored in the trust register.\\nPrivacy concern (control) The extent of concerns regarding control over personal information stored in the trust register.\\nTrust in government The extent of trusting beliefs in the government.\\n3.4 Data collection\\nThe survey was available from August to December 2021. The invitation to take the survey was posted in 69 Facebook\\ngroups and shared 11 times on Instagram, implying a convenience sample. Participation in our survey was voluntary\\nand anonymous. 155 respondents participated in our survey. We excluded seven responses with over 10 percent missing\\nvalues and one response indicating respondent non-engagement. We ended up with ùëÅ=147usable responses for the\\nanalysis. Characteristics of the sample are presented in Table 2. The age of respondents ranged from 17 to 69 years old\\n(ùëÄ=32.1,ùëÜùê∑=12.0). The primary source of information for 77.6 percent of respondents was the internet indicating a\\nsample biased towards internet users. This is likely a consequence of conducting the survey as an online questionnaire.\\n3.5 Data analysis\\nTo analyze the data, we employed covariance-based structural equation modeling (CB-SEM). The key advantage of this\\ndata analysis method is that it integrates into a concurrent evaluation latent variables with multiple indicators and their\\ninter-relations. We used R ver. 4.3.1, lavaan ver. 0.6‚Äì15 and semTools ver. 0.5-6 to analyze the data. Prior to the analysis,\\nwe imputed missing values (0.2 percent) with medians. We used standard model fit indices and thresholds: ùúí2/ùëëùëì‚Äì\\n<2.0excellent fit, 2.0‚àí5.0good fit, >5.0poor fit; CFI ‚Äì >0.95excellent fit, 0.90‚àí0.95good fit, <90.0poor fit; TLI\\n‚Äì>0.95excellent fit, 0.90‚àí0.95good fit, <90.0poor fit; RMSEA ‚Äì <0.06excellent fit, 0.06‚àí0.10good fit, >0.10\\npoor fit; and SRMR ‚Äì <0.06excellent fit, 0.06‚àí0.08good fit, >0.08poor fit. We conducted a confirmatory factor\\nanalysis (CFA) to validate the survey instrument. First, we determined convergent validity by evaluating AVE and factor\\nloadings of questionnaire items. AVE values above the 0.50threshold are considered acceptable. Next, we evaluated\\ndiscriminant validity with a HTMT analysis. Ratios of correlations below the 0.90threshold are considered acceptable.\\nFinally, we determined reliability with CR and CA. CR and CA values above the 0.70threshold are considered acceptable.\\nA structural model was constructed to test the hypothesized associations.Factors associated with citizens‚Äô attitudes towards trust registers 7\\nTable 2. Sample characteristics.\\nCharacteristic Frequency Percent\\nGender Female 105 71 .4\\nMale 41 27 .9\\nN/A 1 0 .7\\nEmployment status Student 47 32 .0\\nEmployed / Self-employed 82 55 .8\\nFarmer / Housewife 2 1 .4\\nUnemployed 9 6 .1\\nRetired 7 4 .8\\nFormal education Finished high school or less 58 39 .5\\nAcquired Bachelor‚Äôs degree 54 36 .7\\nAcquired Master‚Äôs degree 28 19 .0\\nAcquired PhD degree 7 4 .8\\nLiving environment Urban 91 61 .9\\nRural 56 38 .1\\nStatus Single 51 34 .7\\nIn a relationship ‚Äì not living together 19 12 .9\\nIn a relationship ‚Äì living together 38 25 .9\\nMarried 35 23 .8\\nDivorced 4 2 .7\\nPrimary source of information Printed media 2 1 .4\\nRadio 7 4 .8\\nTV 15 10 .2\\nInternet 114 77 .6\\nFamily and friends 9 6 .1\\n4 RESULTS\\n4.1 Instrument validation\\nWe first developed a measurement model to validate the measurement instrument. Model fit of the measurement\\ninstrument is presented in Table 3. It shows that the data fits the model well.\\nTable 3. Fit indices of the measurement model.\\nMeasure Threshold Estimate Interpretation\\nùúí2641.186\\nùëëùëì 440\\nùúí2/ùëëùëì‚â§5 1 .457 Excellent\\nCFI‚â•0.90 0 .965 Excellent\\nTLI‚â•0.90 0 .959 Excellent\\nRMSEA‚â§0.08 0 .056 Excellent\\nSRMR‚â§0.08 0 .040 Excellent\\nNotes : CFI ‚Äì comparative fit index; TLI ‚Äì Tucker-Lewis index; RMSEA ‚Äì root mean square error of approximation; SRMR ‚Äì standardized root mean square\\nresidual.\\nTable 4 presents the results of analyses relevant for determining the validity and reliability of the survey instrument.\\nFirst, CA ranged from 0.859 to 0.981 and CR ranged from 0.861 to 0.981 demonstrating adequate reliability of all\\nconstructs. Second, AVE ranged from 0.676 to 0.945 indicating adequate convergent validity. Third, HTMT analysis\\nindicates that discriminant validity of the measurement instrument is adequate.8 Turha et al.\\nTable 4. Survey instrument validation. Cronbach‚Äôs alpha (CA), composite reliability (CR), average variance extracted (AVE), and\\nheterotrait-monotrait ratio of correlations (HTMT) analysis.\\nConstruct CA CR AVE 1 2 3 4 5 6 7 8 9 10\\n1: PCc 0.910 0.908 0.768\\n2: PCsu 0.957 0.958 0.885 0.761\\n3: PCe 0.926 0.925 0.805 0.541 0.593\\n4: PCia 0.957 0.958 0.884 0.766 0.832 0.646\\n5: PCctl 0.937 0.937 0.833 0.759 0.852 0.609 0.767\\n6: TiG 0.859 0.861 0.676 0.250 0.284 0.314 0.289 0.338\\n7: PU 0.953 0.954 0.873 0.128 0.131 0.047 0.220 0.038 0.105\\n8: PUns 0.981 0.981 0.945 0.050 0.023 0.090 0.073 0.039 0.106 0.828\\n9: PUc 0.971 0.971 0.919 0.070 0.017 0.074 0.089 0.041 0.155 0.759 0.764\\n10: EoU 0.888 0.887 0.725 0.054 0.118 0.092 0.168 0.019 0.101 0.689 0.553 0.549\\n11: AtTR 0.891 0.890 0.729 0.095 0.200 0.028 0.247 0.111 0.091 0.864 0.777 0.685 0.612\\nNotes : PCc ‚Äì privacy concern (collection); PCsu ‚Äì privacy concern (secondary use); PCe ‚Äì privacy concern (errors); PCia ‚Äì privacy concern (improper\\naccess); PCctl ‚Äì privacy concern (control); TiG ‚Äì trust in government; PU ‚Äì perceived usefulness; PUns ‚Äì perceived usefulness (national security); PUc ‚Äì\\nperceived usefulness (crime); EoU ‚Äì ease of use; AtTR ‚Äì attitude towards trust register.\\nTable 5. Questionnaire items.\\nConstruct Loading Prompt / Item Source\\nPrivacy concerns (collection) 0.865 PCc1. It usually bothers me when e-government websites ask me for personal information. [23]\\n0.848 PCc2. When e-government websites ask me for personal information, I sometimes think twice before\\nproviding it.\\n0.911 PCc3. I am concerned that e-government websites are collecting too much personal information about me.\\nPrivacy concerns (secondary usage) 0.909 PCsu1. I am concerned that when I give personal information to a e-government website for some reason,\\nthe website would use the information for other reasons.[23]\\n0.964 PCsu2. I am concerned that e-government websites would sell my personal information in their computer\\ndatabases to public or private companies.\\n0.946 PCsu3. I am concerned that e-government websites would share my personal information with public or\\nprivate companies without my authorization.\\nPrivacy concerns (errors) 0.909 PCe1. I am concerned that e-government websites do not take enough steps to make sure that my personal\\ninformation in their files is accurate.[23]\\n0.874 PCe2. I am concerned that e-government websites do not have adequate procedures to correct errors in my\\npersonal information.\\n0.910 PCe3. I am concerned that e-government websites do not devote enough effort to verifying the accuracy of\\nmy personal information in their databases.\\nPrivacy concerns (improper access) 0.916 PCia1. I am concerned that e-government website databases that contain my personal information are not\\nprotected from unauthorized access.[23]\\n0.956 PCia2. I am concerned that e-government websites do not devote enough effort to preventing unauthorized\\naccess to my personal information.\\n0.950 PCia3. I am concerned that e-government websites do not take enough steps to make sure that unauthorized\\npeople cannot access my personal information.\\nPrivacy concerns (control) 0.920 PCctl1. It usually bothers me when I do not have control of personal information that I provide to e-\\ngovernment.[23]\\n0.914 PCctl2. It usually bothers me when I do not have control over decisions about how my personal information\\nis collected, used, and shared by e-government websites.\\n0.905 PCctl3. I am concerned when control over my personal information is lost as a result of a marketing\\ntransaction with e-government websites.\\nTrust in government 0.933 TiG1. I believe that the government would act in my best interest. [25]\\n0.641 TiG2. The government is interested in my well-being not just its own.\\n0.933 TiG3. I would characterize the government as honest.\\nPerceived usefulness 0.954 PU1. Being included in a trust register would be useful for me. [34]\\n0.963 PU2. Being included in a trust register would be very beneficial for me.\\n0.890 PU3. Being included in a trust register would give me access to useful information.\\nPerceived usefulness (national security) 0.966 PUns1. A trust register would make it easier to ensure national security. [54]\\n0.989 PUns2. I would find a trust register useful in ensuring national security.\\n0.962 PUns3. A trust register would enhance the effectiveness of ensuring national security.\\nPerceived usefulness (crime) 0.952 PUc1. A trust register would make it easier to fight crime. [54]\\n0.944 PUc2. I would find a trust register useful in fighting crime.\\n0.979 PUc3. A trust register would enhance the effectiveness of fighting crime.\\nEase of use 0.903 EoU1. My interaction with a trust register would be clear and understandable. [54]\\n0.771 EoU2. It would be easy for me to become skillful at using a trust register.\\n0.876 EoU3. I would find a trust register easy to use.\\nAttitude towards trust register In general, how do you feel about trust registers? [45]\\n0.840 AtTR1. Negative ... Positive\\n0.891 AtTR2. Undesirable ... Desirable\\n0.828 AtTR3. Harmful ... BeneficialFactors associated with citizens‚Äô attitudes towards trust registers 9\\n4.2 Structural model\\nWe developed a structural model to test the hypothesized associations. Model fit of the structural model is presented in\\nTable 6. It indicates that the model fits the data well.\\nTable 6. Fit indices of the structural model.\\nMeasure Threshold Estimate Interpretation\\nùúí2650.845\\nùëëùëì 448\\nùúí2/ùëëùëì‚â§5 1 .453 Excellent\\nCFI‚â•0.90 0 .965 Excellent\\nTLI‚â•0.90 0 .959 Excellent\\nRMSEA‚â§0.08 0 .055 Excellent\\nSRMR‚â§0.08 0 .041 Excellent\\nNotes : CFI ‚Äì comparative fit index; TLI ‚Äì Tucker-Lewis index; RMSEA ‚Äì root mean square error of approximation; SRMR ‚Äì standardized root mean square\\nresidual.\\nStandardized results of the structural model are presented in Figure 2. The results of the structural model support\\nhypotheses H1 ( ùëù<0.001), H2b (ùëù<0.001), H3a (ùëù<0.001), H3b (ùëù<0.001) and H4a (ùëù=0.003). However, the\\nresults do not indicate support for hypotheses H2a ( ùëù=0.260), H4b (ùëù=0.558), H4c (ùëù=0.860), H4d (ùëù=0.487), H4e\\n(ùëù=0.089) and H5 (ùëù=0.065).\\nFig. 2. Structural model.10 Turha et al.\\n5 DISCUSSION\\nThis study is among the first to investigate factors associated with attitude towards trust registers by leaning on the\\ntechnology acceptance and privacy concerns theory. It is also among the first such studies focusing on a population\\noutside China. It makes a number of contributions to the literature on attitudes toward state data surveillance, tech-\\nnology acceptance and privacy concerns, and provides some practical implications regarding messaging for would-be\\nimplementers of such systems in Western contexts. First, this study defines the concept of trust register as an umbrella\\nterm for surveillance systems comparable to the SCS. SCS and comparable systems in the West are essentially registers\\nin which data are accumulated and processed to determine an individual‚Äôs or other entity‚Äôs trustworthiness score. The\\naim of defining a new neutral concept instead of focusing the study on SCS was to avoid the effect of the potential\\nnegative connotation that SCS may have in Western countries, such as Slovenia. Although studies focusing on SCS\\nmay have some merit in Western countries too, the negative connotation due to the Western media exposure and the\\nfact that SCS originates from China may affect the validity of such studies. Therefore, the implications of such studies\\nmight have little relevance for potential implementations of SCS-like systems in Western countries. Since trust registers\\nencompass both SCS and other surveillance systems, using this concept may enable researchers to study the impact of\\nsuch (non-)negative connotations of both SCS and other comparable systems.\\nSecond, this is one of the first studies to investigate which factors are associated with attitude towards trust register.\\nThe results of our study indicate that the associations based on the original TAM have merit in the context of our study.\\nMore specifically, perceived usefulness is directly associated with attitude towards trust registers. Even though ease of\\nuse was not directly related to attitude towards trust register as predicted by the original TAM, there is an indirect\\nrelation mediated by perceived usefulness as predicted by the original TAM [ 36]. We additionally found that perceived\\ngeneral usefulness is associated with both perceived usefulness of the trust register for ensuring national security and\\nfighting crime. These findings indicate that these often emphasized SCS goals are indeed relevant in shaping the attitude\\ntowards trust register and thus effective in promoting SCS to the public. Although trust registers are enabled and may\\nbe ultimately controlled by governments, trust in government was not associated with attitude towards trust register.\\nThis may be a consequence of the study settings since it was conducted in a Western democracy as countries vary\\nconsiderably according to their residents‚Äô surveillance concern, fear of government intrusions into privacy and trust in\\ngovernment [ 19]. Future studies in other countries around the globe (e.g., autocratic regimes) would be necessary to\\ndetermine whether there are significant differences in other political contexts.\\nThird, the results of our study indicate that privacy concern about data collection is indirectly associated with attitude\\ntowards trust register through perceived usefulness. Although the published literature emphasizes privacy concern of\\ncitizens due to the overreaching nature of trust registers [ 1,26,39,42], this is one of the first studies that empirically\\ntest these assumptions. The results of this study suggest that only one out of five tested dimensions of privacy concern\\nis associated with attitude towards trust register. These findings is therefore only partially in line with the published\\nSCS literature as respondents seem to associate only data collection with their attitude toward trust register, and not\\nother privacy-related issues, such as errors in collected data, secondary use of the data, improper access to the data (e.g.,\\nby hackers) or the lack of control over collected data. Nevertheless, this may be yet another example of the privacy\\nparadox according to which people engage in privacy infringing behavior despite voicing concern about it [29].\\nFourth, the results of this study provide some insights into the possible implementation of trust registers in Western\\ncountries. As already noted, trust registers are not unheard of in these countries (e.g., EDGAR, Schufa, FICO [ 24,27]).\\nHowever, these systems focus solely on the financial aspect [ 61]. Even though Western trust registers are not associatedFactors associated with citizens‚Äô attitudes towards trust registers 11\\nwith the social aspect of surveillance, surveillance of social media data is extensive in Western countries too [ 43,44].\\nThe results of our study provide some insights into acceptance factors for potential implementers of trust registers\\nwith broadened scopes (e.g., the social aspect) in Western countries. Messaging regarding the implementation of a trust\\nregister with a broadened scope may focus on its perceived usefulness. The results of our study suggest that perceived\\ngeneral usefulness of trust registers may be shaped by its ease of use and usefulness for ensuring national security\\nand fighting crime. Key messaging points could therefore focus on these topics to shape an adequate attitude towards\\ntrust register. These topics are already ingrained in Western societies as acceptable reasons for exchanging privacy for\\nsecurity [ 11]. Additionally, the results of our study indicate that messaging aiming to relieve the privacy concern may\\nprimarily focus on alleviating people‚Äôs concern regarding the collection of data. These practical implication should\\nhowever be taken with some reserve as it assumes that the target population does not associate such a trust register with\\nSCS or similar systems. Should the population relate the implementation of a trust register to the implementation of a\\ncontroversial surveillance system, the negative connotation of such a trust register might significantly alter acceptance\\nfactors. Future studies, such as those focusing on a potential implementation of a SCS-like surveillance system in the\\nWest, may be needed to estimate the extent of these changes.\\nThis study has some limitations that the readers should note. First, the sample is not representative therefore the\\nreaders should be careful when generalizing the results to the studied population. Future studies may aim to include\\nrespondents that were underrepresented in our sample, notably people whose primary source of information is not the\\ninternet. Data collection methods beyond online surveys may be employed to achieve these aims. Second, the study\\nwas conducted in a single Western country. For improving the ecological validity of the study, future studies may\\nfocus on other countries in the Western cultural context, too. To better understand the differences between the Eastern\\nand Western cultural contexts as well as in countries with varying surveillance concerns [ 19], comparative studies\\nwould be highly beneficial as well. Third, the study described trust registers to the respondents without providing them\\nwith any real-world examples of such registers. Although the description was based on the SCS, the respondents may\\nnot have grasped all implications of implementing trust registers on a large scale. Future studies focusing on existing\\ntrust registers, such as SCS, EDGAR, Schufa or FICO, or implementations of other pilot trust registers, perhaps for the\\nsole purpose of conducting experiments, may thus be beneficial. Fourth, this study focused on attitude towards trust\\nregister. Attitude is an important albeit not the only acceptance factor. Future studies may include other acceptance\\nfactors, such as social influence, in their investigations. However, the role of some of these factors may be hard to study\\nwithout actual trust register implementations. Additionally, studying factors associated with actual acceptance in pilot\\nimplementations of trust registers would be highly beneficial. Finally, our study focused on individuals. Trust registers\\ncan be used for both individuals and organizations. Therefore, future studies may explore the factors associated with\\nacceptance of trust registers by organizations.\\nACKNOWLEDGMENTS\\nThe authors thank the respondents for taking their time to participate in the study.\\nREFERENCES\\n[1] Shazeda Ahmed. 2017. Consumer Protection Oversights in the Chinese Social Credit System . Technical Report. Digital Credit Observatory.\\n[2] Madini O Alassafi. 2022. E-learning intention material using TAM: A case study. Materials Today: Proceedings 61 (2022), 873‚Äì877.\\n[3]Shweta Arya, Catherine Eckel, and Colin Wichman. 2013. Anatomy of the credit score. Journal of Economic Behavior & Organization 95 (2013),\\n175‚Äì185.12 Turha et al.\\n[4]Jonathan Bach. 2020. The red and the black: China‚Äôs social credit experiment as a total test environment. The British Journal of Sociology 71, 3\\n(2020), 489‚Äì502.\\n[5] Larry Cat√° Backer. 2019. China‚Äôs Social Credit System. Current History 118, 809 (2019), 209‚Äì214.\\n[6] Meriem Benyahya, Anastasija Collen, Sotiria Kechagia, and Niels Alexander Nijdam. 2022. Automated city shuttles: Mapping the key challenges in\\ncybersecurity, privacy and standards to future developments. Computers & Security 122 (2022), 102904. https://doi.org/10.1016/j.cose.2022.102904\\n[7] Kenneth P Brevoort, Philipp Grimm, and Michelle Kambara. 2016. Credit invisibles and the unscored. Cityscape 18, 2 (2016), 9‚Äì34.\\n[8] Satyajit Chatterjee, Dean Corbae, and Jose-Victor Rios-Rull. 2005. Credit scoring and competitive pricing of default risk. In USC FBE Macroeconomics\\nand International Finance Workshop . University of Texas at Austin, Austin, TX.\\n[9] Sofia Elena Colesca. 2009. Understanding trust in e-government. Engineering Economics 63, 3 (2009).\\n[10] Rowena Cullen and Patrick Reilly. 2008. Information Privacy and Trust in Government: a citizen-based perspective from New Zealand. Journal of\\nInformation Technology & Politics 4, 3 (2008), 61‚Äì80.\\n[11] Joseph Da Silva. 2022. Cyber security and the Leviathan. Computers & Security 116 (2022), 102674. https://doi.org/10.1016/j.cose.2022.102674\\n[12] Fred D Davis. 1989. Perceived usefulness, perceived ease of use, and user acceptance of information technology. MIS quarterly (1989), 319‚Äì340.\\n[13] Fred D Davis, Richard P Bagozzi, and Paul R Warshaw. 1992. Extrinsic and intrinsic motivation to use computers in the workplace 1. Journal of\\napplied social psychology 22, 14 (1992), 1111‚Äì1132.\\n[14] Yuliya Demyanyk. 2010. Your credit score is a ranking, not a score. Economic Commentary 2010-16 (2010).\\n[15] Devendra Dhagarra, Mohit Goswami, and Gopal Kumar. 2020. Impact of trust and privacy concerns on technology acceptance in healthcare: an\\nIndian perspective. International journal of medical informatics 141 (2020), 104164.\\n[16] Robert M Doroghazi. 2020. Fico scores. American Journal of Cardiology 130 (2020), 157‚Äì158.\\n[17] Diana Elliott, Ricki Granetz Lowitz, and Working Credit NFP. 2018. What Is the Cost of Poor Credit? Washington, DC: Urban Institute (2018).\\n[18] Nuno Fortes and Paulo Rita. 2016. Privacy concerns and online purchasing behaviour: Towards an integrated model. European Research on\\nManagement and Business Economics 22, 3 (2016), 167‚Äì176.\\n[19] D. Fujs and S.L.R. Vrhovec. 2019. Cyber Landscape of Trust, Fear and Surveillance Concerns: How Slovenians Around the Globe Perceive the\\nCyberspace. Journal of Criminal Justice and Security 21, 4 (2019), 333‚Äì345.\\n[20] Petra Grd, Ena Barƒçiƒá, Igor Tomiƒçiƒá, and Bogdan Okre≈°a √êuriƒá. 2023. Analysing the Impact of Gender Classification on Age Estimation. In European\\nInterdisciplinary Cybersecurity Conference . ACM, Stavanger, Norway, 134‚Äì137. https://doi.org/10.1145/3590777.3590813\\n[21] Hans Krause Hansen. 2023. Governing through metrics in the digital age. Globalizations 20, 1 (2023), 137‚Äì152. https://doi.org/10.1080/14747731.\\n2022.2156700\\n[22] Evan Hendricks. 2011. Credit Reports, Credit Checks, Credit Scores. GPSolo 28 (2011), 33.\\n[23] Weiyin Hong and James YL Thong. 2013. Internet privacy concerns: An integrated conceptualization and four empirical studies. Mis Quarterly\\n(2013), 275‚Äì298.\\n[24] Joshua Ignatius, Adel Hatami-Marbini, Amirah Rahman, Lalitha Dhamotharan, and Pegah Khoshnevis. 2018. A fuzzy decision support system for\\ncredit scoring. Neural Computing and Applications 29 (2018), 921‚Äì937.\\n[25] Luka Jelovƒçan, Simon Vrhovec, and Damjan Fujs. 2021. Survey about protection motivation on social networking sites: University of Maribor\\nstudents, 2018. arXiv preprint arXiv:2104.07712 (2021), 9 pages.\\n[26] Genia Kostka. 2019. China‚Äôs social credit systems and public opinion: Explaining high levels of approval. New media & society 21, 7 (2019),\\n1565‚Äì1593.\\n[27] Theresa Krause, Mo Chen, Lena Wassermann, Doris Fischer, and Jens Grossklags. 2023. China‚Äôs corporate credit reporting system: A comparison\\nwith the United States and Germany. Regulation & Governance 17, 3 (2023), 755‚Äì771. https://doi.org/10.1111/rego.12491\\n[28] Loveleen Kumar and Manish Jain. 2022. A Novel Image Super-Resolution Reconstruction Framework Using the AI Technique of Dual Generator\\nGenerative Adversarial Network (GAN). Journal of Universal Computer Science 28, 9 (2022), 967‚Äì983. https://doi.org/10.3897/jucs.94134\\n[29] Julia Lenz, Zdravko Bozakov, Steffen Wendzel, and Simon Vrhovec. 2023. Why People Replace their Aging Smart Devices: A Push‚ÄìPull‚ÄìMooring\\nPerspective. Computers & Security 130 (2023), 103258:1‚Äì22. https://doi.org/10.1016/j.cose.2023.103258\\n[30] Enshen Li. 2023. The Qianke system in China: Disorganisation, discrimination and dispersion. Criminology & Criminal Justice 23, 4 (2023), 568‚Äì587.\\nhttps://doi.org/10.1177/17488958231161436\\n[31] Helin Li, Hui Zhu, Xiaodong Lin, Rongxing Lu, Zhipeng Yu, and Wei Lan. 2022. Everything you control is not everything: Achieving intention-\\nconcealed visit on social networks. Computers & Security 119 (2022), 102778. https://doi.org/10.1016/j.cose.2022.102778\\n[32] Lauren Yu-Hsin Lin and Curtis J. Milhaupt. 2023. China‚Äôs Corporate Social Credit System: The Dawn of Surveillance State Capitalism? The China\\nQuarterly (2023), 1‚Äì19. https://doi.org/10.1017/S030574102300067X\\n[33] Yuanye Ma. 2019. Unmapped privacy expectations in China: Discussions based on the proposed Social Credit System. In Information in Contemporary\\nSociety: 14th International Conference, iConference 2019, Washington, DC, USA, March 31‚ÄìApril 3, 2019, Proceedings 14 . Springer, 799‚Äì805.\\n[34] Yoon Jin Ma, Hae Jin Gam, and Jennifer Banning. 2017. Perceived ease of use and usefulness of sustainability labels on apparel products: application\\nof the technology acceptance model. Fashion and Textiles 4 (2017), 1‚Äì20.\\n[35] Daith√≠ Mac S√≠thigh and Mathias Siems. 2019. The Chinese social credit system: A model for other countries? The Modern Law Review 82, 6 (2019),\\n1034‚Äì1071.Factors associated with citizens‚Äô attitudes towards trust registers 13\\n[36] Nikola Maranguniƒá and Andrina Graniƒá. 2015. Technology acceptance model: a literature review from 1986 to 2013. Universal access in the\\ninformation society 14 (2015), 81‚Äì95.\\n[37] Yael Mathov, Tal Ben Senior, Asaf Shabtai, and Yuval Elovici. 2022. Stop bugging me! Evading modern-day wiretapping using adversarial\\nperturbations. Computers & Security 121 (2022), 102841. https://doi.org/10.1016/j.cose.2022.102841\\n[38] A Alharbi Nesreen Nasser, A Alharbi Nesreen Nasser, et al .2020. Impacts of Trust in Government and Privacy Risk Concern on Willingness to\\nProvide Personal Information in Saudi Arabia. International Journal of Management Science and Business Administration 6, 2 (2020), 7‚Äì18.\\n[39] Mareike Ohlberg, Shazeda Ahmed, and Bertram Lang. 2017. Central planning, local experiments: The complex implementation of China‚Äôs Social Credit\\nSystem . Technical Report. Mercator Institute for China Studies (MERICS).\\n[40] Wenxi Pu, Siyuan Li, Gregory J. Bott, Marie Esposito, and Jason Bennett Thatcher. 2022. To Disclose or Not to Disclose: An Evaluation of the Effects\\nof Information Control and Social Network Transparency. Computers & Security 112 (2022), 102509. https://doi.org/10.1016/j.cose.2021.102509\\n[41] Wessel Reijers, Liav Orgad, and Primavera De Filippi. 2023. The rise of cybernetic citizenship. Citizenship Studies 27, 2 (2023), 210‚Äì229. https:\\n//doi.org/10.1080/13621025.2022.2077567\\n[42] Marc Oliver Rieger, Mei Wang, and Mareike Ohlberg. 2020. What do young Chinese think about social credit? It‚Äôs complicated . Technical Report.\\nMercator Institute for China Studies (MERICS).\\n[43] Karl Van Der Schyff, Stephen Flowerday, and Steven Furnell. 2020. Duplicitous social media and data surveillance: An evaluation of privacy risk.\\nComputers & Security 94 (2020), 101822. https://doi.org/10.1016/j.cose.2020.101822\\n[44] Jarashanth Selvarajah and Ruwan Nawarathna. 2022. Identifying Tweets with Personal Medication Intake Mentions using Attentive Character and\\nLocalized Context Representations. Journal of Universal Computer Science 28, 12 (2022), 1312‚Äì1329. https://doi.org/10.3897/jucs.84130\\n[45] Jason T Siegel, Mario A Navarro, Cara N Tan, and Melissa K Hyde. 2014. Attitude‚Äìbehavior consistency, the principle of compatibility, and organ\\ndonation: A classic innovation. Health psychology 33, 9 (2014), 1084.\\n[46] H Jeff Smith, Sandra J Milberg, and Sandra J Burke. 1996. Information privacy: Measuring individuals‚Äô concerns about organizational practices. MIS\\nquarterly (1996), 167‚Äì196.\\n[47] Rob T Stewart. 2011. A profit-based scoring system in consumer credit: making acquisition decisions for credit cards. Journal of the Operational\\nResearch Society 62, 9 (2011), 1719‚Äì1725.\\n[48] Eugene F Stone, Hal G Gueutal, Donald G Gardner, and Stephen McClure. 1983. A field experiment comparing information-privacy values, beliefs,\\nand attitudes across several types of organizations. Journal of applied psychology 68, 3 (1983), 459.\\n[49] Caroline J Tolbert and Karen Mossberger. 2006. The effects of e-government on trust and confidence in government. Public administration review 66,\\n3 (2006), 354‚Äì369.\\n[50] Alexander Trauth-Goik. 2023. Civilized cities or social credit? Overlap and tension between emergent governance infrastructures in China. Global\\nMedia and China (2023), 205943642311634. https://doi.org/10.1177/20594364231163444\\n[51] Craig Van Slyke, JT Shim, Richard Johnson, and James J Jiang. 2006. Concern for information privacy and online consumer purchasing. Journal of\\nthe Association for Information Systems 7, 6 (2006), 1.\\n[52] VA Vasilyeva, IA Vetrenko, et al .2020. The social credit system of the people‚Äôs Republic of China through the Eyes of foreign researchers.\\nAdministrative Consulting (Ypravlencheskoe Konsul‚Äôtirovanye) 7 (139) (2020), 20‚Äì31.\\n[53] Viswanath Venkatesh and Fred D Davis. 2000. A theoretical extension of the technology acceptance model: Four longitudinal field studies.\\nManagement science 46, 2 (2000), 186‚Äì204.\\n[54] Viswanath Venkatesh, Michael G Morris, Gordon B Davis, and Fred D Davis. 2003. User acceptance of information technology: Toward a unified\\nview. MIS quarterly (2003), 425‚Äì478.\\n[55] Marianne Von Blomberg. 2023. Credibility Standards: A new social credit mode of regulation? China Information (2023), 0920203X231191098.\\nhttps://doi.org/10.1177/0920203X231191098\\n[56] Marianne Von Blomberg and Wessel Reijers. 2023. Who Deserves Credit? Banks for the Virtuous in Rural China. Journal of Contemporary China\\n(2023), 1‚Äì16. https://doi.org/10.1080/10670564.2023.2248034\\n[57] Marianne Von Blomberg and Haixu Yu. 2023. Shaming the Untrustworthy and Paths to Relief in China‚Äôs Social Credit System. Modern China (2023),\\n009770042311521. https://doi.org/10.1177/00977004231152138\\n[58] Jing Wang, Hongmei Li, Weiai Wayne Xu, and Wei Xu. 2023. Envisioning a credit society: social credit systems and the institutionalization of moral\\nstandards in China. Media, Culture & Society 45, 3 (2023), 451‚Äì470. https://doi.org/10.1177/01634437221127364\\n[59] Merrill Warkentin, Jordan Shropshire, and Allen Johnston. 2007. The IT security adoption conundrum: An initial step toward validation of applicable\\nmeasures. AMCIS 2007 Proceedings (2007), 276.\\n[60] Ping Xu, Brian Krueger, Fan Liang, Mingxin Zhang, Marc Hutchison, and Mingzhi Chang. 2023. Media framing and public support for China‚Äôs\\nsocial credit system: An experimental study. New Media & Society (2023), 14614448231187823. https://doi.org/10.1177/14614448231187823\\n[61] Jinghua Yin and Haiying Song. 2023. Does the perception of smart governance enhance commercial investments? Evidence from Beijing, Shanghai,\\nGuangzhou, and Hangzhou. Heliyon 9, 8 (2023), e19024. https://doi.org/10.1016/j.heliyon.2023.e19024\\n[62] Marko Zekan, Igor Tomiƒçiƒá, and Markus Schatten. 2022. Low-sample classification in NIDS using the EC-GAN method. Journal of Universal\\nComputer Science 28, 12 (2022), 1330‚Äì1346. https://doi.org/10.3897/jucs.85703\\n[63] Yu Zeng and Stan Hok-wui Wong. 2023. Social media, fear, and support for state surveillance: The case of China‚Äôs social credit system. China\\nInformation 37, 1 (2023), 51‚Äì74. https://doi.org/10.1177/0920203X22108814114 Turha et al.\\n[64] Jingjing Zuo, Changqing Huang, Baoyin Qiu, and Ruidong Mai. 2023. The construction of social credit system and corporate innovation: Evidence\\nfrom China. Pacific-Basin Finance Journal 81 (2023), 102116. https://doi.org/10.1016/j.pacfin.2023.102116',\n",
       " 'Looking AT the Blue Skies of Bluesky\\nLeonhard Balduf\\nTechnical University of Darmstadt\\nDarmstadt, Germany\\nleonhard.balduf@tu-darmstadt.deSaidu Sokoto\\nCity, University of London\\nLondon, United Kingdom\\nsaidu.sokoto@city.ac.ukOnur Ascigil\\nLancaster University\\nLancaster, United Kingdom\\no.ascigil@lancaster.ac.uk\\nGareth Tyson\\nHong Kong University of Science and\\nTechnology (GZ)\\nGuangzhou, China\\ngtyson@ust.hkBj√∂rn Scheuermann\\nTechnical University of Darmstadt\\nDarmstadt, Germany\\nscheuermann@kom.tu-darmstadt.deMaciej Korczy≈Ñski\\nUniversity of Grenoble Alps\\nGrenoble, France\\nmaciej.korczynski@univ-grenoble-\\nalpes.fr\\nIgnacio Castro\\nQueen Mary, University of London\\nLondon, United Kingdom\\ni.castro@qmul.ac.ukMicha≈Ç Kr√≥l\\nCity, University of London\\nLondon, United Kingdom\\nmichal.krol@city.ac.uk\\nAbstract\\nThe pitfalls of centralized social networks, such as Facebook and\\nTwitter/X, have led to concerns about control, transparency, and\\naccountability. Decentralized social networks have emerged as a\\nresult with the goal of empowering users. These decentralized ap-\\nproaches come with their own trade-offs, and therefore multiple\\narchitectures exist. In this paper, we conduct the first large-scale\\nanalysis of Bluesky, a prominent decentralized microblogging plat-\\nform. In contrast to alternative approaches ( e.g.Mastodon), Bluesky\\ndecomposes and opens the key functions of the platform into sub-\\ncomponents that can be provided by third party stakeholders. We\\ncollect a comprehensive dataset covering all the key elements of\\nBluesky, study user activity and assess the diversity of providers\\nfor each sub-components.\\nCCS Concepts\\n‚Ä¢Networks‚ÜíNetwork measurement ;Social media networks ;\\n‚Ä¢Information systems ‚ÜíSocial networks .\\nKeywords\\nBluesky, Decentralized Social Networks, Social Network Analysis\\n1 Introduction\\nSocial media platforms like Facebook and Twitter have become\\nubiquitous, attracting vast user bases and wielding significant in-\\nfluence [ 29]. However, their centralized structure raises concerns\\nabout control, transparency, and accountability. These concerns\\nstem from the dominance of these platforms and their unchecked\\ndiscretion over user behavior and content moderation, which has\\nsparked regulatory interest and public debate [16, 31].\\nIn response, decentralized social network (DSN) platforms have\\nemerged. Aiming to foster a more democratic and open online envi-\\nronment, DSNs have pioneered diverse architectures with varying\\ndegrees of decentralization. The ‚Äúfediverse‚Äù, with its server-based\\nfederated services like Mastodon, has attracted attention and users,\\nparticularly after the change of ownership in Twitter [ 20]. These\\nplatforms deconstruct their service into independent, user-creatableserver instances. This approach to decentralization shifts control to\\ninstance administrators, who moderate content and users [ 30]. The\\ndownside of this is that the failure of an individual server can result\\nin data and account loss [ 29]. Other approaches, such as Nostr [ 4],\\novercome these problems by greater decentralized replication, but\\nconsequently lack important server-centric features such as content\\nrecommendations.\\nBluesky [ 25] attempts to overcome these deficiencies. Deployed\\nin 2022, Bluesky operates as a microblogging service that resembles\\nTwitter/X where users can follow each other and share short posts\\n(including images and videos). Bluesky, however, proposes a radical\\ndeparture from Twitter/X or existing fediverse implementations.\\nIts key innovation is to decompose and open the key functions\\nof a social microblogging platform into sub-components that can\\nbe provided by stakeholders other than Bluesky. In contrast to fe-\\ndiverse applications, which embed all the functions into a single\\nserver, Bluesky encourages multiple stakeholders to take on the re-\\nsponsibility of delivering particular sub-aspects of the social media\\nexperience. This means that multiple actors can develop particular\\ncomponents within the overall system. Subsequently, users can\\nthen select between these competing providers of each component\\nto compose their own personalized social media experience.\\nTo attain this, Bluesky defines five key system components:\\n(1) Decentralized User Identifiers (DIDs): To detach users from\\nany specific operator, each user can create their own distinct (cryp-\\ntographically verified) identifier, which they can use across differ-\\nent providers. This identifier is linked to their user handle, a hu-\\nman-readable identifier with a domain of the user‚Äôs choice. (2) Per-\\nsonal Data Servers (PDSes): To detach users from relying on a\\nspecific server to host their data, users can port their data (associ-\\nated with their DID) to any data server and even create their own\\nPDS. (3) Relays: To minimize complexity and overheads, data from\\nmultiple PDSes can be aggregated within a single Relay server that\\noffers high-performance delivery to end users. This is optional and\\nclients can still retrieve posts directly from the PDSes. (4) Feed\\nGenerators: To allow a plurality of feed algorithms, anybody can\\ndevelop their own Feed Generators, which define the selection andarXiv:2408.12449v2  [cs.NI]  29 Oct 2024Leonhard Balduf et al.\\norder of posts seen on a user‚Äôs timeline. Importantly, users can\\nselect between competing Feed Generators to configure how they\\nview content. (5) Labelers: To facilitate content moderation, any-\\nbody can develop a Labeler which assigns labels ( e.g.hate speech)\\nto objects, including posts and accounts. These can also be used\\nlocally by clients to decide content that should be filtered.\\nAny competing operator can build the above components, and\\nusers can freely select between them. For example, users can mi-\\ngrate their data between competing PDSes or configure the use\\nof alternative Feed Generators. This plurality constitutes a radical\\nshift from the walled-garden approach espoused by existing major\\nplayers and proposes novel innovations with respect to other DSNs.\\nBluesky produces a complex system of interconnected components,\\noperators, and users with a diversity of experiences. In this paper,\\nwe examine the operation of these components, study whether\\noperators uptake the opportunity of providing them, whether this\\nresults in competing offerings and how users choose among offer-\\nings and operators. We particularly focus on how multiple operators\\nmanage the critical components: PDSes, Feed Generators, and La-\\nbelers. With this in mind, we gather the first large-scale Bluesky\\ndataset, covering 5,523,919users, 225,461,969posts, 40,398Feed\\nGenerators, and 62Labelers.\\nWe start by investigating the competing set of domains hosting\\nhandles. We discover that, despite the supposed openness, the vast\\nmajority of users‚Äô handles are linked to bsky.social. We then in-\\nspect the availability and offerings of Labelers. Although Bluesky\\noperates the most popular, we do observe a growing ecosystem of\\noperators now issuing a majority of labels just two months after\\nopening this system component. We find that some community\\noperators deviate from the original Labeler goal of filtering content\\nand rather focus on content tagging. We then investigate the Feed\\nGenerators. This is the most popular and competitive component of\\nthe ecosystem, with tens of thousands of active Feed Generators in\\noperation. We discover a wide range of feeds, from spam accounts\\nto ones dedicated to explicit content. To the best of our knowl-\\nedge, this is the first large-scale study of the Bluesky component\\necosystem.\\n2 Bluesky Primer\\nBluesky is a decentralized network where each system component\\n(e.g.data storage, moderation engine) is open-source and can be\\nreplicated, operated, and modified by the community. Users have\\ncontrol over their data, and can migrate between competing op-\\nerators freely. Bluesky attempts to be scalable and easy to use, by\\navoiding redundant communication and providing a user experi-\\nence similar to centralized social networks. We use Bluesky to refer\\nto the social network platform and Bluesky PBC when referring to\\nthe company developing the platform. In this section, we provide\\nan overview of the key concepts of Bluesky.\\nThe AT Protocol. The Authenticated Transfer Protocol ( ATProto )\\nwas developed as a general protocol to underpin social networks\\nand forms the basis of Bluesky. The protocol defines high-level\\ninteractions between components of distributed social applications\\n(e.g.reading data from a user repository). ATProto is easily exten-\\nsible and does not define the exact exchanged data types. Thosetypes are defined in lexicons that can be created by the commu-\\nnity and are organized in namespaces identified by Domain Name\\nSystem ( DNS)-like names. For example, the lexicon app.bsky de-\\nfines the type app.bsky.feed.post , which corresponds to a social\\nmedia post in the Bluesky application.\\nDecentralized Identities. Users in Bluesky are identi-\\nfied via a unique Decentralized Identifier ( DID) [32],e.g.,\\ndid:plc:ewvi7nxzyoun6zhxrhs64oiz . ADID is immutable and\\nidentifies a user uniquely in the network. This enables users\\nto migrate between different servers hosting their data while\\nmaintaining their social graph.\\nEach DID points to its associated DID Document , a document\\nthat stores service information about the user. This includes the\\nendpoint of the Personal Data Server ( PDS) storing the user‚Äôs data\\nrepository , the user‚Äôs handle, as well as public keys to verify sig-\\nnatures on user content. There are currently two supported DID\\nschemata: PLC andWEB . They differ in how the DID Document\\nare retrieved: (1) for PLC DIDs, the associated document is down-\\nloaded from the plc.directory service, which is operated by\\nBluesky PBC. (2) WEB DIDs consist of a Fully-Qualified Do-\\nmain Name ( FQDN ). The associated document must be located\\nathttps://<fqdn>/.well-known/did.json .\\nUser Handles. To provide human-friendly identification, users\\nare addressable via mutable handles . Handles are stored in the DID\\ndocument, linking mutable and immutable identifiers. Handles are\\na FQDN and utilize DNS for proof of ownership.\\nThere are two mechanisms to prove ownership\\nof a handle, e.g. @example.com : (1) via a DNS TXT\\nrecord located at _atproto.example.com , contain-\\ning the DID of the user, or (2) via a file located at\\nhttps://example.com/.well-known/atproto-did . Both\\nmechanisms ensure the owner of the handle also has ownership of\\nthe associated domain, enabling data ownership and user identity\\nverification. By default, Bluesky automatically manages user keys\\nand creates a handle in the form of @username.bsky.social ,\\nhiding the complexity of decentralized identities.\\nUser Data Repositories. Repositories, or repos , are the main\\ndata structure that stores user data. The repos constitute a\\nkey-value store of records and contain users‚Äô posts, likes, fol-\\nlows, blocks, etc. All Bluesky-related records are encoded as\\nConcise Binary Object Representation ( CBOR ), as defined by\\nthecom.atproto and app.bsky lexicons. Updates to a user‚Äôs\\nrepo are signed using one of the keys contained in their\\nDID Document , via repo commits . Entries in repositories can\\nbe identified uniquely within the network via a Uniform Re-\\nsource Identifiers ( URIs), of the form at://<did>/<key> ,e.g.,\\nat://<did>/app.bsky.feed.post/3kdgeujwlq32y , where the\\nlast component of the path marks a unique ID to distinguish repeat-\\nable records.\\nPersonal Data Servers. Repos are hosted by PDSes. A PDS can\\nhold multiple repositories, but each repository is served by just one\\nPDS that the user can choose. Bluesky PBC operates the default\\nPDSes. However, it has recently become possible to self-host PDSes\\nand federate with the network.1Thus, users can migrate to different\\n1https://docs.bsky.app/blog/self-host-federationLooking AT the Blue Skies of Bluesky\\nPDSes while maintaining their social graph. This requires updating\\nthe endpoint in the user‚Äôs DID Document . The PDSes also store\\nuser preferences, which are only accessible by the authenticated\\nuser.\\nThe Relay. Checking for repo updates individually is resource-\\nintensive, as each client would need to contact many different\\nPDSes. To streamline the process, a centralized Relay aggregates\\nuser interactions across PDSes. This is a central store that repli-\\ncates the repo data structures from all known PDSes. The relay\\nthen provides the Firehose ‚Äî a real-time feed of all activity in the\\nnetwork that anyone can subscribe to. The Firehose has a retention\\ntime of three days and includes user repo updates as well as infor-\\nmational and service-related events ( e.g.updates to user handles).\\nBluesky PBC runs the default Relay and Firehose at bsky.network .\\nHowever, other providers could offer a competing service if they\\nwish.\\nFeed Generators. A major challenge with existing social networks\\nis the use of (opaque) commercial algorithms to generate the feeds,\\nselecting which posts to display to which user. In contrast, Bluesky\\nimplements an open ecosystem for content recommendation. Any\\nuser can run their own Feed Generator that other users can sub-\\nscribe to. When creating a Feed Generator, its creators add a special\\nrecord in their repository, which points to the data dissemination\\nendpoint for the feed. The Feed Generator then consumes the Fire-\\nhose and produces a bespoke feed, consisting of URIs pointing to\\nchosen posts. Feed generators can be self-hosted, or created using\\none of multiple feed-generator-as-a-service.\\nLabelers. To support feed generation and other content classifica-\\ntion activities ( e.g.filtering hate speech), Bluesky introduces the\\nconcept of Labelers . Labelers are services that attach labels to ob-\\njects in the network. Users can then leverage these labels to filter\\ncontent. In practice, these labels are a simple short string ( e.g.nsfw ).\\nSome labels are predefined and have hardcoded behavior in other\\ncomponents, such as !hide , which hides the content without an\\noption to click through. Others are custom labels, with custom be-\\nhaviors. For example, a label might indicate that a warning message\\nshould be placed over the content. Note, labels can also be rescinded\\nby a Labeler publishing the same label for the same target with the\\naddition of a negation mark.\\nLabelers operate as regular accounts with a repository for their\\nactivity. Each Labeler publishes a service information record in its\\nrepository, describing the values and default actions to be taken for\\nits labels, e.g.to display a warning. Functionally, a labeling service\\nimplements an endpoint that publishes the labels. The endpoint is\\nlisted in the DID Document of the hosting account and is publicly\\naccessible without authentication. Any entity can connect to this\\nendpoint to retrieve the stream of labels produced. Recently, the\\necosystem opened enabling anyone to run a Labeler.2\\nUser Preferences. The goal of the moderation system is to give\\neach individual the flexibility to decide how they interpret and ac-\\ntion the labels. Bluesky allows users to determine their moderation\\npolicy by describing their preferences, in terms of which labels\\nshould trigger which actions. The preferences are a non-public\\nsetting defining Labelers the user subscribes to and reactions to\\n2https://docs.bsky.app/docs/advanced-guides/moderationlabels produced by them. For each Labeler and label, the users can\\nchoose to ignore it, show a warning, or have the content hidden\\nentirely.\\nThe AppView. The AppView collates the data produced across\\nthe network into a usable format, and makes it available to clients\\nvia a public API. In practice, the AppView consumes the Firehose\\nand information from other system components, stores them in a\\ndatabase, and provides the results in an easy way for a final app\\nto display. There is currently one Bluesky AppView, operated by\\nBluesky PBC.\\nThe Client. Client applications provide a usable frontend to the\\nnetwork. They communicate with a user‚Äôs PDS and the AppView\\nto build the timeline displayed to users. Note, Bluesky does not\\nmandate a single client implementation.\\n3 Datasets\\nUser Identifier Dataset. Each user in Bluesky is identified via\\na unique DID. Utilizing the sync.listRepos call offered by the\\nBluesky Relay, we obtain a list of all active Bluesky users and\\ntheir DIDs. This additionally returns each user‚Äôs latest respective\\nrepo commit version. We query the endpoint weekly to learn of\\nchanged repositories during March and April 2024, obtaining a total\\nof5,591,824identifiers.\\nDID Documents and FQDN Handles. We then download the\\nDID Document s (if available) for all user identifiers obtained in the\\nprevious step. Recall that these documents list fully qualified do-\\nmain name (FQDN) handles, endpoints for users‚Äô PDSes, and other\\nservice information. We obtain documents both from the central-\\nized plc.directory service,3operated by Bluesky PBC, used as\\nthe default for account creations (the did:plc method), as well as\\n.well-known/did.json paths via HTTPs (the did:web method)\\n(see Section 2). We download the full snapshot over the course of one\\nweek in March 2024. This yields a total of 5,077,159 DID Document s\\nfrom the PLC server, with an additional six using the did:web\\nmethod. We extract FQDN handles from the DID Document s for\\nfurther analysis.\\nRepositories Dataset. We download a snapshot of all users‚Äô repos-\\nitories on April 24th containing the public actions of users ( e.g.\\nposts, likes, blocks, etc.). We do this for the complete list of user\\nidentities and corresponding repository versions from the User\\nIdentifier Dataset. We utilize the sync.getRepo endpoint of the\\nRelay service to download a copy of each repository. Since the\\nRelay constantly crawls PDSes and caches repositories locally, we\\nare able to download allrepositories, irrespective of whether they\\nare served by self-hosted PDSes. This is the recommended method\\nof obtaining repo data, and reduces load elsewhere in the network.\\nWe gather 5,523,919repositories over a period of 10 days during\\nApril 2024.\\nFirehose Dataset. The Firehose, provided by Bluesky‚Äôs Relay, offers\\na stream of users posts from across the network (akin to Twitter‚Äôs\\nStreaming API). We subscribe to the Firehose offered by the Relay\\nto obtain real-time updates from the network. This includes user\\nactivity ( e.g., posts, likes, or follows, both additions and deletions)\\n3https://plc.directory/Leonhard Balduf et al.\\nas well as informational and service events published by the Fire-\\nhose. Since the Relay gathers all federated repositories, we receive\\nupdates from the entire visible network. We have been continu-\\nously subscribed to the firehose from 2024-03-06. Since then, until\\nApril 30th, we have collected 279,289,739events (Table 1). The vast\\nmajority of events are repo commits, which mark an update to\\nthe content of a user‚Äôs repository. Updates can be record creations,\\ndeletions, or replacements. Apart from repo commits, the Firehose\\npublishes updates to a user‚Äôs handle, cache invalidation messages\\nfor DID documents, and tombstones for deleted accounts.\\nTable 1: Overview of Firehose event types.\\nEvent Type # Total Share (%)\\nRepo Commit 278 ,677 ,401 99 .78\\nIdentity Update 531 ,295 0 .19\\nUser Handle Update 44,456 0 .02\\nRepo Tombstone 36,587 0 .01\\nFeed Generators Dataset. Feed Generators form the basis of con-\\ntent curation and moderation in Bluesky (see Section 2). We com-\\npile a complete list of all Feed Generators operating in the network.\\nSince they can be identified via records in the repositories, we uti-\\nlize the downloaded repositories as well as real-time updates from\\nthe Firehose to obtain a list of all Feed Generator identifiers. For\\neach Feed Generator, we determine the DID of the service respon-\\nsible for hosting it, as well as the associated endpoint. In total, we\\ndiscover 43,063 Feed Generators. We download Feed Generator\\nmetadata ( e.g.descriptions and information about the creators) via\\nthegetFeedGenerator method of the AppView.\\nFeed Post Dataset. We collect URIs of all Feed Generator posts\\nfrom the Appview getFeed endpoint bi-weekly. We then correlate\\ntheURIs with the posts in the Repositories Dataset to obtain the\\nfull content of each post. Although 100% of Feed Generators with\\nmetadata were marked as both online and valid, we were only able\\nto get posts for 93% of them. While there is a way to delete Feed\\nGenerator records from the repos, provided this record is still there,\\nthere is no way to distinguish between permanent and temporary\\nunavailability. We thus exclude 4.3% of Feed Generators without\\nmetadata from our analysis.\\nFrom 2024-04-16 to 2024-05-10, we collect 21,520,083 posts from\\n40,398 Feed Generators. A challenge for our data collection is that\\nFeed Generators have different policies regarding their retention of\\nhistorical posts. While some provide all the historical posts, others\\nimpose limits based on duration ( e.g.only posts no older than 10\\ndays) or the number of posts ( e.g.only the last 100 posts). As a\\nresult, we are not able to collect all the Feed Generator posts from\\nbefore our measurement period.\\nLabeling Services. To become a labeling service, an account cre-\\nates a service information record in their repository and a service\\nendpoint entry in their DID Document . We utilize this to compile\\na comprehensive list of Labelers using the repository data and\\nreal-time updates from the Firehose.\\nAs of 2024-05-01, we identify 62unique labeling services. For\\neach, we subscribe to the public endpoint listed in their DID Document\\nFigure 1: Daily operation and active user counts.\\nand receive a stream of labels produced by the Labeler. We collect all\\nlabels produced, including ones emitted before our collection period,\\nby consuming the entire stream of labels produced by each Labeler.\\nIf a Labeler‚Äôs endpoint is temporarily unavailable, we backfill any\\nmissed updates as soon as the endpoint is functional again. This\\nincludes rescinded labels, as indicated by a negation of a previously\\nemitted label.\\nWe attempt to reconnect to the service endpoints on a daily basis,\\nbut find only 46labeling services functional, of which 36issued at\\nleast one label. Overall, as of May 2024, we collect 3,402,009inter-\\nactions from labeling services, including 23,394rescinded labels.\\n4 User Activity\\nFirst, we analyze the user activity on Bluesky. We study the growth\\ndynamics of the platform, the most popular accounts, and its current\\nsize compared to other, established social networks. This allows us\\nto understand the platform‚Äôs current status and its user base. We\\ndeliberately omit a deeper analysis of the social graph, as Bluesky\\ndoes not introduce novel solutions in this regard. Finally, we inves-\\ntigate whether the theoretically flexible ATProto infrastructure is\\nused to distribute content not related to the social network itself.\\nThe repositories dataset reveals an accumulation of operations\\n(e.g.follows, likes) through which Bluesky users have interacted\\nsince the creation of the platform in Nov 2022. We observe a total of\\n740M likes, 225M posts, 160.9M follows, 77.9M reposts, and 10.8M\\nuser blocking operations. In this section, we analyze the platform‚Äôs\\ngrowth, its current status and language-specific communities.\\nGrowth. We observe the initial activity in users‚Äô repositories dating\\nback to November 2022, when Bluesky was launched (Figure 1).\\nThis coincides with Elon Musk‚Äôs acquisition of Twitter and the\\nsubsequent firing of half of the employees. The events following\\nthe takeover spurred interest in decentralized social media plat-\\nforms [ 20]. Bluesky experienced a large growth in daily user activity\\nover the subsequent eight months following November 2022. The\\nnumber of active users increased from mere hundreds in December\\n2022 to hundreds of thousands by July 2023. Another significant\\nsurge in daily user activity occurred in February 2024, when Bluesky\\ntransitioned from an invite-only platform to a public one. While\\nBluesky experienced initial growth, we also observe stagnation and\\neven a decrease in daily active users. The number of daily active\\nusers decreased by ‚âà60K between March and May 2024.Looking AT the Blue Skies of Bluesky\\nFigure 2: Active user counts of individual language-specific\\ncommunities.\\nWe further investigate the activity of ‚âà2M Bluesky users, who\\nposted at least once, based on the self-assigned language tags at-\\ntached to their posts (Figure 2). We verify a small ( ‚âà0.1%) random\\nportion of these posts and find that they indeed correspond to\\nthe language indicated in the tags. Language-specific communities\\nroughly follow the global trend of user activity but we also notice\\nsome discrepancies. For instance, opening the platform to the pub-\\nlic in February 2024 significantly increased the number of active\\nJapanese-speaking users while the German-speaking community\\nremained largely unaffected. In April, the Portuguese-speaking\\ncommunity experienced a sharp growth from ‚âà3K to‚âà30K, most\\nlikely caused by direct marketing actions and recent documentation\\ntranslation [5].\\nCurrent Status. As of April 2024, we observe a consistent pres-\\nence of around 500K active users daily on the platform, contributing\\napproximately 3M likes, 800K posts, and 300K reposts daily (Fig-\\nure 1). The network size is comparable to Mastodon but significantly\\nsmaller than Twitter with 1M [ 3] and 245M [ 6] daily active users,\\nrespectively. While English remains the platform‚Äôs main language\\n(‚âà800K users), more than 700K users use Japanese language tags,\\nsuggesting a diverse user base. The platform has recognized the\\nimportance of the community, for instance, by implementing a dedi-\\ncated kawaii mode.4Additionally, Portuguese and German emerged\\nas the next popular languages among users.\\nAccount popularity. We observe the account popularity based\\non likes and block operations. The most popular account is the\\nBluesky official account (775K followers as of April 2024), which\\nposts updates on the platform. The other popular accounts belong\\nto popular American newspapers and independent journalists, such\\nas the Washington Post and the NY Times, each with over 200K\\nfollowers. On the other hand, the most blocked accounts belong\\nto celebrity impersonators and propagandists. For example, the\\nmost blocked account impersonates Jordan B. Peterson, while the\\nnext most blocked account is an anti-vaccine propagandist. Both\\naccounts received‚âà15K blocks.\\nNon-Bluesky content. We find that the Firehose also distributes\\ncontent notcovered by the Bluesky-related lexicons.These are\\nrecords in user repositories targeted for a different AppView, i.e.,\\nBluesky cannot decode or display them. We find 1,855events (out\\n4https://bsky.app?kawaii=trueof‚âà280M) relating to these records. Among the targeted applica-\\ntions, we find WhiteWind5to be the most popular public applica-\\ntion. WhiteWind attempts to bring long-form blogging to ATProto .\\nUsers can log in using their Bluesky account and write articles\\nusing markdown. The articles are then saved in the user‚Äôs repo,\\nhosted on their PDS. The WhiteWind AppView and Frontend can\\ndecode the entries and display them on the website.\\nThese alternative applications require the Bluesky infrastructure\\n(e.g.the Firehose) to index and re-publish non-Bluesky content. The\\nability to use already deployed infrastructure without additional\\ncost is beneficial to the growth of the ecosystem. Due to the current\\nlimited size of those applications, they have a limited impact on the\\ninfrastructure. However, it remains to be seen whether this remains\\nviable as those platforms grow.\\nTakeaways. Bluesky has experienced significant growth since its\\nlaunch in November 2022. However, the growth has been driven\\nmostly by specific events such as the public launch and stagnates\\nbetween those events. Bluesky is still significantly smaller than\\nits centralized counterpart and does not yet show signs of the\\nsnowball effect or mass user migration. This is further confirmed\\nby the current lack of popular celebrity or institutional accounts.\\nAt the same time, the good user experience and a rapidly increasing\\nnumber of features could enable Bluesky to attract specific country-\\nor interest-specific communities. This is exemplified by a recent\\nmass migration of Brazilian users after X/Twitter was banned in the\\ncountry [ 14].6Finally, opening the platform infrastructure to third-\\nparty applications could further boost the growth and diversity of\\nthe ecosystem but for now, remain in their infancy.\\nFigure 3: Number of subdomain handles per registered do-\\nmain name (effective second-level domain); for clarity, we\\nexclude subdomains of bsky.social , which account for 98.9%\\nof all observed FQDN handles.\\n5 (De)centralized Identity\\nBluesky supports decentralized identities, allowing users to link\\ntheir accounts to their domain names and manage their own keys.\\nThis frees the user from a dependence on a single identity provider.\\nHowever, to simplify the account creation process, the platform\\nalso offers a custodial identity creation that automatically creates a\\nsubdomain handle under the bsky.social domain. We investigate\\nuser choices, adoption trends, and their possible implications for\\nsecurity and trust.\\n5https://whtwnd.com/\\n6Unfortunately, the migration happened outside of our data collection period.Leonhard Balduf et al.\\nSubdomain Handles Concentration. We analyze 5,077,159 FQDN\\nhandles, sourced from DID documents, to investigate the concen-\\ntration of subdomain providers. Despite the theoretical openness\\nof handle creation, we observe a notable concentration of subdo-\\nmain name handles with 98.9% of them under bsky.social . The\\nprevalence of such handles is expected, given their convenient man-\\nagement. Bluesky offers subdomain handles at no cost, making\\nthem readily accessible during the Bluesky account creation pro-\\ncess. Moreover, users can employ these handles without having to\\nlink them to a DID.\\nNext, we investigate the remaining 57,202 FQDN handles to\\nidentify alternative operators and services (Figure 3). This helps\\nus study to what extent the openness of the Bluesky design trans-\\nlates into actual diversity. Interestingly, no operator exceeds a few\\nhundred FQDNs per registered name. For instance, we find 256\\nFQDNs under swifties.social , and 179 and 133 FQDNs under\\ntired.io and vibes.cool , respectively, both managed by Sky-\\nname.7Those domains offer dedicated support for Bluesky and\\nfacilitate the handle migration process. We also observe generic\\nsubdomains. For instance, we identify 35 accounts that use their\\ngithub.io subdomain as Bluesky handles.\\nUsing services like bsky.social and other dedicated subdomain\\nproviders offers the benefit of easy integration into the Bluesky\\necosystem. However, this convenience may come at the cost of\\ngiving control over data custody and management to the platform‚Äôs\\noperator.\\nSelf-Managed Domain Names. Organizations and individuals\\nwith existing domains can leverage them to confirm the legitimacy\\nof their Bluesky accounts. This approach offers more control over\\ndata privacy but adds security and infrastructure management\\nresponsibilities.\\nWe extract 51,879 registered domains ( i.e., effective second-level\\ndomain names) from FQDNs using the Public Suffix List [ 27] to iden-\\ntify prominent brands within the ecosystem. We cross-reference\\nthe registered domain names with the Tranco popularity list [ 26]\\nand identify only 1,436 (2.8%) entries within the top 1 million rank-\\ning. These include domains associated with tech companies ( e.g.\\namazonaws.com ,microsoft.com ,cloudflare.com ), media outlets\\n(e.g.cnn .com ,nytimes.com ,washingtonpost.com ), and univer-\\nsities ( e.g.stanford.edu ,columbia.edu ). The limited represen-\\ntation of domains associated with major organizations suggests\\nlimited engagement with the Bluesky platform.\\nValidating Handle Ownership. Bluesky services currently recog-\\nnize DIDs derived from either did:web , defined by the W3C Creden-\\ntials Community Group [ 17], or did:plc , established by Bluesky\\nPBC for Atproto [ 21]. Interestingly, we identify only six did:web\\nidentities. This observation could stem from the accessibility of the\\nmechanism supported by Bluesky PBC, along with the distinction\\nfrom domain name handles, particularly in the immutability of\\nthe identifier. Unlike domain name handles, the did:web domains\\nassociated with a DID cannot be changed [25].\\nWith bsky.social (did:plc ) domain handles, subdo-\\nmains are automatically linked to their DIDs by hosting\\n/.well-known/atproto-did files. We further explore the two\\n7https://skyna.meTable 2: Domain name handles per registrar.\\nIANA ID Registrar Name # Total Share (%)\\n1068 NameCheap, Inc. 8,252 20.94%\\n1910 CloudFlare, Inc. 4,514 11.46%\\n895 Squarespace Domains 4,453 11.30%\\n146 GoDaddy.com, LLC 2,835 7.19%\\n1861 Porkbun, LLC 2,698 6.85%\\n69 Tucows Domains Inc. 2,337 5.93%\\n49 GMO Internet Group 1,796 4.56%\\nmechanisms employed by other FQDN handles to validate handle\\nownership using active measurements. We gather 52,160 DIDs\\ncorresponding to FQDN handles beyond the bsky.social domain.\\nThe vast majority of 51,497 (98.7%) FQDN handles contain DID\\nentries ( e.g.,did=did:plc:cpa2egh7gaaesf2hqc2vuosp ) stored\\nin the DNS TXT records of the _atproto subdomains, while\\nonly 663 (1.3%) are stored in the /.well-known/atproto-did\\nfiles. Several factors could influence the discrepancy. Multiple\\nonline guidelines indicate step-by-step procedures for adding\\nTXTrecords via registrar platforms without requiring in-depth\\nknowledge of DNS. Furthermore, storing this information in DNS\\navoids configuring and running a webserver just to prove domain\\nownership.\\nRegistrar Concentration. Next, we perform a WHOIS scan to\\nevaluate the concentration of registered domain names among\\nvarious registrars identified using IANA IDs. We collect WHOIS\\ndata for 47,728 (92%) registered domain names and successfully\\nextract the IANA ID for 39,403 (76%) domain names. We cannot\\nretrieve the IANA ID for all registrars. While ICANN-accredited\\nregistrars must display the IANA IDs for new and legacy generic\\ntop-level domains (TLDs), public WHOIS records for country-code\\nTLD (ccTLD) operators might not always contain this information.\\nThis could be because ccTLDs opt to provide limited data in WHOIS\\nor because registrars are locally accredited by ccTLDs, exempting\\nthem from the requirement for ICANN-accredited registrars to have\\nan IANA ID [8].\\nWe find a high degree of centralization of domain handles. While\\nwe identify 39,403 registered domains spread across 249 registrars,\\n50% of the domains are registered in just four registrars. Namecheap,\\nan ICANN-accredited registrar, accounts for 21%. The collabora-\\ntion announced in May 2023 between Bluesky and Namecheap to\\nstreamline the process of purchasing and linking domain names\\nto Bluesky explains the latter.8We argue that dedicated support\\nfrom additional registrars will be necessary to avoid the risks of\\nexcessive registrar centralization in the future.\\nUser Handles Updates. Finally, we analyze recent changes in\\nuser identities via the Firehose dataset. We observe 44,449handle\\nchanges by 31,494unique DIDs. This indicates that some users\\nchanged their handles more than once during our observation\\nperiod. In these changes, we register only 41,359unique handles,\\nindicating that some users switch back and forth between handles.\\nWhile the source handle is unavailable in the update event, we\\n8https://bsky.social/about/blog/7-05-2023-namecheapLooking AT the Blue Skies of Bluesky\\ninvestigate the final handles the users settled on with the updates.\\nWe observe that 23,817(75.74 %) of the final handles are under\\nbsky.social , while the remaining 7,630(24.26 %) are under other\\ndomains. This might suggest that as the ecosystem develops, users\\nare more likely to switch to custom domain names, possibly due\\nto the increasing support from alternative services. This perhaps\\nhighlights the flexibility of the overall architecture.\\nTakeaways. Bluesky offers a variety of options for identity manage-\\nment ranging from custodial subdomain handles to self-managed\\ndomain names. Unsurprisingly, the vast majority of users opt for the\\nconvenience of automatic subdomain handles under bsky.social\\nthat resembles traditional social media authentication. This leaves\\nBluesky in full control over user identities but also enables the\\nplatform to avoid the trap of fully decentralized, complex iden-\\ntity management that could deter regular users. At the same time,\\ninstitutions or tech-savvy users have the option to manage their\\nkeys and link their accounts to their domain names making their\\nidentity largely independent of the Bluesky operators. We observe\\nthat more decentralized options to identity management can attract\\nusers when made more accessible. It remains to be seen whether\\nsuch a flexible approach to identity management will enable the\\nonboarding of a large number of users with the centralized ap-\\nproach while later allowing them to take more control over their\\nidentity as the decentralized identity solutions mature and develop\\nuser-friendly interfaces.\\n6 Content Moderation\\nContent moderation is a crucial aspect of any social media platform.\\nThe traditional social media platforms have faced criticism for their\\nopaque and inconsistent moderation practices. Bluesky presents\\na novel approach that relies on two main elements: (1) Labelers ,\\nwhich produce short textual labels attached to objects, and (2) User\\npreferences indicating (a) which Labelers to subscribe to ( i.e.use\\nlabels from) and (b) how to react to a given label ( e.g.by hiding\\nthe post). In this section, we focus on the Labelers producing the\\nlabels, and later inspect the labels themselves and the process of\\ntheir issuance. We try to reveal how the moderation process is split\\nbetween the official Bluesky Labeler and community-run Labelers,\\nand whether there is a significant overlap between them. Note, the\\nuser preferences are not publicly visible and we make no attempt\\nto reveal them due to ethical considerations.\\n6.1 Labelers\\nGrowth. Figure 4 plots the number of Labelers over time. In April\\n2023, Bluesky launched its first official Labeler which remained the\\nonly one for 11 months. On March 15th 2024, the platform opened\\nup to community-driven Labelers, leading to a rapid increase in\\nthe number of Labelers. The new Labelers produce an increasing\\nnumber of labels and in April 2024 issued 1,082,207(88.7 %) of all\\nthe labels.\\nWe make two observations. (1) In the federated architecture\\nof Bluesky, the centralized AppView component subscribes to all\\nknown Labelers and needs to store alllabels. This approach makes\\nit relatively easy to run a Labeler. The required bandwidth is low\\nand the required computing capacity depends on the implemented\\nalgorithm. (2) Conversely, running an AppView becomes ever-more\\n0.000.250.500.751.001.25\\n015304560\\nApr 2023 Jul 2023 Oct 2023 Jan 2024 Apr 2024\\nDate# Labels (√ó10‚Å∂)# Community Labelers Source\\nCommunity\\nBlueskyFigure 4: Number of labels produced by source per month\\n(left-hand axis), and number of community-run labeler ser-\\nvices over time (right-hand axis).\\nresource demanding, the more Labelers are active. It remains to be\\nseen whether this approach is scalable in the long term.\\nCurrent State. As of 2024-05-01, 62unique accounts announced\\nthemselves as Labelers. However, only 36issued at least one label.\\nTable 3 lists the top 5 community-run Labelers by the number of\\napplied labels. Some are transparent and their authors are publicly\\nknown. They generally post about their implementation, technical\\ndetails, and challenges. For instance, the 5th most active community\\nLabeler caters specifically to the Japanese community of Final Fan-\\ntasy 14 game players. This service is intended to prevent accidental\\nexposure to spoilers about new game content. In an accompany-\\ning blog post,9the author describes the implementation and notes\\nchallenges with false positives. In other cases, the operators remain\\nanonymous and do not provide details about their service ( e.g.the\\n‚ÄúAI Imagery Labeler‚Äù, which is operated by multiple anonymous\\nindividuals following a ‚ÄúModerator Handbook‚Äù).10\\nWe analyze the IP addresses of the Labelers. Most ( 40, or 65 %\\nof all) services run on cloud-based hosting infrastructure or are\\nreverse-proxied. However, we find that 6(10 %) are operated from\\nISP-assigned residential addresses. The remaining 16(26 %) were\\nnot functional and no endpoint could be determined.\\n6.2 Labels\\nNumber of labeled objects. So far, Labelers have applied labels to\\na total of 3,160,851unique objects. Due to the dynamic development\\nof the Labeler ecosystem, 1,122,226(36 %) of those objects were\\nlabeled in April 2024 alone. For context, in April, the entire network\\nproduced a total of 26,467,002posts. Of those, 1,114,848(4.21 %)\\nwere labeled with at least one label. The most commonly labeled\\nobjects are posts ( 99.63 %), followed by entire accounts ( 0.23 %), and\\nprofile pictures/banners ( 0.14 %).\\nLabels cause different behavior depending on the object they\\nare applied to. For labeled posts, the post itself, or just the media\\nattached to it, is subject to action by the client. Posts can be hidden,\\ntheir media blurred, etc. Profiles usually receive labels for their\\n9https://blog.usounds.work/posts/bluesky-ff14-labeler\\n10https://trail-buckthorn-014.notion.site/Bluesky-AI-Imagery-Labeler-Moderator-\\nHandbook-65ef75d92a0e4faab0ad2925fc35c85cLeonhard Balduf et al.\\nTable 3: Top 5 community labelers by number of labels applied.\\nRank # Applied Name Likes Operator Description\\n1 1,360 ,224 Bad Accessibility / Alt Text Labeler 99 @baatl.bsky.social Labels posts for missing/invalid alt text.\\n2 76,599 XBlock Screenshot Labeler 301 @aendra.com Uses a machine-learning model to classify screen-\\nshots by origin.\\n3 73,875 No GIFS Please 88 Labels GIFs.\\n4 56,517 AI Imagery Labeler 546 Labels AI-related posts by hashtags.\\n5 10,024 @ff14labeler.bsky.social 15 @usounds.work Labels Final Fantasy 14 content spoilers.\\nprofile picture or banner image. In these cases, the labels only have\\nminor effects: content is still shown, although the profile picture or\\nbanner of the account is blurred or hidden. Finally, labels can also\\nbe applied to entire accounts , as identified by their DID. Configured\\nlabel behavior applies to the entire account, e.g., hides all posts\\nfrom that account, if chosen by the user.\\nLabel values. We identify a total of 222distinct label values. After\\ncleaning the data ( e.g.removing negations without previous appli-\\ncations), there are 196distinct label values. We find that Labelers\\nmostly deal with disjoint parts of the network. Only 100,888(3.2 %)\\nof the labeled objects have labels by multiple services applied on\\nthem and 9objects are labeled with the same label by different\\nLabelers.\\nLooking at Table 4, the most-applied label for posts is no-alt-text ,\\napplied by the most popular community Labeler. This label marks\\nposts with attached media missing an alternative text description\\nfor the media. The next most frequently applied labels ( porn and\\nsexual ) describe Not Safe For Work ( NSFW ) content and are almost\\nexclusively applied by the official Bluesky Labeler. The tenor-gif\\nlabel is applied by a community Labeler. It marks posts containing\\na reaction GIF from the popular Tenor11GIF keyboard.\\nLabel values prefixed with an exclamation mark are reserved\\nand hold special meanings. They are valid only when issued by\\nthe official Bluesky Labeler. All users are subscribed to the official\\nBluesky Labeler and unsubscribing is not an option. Their behavior\\nis hardcoded in the client implementation and other system compo-\\nnents. As an example, the !takedown label causes labeled content\\nto be purged from the network. This label can be applied to posts,\\nbut also to entire accounts via their DID. For the latter, it causes\\nthe entire account to be removed from system components and re-\\nquests for its content to be discarded. The labels porn ,sexual , and\\ngraphic-media also have hardcoded behavior, but can be emitted\\nby any Labeler. Content labeled with them becomes inaccessible to\\nusers under the age of 18.\\nAnother challenge is that there is no official list of potential label\\nvalues to apply (apart from 7labels, of which some are exclusive to\\nthe official Bluesky Labeler as outlined earlier). As such, different\\nLabelers use different labels with similar meanings. While this can\\npose issues regarding coherent labeling, it also offers flexibility to\\ndevelopers. Note that Labelers have to provide descriptive meta-\\ndata when declaring the list of label values they emit. Ultimately,\\nLabelers play just one part in the overall moderation infrastruc-\\nture: users choose which Labelers to subscribe to, and how their\\nclient should react to specific labels produced by these services.\\n11https://tenor.com/Because users choose which Labelers to subscribe to, there is a\\npotential challenge in how to discover appropriate Labelers, and\\nmake informed decisions about which labels to trust.\\nIn general, we find that label values generally show little overlap\\nbetween the Bluesky Labeler and community Labelers, i.e., they\\nseem to deal with mostly disjoint topics: Only 56,856(1.8 %) objects\\nare labeled by the Bluesky Labeler and any community Labeler. The\\nBluesky Labeler mostly labels NSFW content and upholds some\\ncommunity standards whereas the community Labelers seem to\\noperate in specific niches. This is enabled by the flexibility (or lack\\nof predefined) label values and user choice in how to react to labels.\\n6.3 Label issuance\\nFinally, we gather insights about the process of issuing labels. We\\n0.1s1s10s1m1h1d\\n10¬π 10¬≥ 10‚Å∂\\n# Labels ProducedReaction TimeSource\\nCommunity\\nBluesky\\nFigure 5: Number of labels produced by source vs. reaction\\ntime ‚Äî median and quartiles shown.\\nfirst analyze how long it takes for a label to be issued. Figure 5 shows\\nthe reaction time (median and 1st/3rd quartile) and the number\\nof labels produced per Labeler. To calculate the reaction time, we\\nexclusively look at new posts received from the Firehose since\\n2024-03-06. We do this to avoid other objects which retrieve labels\\nless frequently ( e.g.accounts), and labels applied retroactively to\\nolder posts. The more labels a Labeler produces, the faster they\\nare generally in reacting to new posts, suggesting a high degree of\\nautomation. This is reinforced by the variability in reaction times.\\nLabelers producing fewer objects are generally more variable in\\ntheir reaction time, indicative of a manual process rather than an\\nautomated one. Note that the Labeler with the most labels in total\\nis the one operated by Bluesky, which has been running for ‚âà11\\nmonths more than the other ones.Looking AT the Blue Skies of Bluesky\\nTable 4: Label targets with most-applied labels.\\nObject Type # Objects Share (%) Top Labels\\nPost 3,332 ,727 99 .63 no-alt-text ( 1,359 ,752), porn ( 1,256 ,305), sexual ( 375 ,620), ai-imagery ( 56,603), tenor-gif ( 54,968)\\nAccount 7,601 0 .23 !takedown ( 2,643), spam ( 1,067), ai-imagery ( 582), impersonation ( 575), transphobia ( 311)\\nBanner/Avatar 4,706 0 .14 sexual ( 2,538), porn ( 1,742), nudity ( 208), gore ( 104), self-harm ( 35)\\nOther 121 <0.01 porn ( 65), sexual ( 30), !takedown ( 12), nudity ( 5), gore ( 2)\\nIn Figure 6 we investigate the number of objects labeled per label\\nvalue (e.g.,porn ) and the labeler‚Äôs reaction time. The figure shows\\nthe median and 1st/3rd quartile and the color indicates the produc-\\ning Labeler. On the lower right-hand side, we find the most-active\\ncommunity Labelers, applying e.g.no-alt-text andai-imagery ,\\nas well as the screenshot-classifying Labeler. Reaction time for\\nthese is generally very low ( e.g.<10s), as the systems are likely\\nautomated. The labels in the upper left-hand corner are applied\\nrarely and are mostly produced by community Labelers. We find\\nthat some of these are simply experiments by early adopters, while\\nthe majority are probably applied manually.\\nFor the official Bluesky Labeler, we observe two groups of labels:\\nIn the lower half of the plot, we find porn ,nudity ,corpse etc.,\\nwhich are applied within seconds, indicating automated systems.\\nOn the other hand, labels such as, e.g.,spam ,sexual-figurative ,\\nintolerant , and !takedown take longer to be applied, pointing to\\nmanual processes. It seems that heavy-handed moderation decisions\\nsuch as removing data are deliberated instead of automated. This\\nis reassuring, especially for the !takedown label having significant\\nconsequences for the affected users.\\n!takedown!warn\\nai-imagery\\nbluesky-screenshotcorpse\\ngoregraphic-media\\nicon-intolerantintolerantno-alt-textnudityporn\\nrude\\nself-harmsexualsexual-Ô¨Ågurativespam\\ntenor-giftenor-gif-no-textthreattwitter-screenshot!takedown!warn\\nai-imagery\\nbluesky-screenshotcorpse\\ngoregraphic-media\\nicon-intolerantintolerantno-alt-textnudityporn\\nrude\\nself-harmsexualsexual-Ô¨Ågurativespam\\ntenor-giftenor-gif-no-textthreattwitter-screenshot\\n0.1s1s10s1m1h1d\\n10¬π 10¬≥ 10‚Å∂\\n# Objects LabeledReaction Time\\nLabeler Community Bluesky\\nFigure 6: Number of labels produced by source vs. reaction\\ntime ‚Äî median and quartiles shown.Takeaways. Bluesky combines mandatory12platform-run modera-\\ntion with a plethora of custom, user-led Labelers.The open Labeler\\necosystem is still in its infancy and mostly issues specific labels ( e.g.\\nGIFs from a specific platform) avoiding more controversial topics\\n(e.g.fake news or hate speech). Notably, the ease of running a La-\\nbeler and the flexibility of the system already enable it to be used\\nnot only for specific, niche topics, but also for downstream content\\nrecommendation (cf. Section 7). While we observe a high degree\\nof automation in the label issuance process, some labels are still\\nlikely applied manually. This is especially true for the more subtle\\nlabels, such as threat orintolerant . We expect more labels to be\\nissued automatically in the future as the content rate increases and\\ntechnology matures.\\nAs Bluesky grows, the platform might be also exposed to increas-\\ning regulatory pressure to moderate content.13This creates the\\nrisk of running forced, centralized moderation before the content\\nis distributed to the network and limiting the role of decentralized\\nLabelers. We note that it is already possible for Bluesky PBC to\\nperform ‚Äúinfrastructure takedowns‚Äù, instantly removing data from\\ntheir infrastructure in cases of clearly illegal content. While running\\na Labeler is relatively easy, the incentives for their operators to scale\\nup their operations are unclear. In the long run, the decentralized\\nLabelers might thus cover the issuance of high-quality labels for\\nniche use cases, while the centralized moderation would handle the\\nbulk of the sensitive content ( e.g.CSAM) and remove it from the\\nnetwork.\\n7 Content Recommendation\\nBluesky allows users to personalize the content displayed on their\\ntimelines by subscribing to Feed Generators. Users can subscribe\\nto multiple feeds and switch between them seamlessly. We analyze\\nthe Feed Generators‚Äô ecosystem, their impact on the network and\\nthe platforms used to run them.\\n7.1 Feed Generators\\nGrowth. The Feed Generators were introduced in May 2023. Since\\nthen, their number has been steadily increasing (Figure 7). We also\\nobserve a significant increase in the number of likes on Feed Gener-\\nators and followers of Feed Generator creators. As the Feed Gener-\\nator subscriptions are not public, the number of likes they receive\\nsheds light on their popularity. Additionally, creators responsible\\nfor Feed Generators seem to be attracting more followers. This\\nsuggests that creating Feed Generators might be a way for users to\\n12We note that, theoretically, one could run an alternative AppView component that\\nwould ignore labels issued by Bluesky PBC.\\n13cf. https://bsky.app/profile/aaron.bsky.team/post/3l3gerugkbt27 for a recent example\\nof increased need for moderation.Leonhard Balduf et al.\\nreach a wider audience. Similar to other Bluesky components, we\\nobserve a growth increase for all those metrics in February 2024,\\nwhen the platform was opened to public use.\\n# Follows on Feed Generator Creator Accounts# Likes on Feed Generators# Feed Generators\\n2023-01 2023-07 2024-01010√ó10¬≥20√ó10¬≥30√ó10¬≥40√ó10¬≥\\n00.1√ó10‚Å∂0.2√ó10‚Å∂0.3√ó10‚Å∂\\n02√ó10‚Å∂4√ó10‚Å∂6√ó10‚Å∂\\nDate\\nFigure 7: Cumulative sum of number of Feed Generators,\\nlikes on Feed Generators, and followers of Feed Generator\\ncreator accounts, over time.\\nOur investigation shows that most of the generators provide\\nhistorical data from 1 to 7 days back. As a result, we acquire a\\ncomplete list of posts only for the period between April and March\\n2024 when we performed our measurements. We thus do not present\\nhistorical data on the number of Feed Generator posts.\\nCurrent State. As of 2024-04-30, the network contains 40,398 reach-\\nable Feed Generators. However, 3,782 (9.4%) have never curated\\nany posts, while 8,792 (21.8%) did not curate posts in the last month\\nand seem inactive. Interestingly, 2,202 (0.01%) Feed Generator posts\\nhave timestamps predating Bluesky‚Äôs launch. These include times-\\ntamps from years such as 1185, 1776 or 1923, well before even the\\nstart of the Unix timestamp. This suggests an implementation error\\nthat we reported to the Bluesky team.\\nUsing langdetect [ 15] on the Feed Generator descriptions, we\\ndetect a total of 46 languages. While English is the most common\\nlanguage (45%), we also find a significant number of Feed Genera-\\ntors in Japanese (36%), German (4.1%), Korean (2.0%), and French\\n(1.9%). The distribution is roughly consistent with the overall lan-\\nguage distribution on Bluesky (Figure 2). However, we observe a\\nlower proportion of Portuguese Feed Generators compared to the\\noverall Portuguese-speaking user base. This might be caused by\\nthe recent increase in Portuguese-speaking users, who might not\\nhave had enough time to create Feed Generators.\\nWe further extract the most common words present in the de-\\nscription (Figure 8). We observe that the art community is partic-\\nularly active in utilizing Feed Generators ( e.g.‚Äúart‚Äù, ‚Äúartists‚Äù). It\\nprovides artists with a way to showcase and potentially promote\\nFigure 8: Word cloud showing the most common words found\\nin the description of Feed Generators.\\ntheir works within the network. This is further confirmed by links\\nto artist content-sharing platforms ( e.g.tumblr, deviantart, pixiv)\\nfound in the descriptions. Furthermore, some Labelers explicitly tag\\ntheir content in the descriptions by using ‚Äúnsfw‚Äù or ‚Äúsfw‚Äù keywords.\\nFigure 9: Top labels associated with posts curated by Feed\\nGenerators.\\nWe next delve deeper into the content of Feed Generators by\\nanalyzing the labels associated by Labelers with the posts. Only\\n12.6% of Feed Generators have some of their content labeled. We\\nfocus on the 0.53% of Feed Generators that have 10% or more of\\ntheir content labeled and report their most frequent label (Figure 9).\\nMost of these Feed Generators (0.096%) are dedicated to explicit\\ncontent ( e.g.‚Äúporn‚Äù, ‚Äúsexual‚Äù, ‚Äúnudity‚Äù) or spam. The dominance of\\nsuch labels is caused by the Bluesky official Labeler that focuses\\non filtering this kind of content and issues the majority of all the\\nlabels. While some independent Labelers issue labels dedicated to\\ndescribing the content rather than filtering it out ( e.g.the Final\\nFantasy Labeler), such use is not yet widespread. Importantly, the\\nflagged content is not removed from the platform. Rather, each user\\ncan decide how to react to each specific label.\\nFigure 10 shows the number of curated posts and likes received\\nby each Feed Generator. We observe that the majority of Feed\\nGenerators have a low number of posts and likes. However, a few\\nFeed Generators curate a large number of posts ( >400,000) or\\nreceive a high number of likes ( >16,000). Interestingly, the number\\nof likes received by Feed Generators is not directly proportional to\\nthe number of posts curated by them.\\nTo explore this, we manually investigate the Feed Generators on\\nboth extremes. Highly-liked Feed Generators returning no posts are\\npersonalized. For instance, ‚Äúthe-algorithm‚Äù tailors feeds based on\\nuser likes, while ‚Äúwhats-hot‚Äù aggregates trending content from\\na user‚Äôs personal network. They do not return any content toLooking AT the Blue Skies of Bluesky\\nFigure 10: Scatter plot showing the number of posts in rela-\\ntion to the like count.\\n‚Äúempty‚Äù accounts that we use for our crawls. On the other extreme,\\nthere are automatic and aggregating Feed Generators. For instance,\\n‚Äú4dff350a5a3e‚Äù is a Japanese language feed tracking hundreds of\\nthousands of posts related to the popular noodle dish ‚Äúramen‚Äù, while\\n‚Äúhebrew-feed‚Äù automatically reposts all the content in Hebrew. We\\nalso find active and highly-liked Feed Generators that curate their\\ncontent, potentially manually. This includes content relevant to\\nspecific communities such as ‚Äúblacksky‚Äù or ‚Äúfurry-new‚Äù.\\nFigure 11: Degree distributions of users based on follow op-\\nerations, with Feed Generators highlighted in red.\\nGaining Popularity. We also investigate additional factors im-\\npacting the popularity of Feed Generators. Figure 11 presents the\\nin-degree (top) and out-degree (bottom) distributions of users based\\non follow operations. A red shading in both plots indicates the den-\\nsity of accounts that created Feed Generators. The shade of red\\nintensifies as the in-degrees increase and the out-degrees decrease.\\nThis indicates that the Feed Generator accounts correspond to users\\nwith many followers. Intuitively, popular users are more likely to\\ncreate Feed Generators and Feed Generators increase the popularity\\nof their creators. We observe a reverse trend for the out-degree\\nthough. Users creating Feed Generators follow a small number of\\nother accounts. We calculate the Pearson‚Äôs Coefficients for vari-\\nous factors to estimate whether they are predictive of an account\\nattracting followers. We find that the number of Feed Generatorscreated does not correlate with the number of followers on the\\ncreator account ( ùëü=0.005). When calculating the sum of likes on\\nthe Feed Generators created, however, we find correlation at ùëü=\\n0.533. This indicates that creating good Feed Generators is a way\\nfor users to gain a larger followership.\\nFinally, we investigate the number of feeds created per account.\\nA majority of users (62.1%) manage only one Feed Generator, while\\n37% of users manage between 1 and 10. Interestingly, a small fraction\\n(0.02%) of accounts manage a large number of Feed Generators,\\nexceeding 100. An account creating the most (1799) feeds belongs\\nto aFeed Generator As a Service platform that simplifies feed creation.\\nFeeds created via this platform remain associated with the platform\\nrather than the user account justifying the high feeds per account\\nratio. It motivates us to investigate such platforms.\\n7.2 Feed Generator As a Service\\nWe look into the Feed Generator As a Service ecosystem by analyz-\\ning the servers hosting existing Feed Generators (Figure 12). The top\\nthree services, Skyfeed, Bluefeed, and Goodfeeds, collectively host\\n95.8% of all the Feed Generators, with Skyfeed alone hosting 85.86%\\nof them. This may indicate another example of centralization.\\nThe number of Feed Generators does not necessarily correlate\\nwith the number of created posts. For instance, Skyfeed, hosting\\n85.86% Feed Generators, produces only 30.3% of the posts but ac-\\ncounts for 61.2% of likes. On the other hand, Goodfeeds, despite\\nhosting only 4.36% of the Feed Generators, is responsible for 35.6%\\nof the posts but receives 1.2% of likes.\\nFigure 12: Percentage of providers hosting feed generators\\n(top) and simplified Pareto chart of feed providers (bottom).\\nTo better understand these differences, we analyze the features\\noffered by each Feed Generator As a Service platform. We provide\\na full comparison table in Table 5 in the Appendix. The services\\nallow their user to consume specific inputs ( e.g.a specific user or a\\nfeed) and filter them using labels, languages or regular expressions.\\nSkyfeed provides by far the most comprehensive list of features,\\nexplaining its high market share. For instance, it is the only platform\\nthat supports regular expressions. Additionally, while Skyfeed does\\nsupport personalized feeds, this feature requires manual setup from\\nthe developers and is not automated. As a result, most personal-\\nized Feed Generators are currently run by their creators. Although\\nconsisting of only 0.09% of all Feed Generators, they are among theLeonhard Balduf et al.\\nmost popular ones in the network (Figure 10). Adding such features\\nto the Feed Generator As a Service platforms would require a high\\namount of implementation effort and increases the platform costs.\\nHowever, their lack makes creating highly-quality, personalized\\nFeed Generators challenging.\\nMost of the platforms offer their services for free and are run\\nby platform enthusiasts. Only Blueskyfeedcreator provides paid\\noptions for feeds with additional features. Our conversation with\\nFeed Generator As a Service operators suggests that they try to\\ncover their costs by donations ( e.g.using patreon.com or ko-fi.com)\\nor consider running additional Bluesky-related services to generate\\nprofit. The lack of clear economic incentives puts in question the\\nability to scale the entire Feed Generator ecosystem.\\nTakeaways. In contrast to Labelers, Feed Generators already play\\na prominent role in the Bluesky ecosystem. High quality feed gen-\\nerators are widely used by users, while automatic, spamming feed\\ngenerators are less popular. Running a popular Feed Generator is an\\nefficient way for users to gain new followers providing a potential\\nincentive for users to create them. The already dynamic ecosystem\\nof Feed Generator As a Service platforms and its simplicity of use\\npromises a rapid growth of user-led Feed Generators.\\n8 Related Work\\nSeveral recent works have explored alternative emerging federated\\nsocial networks. Perhaps closest to our work is an initial investiga-\\ntion highlighting the centralization tendencies within Mastodon\\n[29]. Since then, there have been several related studies of these so-\\ncalled ‚Äúfediverse‚Äù applications. For instance, [ 11] study the growth\\nof Mastodon, and others have investigated how user behavior differs\\nacross server instances [ 9,12,13]. Additionally, some very recent\\nworks deal with Bluesky from a social network perspective, which\\nis not the focus of our work: Quelle and Bovet [ 28] investigate the\\nsocial and interaction graphs for Bluesky from a network-scientific\\nperspective. Jeong et al. [23] collect and investigate a dataset of user\\ninteractions with timestamp annotations to find temporal patterns.\\nThey also investigate migration patterns from and to Bluesky [ 24].\\nOne of the main challenges Bluesky attempts to overcome is de-\\ncentralized content moderation, via its labeling architecture. There\\nhave been several studies looking at content moderation labeling in\\nalternative decentralized social networks. For instance, Hassan et al.\\ninvestigate issues with policy implementation in decentralized so-\\ncial networks [ 19], and Zia et al. [ 10] proposed a federated solution\\nto training moderation models in the fediverse. Bluesky offers an\\ninteresting new point on the design space. To the best of our knowl-\\nedge, we are the first to study this new architecture. Beyond the\\nabove federated networks, there have also been studies of various\\nP2P decentralized social networks, perhaps most notably Secure\\nScuttlebutt (SSB). These offer a P2P event-sharing protocol and an\\narchitecture for social applications [ 33]. There are also blockchain-\\nbased social networks, such as including memo.cash [ 37], Steemit\\n[18], and Sapien [ 22]. Finally, some work focused on assessing the\\ncentralization of decentralized platforms such as Interplanterary\\nFilesystem (IPFS) [7, 35].\\nOur work differs as we focus on an entirely different decentral-\\nized architecture: Bluesky. In contrast to prior approaches, Bluesky\\ndecomposes social platform functionality into a set of sub-components.These are then opened to competing providers. To the best of our\\nknowledge, we are the first to offer a comprehensive measurement\\nstudy of Bluesky and its novel architecture.\\n9 Discussion and conclusion\\nWe have presented the first comprehensive measurement study of\\nthe Bluesky network. The platform implements a unique, hybrid\\napproach to federation, content moderation, and recommendation,\\nwhich presents its own set of opportunities and challenges.\\nEase of use and decentralization. Fully decentralized platforms,\\ndespite their benefits, tend to be more difficult to use than central-\\nized ones. Manually handling cryptographic keys or server migra-\\ntions is too difficult for the majority of users [ 36]. The Bluesky\\napproach tries to strike the right balance. By default, the platform\\nautomatically handles key management and DNS domain creation.\\nWhile giving full control to Bluesky PBC, this procedure looks ac-\\nceptable for most users and enables easy uptake. At the same time,\\ntech-savvy users can opt for more control by managing their keys\\nand domains themselves. Currently, only a small fraction of users\\n(1.1%) have chosen this option, suggesting that few people wish\\nto exploit this opportunity, or the technical challenges remain too\\ndifficult for most to overcome. However, the recent development\\nof alternative services with dedicated support facilitating Bluesky\\nidentity management ( e.g.NameCheap) might increase this number.\\nOpenness and diversity. Openness translates into diversity when\\nthe barrier to entry is low. Bluesky has opened a number of compo-\\nnents to the community, allowing anybody to implement competi-\\ntors. Most notably, the content moderation and recommendation\\nsub-components enjoy a diverse and growing ecosystem. There are\\ncurrently >40k Feed Generators providing personalized feeds ( e.g.\\nthe-algorithm), focusing on niche communities ( e.g.furry-art) or\\nserving explicit content ( e.g.feed-me-porn). While Bluesky PBC\\nhides some of this content from the default view, users can still ac-\\ncess them by adjusting their settings. Our results show that the use\\nof content moderation is also growing. We find 62Labeler accounts\\nin total, of which 34are active. Community-operated Labelers al-\\nready issue the majority of labels in the network after only two\\nmonths of their introduction. The freedom to create custom labels\\nand to re-adapt existing ones provides a high level of flexibility. It\\nremains to be seen whether some standardization will be neces-\\nsary to avoid inconsistencies and misinterpretations as the system\\ngrows.\\nImportantly, Bluesky PBC still controls the Firehose and the Ap-\\npView, which are arguably the main choke points of the system.\\nThis enables deleting user accounts, enforcing rules, and vetting\\nexternal services. However, it is not clear how a potential migra-\\ntion would work, and the network effect might prevent users from\\nswitching to a new service.\\nScalability. Maintaining centralized components by Bluesky PBC\\nensures a good user experience and simplifies the development\\nof federated components. For instance, running an independent\\nLabeler requires only lightweight operations, while the Firehose\\nand AppView handle the heavy lifting of aggregating a global view\\nof the network. However, as the platform grows, these centralizedLooking AT the Blue Skies of Bluesky\\ncomponents might become bottlenecks. Based on our measure-\\nments, we estimate that the Firehose already outputs ‚âà30GB of\\ndata per day per subscribed client. In the long run, the platform\\nmight need to decentralize these components to scale further.\\nLegal compliance. Bluesky PBC introduces an inclusive approach\\nto content moderation. While some content is hidden by default, it\\nis still processed and served by the platform. This approach might\\nbe problematic if users introduce problematic content such as copy-\\nrighted material or child sexual abuse material. Furthermore, the\\nplatform uses a git-like structure for storing data. Content can be\\nmarked as deleted, but can be still recovered from the reposito-\\nries. This might be problematic from the perspective of privacy-\\nprotecting laws such as GDPR.\\nInteroperability. Bluesky is built on top of the AT protocol [ 25],\\nwhich is extensible and designed to host multiple applications. Our\\nanalysis of the repositories shows that non-Bluesky content is al-\\nready present in the network, indicating that the ATProto fulfills\\nits role as an extendable base layer to build social applications upon.\\nHowever, the platform is currently not interoperable with exter-\\nnal social applications ( e.g.Mastodon) that run on another open\\nprotocol ‚Äì ActivityPub [ 1]. Greater interoperability could be key in\\nincreasing activity in Bluesky. Given the similar focus on openness\\nand user portability of applications supporting the ActivityPub\\nprotocols would be a good candidate and existing bridges already\\npoint to this possibility. We note that discussions on the integration\\nare already ongoing in the community.14\\nEconomics. The Bluesky network is currently fueled by the Bluesky\\nPBC, enthusiasts, and early adopters. Our analysis of account and\\nFeed Generator descriptions suggests that multiple users raise\\nmoney via dedicated services ( e.g.patreon.com, ko-fi.com) or point\\nto alternative services hosting the users‚Äô and compensating cre-\\nators ( e.g.youtube.com, twitch.tv). Bluesky PBC does not currently\\nsuggest introducing advertisements. However, our analysis of mul-\\ntiple forums and discussions around Bluesky suggests that multiple\\ncreators consider introducing ads as posts included in their feed\\nor Feed Generators. In the long run, the ecosystem will require\\neconomic incentives to become sustainable and compete with cen-\\ntralized platforms such as Twitter/X that recently started sharing\\nits revenue with content creators [2].\\nAcknowledgments\\nThe authors would like to thank the anonymous referees for their\\nvaluable comments and helpful suggestions. This work was sup-\\nported by the German Research Foundation (DFG) within the Col-\\nlaborative Research Center (CRC) SFB 1053: MAKI (https://gepris.\\ndfg.de/gepris/projekt/210487104).\\nReferences\\n[1] [n. d.]. ActivityPub Specification. https://www.w3.org/TR/activitypub/. Accessed\\non 15 May, 2024.\\n[2] [n. d.]. Ads Revenue Sharing. https://help.twitter.com/en/using-x/creator-ads-\\nrevenue-sharing. Accessed on 15 May, 2024.\\n[3] [n. d.]. Mastodon Statistics. https://mastodon-analytics.com/. Accessed on 15\\nMay, 2024.\\n[4] [n. d.]. NOSTR: A decentralized social network with a chance of working. https:\\n//nostr.com/. Accessed on 29 April, 2024.\\n14https://github.com/bluesky-social/atproto/discussions/1716[5] [n. d.]. Perguntas Frequentes do Usu√°rio Bluesky (Portugu√™s). https://bsky.social/\\nabout/blog/04-10-2024-user-faq-br. Accessed on 15 May, 2024.\\n[6] [n. d.]. X (Twitter) Statistics: How Many People Use X? https://backlinko.com/\\ntwitter-users. Accessed on 15 May, 2024.\\n[7]Leonhard Balduf, Maciej Korczy≈Ñski, Onur Ascigil, Navin V Keizer, George\\nPavlou, Bj√∂rn Scheuermann, and Micha≈Ç Kr√≥l. 2023. The Cloud Strikes Back:\\nInvestigating the Decentralization of IPFS. In Proceedings of the 2023 ACM on\\nInternet Measurement Conference . 391‚Äì405.\\n[8] J Bayer, Y Nosyk, O Hureau, S Fernandez, S Paulovics, A Duda, and M Korczynski.\\n2022. Study on Domain Name System (DNS) abuse ‚Äì Technical report. Appendix 1 .\\nPublications Office of the European Union. https://doi.org/doi/10.2759/473317\\n[9]Haris Bin Zia, Jiahui He, Ignacio Castro, and Gareth Tyson. 2024. Fediverse\\nMigrations: A Study of User Account Portability on the Mastodon Social Network.\\nInProc. of ACM Internet Measurement Conference (IMC) .\\n[10] Haris Bin Zia, Aravindh Raman, Ignacio Castro, Ishaku Hassan Anaobi, Emil-\\niano De Cristofaro, Nishanth Sastry, and Gareth Tyson. 2022. Toxicity in the\\ndecentralized web and the potential for model sharing. Proceedings of the ACM\\non Measurement and Analysis of Computing Systems 6, 2 (2022), 1‚Äì25.\\n[11] Lucio La Cava, Sergio Greco, and Andrea Tagarelli. 2021. Understanding the\\ngrowth of the Fediverse through the lens of Mastodon. Applied Network Science\\n6 (2021), 1‚Äì35.\\n[12] Lucio La Cava, Sergio Greco, and Andrea Tagarelli. 2022. Information consump-\\ntion and boundary spanning in decentralized online social networks: the case of\\nmastodon users. Online Social Networks and Media 30 (2022), 100220.\\n[13] Lucio La Cava, Sergio Greco, and Andrea Tagarelli. 2022. Network analysis of the\\ninformation consumption-production dichotomy in mastodon user behaviors.\\nInProceedings of the International AAAI Conference on Web and Social Media ,\\nVol. 16. 1378‚Äì1382.\\n[14] CNBC. [n. d.]. Social media platform Bluesky attracts millions in Brazil after\\njudge bans Musk‚Äôs X . https://www.cnbc.com/2024/09/04/social-media-platform-\\nbluesky-attracts-millions-in-brazil-after-judge-bans-musks-x-.html. Accessed\\non 11 Sep, 2024.\\n[15] Michal Danilak et al. 2021. langdetect. https://pypi.org/project/langdetect/.\\n[16] Sarah Frier, Naomi Nix, and Sarah Kopit. 2021. Why Free Speech on the Internet\\nIsn‚Äôt Free for All. International New York Times (2021), NA‚ÄìNA.\\n[17] Christian Gribneau, Michael Prorock, Orie Steele, Oliver Terbu, Mike Xu, and\\nDmitri Zagidulin. 2023. DID WEB Method (did:web). https://perma.cc/WB8M-\\n8ECW.\\n[18] Barbara Guidi, Andrea Michienzi, and Laura Ricci. 2020. A graph-based socioe-\\nconomic analysis of steemit. IEEE Transactions on Computational Social Systems\\n8, 2 (2020), 365‚Äì376.\\n[19] Anaobi Ishaku Hassan, Aravindh Raman, Ignacio Castro, Haris Bin Zia, Emiliano\\nDe Cristofaro, Nishanth Sastry, and Gareth Tyson. 2021. Exploring content\\nmoderation in the decentralised web: The pleroma case. In Proceedings of the 17th\\nInternational Conference on emerging Networking EXperiments and Technologies .\\n328‚Äì335.\\n[20] Jiahui He, Haris Bin Zia, Ignacio Castro, Aravindh Raman, Nishanth Sastry, and\\nGareth Tyson. 2023. Flocking to mastodon: Tracking the great twitter migration.\\nInProceedings of the 2023 ACM on Internet Measurement Conference . 111‚Äì123.\\n[21] Daniel Holmgren, Bryan Newbold, Devin Ivy, and Jake Gold. 2023. DID PLC\\nMethod (did:plc). https://github.com/did-method-plc/did-method-plc.\\n[22] Lars Andreassen Jaatun, Anders Ringen, and Martin Gilje Jaatun. 2022. Yet\\nAnother Blockchain-based Privacy-friendly Social Network. In 2022 IEEE Interna-\\ntional Conference on Cloud Computing Technology and Science (CloudCom) . IEEE,\\n222‚Äì229.\\n[23] Ujun Jeong, Bohan Jiang, Zhen Tan, H. Russell Bernard, and Huan Liu. 2024.\\nBlueTempNet: A Temporal Multi-network Dataset of Social Interactions in\\nBluesky Social. arXiv:2407.17451 [cs.SI] https://arxiv.org/abs/2407.17451\\n[24] Ujun Jeong, Ayushi Nirmal, Kritshekhar Jha, Susan Xu Tang, H. Russell Bernard,\\nand Huan Liu. 2024. User Migration across Multiple Social Media Platforms.\\narXiv:2309.12613 [cs.SI] https://arxiv.org/abs/2309.12613\\n[25] Martin Kleppmann, Paul Frazee, Jake Gold, Jay Graber, Daniel Holmgren, Devin\\nIvy, Jeromy Johnson, Bryan Newbold, and Jaz Volpert. 2024. Bluesky and the\\nAT Protocol: Usable Decentralized Social Media. arXiv preprint arXiv:2402.03239\\n(2024).\\n[26] Victor Le Pochat, Tom Van Goethem, Samaneh Tajalizadehkhoob, Maciej Ko-\\nrczy≈Ñski, and Wouter Joosen. 2019. Tranco: A Research-Oriented Top Sites\\nRanking Hardened Against Manipulation. In NDSS .\\n[27] Stephen McQuistin, Peter Snyder, Colin Perkins, Hamed Haddadi, and Gareth\\nTyson. 2023. A first look at the privacy harms of the public suffix list. In Proceed-\\nings of the 2023 ACM on Internet Measurement Conference . 383‚Äì390.\\n[28] Dorian Quelle and Alexandre Bovet. 2024. Bluesky: Network Topology, Polariza-\\ntion, and Algorithmic Curation. arXiv:2405.17571 [cs.SI] https://arxiv.org/abs/\\n2405.17571\\n[29] Aravindh Raman, Sagar Joglekar, Emiliano De Cristofaro, Nishanth Sastry, and\\nGareth Tyson. 2019. Challenges in the decentralised web: The mastodon case. In\\nProceedings of the internet measurement conference . 217‚Äì229.\\n[30] Alan Z Rozenshtein. 2023. Moderating the fediverse: Content moderation on\\ndistributed social media. J. Free Speech L. 3 (2023), 217.Leonhard Balduf et al.\\n[31] Adam Satariano. 2021. Facebook Hearing Strengthens Calls for Regulation in\\nEurope. International New York Times (2021), NA‚ÄìNA.\\n[32] Manu Sporny, Dave Longley, Markus Sabadello, Drummond Reed, Orie Steele,\\nand Christopher Allen. 2022. Decentralized Identifiers (DIDs) v1.0. https://www.\\nw3.org/TR/did-core/.\\n[33] Dominic Tarr, Erick Lavoie, Aljoscha Meyer, and Christian Tschudin. 2019. Se-\\ncure Scuttlebutt: An Identity-Centric Protocol for Subjective and Decentralized\\nApplications (ICN ‚Äô19) . Association for Computing Machinery, New York, NY,\\nUSA, 1‚Äì11. https://doi.org/10.1145/3357150.3357396\\n[34] Leanne Townsend and Claire Wallace. 2017. The ethics of using social media\\ndata in research: A new framework. In The ethics of online research . Emerald\\nPublishing Limited, 189‚Äì207.\\n[35] Yiluo Wei, Dennis Trautwein, Yiannis Psaras, Ignacio Castro, Will Scott, Aravindh\\nRaman, and Gareth Tyson. 2024. The Eternal Tussle: Exploring the Role of\\nCentralization in{IPFS}. In21st USENIX Symposium on Networked Systems\\nDesign and Implementation (NSDI 24) . 441‚Äì454.\\n[36] Alma Whitten and J Doug Tygar. 1999. Why Johnny Can‚Äôt Encrypt: A Usability\\nEvaluation of PGP 5.0.. In USENIX security symposium , Vol. 348. 169‚Äì184.\\n[37] Wenrui Zuo, Aravindh Raman, Raul J Mondrag√≥n, and Gareth Tyson. 2023.\\nSet in Stone: Analysis of an Immutable Web3 Social Media Platform (WWW\\n‚Äô23). Association for Computing Machinery, New York, NY, USA, 1865‚Äì1874.\\nhttps://doi.org/10.1145/3543507.3583510\\nA Additional Data\\nWe provide additional information on Feed Generator As a Service\\nplatforms (Table 5) and the complete table of reaction times of\\nlabelers to posts published via the Firehose (Table 6).B Ethics\\nWe believe that the benefits of our research significantly outweigh\\npotential harms. This work helps understand the implications of\\nopening and decentralizing social network platforms. We take mul-\\ntiple actions to minimize any potential harm. We exclusively collect\\npublicly available information and follow well established ethical\\nprocedures for social data [ 34]. We make no attempt to link activi-\\nties to other accounts or real-world identifies and the collected data\\nis stored securely within a university silo, and no external access is\\ngiven. Prior to initiating our scans, we contacted the Bluesky team\\nto agree upon a scanning rate that would not disrupt the normal\\nfunctioning of their service. Additionally, and to ensure a mini-\\nmal impact, we implement a solution that exclusively downloads\\nrepositories if their content has changed since the last snapshot.\\nWe identified issues preventing this originally and upstreamed fixes\\nto the Bluesky open-source projects.\\nWhen analyzing the public endpoints of community labeling\\nservices we deduce their IP addresses while subscribing to them\\nfor labels, which is intended behavior. Analyses of the IP addresses\\nthemselves happened locally on university machines.Looking AT the Blue Skies of Bluesky\\nTable 5: Comparing the top 5 feed generator builders/services.\\nFeature Skyfeed Bluefeed Blueskyfeeds goodfeeds Blueskyfeedcreator\\nInputs\\nWhole network ‚úì ‚úì ‚úì ‚úì\\nTags ‚úì ‚úì ‚úì ‚úì\\nSingle user ‚úì ‚úì ‚úì ‚úì ‚úì\\nList ‚úì ‚úì ‚úì ‚úì\\nFeed ‚úì ‚úì\\nSingle post ‚úì ‚úì ‚úì\\nLabels ‚úì ‚úì\\nToken ‚úì\\nSegment ‚úì\\nFilters\\nItem ‚úì ‚úì\\nLabels ‚úì ‚úì ‚úì ‚úì\\nImage count ‚úì\\nLink count ‚úì\\nRepost count ‚úì\\nEmbed ‚úì\\nDuplicate ‚úì\\nList of users ‚úì ‚úì ‚úì\\nLanguage ‚úì ‚úì ‚úì\\nRegex\\nText ‚úì\\nImage Alt ‚úì\\nLink ‚úì\\nOther Features\\nNumber of Feeds 35,415 2,302 1,797 929 158\\nPaid or Free free free free free free & paidLeonhard Balduf et al.\\nTable 6: Reaction time of Labelers to Posts Published via the Firehose.\\nLabels Applied Reaction Time [s]\\nRank DID Top Values # Unique # Total Share (%) Median IQD\\n1 did:plc:wp7hxfjl5l4zlptn7y6774lk no-alt-text, non-alt-text, mis-alt-\\ntext4 1 ,360 ,224 72 .91 0 .58 0 .10\\n2 did:plc:ar7c4by46qjdydhdevvrndac porn, sexual, nudity 32 279 ,002 14 .95 1 .76 0 .70\\n3 did:plc:newitj5jo3uel7o4mnf3vj2o twitter-screenshot, bluesky-\\nscreenshot, uncategorised-\\nscreenshot14 76 ,599 4 .11 3 .70 3 .81\\n4 did:plc:mjyeurqmqjeexbgigk3yytvb tenor-gif, tenor-gif-no-text 2 73 ,875 3 .96 0 .35 0 .20\\n5 did:plc:bpkpvmwpd3nr2ry4btt55ack ai-imagery 1 56 ,517 3 .03 0 .82 0 .21\\n6 did:plc:fcikraffwejtuqffifeykcml shadowbringers, endwalker, dawn-\\ntrail6 10 ,024 0 .54 2 .07 0 .82\\n7 did:plc:3eivfiql4memqxkryeu4tqnk ai-related-content, spoiler, test-\\nlabel3 7 ,646 0 .41 1 .32 0 .78\\n8 did:plc:j67mwmangcbxch7knfm7jo2b trolling, transphobia, racial-\\nintolerance13 876 0 .05 13 ,911 .90 53 ,085 .19\\n9 did:plc:vrjubqujt3v46z5poehh4qfg pup, fatfur, diaper 18 631 0 .03 34 ,408 .43 65 ,282 .36\\n10 did:plc:3ehw5dwwptcy3xuzugwq2u6t beans 1 49 <0.01 90 .39 5 ,089 .05\\n11 did:plc:skibpmllbhxvbvwgtjxl3uao simping, bad-selfies, cringe 5 32 <0.01 70 ,413 .53 121 ,503 .24\\n12 did:plc:olmiw2wkm3qoxinal7w5fbnl lowquality, shorturl, unknown-\\nsource6 26 <0.01 104 ,584 .57 236 ,752 .45\\n13 did:plc:e4elbtctnfqocyfcml6h2lf7 alf, sensual-alf, the-format 3 18 <0.01 38 ,417 .71 61 ,154 .18\\n14 did:plc:exlb5xx2t4pgtjqzdm6ntsgh severity-alert-blurs-content,\\nseverity-alert-blurs-media, severity-\\nalert-blurs-none9 18 <0.01 937 .55 584 .76\\n15 did:plc:4vf7tgwlg6edds2g2nixyjda spam-aff-ja, spam, porn 4 16 <0.01 534 ,935 .10 429 ,626 .79\\n16 did:plc:gcbmhqcuvuoz7jgmlanabiuv so-true, epic, based 4 16 <0.01 526 .03 3 ,413 .47\\n17 did:plc:5o2g6wwchb3tgwrhl2atauzu !warn, threat, triggerwarning 10 14 <0.01 109 ,931 .10 373 ,967 .40\\n18 did:plc:36inn6r2ttwfrt6tpywsjcmt coulro, arachno, lepidoptero 6 11 <0.01 260 ,511 .95 297 ,492 .05\\n19 did:plc:cnn3jrtucivembf66xe6fdfs neutral-pro-discourse, anti-\\ndiscourse2 10 <0.01 2 ,120 .64 47 ,340 .94\\n20 did:plc:mcskx665cnmnkgqnunk6lkrk spoilers, !no-promote, !no-\\nunauthenticated3 4 <0.01 1 ,585 ,404 .55 3 ,100 ,279 .22\\n21 did:plc:z2i5ah5elywxdcr64i7xai3z nipps, no-church, non-handshake 3 4 <0.01 154 ,416 .53 95 ,557 .08\\n22 did:plc:7fkqmr7dfu6vanyxvjtloos3 !warn, porn, spam 3 3 <0.01 5 ,203 .95 95 ,853 .62\\n23 did:plc:j2zujaxuq33c7nbcqyvgvyvk amplifying-disinfo 1 3 <0.01 5 ,445 .06 9 ,348 .35\\n24 did:plc:hxgctysbwhc3bap3a5c7gdu3 beanhate, feature-scold 2 2 <0.01 5 ,900 .41 4 ,489 .93',\n",
       " 'arXiv:2408.12743v2  [cs.CR]  7 Dec 2024The Matrix Reloaded: A Mechanized Formal\\nAnalysis of the Matrix Cryptographic Suite\\nJacob Ginesin\\nNortheastern UniversityCristina Nita-Rotaru\\nNortheastern University\\nAbstract ‚ÄîSecure instant group messaging applications such as\\nWhatsApp, Facebook Messenger, Matrix, and the Signal Appli -\\ncation have become ubiquitous in today‚Äôs internet, cumulat ively\\nserving billions of users. Unlike WhatsApp, for example, Ma trix\\ncan be deployed in a federated manner, allowing users to choo se\\nwhich server manages their chats. To account for this differ ence in\\narchitecture, Matrix employs two novel cryptographic prot ocols:\\nOlm, which secures pairwise communications, and Megolm,\\nwhich relies on Olm and secures group communications. Olm\\nand Megolm are similar to and share security goals with Signa l\\nand Sender Keys, which are widely deployed in practice to sec ure\\ngroup communications. While Olm, Megolm, and Sender Keys\\nhave been manually analyzed in the computational model, no\\nsymbolic analysis nor mechanized proofs of correctness exi st. Us-\\ning mechanized proofs and computer-aided analysis is impor tant\\nfor cryptographic protocols, as hand-written proofs and an alysis\\nare error-prone and often carry subtle mistakes.\\nUsing P ROVERIF , we construct formal models of Olm and\\nMegolm, as well as their composition. We prove various prope rties\\nof interest about Olm and Megolm, including authentication , con-\\nÔ¨Ådentiality, forward secrecy, and post-compromise securi ty. We\\nalso mechanize known limitations, previously discovered a ttacks,\\nand trivial attacker wins from the speciÔ¨Åcations and previo us\\nliterature. Finally, we model Sender Keys and the compositi on\\nof Signal with Sender Keys in order to draw a comparison with\\nOlm, Megolm, and their composition. Our mechanized analysi s\\nindicates subtle yet critical differences in forward secre cy between\\nOlm and Signal, which in turn affects the post-compromise\\nsecurity of Megolm and Sender Keys respectively. From our\\nanalysis we conclude the composition of Olm and Megolm has\\ncomparable security to the composition of Signal and Sender Keys\\nif Olm pre-keys are signed, and provably worse post-comprom ise\\nsecurity if Olm pre-keys are not signed.\\nI. I NTRODUCTION\\nEnd-to-end encryption represents the foundation for se-\\ncurity, privacy, trust, and compliance for all services on t he\\ninternet. Many end-to-end encryption protocols exist, all serv-\\ning different purposes. SpeciÔ¨Åcally, Transport Layer Secu rity\\n(TLS) [73] and Quick UDP Internet Connections (QUIC)\\n[47] secure web trafÔ¨Åc, Pretty Good Privacy (PGP) [88]\\nsecures email communications and Ô¨Åles, Zimmermann Real-\\nTime Transport Protocol (ZRTP) [36] secures V oice over IP\\n(V oIP) communications, while Wireguard [66] and OpenVPN\\n[86] secure point-to-point tunneling. In addition, with th e rise\\nof popularity of secure instant messaging applications suc h\\nas WhatsApp [85], iMessage [15], Telegram [48], Facebook\\nMessenger [50], and Matrix [38], secure instant messaging\\nprotocols such as Signal [39], OMEMO [80], and MTProto\\n[48] have become ubiquitous in today‚Äôs internet.\\nWhile secure instant messaging protocols have the samehigh-level guarantees as protocols such as TLS, there are su b-\\ntleties to instant messaging that lead to differences in pro tocol\\ndesign and desired security properties. First, messages ar e\\nasynchronous ; an online peer must be able to send messages\\nto an ofÔ¨Çine peer. This means that parties must rely on a\\npotentially untrusted central server to initiate authenti cation\\nand key exchange. Second, conversations are long-lived ; unlike\\nTLS connections, which typically last for a few seconds,\\ninstant messaging conversations may go on for years and\\nhave many sensitive messages. Thus, it is highly likely an\\nendpoint may be compromised during a conversation lifetime ,\\nmaking the protection of conversation contents in the case\\nof long-term key compromise a major security goal. Third,\\ntranscripts are ideally restorable ; users often expect to be able\\nto restore their message conversation history from a server ,\\nwhich introduces additional challenges in terms of ensurin g\\nthe integrity and conÔ¨Ådentiality of stored messages. Thus,\\nbesides authentication and conÔ¨Ådentiality of messages whe n\\nlong-term keys are not compromised, secure instant messagi ng\\nprotocols are designed to provide security when long-term\\nkeys are compromised. The primary properties of interest ar e\\nforward secrecy , ensuring past messages remain secure in the\\nevent long-term keys are compromised, and post-compromise\\nsecurity , ensuring future messages remain secure in the event\\nlong-term keys are compromised.\\nForward secrecy and post-compromise security are well\\nunderstood for secure two-peer messaging protocols such\\nas Signal [39]. However, secure group messaging has com-\\nparatively seen much less attention [83]. Group messaging\\nintroduces additional complexity, as members join and leav e\\nover time, motivating novel protocol designs. One notable\\nprotocol for secure instant group messaging is the Sender\\nKeys variant of Signal [67] which, among other applications,\\nis employed by WhatsApp [85], Facebook Messenger [50], the\\nSignal Application [76], and the Session Application [75].\\nMatrix is a recently introduced open standard for secure,\\ndecentralized, real-time communication that promises int erop-\\nerability and end-to-end encryption [38]. Unlike Signal, M atrix\\nis designed to be deployed in a federated way, giving users\\ncontrol over which servers host and manage their conversa-\\ntions. Matrix has seen wide adoption across governments, th e\\nprivate sector, and the general public. It is used by governm ent\\norganizations in France, Germany, Sweden, and Luxembourg;\\nfor example, France‚Äôs central administration takes place o n\\na Matrix-based internal network, Tchap [49], and Germany‚Äôs\\ndefense and healthcare organizations employ Matrix in the Ô¨Å eld\\n[56]. Mozilla and KDE use Matrix internally [41]. A number\\nof existing applications integrate Matrix, including the f orumsoftware Discourse [74] and the enterprise messaging platf orm\\nRocket.Chat [74]. As of September 2024 Matrix reports over\\n115 million users on public, data-reporting home servers [8 2].\\nThe Matrix stack consists of a number of speciÔ¨Åcations [38]\\nwhich cumulatively deÔ¨Åne a federated secure group messagin g\\nprotocol. The cumulative protocol allows for users to choos e a\\nhome server, which in turn federates with other home servers\\nin order to exchange messages. Users construct ‚Äùrooms‚Äù whos e\\nchat history is shared between the users‚Äô respective home\\nservers. Every chat in Matrix is a room, including one-on-\\none chats. The Matrix stack contains three novel protocols:\\nthe Matrix protocol, which speciÔ¨Åes the various APIs the\\nclients and homeservers must implement, the Olm protocol,\\nwhich implements peer-to-peer encryption and can be com-\\npared to the Signal protocol in terms of security guarantees ,\\nand the Megolm protocol, which implements peer-to-multipe er\\nencryption and is comparable to the Sender Keys variant of\\nSignal. The Matrix stack employs TLS to secure individual\\ncommunications between clients and servers and between\\nservers and other servers (via federation). However, full e nd-\\nto-end encryption is realized by employing Olm and Megolm\\nin composition.\\nSince the introduction of Matrix in 2014, several cryp-\\ntographic vulnerabilities have been reported across both t he\\nspeciÔ¨Åcations and implementations of Olm and Megolm. An\\naudit of Olm and Megolm was conducted by the NCC group in\\n2016 which, among other issues, disclosed an unknown key-\\nshare attack in Megolm [58]. Several cryptographic vulner-\\nabilities in the Matrix speciÔ¨Åcation and the Ô¨Çagship matrix\\nclient, Element, were reported in 2022 [10]. Many additiona l\\ncryptographic vulnerabilities have been reported, includ ing\\nCVE-2021-34813 [4], CVE-2021-40824 [5], CVE-2022-39251\\n[7], and CVE-2022-39248 [6]. Largely in response to [10],\\nan audit of vodozemac , a library implementing Olm and\\nMegolm, was conducted by Least Authority. This audit discov -\\nered several implementation-level cryptographic issues [ 16].\\nImplementation-level weaknesses were discovered in libolm , a\\nrecently depreciated yet widely used reference implementa tion\\nof Olm as of August 2024 [77]. These vulnerabilities highlig ht\\nthe need for precise, formal, and rigorous cryptographic an aly-\\nsis, especially to understand precisely how weaknesses in O lm\\nor Megolm may affect the entire Matrix stack.\\nFormal security proofs of cryptographic protocols and\\ncryptosystems are generally classiÔ¨Åed into two categories : (1)\\ncomputational proofs, which make precise computational as-\\nsumptions and show the complexity of breaking the cryptosys -\\ntem reduces to the complexity of breaking the assumptions;\\n(2)symbolic or Dolev-Yao proofs, which rely on algebraic\\nabstractions of primitives and directly reason about the ac tions\\nof a channel-controlling attacker. In general, both method s can\\nbe used to provide complementary beneÔ¨Åts as some attacks\\nmay arise from one approach but not the other [62]. A full\\nbreakdown of the differences between the two approaches\\nis discussed in greater detail in [32]. In recent years it has\\nbecome the standard to analyze cryptographic protocols bot h\\nsymbolically and computationally, as demonstrated by the\\nmany works on Signal [62] [39] [12] [27] [31] [63], TLS [42],\\nand Wireguard [45] [46] [65] [61], as well as the dual analysi s\\nof iMessage PQ3 [78] [21].\\nOur contribution. In this work we take an approachrooted in formal methods and computer-aided reasoning to\\nstudy the cryptographic security of the Matrix protocol sui te,\\nallowing us to construct computer-veriÔ¨Åed proofs or veriÔ¨Åe d\\nand explicit counterexamples. We employ P ROVERIF as our\\nsymbolic analysis tools of choice. We make the following\\ncontributions:\\nModels . We present the Ô¨Årst symbolic analysis and mecha-\\nnization of the Matrix protocol suite; taking such an approa ch\\nallows us to precisely and automatically compare the cryp-\\ntographic guarantees of the Matrix cryptographic suite to i ts\\ncontemporaries. We construct the Ô¨Årst formal models for Olm ,\\nMegolm, and their composition. To understand how Olm and\\nMegolm compare with Signal and Sender Keys, we employ\\na pre-existing P ROVERIF Signal model [62], we implement\\nSender Keys, and we model the composition of Signal and\\nSender Keys.\\nVeriÔ¨Åcation . Using P ROVERIF for automated symbolic\\nanalysis, we derive various proofs, limitations, and previ ously\\nfound attacks from the Matrix and Sender Keys speciÔ¨Åcations\\nand previous formalizations. First, we mechanically prove\\nthe conÔ¨Ådentiality, authentication, forward secrecy, and post-\\ncompromise security properties of Olm and Megolm, mecha-\\nnizing the previous hand-written computational analysis o f the\\nMatrix cryptographic core [11] and the stated security goal s\\nin the Olm [52] and Megolm [51] speciÔ¨Åcations. We then\\nmechanize previously known attacks and limitations, inclu ding\\nthe Megolm unknown key-share attack described in the Matrix\\nNCC audit [58], the tradeoff between forward secrecy and\\ndeniability in Olm [52], and the unauthorized injection att ack\\ndescribed in the computational analysis of Sender Keys [17] .\\nWe mechanically prove the extension of Sender Keys proposed\\nby Balbas et al. satisÔ¨Åes the intended security properties,\\nconÔ¨Ådentiality and authentication [17].\\nComparison . Using our P ROVERIF models, we precisely\\ncompare the cryptographic guarantees of Olm and Megolm\\nwith Signal and Sender keys. Among other insights, we\\nobserve Olm channels without pre-key signing result in the\\npost-compromise security of the composition of Megolm bein g\\nsigniÔ¨Åcantly weaker than Sender Keys. We note that the\\nMegolm speciÔ¨Åcation [51] does not require Olm pre-keys to be\\nsigned while the Matrix protocol [2] requires the Olm pre-ke ys\\nto be signed but provides no reason why. Our work elucidates\\nthis question providing formal proofs that in the absence of\\nOlm pre-keys signed, the composition of Olm and Megolm\\nprovides weaker security.\\nEthics consideration. We have alerted the Matrix founda-\\ntion‚Äôs security contact point of our Ô¨Åndings and risks. They\\npointed us to the design document speciÔ¨Åying the Matrix\\narchitecture [2] that require Olm pre-keys to be pre-signed ,\\nthus conÔ¨Årming our Ô¨Åndings.\\nCode . Our models are avail-\\nable in our anonymized git repository\\nhttps://anonymous.4open.science/r/Matrix-Symbolic-A nalysis-7D9D.\\nII. B ACKGROUND\\nIn this section, we present the necessary background for\\nour work, including a description of Signal, Sender Keys, Ol m,\\nMegolm, and a brief overview of their security. We then clari fy\\nthe scope of our analysis.\\n2A. Overview of Signal and Sender Keys\\nSignal . The Signal protocol is a cryptographic protocol\\nused for encrypted instant messaging between two parties.\\nAmong other properties, Signal provides conÔ¨Ådentiality, i n-\\ntegrity, forward secrecy, and post-compromise security [8 3].\\nSignal provides end-to-end encryption for Whatsapp [85] an d\\nFacebook Messenger [50] among many other services, serving\\nover a billion users [39].\\nThe core of Signal‚Äôs design is the double ratchet algorithm,\\nwhich provides forward secrecy and post-compromise securi ty\\nsimultaneously [39]. Signal connections are chunked into S ig-\\nnal ‚Äùsessions,‚Äù which indicate stateful cryptographic con text in\\nwhich forward secrecy is held between. The symmetric key for\\neach (short-lived) Signal session is derived from a combina tion\\nof (1) a ratchet, deÔ¨Åned as a key which is continually ‚Äùad-\\nvanced‚Äù via a hashing algorithm per each new Signal session,\\nagreed upon at the beginning of the Signal connection, and\\n(2) a ‚Äùfresh‚Äù shared key derived from a DifÔ¨Åe-Hellman key\\nexchange, which both peers take turns in initiating. To secu rely\\ninitiate an authenticated connection and agree upon a share d\\ninitial ratchet key, Signal relies on the Extended Triple Di fÔ¨Åe-\\nHellman (X3DH) key agreement protocol [3].\\nIn more detail, the protocol works as follows. Each\\npeer generates an asymmetric long-term identity key pair, a\\nmedium-term asymmetric pre-key pair, and a one-time asym-\\nmetric pre-key pair. Then, each peer uploads a ‚Äùpre-key bun-\\ndle‚Äù to the Signal server, containing their public identity key,\\ntheir public pre-key signed with the identity key, and the pu blic\\none-time pre-key. If Alice wanted to initiate a connection\\nwith Bob, Alice would retrieve Bob‚Äôs pre-key bundle from\\nthe Signal server, generate an asymmetric ephemeral key pai r,\\nand compute four DifÔ¨Åe-Hellman key agreements to derive\\nthe shared initial master key: (1) between Alice‚Äôs ephemera l\\nkey and Bob‚Äôs identity key, (2) between Alice‚Äôs identity key\\nand Bob‚Äôs signed pre-key, (3) between Alice‚Äôs ephemeral key\\nand Bob‚Äôs signed pre-key, and (4) between Alice‚Äôs ephemeral\\nkey and Bob‚Äôs one-time pre-key. Alice then sends Bob her\\nephemeral public key, her identity key, and some additional\\nmetadata; Bob then computes the DifÔ¨Åe-Hellman exchanges\\nin turn, deriving the same master key as Alice. Once Alice\\nand Bob agree on a shared master key, they derive an initial\\nratchet key using a key derivation function. Importantly, t his\\nkey exchange provides post-compromise security in the case\\nlong-term identity keys are leaked. However, Signal is reli ant\\non off-band veriÔ¨Åcation of identity keys [39], and without s uch\\nthe protocol is vulnerable to an unknown key-share attack.\\nThe Signal handshake is visualized in Figure 5 in the\\nAppendix, and we refer the reader to [39] for a complete\\nformal description of the protocol.\\nSender Keys . The Sender Keys protocol is a cryptographic\\nprotocol used for encrypted instant messaging within a grou p\\nwithout group key agreement. In comparison to protocols with\\ngroup key agreement such as Message Layer Security, Sender\\nKeys offers different performance guarantees, as fully det ailed\\nin [26]. Sender Keys provides conÔ¨Ådentiality, forward secr ecy,\\nand authentication assuming there exists a secure peer-to-\\npeer channel (such as Signal). However, because Sender Keys\\ndoes not do group key agreement, transcript equivalence is\\nnot maintained [19]. That is, the message histories betweenSender Keys peers may differ due to asynchronous message\\ndelivery or the re-ordering of messages. Sender Keys has see n\\nwidespread deployment, being used by the Signal Applicatio n\\n[76], Whatsapp [85], Facebook Messenger [50], and the Ses-\\nsion Application [75].\\nThe core mechanism of the Sender Keys protocol is the\\nnotion of a session . Note, sessions in the context of Sender\\nKeys or Megolm are entirely different from sessions in Signa l\\nor Olm; notionally, we will preÔ¨Åx each mention of a session\\nwith the associated protocol for the remainder of the paper.\\nThe protocol works as follows. Each time the state of\\nthe group changes (e.g., a member joins, leaves, etc.), each\\npeer generates a new Sender Keys session and transmits it\\nto all other peers in the group via a secure peer-to-peer\\nchannel. A Sender Keys session consists of an asymmetric\\n‚Äùsender‚Äù keypair (of which only the public key is transmitte d),\\na ratchet key, and a ratchet index. Sent messages are signed\\nwith the private key of the sender keypair and encrypted with a\\nsymmetric key derived from the ratchet key, which is advance d\\nas necessary with a hash function. All receiving peers verif y\\nthe signature with the public key from the sender keypair,\\nadvance the ratchet key, derive the symmetric key, and decry pt\\nthe message. Thus, each peer stores the Sender Keys sessions\\nof all other peers. A full Sender Keys exchange is deÔ¨Åned in\\nFigure 2.\\nB. Overview of Olm and Megolm\\nWhile the Signal and Sender Keys protocol scheme as-\\nsumes a single, reliable server, Matrix is a federated system\\nwith no central server. This motivated the development of\\nOlm and Megolm (adjacent to the development of Signal and\\nSender Keys), cryptographic protocols that are better desi gned\\nto be routed between multiple servers as deÔ¨Åned via the Matri x\\nprotocol‚Äôs federation scheme [38]. The Matrix protocol spe ci-\\nÔ¨Åcation, which deÔ¨Ånes room construction and message routin g\\nbetween federated servers and employs Olm and Megolm as\\nsub-protocols, is disjoint from the protocol speciÔ¨Åcation s of\\nOlm and Megolm. For the purpose of our analysis we do\\nnot reason about the overarching Matrix protocol and instea d\\nreason directly about Olm and Megolm.\\nOlm . Olm is used in one-on-one chats between two parties,\\nand can be compared to the Signal protocol in terms of\\nsecurity guarantees [39] [52]. However, Olm is never used\\nindividually, and its purpose is to serve as a subroutine for\\ninitial key exchange for Megolm. Precisely like Signal, Olm\\nis a double ratchet algorithm that derives each Olm session\\nkey from advancing a ratchet key and a deriving a shared key\\nvia DifÔ¨Åe-Hellman (which, like Signal, both peers take turn s\\nin initiating). Where Olm differs from Signal, however, is t he\\nalgorithm for which Olm initiates connections: Olm utilize s\\nTriple DifÔ¨Åe-Hellman (3DH), while Signal utilizes Extende d\\nTriple DifÔ¨Åe-Hellman (X3DH).\\nThe protocol works as follows. Each peer generates an\\nasymmetric long-term identity key pair and an asymmetric\\none-time key-pair. Then, each peer uploads their pre-key bu n-\\ndle to the Olm server, consisting only of the peer‚Äôs identity key\\nand public one-time key. If Alice wants to initiate a connect ion\\nwith Bob, Alice would retrieve Bob‚Äôs pre-key bundle from the\\nOlm server and compute three DifÔ¨Åe-Hellman key agreements\\n3to derive the initial shared ratchet key: (1) between Alice‚Äô s\\nidentity key and Bob‚Äôs one-time pre-key, (2) between Alice‚Äô s\\none-time pre-key and Bob‚Äôs identity key, and (3) between\\nAlice‚Äôs one-time pre-key and Bob‚Äôs one-time pre-key.\\nA full breakdown of the differences between Olm and\\nMegolm is available in Table I. Most notably, Olm does\\nnot require one-time pre-keys be signed, citing a tradeoff\\nbetween deniability and forward secrecy [52]. Additionall y,\\nbecause Olm uses Curve25519 as opposed to Signal‚Äôs XEd-\\nDSA scheme, Olm must generate its own Fingerprint key ‚Äì a\\nshort sequence of bytes used to identify a longer public key ‚Äì\\nas opposed to deriving it from the identity key as Signal does\\n[71]. We note that the Matrix protocol speciÔ¨Åcation diverge s\\nfrom Olm [52] and requires Olm pre-keys to be signed [68],\\nyet there does not exist any justiÔ¨Åcation for this decision.\\nMegolm . Megolm is used to ensure group communications\\nare encrypted. Originally introduced in 2016, before Sende r\\nKey (group messaging variant of Signal) [67] was published,\\nMegolm has very similar functionality to Sender Keys . How-\\never, Megolm utilizes Olm as opposed to Signal as the secure\\npeer-to-peer channel in order to share Megolm session infor -\\nmation between peers. Additionally, Megolm utilizes a cust om\\nratchet scheme that can be advanced an arbitrary amount\\nforward while needing at most 1020 hash computations [51],\\nwhich is further described in Figure VII-A. Aside from the\\ndetails in cryptographic primitives between implementati ons\\nof Sender Keys and Megolm as shown in Table II, the secure\\nchannel protocol, and the ratcheting scheme, all other deta ils\\nbetween the two protocols remain the same.\\nFor precise game-theoretic cryptographic deÔ¨Ånitions of\\nOlm and Megolm, we defer to [11], of which our models and\\nanalysis are closely based upon. Our derived Olm and Megolm\\nhandshakes are shown in Figures 1 and 2, respectively.\\nC. Security of Matrix Cryptographic Suite\\nSince Olm and Megolm have similar security goals as\\nSignal and Sender Keys, we discuss the security of all four\\nprotocols.\\nSignal . The Signal protocol has seen ample formal analysis\\nsince its inception in 2013, including both symbolic [63] [6 2]\\n[27] [31] and computational [13] [39] analysis. These anal-\\nyses demonstrated the post-compromise and forward secrecy\\nguarantees of the double ratchet scheme and the authenticat ion\\nand security guarantees of the X3DH handshake in the man-\\nual, computational setting [39] and the symbolic, mechaniz ed\\nsetting [62] [12].\\nSender Keys . Sender Keys, in comparison with Signal,\\nis relatively understudied, as there only exists a single co m-\\nputational analysis [17] and, to the best of our knowledge, n o\\nmechanical nor symbolic analysis. Balbas et al. formally de Ô¨Åne\\na ‚ÄùGroup Messenger‚Äù primitive and the Sender Keys protocol,\\nthen they prove the security of the Sender Keys protocol with\\nrespect to their custom primitive. In the post-compromise s ecu-\\nrity case, Balbas et al. observe leaking the sender key can re sult\\nan outside attacker being able to forge a message. To prevent\\nthis, the authors propose Sender Keys+ which, among other\\nnon-cryptographic improvements, proposes MACing message\\ncontents with a hash of the chain key (which prevents leakedmessage keys from exposing the corresponding messages), an d\\nratcheting signature keys in addition to ratchet keys (whic h\\nprevents unauthorized injections in the case the identity k ey is\\ncompromised). The authors prove the security of Sender Keys+\\nby hand.\\nOlm and Megolm . There has been just a single com-\\nputational formal analysis of Olm and Megolm [11] and,\\nto the best of our knowledge, no mechanical nor symbolic\\nanalysis. Albrecht et al. formally deÔ¨Åne a custom primitive ,\\nDevice-Oriented Group Messaging (DOGM) , and prove the\\nsecurity and authentication of the composition of Olm and\\nMegolm with respect to it by hand. The authors also describe\\nsome trivial attacker wins with respect to their threat mode l;\\ncompromising the long-term Megolm session compromises\\nthe messages, compromising a Megolm identity key allows\\nthe initialization of forged Olm sessions, compromising a\\nMegolm identity allows the adversary to forge messages,\\nand compromising a Megolm identity allows an adversary to\\ninitiate malicious epochs.\\nD. Scope of Our Analysis\\nIn our analysis, we take a symbolic approach to reasoning\\nabout Olm and Megolm. Our analysis is in complement to the\\nprevious formal, computational analysis of Olm and Megolm\\n[11]. We also seek to mechanize the veriÔ¨Åcation of Olm\\nand Megolm using a computer-aided cryptographic analysis\\ntool, complementing the manual on-paper proofs provided in\\n[11]. Using mechanized proofs and computer-aided analysis\\nis particularly necessary for cryptographic protocols due to\\nnot just their importance, but the high level of rigor and\\nprecision required to correctly specify them [18]. Hand-wr itten\\nproofs are error-prone and often carry subtle mistakes that\\ngo unnoticed. This has consistently shown to be the case\\nfor both mathematical literature [23] [64] and distributed\\nprotocol literature [87]. Thus, we stress the importance of\\nmechanization for our analysis.\\nIII. O LM AND MEGOLM FORMAL MODELS\\nWe fully model the Matrix protocol suite as well as Sender\\nKeys using P ROVERIF , an automated symbolic analysis tool\\nin the Dolev-Yao model. In this section we overview the\\nPROVERIF automated symbolic reasoning tool, the subsequent\\nabstractions and limitations of our models, the details of o ur\\nmodels, and the properties of interest we seek to reason abou t.\\nA. Modeling in ProVerif\\nWe choose P ROVERIF as our symbolic protocol analysis\\ntool of choice, boasting an efÔ¨Åcient pi-calculus-based pro of\\nengine which reduces the term rewriting search problem that\\ncharacterizes symbolic cryptanalysis to horn clauses, as w ell\\nas an expressive syntax for specifying protocols [34].\\nTo model protocols in P ROVERIF , we begin by deÔ¨Åning\\neach participant as a process, representing the roles they\\nplay within the protocol. Each process consists of message\\nexchanges, including cryptographic operations such as enc ryp-\\ntion, decryption, signing, and veriÔ¨Åcation. Cryptographi c prim-\\nitives are modeled symbolically using predeÔ¨Åned functions\\nin P ROVERIF , which capture their idealized behavior without\\nconsidering their computational implementation.\\n4Alice\\nidenalong\\ngalong =GalongBob\\nidenblong\\ngblong=Gblong\\ngenerates bo\\ngbo=Gbo\\ngbosig = SIGN(blong,gbo ) gblong ,gbosig ,gbo\\noff-band veriÔ¨Åcation of gblong\\ngenerates ae1\\ngae1=Gae1\\namaster =HASH(gblongalong,gboalong,gblongae1)\\narkba1,ackba1= HKDF( amaster )\\ngenerates m1,ae2\\ngae2=Gae2\\nSIGNVERIF( gblong,gbs,gbssig )\\narkab1,ackab 1= HKDF( akshared 1,arkba 1)\\nakenc1=HKDF(MAC( ackab1))\\nx1=AEAD ENC(akenc1,m1,HASH(galong,gblong,gae 2))\\ngalong ,gae1,gae2,x1\\noff-band veriÔ¨Åcation of galong\\nbmaster =HASH(galongblong,galongbo,gaeblong\\n1)\\nbrkba1,bckba 1= HKDF( bmaster )\\nbkshared 1=gaebs\\n2\\nbrkab1,bckab 1=HKDF(bkshared 1,brkba 1)\\nbkenc1,bkenc 2=HKDF(MAC( bckab1))\\nmd1=AEAD DEC(bkenc1,x1,HASH(galong,gblong,gae 2))\\ngenerates m2,be\\ngbe=Gbe\\nbkshared 2=gaegb\\n2\\nbrkba2,bckba 2= HKDF( bkshared 2,brkab 1)\\nbkenc3=HKDF(MAC( bckba2))\\nx2=AEAD ENC(bkenc1,m2,HASH(gblong,galong,gbe ))gbe,x2\\nakshared 2=gbeae2\\narkba2,ackba 2=HKDF(akshared 2,arkab 1)\\nakenc3,bkenc 2=HKDF(MAC( ackba2))\\nmd2=AEAD DEC(akenc3,x2,HASH(gblong,galong,gbe ))\\naesa1,hmaca 1= HKDF( ra1)\\nakenc3,bkenc 2=HKDF(MAC( ackba2))\\nmd2=AEAD DEC(akenc3,x2,HASH(gblong,galong,gbe ))\\nFig. 1: Complete Olm Protocol including optional pre-key si gning, describing the initialization and transmission of t wo messages.\\nBrackets around a message in transit indicates off-band ver iÔ¨Åcation, where an attacker can observe but not modify the me ssage.\\n5Protocol Olm [52] Signal [39]\\nIdentity Key Curve25519 X25519\\nFingerprint Key Ed25519 N/A\\nPre-keys Curve25519 X25519\\nEphemeral Key N/A X25519\\nSigned Pre-keys (Optional) Curve25519 X25519\\nKey Exchange Triple DifÔ¨Åe-Hellman (3DH) Extended Triple DifÔ¨Åe-Hellman (X3DH)\\nRatcheting Double Ratchet Double Ratchet\\nTABLE I: Comparison of Olm and Signal\\nProtocolP2P Encryption\\nProtocolMessage\\nEncryptionRatchet\\nMechanismSignature Key\\nMegolm [51] Olm AES-256 (CBC) Custom ED25519\\nWhatsApp [85] Signal AES-256 (CBC) HMAC-SHA-256 Curve25519\\nSignal Sender Keys [76] Signal AES-256 (CBC) HMAC-SHA-256 Curve25519\\nSession Application [75] Signal AES-256 (CBC) HMAC-SHA-256 ED25519\\nFacebook Messenger [50] Signal AES-256 (CBC) HMAC-SHA-256 Curve25519\\nTABLE II: Comparison of Sender Keys implementations and Meg olm\\nThe adversary is explicitly modeled with the ability to\\nintercept, modify, and inject messages into the communicat ion\\nchannel. However, they are restricted by the symbolic secur ity\\nassumptions: they cannot invert cryptographic functions u nless\\nexplicitly allowed by the model. These symbolic constraint s\\nallow us to analyze potential attacks, such as replay attack s,\\nman-in-the-middle attacks, or key compromise impersonati on,\\nwithin the Dolev-Yao framework.\\nIn addition to the processes and messages, we specify the\\nsecurity properties of interest using queries, which P ROVERIF\\nattempts to verify or refute. Typical properties include au then-\\ntication, secrecy, and integrity. These properties are for mally\\nexpressed in terms of reachability, correspondence assert ions,\\nor observational equivalence, depending on the speciÔ¨Åc sec u-\\nrity goals of the protocol under analysis.\\nBy following this structured modeling methodology, we\\nensure that the symbolic model accurately reÔ¨Çects the inten ded\\nbehavior of the protocol, while accounting for the adversar ial\\ncapabilities. This allows for the comprehensive veriÔ¨Åcati on of\\nthe security properties under consideration. Additionall y, in\\norder to ensure P ROVERIF is always able to complete a proof\\n(or discover a counterexample), we ran our experiments on a\\nresearch computing cluster with 88 cascade lake CPUs and\\n5TB of RAM.\\nB. Model Details\\nTo construct our P ROVERIF models, we carefully and\\nexhaustively analyzed the existing speciÔ¨Åcations of Olm [5 2],\\nMegolm [51], and Sender Keys [50] [85], the canonical im-\\nplementations of Olm and Megolm provided by the Matrix\\nfoundation [55] [54], the Matrix protocol speciÔ¨Åcation [2] , and\\nmuch of the relevant online discussion [77]. We also closely\\ncompared our symbolic models with the existing computa-\\ntional constructions of Olm, Megolm, and Sender Keys [11]\\n[17] to best ensure our models accurately capture the intend ed\\nbehaviors of the protocols. Additionally, although not cen tralto our analysis, we construct V ERIFPAL [63] models of of\\nOlm, Megolm, and Sender Keys for the general beneÔ¨Åt of the\\ncommunity.\\nOlm . We model the complete cryptographic handshake and\\nthe ratcheting mechanism of Olm. A diagram of the modeled\\nhandshake is shown in Appendix Section 1.\\nMegolm . We model the initialization of a Megolm session,\\nthe transmission of the ratchet key to peers in the Megolm\\nsession via Olm, the exchange of messages between peers\\nin a Megolm session, and the incrementation of the Megolm\\nratchet. A diagram of the modeled cryptographic exchange is\\nshown in Figure 2.\\nSignal . We employ a previously created Signal model, as\\npresented in [62] and visualized in .\\nSender Keys . We model the initialization of a Sender Keys\\nsession, the transmission of the ratchet key to peers in the\\nSender Keys session via Signal, the exchange of messages\\nbetween peers in a Sender Keys session, and the incremen-\\ntation of the Sender Keys ratchet. A diagram of the modeled\\ncryptographic exchange is shown in Figure 2.\\nC. Abstractions and Limitations\\nOur models are fully faithful to the Olm, Megolm, and\\nSender Keys speciÔ¨Åcations, modulo the following abstracti ons\\nand limitations.\\n‚Ä¢Perfect Underlying Primitives . As in the symbolic\\ncryptanalysis model, we assume the underlying cryptograph ic\\nprimitives are black-boxes . That is, we assume the primitives\\nused in our models are secure and correct, particularly HKDF ,\\nHMAC, AEAD, and key signing. P ROVERIF and other cryp-\\ntographic reasoning in tools in the symbolic model traditio nally\\nassume perfect computational cryptography.\\n‚Ä¢Abstract Message Terms . Our models are abstract with\\nrespect to message formats, treating each message as a sym-\\nbolic constructor. This abstract treatment of protocol mes sages\\n6Alice\\nsender key ska\\ngska=GskaBob\\nsender key skb\\ngskb=Gskb\\nEstablishment of shared secret okvia secure p2p channel\\ngenerates ra1\\nxska=ENC(ok1,gska )\\nxra1=ENC(ok1,ra1)generates rb1\\nxskb=ENC(ok1,gskb )\\nxrb1=ENC(ok1,gskb )\\ngenerates ma1\\naesa1,hmaca1,aesda1= HKDF( ra1)\\nxma1=AEAD ENC(aesa1,ma1,aesad1)\\nxmasig 1=SIGN(ska,xma1)xska,xra 1,xma 1,xmasig 1\\narsk=DEC(ok,xska )\\nrar1=DEC(ok,xra 1)\\nSIGNVERIF( arsk,xma 1,xmasig 1)\\naesab1,hmacab1,aesdab1=HKDF(rar1)\\nmd1= AEAD DEC(aesab1,xma1,aesdbab1)\\ngenerates mb1\\naesb1,hmacb1,aesdb1= HKDF( rb1)\\nxmb1=AEAD ENC(aesb1,mb1,aesdb1)\\nxmbsig 1=SIGN(skb,xmb1)xskb,xrb 1,xmb1,xmbsig 1\\nbrsk=DEC(ok,xska )\\nrbr1=DEC(ok,xra 1)\\nSIGNVERIF( brsk,xmb 1,xmbsig 1)\\naesba1,hmacba1,aesbad1=HKDF(rbr1)\\nmd2= AEAD DEC(aesba1,xmb1,aesdba1)\\nFig. 2: Complete Megolm/Sender Keys protocol, communicati ng 3 messages between Alice and Bob. Note, the constructions of\\nMegolm and Sender Keys are the same besides (1) different rat cheting algorithms, and (2) different secure P2P channel pr otocols.\\nis typical of symbolic tools.\\n‚Ä¢No Ratchet Reseeding . The Megolm ratchet is designed to\\nre-seed after every 224iterations as described in the Appendix\\nVII-A. We do not capture this reseeding in our model ‚Äî only\\nforward iterations ‚Äî as it is not relevant to the analysis und er\\nthe Dolev-Yao model.\\n‚Ä¢Finite Peers . We reason about a Ô¨Ånite number of peers of\\na Megolm or Sender Keys group in our model, and prove our\\nproperties of interest under that assumption.\\n‚Ä¢Perfect Message Ordering . We do not reason about mes-\\nsage orderings in Megolm nor Sender Keys, and assume all\\nmessages arrive in proper order.\\nD. Properties of Interest\\nMotivated by the Olm, Signal, Megolm, and Sender Keys\\nspeciÔ¨Åcations and previous literature, we are concerned wi th\\nthe following properties:‚Ä¢Olm Message ConÔ¨Ådentiality and Authentication . All\\nmessages transmitted by Olm, assuming pre-keys are signed,\\nshould be conÔ¨Ådential and authenticated, given an attacker with\\nfull control over the communication channel. This property is\\nexplicitly derived from the Olm speciÔ¨Åcation [52] and was\\nadditionally proven with pen and paper in [11].\\n‚Ä¢Olm Session Key ConÔ¨Ådentiality . Each Olm session\\nkey remains conÔ¨Ådential, given an attacker with full contro l\\nover the communication channel. This property was explicit ly\\nderived from the Olm speciÔ¨Åcation [52].\\n‚Ä¢Olm Forward Secrecy . Upon leaking Olm identity keys and\\nOlm session keys and assuming Olm chooses to sign its pre-\\nkey, messages from previous sessions should remain secure\\ngiven an attacker with full control over the communication\\nchannel. This property is explicitly derived from the Olm\\nspeciÔ¨Åcation [52] and was additionally proven with pen and\\npaper in [11].\\n7‚Ä¢Olm Post-Compromise Security . Upon leaking Olm\\nidentity keys, all future messages should remain secure, gi ven\\nan attacker with full control over the communication channe l.\\nThis property is explicitly derived from the Olm speciÔ¨Åcati on\\n[52].\\n‚Ä¢Megolm Message ConÔ¨Ådentiality and Authentication .\\nAll messages transmitted by Megolm should be conÔ¨Ådential\\nand authenticated, given an attacker with full control over the\\ncommunication channel. This property was explicitly deriv ed\\nfrom the Megolm speciÔ¨Åcation [51] and was additionally\\nproven with pen and paper in [11].\\n‚Ä¢Megolm Forward Secrecy . Upon leaking the ratchet key\\n, all previous messages encrypted by symmetric keys derived\\nfrom previous ratchets should remain secure. This property\\nwas explicitly derived from the Megolm speciÔ¨Åcation [51].\\nIV. A NALYSIS\\nIn this section we present our mechanized results, includin g\\nthe proofs of the properties of interest mentioned in the\\nprevious section III-D, various proofs and attacks from the\\nprotocol speciÔ¨Åcations and previous literature, and the re sults\\nof our comparative analysis between Megolm and Sender\\nKeys.\\nA. Proven Properties of Interest\\nUsing our P ROVERIF models, we automatically prove\\nthe Olm and Megolm properties as stated and motivated in\\nsection III-D. This includes Olm message conÔ¨Ådentiality an d\\nauthentication, Olm session key conÔ¨Ådentiality, Olm forwa rd\\nsecrecy (assuming Olm chooses to sign pre-keys), Olm post-\\ncompromise security, Megolm message conÔ¨Ådentiality and\\nauthentication, and Megolm forward secrecy.\\nB. Mechanization of Manual Proofs and Attacks\\nUsing our V ERIFPAL models, we mechanize various\\nproofs, attacks, and limitations as described in the litera ture\\nand documentation. This allows us to extract precise, veriÔ¨Å able\\nattack sequences, which we narrate over the rest of this sect ion.\\n1) Compromising a Megolm session compromises all trans-\\nmitted messages via the Megolm session: As shown on pen and\\npaper in the computational formulation of Matrix as a trivia l\\nattacker win, compromising a Megolm session compromises\\nthe ratchet key contained within it, allowing an attacker to\\ndecrypt all messages sent with the Megolm session after the\\ncompromise [11].\\n2) Unknown Key-share in Olm: Olm, like Signal, is reliant\\non the off-band veriÔ¨Åcation of the pairwise identity keys. I f this\\nveriÔ¨Åcation does not occur, Olm is vulnerable to an unknown\\nkey-share attack, as described in the NCC audit of Olm and\\nMegolm [58]. This attack works as follows. Alice initiates a n\\nOlm session with Bob. Bob tries to send his identity and his\\n(signed or unsigned) pre-key to Alice (through the server), but\\nMallory replaces Bob‚Äôs identity key and pre-key, signed or\\nunsigned accordingly, with her own. Alice computes the Olm\\npairwise master key with the malicious values, uses the mast er\\nkey to derive a ratchet key, derives a symmetric key from\\nthe ratchet key, and encrypts a message. Alice then transmit s\\nher identity key and pre-key to Bob, which are observed byMallory. Then, using Alice‚Äôs identity and pre-key, Mallory\\ncomputes the pairwise, master key, derives the ratchet key,\\nderives the message key, and decrypts the message.\\n3) Unknown Key-share in Megolm: At a high level, this\\nattack involves the propagation of the Olm unknown key-share\\nattack through Megolm. That is, an unknown key-share attack\\nin an Olm session will compromise the Megolm session. This\\nattack works as follows. Alice and Bob intend to initialize a\\nMegolm connection. Alice initializes a Olm session with Bob ;\\nhowever, the Olm session, particularly the pairwise master\\nkey, is compromised via an unknown key-share attack as\\ndescribed in IV-B2. Alice then generates her Megolm session\\ninformation, including her sender key pair and her ratchet\\nkey. Using the compromised Olm master key, Alice derives\\na ratchet key, and a message key from the ratchet key. Alice\\nencrypts her Megolm session information using the message\\nkey and sends it to Bob. Mallory captures the encrypted Alice ‚Äôs\\nMegolm session, derives the Olm message key from the Olm\\nmaster key she captured, and decrypts Alice‚Äôs Megolm sessio n.\\nNow, with access to the Megolm ratchet key in Alice‚Äôs Megolm\\nsession, Mallory can decrypt all the messages Alice sends to\\nBob via the Megolm session.\\n4) Compromising an Olm identity key without pre-key sign-\\ning compromises the messages of an Olm exchange before\\nthe Ô¨Årst ratchet key advancement: As stated in the Olm\\nspeciÔ¨Åcation, choosing whether to sign pre-keys or not is le ft to\\nthe protocol implementer [52]. The Olm authors cite a tradeo ff\\nbetween forward secrecy and deniability [52]. As shown in th e\\ncomputational formulation of Megolm [11], we verify leavin g\\npre-keys unsigned does indeed compromise forward secrecy.\\nThe attack works as follows. Alice initiates an Olm session\\nwith Bob. Bob tries to send his identity key and his unsigned\\npre-key to Alice (through the server), but Mallory replaces\\nBob‚Äôs pre-key with her own. Alice, using her identity key\\nand pre-key, computes the pairwise Olm session key with\\nBob‚Äôs identity key and Mallory‚Äôs pre-key. Alice uses the Olm\\npairwise master key to derive a ratchet key, derive a symmetr ic\\nkey from the ratchet key, then encrypts a message with it.\\nAlice transmits her identity key and pre-key to Bob, which\\nMallory captures but does not modify in transit. Then, Bob‚Äôs\\nprivate key gets leaked to Mallory. Mallory then derives the\\npairwise Olm master key by performing DifÔ¨Åe-Hellman with\\n(1) Alice‚Äôs public key and Bob‚Äôs private key, (2) Alice‚Äôs pub lic\\nkey and Mallory‚Äôs injected pre-key, and (3) Alice‚Äôs transmi tted\\npublic pre-key and Bob‚Äôs private key. Using the pairwise Olm\\nmaster key, Mallory derives the message key and decrypts the\\nmessage Alice sent to Bob. We do observe, however, that the\\nOlm session heals after Alice and Bob advance their ratchet\\nkey, exchanging keys for DifÔ¨Åe-Hellman to do so. Thus, this\\nattack only compromises the Ô¨Årst (or Ô¨Årst few) messages. A\\nvisualization of this attacker sequence is given in Figure 3 .\\n5) Compromising an Olm identity key with pre-key signing\\npreserves post-compromise security: We prove this property\\nwith respect to an active and passive attacker. Intuitively , pre-\\nkey signing prevents an attacker from replacing the pre-key s\\nwith malicious ones; thus, the future compromise of an ident ity\\nkey will not compromise the messages of the associated Olm\\nsession. Again note, this is the default behavior of the Sign al\\nprotocol, as was proven in [63] [39] [12].\\n8Alice\\nidenalong\\ngalong =GalongAttackerBob\\nidenblong\\ngblong=Gblong\\ngenerates bo\\ngbo=Gbo\\ngblong ,gbo\\nreplacesgbowithgbm\\ngblong ,gbm\\noff-band veriÔ¨Åcation of gblong\\ngenerates ae1\\ngae1=Gae1\\namaster =HASH(gblongalong,gbmalong,gblongae1)\\narkba1,ackba1= HKDF( amaster )\\ngenerates m1,ae2\\ngae2=Gae2\\narkab1,ackab 1= HKDF( akshared 1,arkba 1)\\nakenc1=HKDF(MAC( ackab1))\\nx1=AEAD ENC(akenc1,m1,HASH(galong,gblong,gae 2))\\ngalong ,gae1,gae2,x1 galong ,gae1,gae2,x1\\noff-band veriÔ¨Åcation of galong\\nbmaster =HASH(galongblong,galongbo,gaeblong\\n1)\\nbrkba1,bckba 1= HKDF( bmaster )\\nbkshared 1=gaebs\\n2\\nbrkab1,bckab 1=HKDF(bkshared 1,brkba 1)\\nbkenc1,bkenc 2=HKDF(MAC( bckab1))\\nmd1=AEAD DEC(bkenc1,x1,HASH(galong,gblong,gae 2))\\nLeaksblong\\nbmaster = HASH( blonggalong,gblonggbm,gaeblong\\n1)\\nbrkba1,bckba1= HKDF( bmaster )\\nbcmba 1= HKDF(MAC( bckba1))\\nm1=AEAD DEC(x1,bcmba 1, HASH(galong,gblong,gae 2))\\nObtainsm1\\nFig. 3: Post-compromise attacker sequence on unsigned Olm k ey exchange, as described in IV-B4\\n6) Megolm & Sender Keys message injection after sender\\nkey compromise: As pointed out in the computational for-\\nmulation of Sender Keys, compromising a sender key pair\\nallows the attacker to inject messages that correspond to ke y\\nmaterial from the respective Megolm session that were used\\nbefore the sender key was compromised [17]. We verify this\\nis indeed the case. The attack works as follows. Alice secure lyinitiates an Olm session with Bob. Alice generates her Megol m\\nsession containing her ratchet key and public sender key, th en\\nsends it to Bob via Olm. Alice sends a few messages to Bob,\\nadvancing the ratchet key as such. Alice‚Äôs sender key is then\\ncompromised by Mallory. At this point, Mallory can inject a\\nmessage corresponding to previous key material used before\\nthe sender key was compromised, signing the message with\\n9the current sender key. Note, Megolm peers will store a copy\\nof the earliest known ratchet value [51], thus this injectio n is\\nfeasible.\\n7) The Sender Keys extension proposed by Balbas et al.\\n[17] maintains authenticity and conÔ¨Ådentiality: Among other\\ncontrol-related changes not relevant to analysis under Dol ev-\\nYao, the Sender Keys extension as proposed by Balbas et al.\\nproposes ratcheting the sender key with a hash algorithm in\\norder to maintain authentication forward secrecy and using\\nMAC as an alternative to AEAD when computing the chain\\nkey [17]. They prove their extension satisÔ¨Åes authenticati on\\nand conÔ¨Ådentiality. We replicate this proof with respect to an\\nactive and passive attacker.\\nC. Megolm vs. Sender Keys Post-Compromise Security\\nMost notably in our analysis, we are the Ô¨Årst to observe\\nand mechanically prove that the post-compromise security o f\\nMegolm is explicitly weaker than in Sender Keys due to the\\ndifferences in the forward secrecy guarantees of the respec tive\\npeer-to-peer secure channels. However, this is the only maj or\\ndifference in symbolic security we discover in our analysis .\\n1) Olm identity key compromise without pre-key signing\\ncompromises the reliant Megolm session: Critically, we ob-\\nserve that Olm only transmits Megolm session data [52].\\nSo, the compromise of any Olm messages will compromise\\nMegolm session data. Similarly to how the unknown key-share\\nattack on Megolm is caused by the unknown key-share attack\\nin Olm, the leakage of Olm messages via the attack described\\nin IV-B4 is sufÔ¨Åcient to leak Megolm session data.\\nThe attack works as follows. Alice and Bob intend to\\ninitialize a Megolm connection. Alice initiates an Olm sess ion\\nwith Bob. Bob tries to send his identity key and his unsigned\\npre-key to Alice (through the server), but Mallory replaces\\nBob‚Äôs pre-key with her own. Alice, using her identity key\\nand pre-key, computes the pairwise Olm session key with\\nBob‚Äôs identity key and Mallory‚Äôs pre-key. Alice uses the Olm\\npairwise master key to derive a ratchet key, derive a symmetr ic\\nkey from the ratchet key, then encrypts a message with it. Ali ce\\ntransmits her identity key and pre-key to Bob, which Mallory\\ncaptures but does not modify in transit. Alice then generate s\\nher Megolm session information, including her sender key pa ir\\nand her ratchet key. Using the Olm pairwise master key, Alice\\nderives an Olm ratchet key, and an Olm message key from the\\nratchet key. Alice encrypts her Megolm session information\\nusing the Olm message key and sends it to Bob. Mallory\\ncaptures the encrypted Megolm session. Alice sends Megolm\\nmessages to Bob via her Megolm session as normal. Then,\\nBob‚Äôs private key gets leaked to Mallory. Mallory then deriv es\\nthe pairwise Olm master key from doing DifÔ¨Åe-Hellman with\\n(1) Alice‚Äôs public key and Bob‚Äôs private key, (2) Alice‚Äôs pub lic\\nkey and Mallory‚Äôs injected pre-key, and (3) Alice‚Äôs transmi tted\\npublic pre-key and Bob‚Äôs private key. Using the Olm pairwise\\nmaster key, Mallory derives the Olm message key and decrypts\\nthe Megolm session message Alice sent to Bob. Using the\\ndecrypted Megolm session, Mallory can now read the previous\\nmessages Alice sent to Bob (as Mallory recorded them), and\\nall the future Alice sends to Bob via the Megolm session. A\\nvisualization of this attacker sequence is given in Figure 4 .2) Signal identity key compromise does not compromise\\nthe reliant Sender Keys session: Because Signal maintains\\nforward secrecy, the Sender Keys protocol (which employs\\nSignal over Olm as the P2P channel) is notvulnerable to the\\naforementioned attack. We prove this for the active and pass ive\\nattacker case.\\nD. Impact of Megolm post-compromise security limitations\\nIn brief, the attack described in IV-C1 implies compromis-\\ning an Olm identity key of a peer that chooses to not sign\\nits pre-keys allows the attacker to compromise all Megolm\\nsessions of peers that initiate Olm sessions with said peer.\\nTo understand the impact of this limitation, we need to\\nprecisely understand how Olm sessions are handled in respec t\\nto Megolm sessions. We observe this detail is not properly\\nspeciÔ¨Åed in the Olm and Megolm speciÔ¨Åcations [51] [52]. We\\nconÔ¨Årmed two key details with the Matrix cryptography team\\nregarding how Olm works in respect to Megolm. First, Olm\\nidentity keys are similar to Signal identity keys, i.e they l ast\\nas long as the device the user is logged in on does. Second,\\na single Olm identity key is used for the connections with\\nmultiple different Olm peers.\\nThus, based on this detail, we can draw larger conclusions\\nof how the compromise of an Olm identity key (in the case\\nOlm pre-keys aren‚Äôt signed) impacts Megolm over multiple\\nepochs.\\n‚Ä¢Only the Ô¨Årst Megolm session sent to a peer with a\\ncompromised Olm identity key will be compromised by an\\nactive attacker . Because Olm ratchets forward (and thus is\\nself-healing), we know only the Ô¨Årst Megolm session sent\\nto a peer whose Olm identity key is compromised will be\\ncompromisable by an active attacker.\\n‚Ä¢Any peer that initiates a Megolm connection with a\\ncompromised peer could have their Ô¨Årst Megolm session\\ncompromised by an active attacker . This is due to Olm\\nidentity keys being shared among multiple Olm sessions.\\n‚Ä¢Newly joining Megolm peers could have their Ô¨Årst Megolm\\nsession compromised by an active attacker; Megolm peers\\nalready in the group will not have their Megolm sessions\\ncompromised in future Epochs . When a user joins a Megolm\\nroom, they must initiate Olm sessions with all Megolm\\npeers, potentially including a peer whose Olm identity key\\nis compromised. However, because Olm is self-healing, we\\nsee new users will only have the messages of their Ô¨Årst\\nMegolm epoch compromised. Informally, we observe Olm‚Äôs\\nself-healing property propagates to Megolm in a way.\\nRegarding the propagation of the aforementioned limita-\\ntions to the Matrix protocol, we conÔ¨Årm the overarching Matr ix\\nprotocol remains unaffected due to mandating the signing of\\nOlm pre-keys [68].\\nV. R ELATED WORK\\nIn this section we review broader related work other than\\nthe Matrix-related work which we reviewed in Section II-C.\\n10Alice\\nidenalong\\ngalong =GalongAttackerBob\\nidenblong\\ngblong=Gblong\\ngenerates bo\\ngbo=Gbo\\ngblong ,gbo\\nreplacesgbowithgbm\\ngblong ,gbm\\noff-band veriÔ¨Åcation of gblong\\ngenerates ae1,ao\\ngae1=Gae1\\ngao=Gao\\namaster =HASH(gblongalong,gbmalong,gblongae1)\\narkba1,ackba1= HKDF( amaster )\\nacmba 1= HKDF(MAC( ackba1))\\n// Megolm session\\nsender key ska\\ngska =Gska\\ngenerates ra1,ta1\\ngta1=Gta1\\nxska = ENC(acmba 1,gska )\\nxra1= ENC(acmba 1,ra1)\\ngenerates ma1\\naesa1,hmaca1,aesda1= HKDF( ra1)\\nxma1=AEAD ENC(aesa1,ma1,aesad1)\\nxmasig 1=SIGN(ska,xma1)\\ngalong,gao galong,gao\\noff-band veriÔ¨Åcation of galong\\nxska,xra 1,xma 1,xmasig 1,gta1xska,xra 1,xma 1,xmasig 1,gta1\\nComputes Olm, Megolm session info\\nDecrypts ma1\\n. . . Continues Megolm interactions\\nLeaksblong\\nbmaster = HASH( blonggalong,gblonggbm,gaeblong\\n1)\\nbrkba1,bckba1= HKDF( bmaster )\\nbcmba 1= HKDF(MAC( bckba1))\\nska = ENC(bcmba 1,xska )\\nra1= ENC(bcmba 1,xra1)\\naesb1,hmacb1,aesdb1= HKDF( ra1)\\nma1=AEAD DEC(ma1,besa1,besad1)\\nObtainsra1,ska,ma 1\\nFig. 4: Post-compromise attacker sequence on unsigned Olm & Megolm composition, as described in IV-C1\\n11A. Computer-Aided Protocol VeriÔ¨Åcation\\nSymbolic Cryptographic Analysis . Most closely related\\nto V ERIFPAL , many other automated symbolic protocol analy-\\nsis tools exist. Most prevalent are T AMARIN , another symbolic\\ncryptanalysis tool [20] and P ROVERIF [33]. T AMARIN has\\nbeen used to analyze Apple iMessage [15], TLS 1.3 [42],\\nthe EMV standard [22], and The Noise protocol suite [57].\\nSimilarly P ROVERIF has been used to analyze Signal [62]\\nand TLS 1.3 [28]. Other symbolic cryptanalysis tools such as\\nSCYTHER [44] and AVISPA [84] have also been historically\\nused for this purpose.\\nIn comparison, manual symbolic cryptanalysis has been\\ntheoretically feasible [29] but seldom employed until rece nt\\nadvances in proof-oriented programming languages. In part icu-\\nlar, F*, a general-purpose proof-oriented language with su pport\\nfor dependent types proving, reÔ¨Ånement times, executable\\nsemantics extractions, and veriÔ¨Åed implementation extrac tions\\nto F#, C, or Wasm [81], has seen usage for this purpose.\\nAdditionally, other dependent types-based theorem prover s\\nsuch as Rocq [24] and Agda [69] are suited for this purpose,\\nas they both offer executable semantics and implementation\\nextractions1. The theoretical framework for verifying protocols\\nsymbolically using dependent-types introduced in [29] has\\nbeen formalized in F* and was used to analyze Signal [27]\\nand later the Message Layer Security protocol [26].2However,\\nconstructing such formalizations in a dependent types prov er\\nsuch as F* is generally much more time consuming than\\nconstructing models in a symbolic cryptanalysis tool.\\nComputational Cryptographic Analysis . The most preva-\\nlent automated computational cryptanalysis tool is C RYP-\\nTOVERIF , which has been used to verify Signal [62], SSH [35],\\nTLS 1.3 [28], and WireGuard [66]. E ASYCRYPT , alternatively,\\nfeatures manual tactic-based reasoning with automated met h-\\nods for simple lemmas [70], and has been used to verify zero-\\nknowledge protocols [53] and TLS [30]. In general, compu-\\ntational cryptographic analysis tools make assumptions ab out\\nthe complexity of cryptographic primitives and reason abou t\\ncryptosystems accordingly. We observe that there exists no\\nconstructive mechanization of the explicit computational com-\\nplexity of cryptographic primitives. In fact, in stark cont rast to\\nother areas of math, complexity theory has seen virtually no\\nmechanization in dependent types-based theorem provers [5 9]\\n[9].\\nB. Group Messaging Protocols With Key Agreement\\nWhile the Megolm and Sender Keys protocols choose\\nto approach group messaging without key agreement, there\\nexists a large body of work analyzing protocols with group\\nkey agreement. Group key agreement protocols have a long\\nhistory of formal [72], [79], [60] and empirical [14] analys is.\\nRecently, asynchronous ratcheting trees have been introdu ced\\n1Note, Rocq and Agda have different type theories: Rocq is bas ed on\\nthe Calculus of Constructions (CoC) while Agda is based on Ma rtin-L¬® of\\nType Theory. This difference results in different approach es to constructing\\nproofs. It is entirely unexplored how this difference affec ts the formulation of\\ncryptographic protocols via the methodology described in [ 29].\\n2Note that F* and dependent-types theorem provers are incred ibly general\\nand are expressive enough to capture all of mathematics. Int erestingly, F* has\\nbeen additionally used to construct veriÔ¨Åed cryptographic libraries [89] and\\nto reason about group key agreement [25].as an effective method to achieve post-compromise security\\n[40], forming the basis for the development of Message\\nLayer Security [19]. Group messaging protocols based on\\nasynchronous ratchet trees have been analyzed and veriÔ¨Åed\\nwith T AMARIN [43] and F* [26].\\nAt the time of writing, the Matrix foundation intends\\nto phase out Olm and Megolm in favor of Message Layer\\nSecurity [8] [1] but has yet to do so. In addition to better pos t-\\ncompromise security guarantees [40], Message Layer Securi ty\\nhas been shown to be more performant than Olm and Megolm\\nin practice [37].\\nVI. C ONCLUSION\\nThis paper has presented a comprehensive formal, symbolic\\nanalysis of the Olm and Megolm protocols within the Ma-\\ntrix cryptographic suite. Utilizing P ROVERIF , we constructed\\nprecise models and conducted automated symbolic analysis\\nto verify key security properties, including conÔ¨Ådentiali ty,\\nauthentication, forward secrecy, and post-compromise sec urity\\nfor Olm and Megolm. Our approach enabled us to precisely\\nand automatically reproduce known vulnerabilities and att acks.\\nIn addition, through our comparative analysis with the\\nSignal and Sender Keys protocols, we highlighted critical\\ndifferences in post-compromise security, emphasizing the need\\nfor ongoing scrutiny and improvement of secure messaging\\nprotocols. We endorse the Matrix protocol speciÔ¨Åcation‚Äôs c ur-\\nrent decision for Olm pre-keys to be signed, as our models\\nconÔ¨Årm that when Olm pre-keys are signed the security\\nguarantees of Olm and Megolm are comparable to Signal and\\nSender Keys.\\nAs for future work, we plan to round out our symbolic\\nanalysis of the Matrix cryptographic suite with a formaliza tion\\nin the dependent types-based symbolic analysis framework\\nDY*, which is implemented in F* [27]. This would allow us\\nto construct secrecy and authentication proofs that are par ame-\\nterized over the number of Olm ratchet iterations and Megolm\\nepoch iterations, as well as extract executable semantics t o\\nmore effectively reason about the implementations.\\nOverall, our Ô¨Åndings underscore the importance of formal,\\nmechanized proofs in ensuring the robustness of cryptograp hic\\nsystems, particularly in the context of federated and decen tral-\\nized communication platforms like Matrix. We hope our work\\nmotivates further advancement of cryptographic veriÔ¨Åcati on\\ntooling and the continued analysis and scrutiny of the proto cols\\nof which we all rely upon.\\nREFERENCES\\n[1] [Online]. Available: https://matrix.org/blog/2023/ 07/a-giant-leap-with-mls/\\n[2] ‚ÄúMatrix SpeciÔ¨Åcation ‚Äî spec.matrix.org,‚Äù https://spe c.matrix.org/, [Ac-\\ncessed 22-04-2024].\\n[3] ‚ÄúThe X3DH key agreement protocol,‚Äù accessible at:\\nhttps://signal.org/docs/specifications/x3dh/.\\n[4] ‚ÄúCVE-2021-34813,‚Äù https://cve.mitre.org/cgi-bin/c vename.cgi?name=CVE-2021-34813 \\n2021, accessed: 2024-07-08. [Online]. Available:\\nhttps://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE- 2021-34813\\n[5] ‚ÄúCVE-2021-40824,‚Äù https://cve.mitre.org/cgi-bin/c vename.cgi?name=CVE-2021-40824 \\n2021, accessed: 2024-07-08. [Online]. Available:\\nhttps://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE- 2021-40824\\n12[6] ‚ÄúCVE-2022-39248,‚Äù https://cve.mitre.org/cgi-bin/c vename.cgi?name=CVE-2022-39248,\\n2022, accessed: 2024-07-08. [Online]. Available:\\nhttps://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE- 2022-39248\\n[7] ‚ÄúCVE-2022-39251,‚Äù https://cve.mitre.org/cgi-bin/c vename.cgi?name=CVE-2022-39251,\\n2022, accessed: 2024-07-08. [Online]. Available:\\nhttps://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE- 2022-39251\\n[8] ‚ÄúAre we mls yet?‚Äù 2024, accessed: 2024-07-27. [Online]. Available:\\nhttps://arewemlsyet.com/\\n[9] ‚ÄúComputational complexity theory discus-\\nsion,‚Äù Online; accessed 7-July-2024, 2024,\\nhttps://leanprover.zulipchat.com/#narrow/stream/113 488-general/topic/Computational.20Complexity.20Theo ry/near/264319554.\\n[10] M. R. Albrecht, S. Celi, B. Dowling, and D. Jones, ‚ÄúPract ically-\\nexploitable cryptographic vulnerabilities in matrix,‚Äù in 2023 IEEE\\nSymposium on Security and Privacy (SP) . San Francisco,\\nCA, USA: IEEE, May 2023, p. 164‚Äì181. [Online]. Available:\\nhttps://ieeexplore.ieee.org/document/10351027/\\n[11] M. R. Albrecht, B. Dowling, and D. Jones, ‚ÄúDevice-orien ted group\\nmessaging: A formal cryptographic analysis of matrix‚Äô core .‚Äù\\n[12] J. Alwen, S. Coretti, and Y . Dodis, The Double Ratchet: Security\\nNotions, Proofs, and Modularization for the Signal Protoco l, ser.\\nLecture Notes in Computer Science. Cham: Springer Internat ional\\nPublishing, 2019, vol. 11476, p. 129‚Äì158. [Online]. Availa ble:\\nhttps://link.springer.com/10.1007/978-3-030-17653-2 5\\n[13] ‚Äî‚Äî, The Double Ratchet: Security Notions, Proofs, and\\nModularization for the Signal Protocol , ser. Lecture Notes\\nin Computer Science. Cham: Springer International Pub-\\nlishing, 2019, vol. 11476, p. 129‚Äì158. [Online]. Available :\\nhttps://link.springer.com/10.1007/978-3-030-17653-2 5\\n[14] Y . Amir, Y . Kim, and C. Nita-Rotaru, ‚ÄúOn the performance of group key\\nagreement protocols,‚Äù ACM Transactions on Information and System\\nSecurity , vol. 7, no. 3.\\n[15] Apple Engineering, ‚Äúimessage with pq3: The new state of the art in\\nquantum-secure messaging at scale,‚Äù Feb. 2024, accessed: 2 024-07-05.\\n[Online]. Available: https://security.apple.com/blog/ imessage-pq3/\\n[16] L. Authority, ‚Äúvodozemac - security audit report,‚Äù\\nhttps://matrix.org/media/Least%20Authority%20-%20Ma trix%20vodozemac%20Final%20Audit%20Report.pdf,\\n[Accessed 22-04-2024].\\n[17] D. Balb¬¥ as, D. Collins, and P. Gajland, WhatsUpp with\\nSender Keys? Analysis, Improvements and Security Proofs , ser.\\nLecture Notes in Computer Science. Singapore: Springer Nat ure\\nSingapore, 2023, vol. 14442, p. 307‚Äì341. [Online]. Availab le:\\nhttps://link.springer.com/10.1007/978-981-99-8733-7 10\\n[18] M. Barbosa, G. Barthe, K. Bhargavan, B. Blanchet,\\nC. Cremers, K. Liao, and B. Parno, ‚ÄúSok: Computer-aided\\ncryptography,‚Äù in 2021 IEEE Symposium on Security and\\nPrivacy (SP) , May 2021, p. 777‚Äì795. [Online]. Available:\\nhttps://ieeexplore.ieee.org/document/9519449/?arnum ber=9519449\\n[19] R. Barnes, B. Beurdouche, R. Robert, J. Millican, E. Oma ra,\\nand K. Cohn-Gordon, ‚ÄúThe Messaging Layer Security\\n(MLS) Protocol,‚Äù RFC 9420, Jul. 2023. [Online]. Available:\\nhttps://www.rfc-editor.org/info/rfc9420\\n[20] D. Basin, C. Cremers, J. Dreier, and R. Sasse, ‚ÄúTamarin: VeriÔ¨Åcation\\nof large-scale, real-world, cryptographic protocols,‚Äù IEEE Security and\\nPrivacy , vol. 20, no. 3, p. 24‚Äì32, May 2022.\\n[21] D. Basin, F. Linker, and R. Sasse, ‚ÄúA formal analysis of t he imessage\\npq3 messaging protocol.‚Äù\\n[22] D. Basin, R. Sasse, and J. Toro-Pozo, ‚ÄúThe emv standard: Break,\\nÔ¨Åx, verify,‚Äù no. arXiv:2006.08249, Feb. 2021, arXiv:2006. 08249 [cs].\\n[Online]. Available: http://arxiv.org/abs/2006.08249\\n[23] J. Bayer, C. Benzm¬® uller, K. Buzzard, M. David, L. Lampo rt, Y . Matiya-\\nsevich, L. Paulson, D. Schleicher, B. Stock, and E. Zelmanov , ‚ÄúMath-\\nematical proof between generations,‚Äù Notices of the American Mathe-\\nmatical Society , vol. 71, no. 01, p. 1, Jan. 2024, arXiv:2207.04779 [cs,\\nmath].\\n[24] Y . Bertot and P. Cast¬¥ eran, Interactive Theorem Proving and\\nProgram Development: Coq‚ÄôArt: The Calculus of Inductive\\nConstructions . Berlin, Heidelberg: Springer, 2004. [Online]. Available :\\nhttps://link.springer.com/book/10.1007/978-3-662-07 964-5\\n[25] B. Beurdouche, ‚ÄúFormal veriÔ¨Åcation for high assurance security soft-\\nware in fstar.‚Äù[26] K. Bhargavan, B. Beurdouche, and P. Naldurg, ‚ÄúFormal mo dels and\\nveriÔ¨Åed protocols for group messaging: Attacks and proofs f or ietf mls.‚Äù\\n[27] K. Bhargavan, A. Bichhawat, Q. H. Do, P. Hosseyni, R. K¬® u sters,\\nG. Schmitz, and T. W¬® urtele, ‚ÄúDy*: A modular symbolic veriÔ¨Åc ation\\nframework for executable cryptographic protocol code.‚Äù\\n[28] K. Bhargavan, B. Blanchet, and N. Kobeissi, ‚ÄúVeriÔ¨Åed mo dels\\nand reference implementations for the tls 1.3 standard cand idate,‚Äù\\nin2017 IEEE Symposium on Security and Privacy (SP) . San\\nJose, CA, USA: IEEE, May 2017, p. 483‚Äì502. [Online]. Availab le:\\nhttp://ieeexplore.ieee.org/document/7958594/\\n[29] K. Bhargavan, C. Fournet, and A. D. Gordon, ‚ÄúModular ver iÔ¨Åcation of\\nsecurity protocol code by typing.‚Äù\\n[30] K. Bhargavan, C. Fournet, M. Kohlweiss, A. Pironti, P.- Y . Strub, and\\nS. Zanella-B¬¥ eguelin, Proving the TLS Handshake Secure (As It Is) ,\\nser. Lecture Notes in Computer Science. Berlin, Heidelberg : Springer\\nBerlin Heidelberg, 2014, vol. 8617, p. 235‚Äì255. [Online]. A vailable:\\nhttp://link.springer.com/10.1007/978-3-662-44381-1 14\\n[31] K. Bhargavan, C. Jacomme, F. Kiefer, and R. Schmidt, ‚ÄúFo rmal veriÔ¨Åca-\\ntion of the pqxdh post-quantum key agreement protocol for en d-to-end\\nsecure messaging.‚Äù\\n[32] B. Blanchet, ‚ÄúSecurity protocol veriÔ¨Åcation: Symboli c and\\ncomputational models,‚Äù in Principles of Security and Trust ,\\nser. Lecture Notes in Computer Science, P. Degano and\\nJ. D. Guttman, Eds., vol. 7215. Berlin, Heidelberg:\\nSpringer Berlin Heidelberg, 2012, p. 3‚Äì29. [Online]. Avail able:\\nhttp://link.springer.com/10.1007/978-3-642-28641-4 2\\n[33] B. Blanchet et al. , ‚ÄúModeling and verifying security protocols with the\\napplied pi calculus and proverif,‚Äù Foundations and Trends¬Æ in Privacy\\nand Security , 2016.\\n[34] B. Blanchet, B. Smyth, V . Cheval, and M. Sylvestre, ‚ÄúPro verif 2.05:\\nAutomatic cryptographic protocol veriÔ¨Åer, user manual and tutorial.‚Äù\\n[35] D. Cade, B. Blanchet, and I. Paris-Rocquencourt, ‚ÄúFrom\\ncomputationally-proved protocol speciÔ¨Åcations to implem entations and\\napplication to ssh.‚Äù\\n[36] J. Callas, A. Johnston, and P. Zimmermann, ‚ÄúZRTP: Media Path Key\\nAgreement for Unicast Secure RTP,‚Äù RFC 6189, Apr. 2011. [Onl ine].\\nAvailable: https://www.rfc-editor.org/info/rfc6189\\n[37] H. Chathi, ‚ÄúMls comparison,‚Äù 2022, accessed: 2024-07- 27. [Online].\\nAvailable: https://gitlab.matrix.org/uhoreg/mls-comp arison\\n[38] T. M. F. C.I.C., ‚ÄúMatrix protocol,‚Äù https://matrix.or g/docs/spec/, 2019,\\naccessed: 2024-07-08. [Online]. Available: https://matr ix.org/docs/spec/\\n[39] K. Cohn-Gordon, C. Cremers, B. Dowling, L. Garratt, and D. Stebila,\\n‚ÄúA formal security analysis of the signal messaging protoco l.‚Äù\\n[40] K. Cohn-Gordon, C. Cremers, L. Garratt, J. Millican, an d K. Milner,\\n‚ÄúOn ends-to-ends encryption: Asynchronous group messagin g with\\nstrong security guarantees,‚Äù in Proceedings of the 2018 ACM\\nSIGSAC Conference on Computer and Communications Security .\\nToronto Canada: ACM, Oct. 2018, p. 1802‚Äì1819. [Online]. Ava ilable:\\nhttps://dl.acm.org/doi/10.1145/3243734.3243747\\n[41] K. Community, ‚ÄúKde is adding matrix to its im framework, ‚Äù\\nhttps://dot.kde.org/2019/02/20/kde-adding-matrix-it s-im-framework,\\n2019, accessed: 2024-07-08. [Online]. Available:\\nhttps://dot.kde.org/2019/02/20/kde-adding-matrix-it s-im-framework\\n[42] C. Cremers, M. Horvat, J. Hoyland, S. Scott, and T. Van De r Merwe,\\n‚ÄúA comprehensive symbolic analysis of tls 1.3,‚Äù in Proceedings of the\\n2017 ACM SIGSAC Conference on Computer and Communications\\nSecurity . Dallas Texas USA: ACM, Oct. 2017, p. 1773‚Äì1788.\\n[Online]. Available: https://dl.acm.org/doi/10.1145/3 133956.3134063\\n[43] C. Cremers, C. Jacomme, and P. Lukert, ‚ÄúSubterm-based p roof\\ntechniques for improving the automation and scope of securi ty\\nprotocol analysis,‚Äù in 2023 IEEE 36th Computer Security Foundations\\nSymposium (CSF) . Dubrovnik, Croatia: IEEE, Jul. 2023, p. 200‚Äì213.\\n[Online]. Available: https://ieeexplore.ieee.org/docu ment/10221880/\\n[44] C. J. F. Cremers, The Scyther Tool: VeriÔ¨Åcation, FalsiÔ¨Åcation,\\nand Analysis of Security Protocols , ser. Lecture Notes\\nin Computer Science. Berlin, Heidelberg: Springer Berlin\\nHeidelberg, 2008, vol. 5123, p. 414‚Äì418. [Online]. Availab le:\\nhttp://link.springer.com/10.1007/978-3-540-70545-1 38\\n[45] J. A. Donenfeld, ‚ÄúFormal veriÔ¨Åcation of the wireguard p rotocol.‚Äù\\n13[46] B. Dowling and K. G. Paterson, ‚ÄúA cryptographic analysi s of the\\nwireguard protocol.‚Äù\\n[47] M. Duke, ‚ÄúQUIC Version 2,‚Äù RFC 9369, May 2023. [Online].\\nAvailable: https://www.rfc-editor.org/info/rfc9369\\n[48] N. Durov, ‚ÄúMtproto mobile protocol,‚Äù https://core.te legram.org/mtproto,\\n2013, accessed: 2024-07-08. [Online]. Available:\\nhttps://core.telegram.org/mtproto\\n[49] Element, ‚ÄúTchap case study,‚Äù https://element.io/cas e-studies/tchap,\\n2023, accessed: 2024-07-08. [Online]. Available:\\nhttps://element.io/case-studies/tchap\\n[50] Facebook Engineering, ‚ÄúMessenger end-to-end encryption overview, ‚Äù\\nDecember 2023, accessed: 2024-07-03. [Online]. Available :\\nhttps://engineering.fb.com/wp-content/uploads/2023/ 12/MessengerEnd-to-EndEncryptionOverview 12-6-2023.pdf\\n[51] T. M. Federation, ‚Äúdocs/megolm.md ¬∑ master ¬∑\\nmatrix-org / Olm ¬∑ GitLab ‚Äî gitlab.matrix.org,‚Äù\\nhttps://gitlab.matrix.org/matrix-org/olm/-/blob/mas ter/docs/megolm.md,\\n[Accessed 22-04-2024].\\n[52] ‚Äî‚Äî, ‚ÄúOlm: A cryptographic ratchet,‚Äù\\nhttps://gitlab.matrix.org/matrix-org/olm, [Accessed 2 2-04-2024].\\n[53] D. Firsov and D. Unruh, ‚ÄúZero-knowledge in easycrypt,‚Äù in\\n2023 IEEE 36th Computer Security Foundations Symposium (CS F).\\nDubrovnik, Croatia: IEEE, Jul. 2023, p. 1‚Äì16. [Online]. Ava ilable:\\nhttps://ieeexplore.ieee.org/document/10221929/\\n[54] M. Foundation, ‚Äúlibolm,‚Äù https://gitlab.matrix.org /matrix-org/olm, 2024,\\naccessed: 2024-09-23.\\n[55] ‚Äî‚Äî, ‚Äúvodozemac,‚Äù https://gitlab.matrix.org/matrix -org/vodozemac,\\n2024, accessed: 2024-09-23.\\n[56] T. M. Foundation, ‚ÄúGermany‚Äôs na-\\ntional healthcare system adopts matrix,‚Äù\\nhttps://matrix.org/blog/2021/07/21/germany-s-nation al-healthcare-system-adopts-matrix/,\\n2021, accessed: 2024-07-08. [Online]. Available:\\nhttps://matrix.org/blog/2021/07/21/germany-s-nation al-healthcare-system-adopts-matrix/\\n[57] G. Girol, L. Hirschi, R. Sasse, and D. Jackson, ‚ÄúA spectr al analysis of\\nnoise: A comprehensive, automated, formal analysis of difÔ¨Å e-hellman\\nprotocols.‚Äù\\n[58] N. Group, ‚ÄúPublic report ‚Äì matrix olm cryptographic rev iew,‚Äù\\nhttps://research.nccgroup.com/2016/11/01/public-rep ort-matrix-olm-cryptographic-review/,\\n[Accessed 22-04-2024].\\n[59] L. G¬® aher and F. Kunze, ‚ÄúMechanising complexity theory : The cook-\\nlevin theorem in coq,‚Äù LIPIcs, Volume 193, ITP 2021 , vol. 193, pp.\\n20:1‚Äì20:18, 2021.\\n[60] Y . Kim, A. Perrig, and G. Tsudik, ‚ÄúTree-based group key a greement,‚Äù\\nACM Trans. Inf. Syst. Secur. , vol. 7, pp. 60‚Äì96, 2004. [Online].\\nAvailable: https://api.semanticscholar.org/CorpusID: 13187206\\n[61] N. Kobeissi and K. Bhargavan, ‚ÄúNoise explorer: Fully au tomated\\nmodeling and veriÔ¨Åcation for arbitrary noise protocols.‚Äù\\n[62] N. Kobeissi, K. Bhargavan, and B. Blanchet, ‚ÄúAutomated veriÔ¨Åcation\\nfor secure messaging protocols and their implementations: A symbolic\\nand computational approach,‚Äù in 2017 IEEE European Symposium on\\nSecurity and Privacy . Paris: IEEE, Apr. 2017, p. 435‚Äì450. [Online].\\nAvailable: https://ieeexplore.ieee.org/document/7961 995/\\n[63] N. Kobeissi, G. Nicolas, and M. Tiwari, ‚ÄúVerifpal: Cryp tographic\\nprotocol analysis for the real world.‚Äù\\n[64] L. Lamport, ‚ÄúHow to write a 21st century proof,‚Äù Journal of Fixed Point\\nTheory and Applications , vol. 11, no. 1, p. 43‚Äì63, Mar. 2012.\\n[65] B. Lipp, ‚ÄúA mechanised computational analysis of the wi reguard virtual\\nprivate network protocol.‚Äù\\n[66] B. Lipp, B. Blanchet, and K. Bhargavan, ‚ÄúA mechanised cr yptographic\\nproof of the wireguard virtual private network protocol,‚Äù i n2019\\nIEEE European Symposium on Security and Privacy (EuroS&P) .\\nStockholm, Sweden: IEEE, Jun. 2019, p. 231‚Äì246. [Online]. A vailable:\\nhttps://ieeexplore.ieee.org/document/8806752/\\n[67] M. Marlinspike, ‚ÄúPrivate group messaging,‚Äù\\nhttps://signal.org/blog/private-groups/, 2014.\\n[68] Matrix.org Foundation CIC, ‚ÄúClient-\\nserver api - one-time and fallback keys,‚Äù\\nhttps://spec.matrix.org/v1.11/client-server-api/#on e-time-and-fallback-keys,\\n2024, accessed: 2024-08-02.[69] U. Norell, ‚ÄúDependently typed programming in agda,‚Äù in Proceedings\\nof the 4th International Workshop on Types in Language Desig n and\\nImplementation , ser. TLDI ‚Äô09. New York, NY , USA: ACM, 2009, pp.\\n1‚Äì2. [Online]. Available: http://doi.acm.org/10.1145/1 481861.1481862\\n[70] V . Pereira, ‚ÄúEasycrypt - a (brief) tutorial.‚Äù\\n[71] T. Perrin, ‚ÄúThe xeddsa and vxeddsa signature schemes.‚Äù\\n[72] B. Poettering, P. R¬® osler, J. Schwenk, and D. Stebila, SoK:\\nGame-Based Security Models for Group Key Exchange , ser.\\nLecture Notes in Computer Science. Cham: Springer Internat ional\\nPublishing, 2021, vol. 12704, p. 148‚Äì176. [Online]. Availa ble:\\nhttps://link.springer.com/10.1007/978-3-030-75539-3 7\\n[73] E. Rescorla, ‚ÄúThe Transport Layer Security (TLS) Proto col\\nVersion 1.3,‚Äù RFC 8446, Aug. 2018. [Online]. Available:\\nhttps://www.rfc-editor.org/info/rfc8446\\n[74] Rocket.Chat, ‚ÄúRocket.chat leverages matrix protocol\\nfor decentralized and interoperable communications,‚Äù\\nhttps://www.rocket.chat/press-releases/rocket-chat- leverages-matrix-protocol-for-dec \\n2023, accessed: 2024-07-08. [Online]. Available:\\nhttps://www.rocket.chat/press-releases/rocket-chat- leverages-matrix-protocol-for-dec \\n[75] Session, ‚ÄúSession protocol technical information,‚Äù D ec. 2020, accessible\\nat: https://getsession.org/blog/session-protocoltech nical-information.\\n[76] Signal, ‚ÄúPrivate group messaging,‚Äù accessible at:\\nhttps://signal.org/blog/private-groups/.\\n[77] soatok, ‚ÄúSoatok‚Äôs matrix disclosure,‚Äù\\nhttps://gist.github.com/soatok/0a3d24710b2ccac2aac1 4820008b06ab,\\n2024, accessed: 2024-08-20.\\n[78] D. Stebila, ‚ÄúSecurity analysis of the imessage pq3 prot ocol.‚Äù\\n[79] M. Steiner, G. Tsudik, and M. Waidner, ‚ÄúDifÔ¨Åe-hellman k ey distribution\\nextended to group communication,‚Äù in Proceedings of the 3rd ACM\\nConference on Computer and Communications Security , ser. CCS ‚Äô96.\\nNew York, NY , USA: Association for Computing Machinery, 199 6, p.\\n31‚Äì37. [Online]. Available: https://doi.org/10.1145/23 8168.238182\\n[80] A. Straub, ‚ÄúOmemo: Multi-end message and object encryp tion,‚Äù\\nhttps://xmpp.org/extensions/xep-0384.html, 2016, acce ssed: 2024-07-\\n08. [Online]. Available: https://xmpp.org/extensions/x ep-0384.html\\n[81] N. Swamy, G. Mart¬¥ ƒ±nez, and A. Rastogi, ‚ÄúProof-oriente d programming\\nin f*.‚Äù\\n[82] T. S. Technology, ‚ÄúMatrix protocol users in\\n2023,‚Äù https://www.thestack.technology/matrix-protoc ol-users-2023/,\\n2023, accessed: 2024-07-08. [Online]. Available:\\nhttps://www.thestack.technology/matrix-protocol-use rs-2023/\\n[83] N. Unger, S. Dechand, J. Bonneau, S. Fahl, H. Perl,\\nI. Goldberg, and M. Smith, ‚ÄúSok: Secure messaging,‚Äù in\\n2015 IEEE Symposium on Security and Privacy . San Jose,\\nCA, USA: IEEE, May 2015, p. 232‚Äì249. [Online]. Available:\\nhttps://ieeexplore.ieee.org/document/7163029/\\n[84] L. Vigan` o, ‚ÄúAutomated security protocol analysis wit h the avispa tool,‚Äù\\nElectronic Notes in Theoretical Computer Science , vol. 155, p. 61‚Äì86,\\nMay 2006.\\n[85] WhatsApp, ‚ÄúWhatsapp security whitepaper,‚Äù July 2024, accessible at:\\nhttps://web.archive.org/web/20240705051221/https:// delta.cs.cinvestav.mx/ ‚àºfrancisco/ssi/WhatsApp- \\n[86] J. Yonan, ‚ÄúOpenvpn,‚Äù https://openvpn.net/, 2001, acc essed: 2024-07-08.\\n[Online]. Available: https://openvpn.net/\\n[87] P. Zave, ‚ÄúUsing lightweight modeling to understand cho rd,‚Äù ACM\\nSIGCOMM Computer Communication Review , vol. 42, no. 2, p. 49‚Äì57,\\nMar. 2012.\\n[88] P. Zimmermann, ‚ÄúPretty good privacy (pgp),‚Äù\\nhttps://www.philzimmermann.com/EN/Ô¨Åndpgp/, 1991,\\naccessed: 2024-07-08. [Online]. Available:\\nhttps://www.philzimmermann.com/EN/Ô¨Åndpgp/\\n[89] J.-K. Zinzindohou¬¥ e, K. Bhargavan, J. Protzenko, and B . Beurdouche,\\n‚ÄúHacl*: A veriÔ¨Åed modern cryptographic library,‚Äù in Proceedings of\\nthe 2017 ACM SIGSAC Conference on Computer and Communicatio ns\\nSecurity . Dallas Texas USA: ACM, Oct. 2017, p. 1789‚Äì1806.\\n[Online]. Available: https://dl.acm.org/doi/10.1145/3 133956.3134043\\n14VII. A PPENDIX\\nA. Megolm Ratchet DeÔ¨Ånition\\nWe brieÔ¨Çy describe the Megolm ratchet scheme as de-\\nscribed in the Megolm speciÔ¨Åcation [51]. The Megolm ratchet\\nRiconsists of four components, Ri,jforj‚àà {0,1,2,3}, each\\n256 bits in this implementation. Initialization is done wit h\\ncryptographically-secure random data and advances as foll ows:\\nRi,0=/braceleftbiggH0/parenleftbig\\nR224(n‚àí1),0/parenrightbig\\nifi= 224n\\nRi‚àí1,0 otherwise\\nRi,1=\\uf8f1\\n\\uf8f2\\n\\uf8f3H1/parenleftbig\\nR224(n‚àí1),0/parenrightbig\\nifi= 224n\\nH1/parenleftbig\\nR216(m‚àí1),1/parenrightbig\\nifi= 216m\\nRi‚àí1,1 otherwise\\nRi,2=\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f3H2/parenleftbig\\nR224(n‚àí1),0/parenrightbig\\nifi= 224n\\nH2/parenleftbig\\nR216(m‚àí1),1/parenrightbig\\nifi= 216m\\nH2/parenleftbig\\nR28(p‚àí1),2/parenrightbig\\nifi= 28p\\nRi‚àí1,2 otherwise\\nRi,3=\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f3H3/parenleftbig\\nR224(n‚àí1),0/parenrightbig\\nifi= 224n\\nH3/parenleftbig\\nR216(m‚àí1),1/parenrightbig\\nifi= 216m\\nH3/parenleftbig\\nR28(p‚àí1),2/parenrightbig\\nifi= 28p\\nH3(Ri‚àí1,3) otherwise\\nHere,H0,H1,H2, andH3are distinct hash functions. In\\nsummary:\\n‚Ä¢ Every28iterations, Ri,3is reseeded from Ri,2.\\n‚Ä¢ Every216iterations, Ri,2andRi,3are reseeded from\\nRi,1.\\n‚Ä¢ Every224iterations, Ri,1,Ri,2, andRi,3are reseeded\\nfromRi,0.\\nThe complete ratchet value Riis hashed to generate\\nencryption keys. This design allows arbitrary forward ratc het\\nadvancement, requiring at most 1020 hash computations, and\\ntherefore minimizing cost. A client can decrypt chat histor y\\nfrom the earliest known ratchet value but cannot decrypt pri or\\nhistory without reversing the hash.\\nVIII. P ROTOCOL HANDSHAKES\\n15Alice\\nidenalong\\ngalong =GalongBob\\nidenblong\\ngblong=Gblong\\ngenerates bs,bo\\ngbs=Gbs\\ngbo=Gbo\\ngbssig = SIGN(blong,gbs )\\ngblong ,gbssig ,gbs,gbo\\noff-band veriÔ¨Åcation of gblong\\ngenerates ae1\\ngae1=Gae1\\namaster =HASH(gbsalong,gblongae1,gbsae1,gboae1)\\narkba1,ackba1= HKDF( amaster )\\ngenerates m1,ae2\\ngae2=Gae2\\nSIGNVERIF( gblong,gbs,gbssig )\\nakshared 1=gbsae2\\narkab1,ackab 1= HKDF( akshared 1,arkba 1)\\nakenc1=HKDF(MAC( ackab1))\\nx1=AEAD ENC(akenc1,m1,HASH(galong,gblong,gae 2))\\ngalong ,gae1,gae2,x1\\noff-band veriÔ¨Åcation of galong\\nbmaster =HASH(galongbs,gaeblong\\n1,gaebs\\n1,gaebo\\n1)\\nbrkba1,bckba 1= HKDF( bmaster )\\nbkshared 1=gaebs\\n2\\nbrkab1,bckab 1=HKDF(bkshared 1,brkba 1)\\nbkenc1,bkenc 2=HKDF(MAC( bckab1))\\nmd1=AEAD DEC(bkenc1,x1,HASH(galong,gblong,gae 2))\\ngenerates m2,be\\ngbe=Gbe\\nbkshared 2=gaegb\\n2\\nbrkba2,bckba 2= HKDF( bkshared 2,brkab 1)\\nbkenc3=HKDF(MAC( bckba2))\\nx2=AEAD ENC(bkenc1,m2,HASH(gblong,galong,gbe ))\\ngbe,x2\\nakshared 2=gbeae2\\narkba2,ackba 2=HKDF(akshared 2,arkab 1)\\nakenc3,bkenc 2=HKDF(MAC( ackba2))\\nmd2=AEAD DEC(akenc3,x2,HASH(gblong,galong,gbe ))\\nFig. 5: Complete Signal Protocol\\n16',\n",
       " 'Contrastive Representation Learning for Dynamic Link Prediction\\nin Temporal Networks\\nAmirhossein Nouranizadeh‚àóFatemeh Tabatabaei Far*\\nMohammad Rahmati\\nDepartment of Computer Engineering\\nAmirkabir University of Technology, Tehran, Iran\\n{nouranizadeh, tabatabaeifateme, rahmati}@aut.ac.ir\\nAbstract\\nEvolving networks are complex data structures that emerge\\nin a wide range of systems in science and engineering. Learn-\\ning expressive representations for such networks that encode\\ntheir structural connectivity and temporal evolution is essential\\nfor downstream data analytics and machine learning appli-\\ncations. In this study, we introduce a self-supervised method\\nfor learning representations of temporal networks and em-\\nploy these representations in the dynamic link prediction task.\\nWhile temporal networks are typically characterized as a se-\\nquence of interactions over the continuous time domain, our\\nstudy focuses on their discrete-time versions. This enables us\\nto balance the trade-off between computational complexity and\\nprecise modeling of the interactions. We propose a recurrent\\nmessage-passing neural network architecture for modeling\\nthe information flow over time-respecting paths of temporal\\nnetworks. The key feature of our method is the contrastive\\ntraining objective of the model, which is a combination of\\nthree loss functions: link prediction, graph reconstruction, and\\ncontrastive predictive coding losses. The contrastive predictive\\ncoding objective is implemented using infoNCE losses at both\\nlocal and global scales of the input graphs. We empirically\\nshow that the additional self-supervised losses enhance the\\ntraining and improve the model‚Äôs performance in the dynamic\\nlink prediction task. The proposed method is tested on Enron,\\nCOLAB, and Facebook datasets and exhibits superior results\\ncompared to existing models.1\\n1. Introduction\\nMany processes in science and engineering are modeled as\\ndynamical systems over evolving networks. These processes\\nrange from contagion processes over networks of individual\\ncontacts, like disease spreading, to information propagation\\nover communication networks [25], such as those observed\\nin social networks. A key feature of processes supported on\\nevolving networks is that their dynamics are highly coupled\\n*Equal Contributions\\n1https://github.com/amrhssn/teneNCE\\nFigure 1. (a) Illustration of a temporal network as a sequence of\\npairwise interactions between system entities over a continuous time\\ninterval. (b) The discrete-time snapshot sequence represents the\\ntemporal network. The discretization operation divides the temporal\\nnetwork‚Äôs time domain into equal-length intervals ‚àÜtand projects\\nthe interactions within each time interval to a static graph.\\nwith the network‚Äôs topological and temporal features. This\\nmakes it essential to consider the evolving nature of networks\\nwhen modeling such systems. In recent years, there have\\nbeen numerous efforts in the machine learning community\\nto encode the structures of graphs into real vector spaces, a\\nfield that is generally referred to as graph representation learn-\\ning. Graph neural networks (GNNs) constitute a significant\\nparadigm in graph representation learning methods. In addi-\\ntion to the abundance and ubiquity of graph-based data and\\nthe availability of Graphical Processing Units (GPUs), the suc-\\ncessful implementation of GNNs can be primarily attributed to\\nthe widespread accessibility of open-source software libraries,\\nsuch as [12, 13, 61].\\nHowever, most graph-based deep learning methods are typ-\\nically applied to static graphs for representation learning. This\\ncontrasts with numerous real-world networks that are inher-\\nently dynamic. Nevertheless, there has been a growing interest\\nin the development of methods that handle temporal networks\\nand their deployment in production environments.\\nIn the present work, our goal is to learn vector representa-\\ntions of temporal networks in a manner that ensures the learned\\n1arXiv:2408.12753v1  [cs.LG]  22 Aug 2024embeddings contain structural and temporal information of the\\nunderlying dynamic graph. To this end, we model the evo-\\nlution of the temporal network as a discrete-time dynamical\\nsystem. The system is parameterized via a novel recurrent\\nmessage-passing neural network architecture that naturally fa-\\ncilitates the flow of information over time-respecting paths of\\nthe dynamic graph.\\nMoreover, to train the model‚Äôs parameters, we formulate a\\nloss objective that encourages the model to extract features of\\nthe temporal network that are informative for both transductive\\nand inductive downstream machine learning tasks, namely the\\nreconstruction and prediction of the dynamic graphs. Addi-\\ntionally, to regularize the training, we include a self-supervised\\ninfoNCE loss that operates at the local and global scales of the\\ntemporal network and guides the model to encode features of\\nthe data that span longer periods into the future. Intuitively,\\nthis training strategy creates a balance between extracting low-\\nlevel features that are informative for predicting the next state\\nof the graph and high-level features that describe the graph far\\ninto the future.\\nWe call the proposed method Temporal Network Noise\\nContrastive Estimation ,teneNCE for short. We evaluate the\\nteneNCE representations in the dynamic link prediction task\\nand numerically show that it achieves competitive results com-\\npared to state-of-the-art existing models. Our main contribu-\\ntions are summarized as follows:\\n‚Ä¢We introduce a novel recurrent message-passing neural net-\\nwork architecture for learning temporal representations in\\ndynamic graphs. This model propagates node information\\nthrough time-respecting paths in the temporal network.\\n‚Ä¢We propose a single loss function that integrates reconstruc-\\ntion, prediction, and self-supervised infoNCE losses. The\\nreconstruction loss helps the model in learning current struc-\\nture of the temporal network, while the prediction loss en-\\nsures that the learned features are informative for predicting\\nthe network‚Äôs structure in the next time step. Moreover, the\\ninfoNCE loss function effectively captures high-level dy-\\nnamics that extend further into the future by operating at\\nboth the local node-level and the global graph-level of the\\ntemporal network. The training process demonstrates that\\ninfoNCE loss decreases continuously, enabling the model to\\nextract more informative representations.\\n‚Ä¢The teneNCE model shows superior performance in the dy-\\nnamic link prediction task on Enron, COLAB, and Facebook\\ndatasets, achieving an average improvement of 4% compared\\nto state-of-the-art models.\\nIn the remainder of this paper, we will revisit the related works\\nthat have guided us in formulating our proposed method in\\nSection 2. Following this, we will elaborate on our method in\\nSection 3 and subsequently discuss the experimental setup and\\nnumerical findings in Section 4. Finally, we will conclude this\\nwork and outline our future research trajectory in Section 5.\\n2. Related Works\\n2.1. GNNs for static graphs\\nThe field of graph representation learning has experienced sig-\\nnificant success, mainly due to advancements in graph neuralnetwork research. These models are message-passing neural\\nnetworks that encode the structure and features of a graph into\\nan embedding space by exchanging neural messages across\\nthe topology of the graph [5, 15, 20, 29, 30, 57, 66]. Recent re-\\nsearch in graph representation learning focuses on addressing\\nthe fundamental limitations of GNNs, such as oversmoothing\\nand over-squashing problems [1, 39, 55]. Furthermore, to en-\\nhance the expressive power of GNNs, proposed methods have\\nemployed graph-rewiring techniques [3, 55], graph transform-\\ners [9, 31, 36, 46, 52, 63, 68], and integration of topological\\ninductive biases into these models to inform them about the\\nunderlying graph structure [4, 10, 33, 34, 60].\\n2.2. GNNs for dynamic graphs\\nGiven the evolving nature of graphs in many real-world scenar-\\nios, dynamic graph representation learning is still in its early\\nstages compared to static graph embedding methods. How-\\never, there has been a recent interest in developing methods\\nspecifically tailored for temporal networks.\\nIn general, deep learning models for dynamic graph rep-\\nresentation learning can be classified into two categories, de-\\npending on how they represent the input data: discrete-time or\\ncontinuous-time models.\\n2.2.1 Discrete-time models\\nDiscrete-time models are applied to a sequence of static graph\\nsnapshots. GCRN [50] generalizes the convLSTM model [51]\\nto graphs, where the 2D Euclidean convolutions in the RNN\\nare replaced with graph convolutions. This modification allows\\nthe RNN model to perform message-passing operations over\\nthe graph structure to update its internal hidden state. However,\\nin their original work, the GCRN model was used to model\\nspatiotemporal sequences of images and text data, with the\\ngraph structure kept fixed during training.\\nVGRNN [19] extends the GCRN model to evolving graph\\nsequences and is trained according to the variational autoen-\\ncoder framework. More specifically, VGRNN introduces latent\\nvariables sampled from a prior Gaussian distribution, which\\nis a function of previous graph structures that are captured by\\ngraph recurrent neural networks. These latent variables are\\nthen optimized to reconstruct future graphs.\\nIn parallel to VGRNN, GC-LSTM [6] proposes an encoder-\\ndecoder architecture to learn the dynamic graph representa-\\ntions. The encoder model is a graph convolutional LSTM and\\nis responsible for capturing temporal-topological information\\nof the snapshot sequence, and the decoder model is a fully\\nconnected MLP that predicts the future structure of the graph.\\nEULER [28] also adopts the standard architecture of in-\\ntegrating GNNs with sequential encoders for temporal link\\nprediction. Their proposed method aims to execute this archi-\\ntecture on large-scale graphs by introducing a parallel compu-\\ntational framework where separate machines run a replicated\\nGNN. This enables the architecture to process multiple snap-\\nshot graphs simultaneously, demonstrating the capability of\\nscaling the sequential architecture in practice.\\nEvolveGCN [41] approaches the temporal graph embedding\\ndifferently. Instead of training the model to encode the dynam-\\n2ics of graph structure into its parameters, EvolveGCN uses a\\nrecurrent neural network to update the parameters of the GNN\\nmodel to react to the evolution of input graphs. This allows\\nEvolveGCN to address the problem of frequently changing\\nnode sets in the dynamic graph.\\nWhile most discrete-time models employ RNNs to capture\\nthe time evolution of the snapshot sequence, DySAT [49]\\nuses self-attention layers to jointly encode the structural and\\ntemporal information of the dynamic graph. Through the use\\nof positional embeddings, DySAT processes the sequence of\\nsnapshot graphs simultaneously, making it computationally\\nmore efficient compared to recurrent processing RNN-based\\nmethods.\\n2.2.2 Continuous-time models\\nContinuous-time models are directly applied to the temporal\\nedge sequence observed over continuous timestamps. These\\nmodels use time encoding functions to embed the time di-\\nmension information into real vector spaces. Time encoders\\ncan have both fixed [8] and trainable parameters [47, 65].\\nTGAT [65] employs a self-attention mechanism to jointly cap-\\nture the structural and temporal information of the temporal\\nnetwork. TGN [47] initially captures each node‚Äôs temporal\\ninformation by aggregating the node‚Äôs interaction information,\\nwhich is represented as edge features, using a recurrent neural\\nnetwork. Subsequently, TGN employs a graph attention neural\\nnetwork on the time-aggregated graph, enabling it to capture\\nstructural and temporal information.\\nIn contrast to other dynamic GNN architectures that use\\nRNNs or attention mechanisms for encoding temporal net-\\nworks, GraphMIXER [8] introduces an architecture composed\\nsolely of MLPs. GraphMIXER uses a link encoder to compute\\nedge information and then applies a node encoder to aggregate\\neach node‚Äôs messages over its links.\\nInterested readers are referred to [26] for a comprehensive\\nsurvey on the dynamic graph representation learning task.\\n2.3. Contrastive learning on graphs\\nContrastive learning (CL) is a popular self-supervised learning\\ntechnique that learns representations by contrasting similar\\nand dissimilar instances sampled from a data distribution. CL\\nhas proved to be successful in computer vision [7, 16, 21, 23],\\nnatural language [11, 17, 67] and audio processing [37, 48, 53].\\nNoise Contrastive Estimation (NCE) is introduced in [18]\\nas a technique for estimating the parameters of unnormalized\\nstatistical models. The method trains a logistic regression clas-\\nsifier to discriminate data points from samples drawn from a\\nknown noise distribution. Intuitively, as the authors articulated,\\nthe idea behind the noise-contrastive estimation is ‚Äúlearning\\nby comparison\".\\nIn the well-known work [40], the authors proposed the Con-\\ntrastive Predictive Coding (CPC) technique for unsupervised\\nrepresentation learning of sequential data. CPC encodes each\\nelement of the sequence via an encoder function. It then uses\\nan autoregressive model to capture context representations\\nthat maximize mutual information with the future represen-\\ntations of the sequence elements. The mutual informationmaximization is done by optimizing the introduced infoNCE\\nloss function. In essence, the infoNCE loss can be interpreted\\nas the binary cross entropy loss between the similarity scores\\nof the context representation and the future encodings, con-\\ntrasted with the similarity scores of the context representation\\nand encodings of negative samples. For further details on CPC\\nand infoNCE loss, see Appendix 6.4.\\nContrastive learning has also been shown to be an effec-\\ntive objective for capturing graph information through GNN\\narchitectures in unsupervised graph representation learning.\\nDeep Graph Infomax (DGI) [58] extends Deep Infomax [24]\\nto graph-structured data. The DIG model‚Äôs learning objective\\nis to differentiate between the similarity scores of local-global\\nembedding pairs of the input graph and those derived from a\\nshuffled version of the same graph.\\nAlternatively, InfoGraph [54] employs deep Infomax to\\nlearn graph-level representations. It aims to maximize the mu-\\ntual information between the graph-level representation and\\nthe representations of substructures at varying scales within the\\ngraph. In the study [22], the authors construct a different struc-\\ntural view of the graph using the graph diffusion process and\\ndifferentiate between the node and graph level representation\\nof the two graph views to learn graph representations.\\nIt is essential to mention that despite the success of con-\\ntrastive learning methods in representation learning, it has been\\nshown that the performance of these methods cannot solely be\\nattributed to the properties of mutual information alone. It also\\ndepends on the choice of encoders and mutual information\\nestimators [56]. For a complete survey on graph representation\\nlearning with contrastive methods, refer to [35, 64].\\n3. Methodology\\nAs briefly mentioned in Section 1, our goal is to learn node rep-\\nresentations of a temporal network that capture the structural\\ninformation of the network as well as its temporal evolution.\\nSpecifically, we seek to learn a function that maps each node\\nof the temporal network to an embedding vector, such that\\nthe embeddings contain sufficient information to predict the\\nnetwork‚Äôs future structure.\\nThe unique aspect of our approach in accomplishing this\\ngoal lies in the application of the infoNCE loss function at both\\nlocal and global scales of the temporal network, which regu-\\nlates the training process. Intuitively, by incorporating both\\nnode-level and graph-level network evolution within the con-\\ntrastive learning framework, we seek to regularize the model\\nin learning node representations that align with the global net-\\nwork dynamics. Our empirical evaluation demonstrates the\\nefficacy of our approach in the dynamic link prediction task\\nacross three datasets: Enron, Colab, and Facebook. We find\\nthat it delivers competitive quantitative results when bench-\\nmarked against existing methods.\\nIn the following subsections, we will detail the develop-\\nment process of the teneNCE model. All the notations used\\nthroughout the paper are summarized in Table 5.\\n3Figure 2. (a) Illustration of the teneNCE model architecture. The model processes a sequence of snapshot graphs. The main components of the\\nteneNCE model include an Encoder for embedding each static graph in the sequence and an Update component that recursively updates the\\nstate representations of each node across time steps. During the forward pass of the model in the training process, at time step k, the updated\\nnode states Skare used to compute the (1) reconstruction loss, (2) prediction loss, and, (3) contrastive predictive coding loss. (b) The prediction\\nmodules include a Decoder for reconstructing the static graph at each time step and a LinkPredictor for predicting the graph‚Äôs structure at the next\\ntime step. (c) An overview of the CPC loss module, which consists of a LocalPredictiveEncoder and a GlobalPredictiveEncoder for predicting\\nthe future structural embeddings of the graph based on the node states, for time steps k+ 1, . . . , N . Additionally, the ReadOut (.)function\\naggregates the node-level embeddings into the graph-level representation.\\n3.1. Preliminaries\\nThe main objects of our interest are temporal networks. In most\\nsystems, where pairwise events occur or other elements within\\nthe system influence the dynamics of individual objects, the\\ndata is represented by a temporal graph. This data consists of\\nan interaction sequence over time where interactions are events\\nbetween system entities. Since temporal networks represent\\nthe internal interactions of a system over time, additional time-\\ndependent information can be attributed to them, such as node\\nand edge features at the time of the interaction. Specifically,\\nwe define temporal networks as follows:\\nDefinition 3.1 (Temporal network) A temporal network\\nG= (V,E)is defined by the set of nodes V={v1, . . . , v n}\\nand the set of events E={et1, . . . , e tm}, where nandm\\nare the number of nodes and events, respectively. Each\\nevent et= (vi, vj, t)‚àà\\x00V\\n2\\x01\\n√óR+represents a pairwise\\ninteraction between nodes viandvjat the time t‚ààR+. In\\nattributed networks, events and nodes can have extra time-dependent feature information, which can be represented by\\net= (vi, vj,xi(t),xj(t),eij(t), t)where xi(t),xj(t)‚ààRdV\\nandeij(t)‚ààRdEare the feature information of the nodes\\nvi, vjand event etat time t, respectively. Here dVanddE\\ndenote the dimensions of feature vectors for nodes and edges.\\nTemporal networks are also referred to as continuous-\\ntime dynamic graphs (CTDG) [26]. Instead of processing\\ncontinuous-time dynamic graphs, we work with discrete-time\\nversions of temporal networks, also known as snapshot se-\\nquences . We project the temporal network into a sequence\\nof static snapshot graphs, each capturing interactions within\\na specific, fixed time interval. The discretization function di-\\nvides the entire timespan of the temporal network into equally-\\ndistanced time intervals and projects the interactions of each\\ninterval into a time-aggregated static graph. Despite losing\\nsome information during the discretization process, this ap-\\nproach allows us to handle a greater number of interactions in\\na single call to the representation model.\\n4Definition 3.2 (Snapshot sequence) Given a temporal net-\\nworkG= (V,E), a snapshot sequence G={G1, . . . , G N}is\\ndefined as the sequence of static graphs Gk= (V, Ek), where\\nthe set of edges is defined as Ek={et‚àà E: (k‚àí1)‚àÜt‚â§t <\\nk‚àÜtfork= 1, . . . , N, ‚àÜt=tm‚àít1\\nN}. The set of edges can be\\nrepresented by the adjacency matrix Ak‚ààRn√ón. Attributed\\nsnapshot graphs can be represented as Gk= (Xk,Ek,Ak),\\nwhere Xk‚ààRn√ódVandEk‚ààR|Ek|√ódEare the feature ma-\\ntrices of nodes and edges, respectively.\\nThe snapshot sequence is also called a discrete-time dy-\\nnamic graph (DTDG) [26]. We model the evolution of a\\ntemporal network as a discrete-time dynamical system that is\\nparameterized by a composition of recurrent and graph neu-\\nral networks. After optimizing the model‚Äôs parameters, the\\nlearned state representations of the nodes encode temporal-\\ntopological information of the system, which are computed as\\na function of historical data. These node state representations\\ncan be used in any downstream machine-learning task defined\\nover the temporal network.\\n3.2. Motivation\\nIn a temporal network, information spreads through time-\\nrespecting paths. A time-respecting path is defined as a set\\nof interactions, ordered by time, that connects subsets of\\nnodes [27] in a temporal network. For any given node vi\\nat a specific time t, the set of all other nodes that have time-\\nrespecting paths to node viup to time t, is called the source set\\nofvi[25]. Node states in a temporal network are influenced by\\ntheir corresponding source sets. This makes the ideal modeling\\nof a temporal network an inherently sequential process since\\nthe node states must be updated after each interaction.\\nTo model the information propagation over a temporal net-\\nwork, we need to consider the trade-off between updating node\\nstates after every interaction and the computational efficiency\\nof our model. This motivates us to work with the discretized\\nversion of the temporal network, which is a sequence of static\\ngraph snapshots, and concurrently update the node states of\\neach static graph by a single forward pass of the model. Fur-\\nthermore, to compensate for the information loss resulting\\nfrom the discretization process, we employ a message-passing\\nmechanism on each static graph within the snapshot sequence\\nbefore updating the node states.\\nTo steer the model towards extracting state representations\\nthat reflect the temporal-topological information of the sys-\\ntem, we define a learning objective that is constituted of three\\ncomponents:\\nI.Initially, we want the node representations to contain the\\ntopological information of the network at any given time.\\nThis results in the formulation of the reconstruction loss ,\\nwhich encourages the learned representations to contain\\nthe necessary information for reconstructing the graph\\nat a given time.\\nII.Next, we want the representations to capture the required\\ntemporal information for predicting the temporal net-\\nwork structure at the next time step. This motivates the\\nprediction loss .\\nIII.Finally, we want the representations to containtopological-temporal features of the network that span\\nmultiple time steps. To this end, we employ the con-\\ntrastive predictive coding loss function [40], which max-\\nimizes the mutual information between node and graph\\nlevel representations and corresponding future features\\nof the temporal network.\\nBy optimizing the learning objective with respect to the\\nmodel‚Äôs parameters, we are able to embed the system‚Äôs infor-\\nmation into state representations and subsequently feed them\\ninto machine learning workflows of temporal networks.\\n3.3. Model\\nOur model consists of five components, as shown in Figure 2.\\n3.3.1 Encoder\\nThe Encoder function maps the structural information of each\\nsnapshot graph into the structural embedding space. It is im-\\nplemented using a graph neural network that acts on each static\\ngraph:\\nZk=Encoder (Xk,Ak)k= 1, . . . , N. (1)\\nHere,Zk‚ààRn√ódencis the structural embedding matrix, and\\ndencis the output dimension of the Encoder . In practice, any\\nGNN can be used as an encoder. In our experiments, we used\\na simple 3-layer graph convolutional network [29] with ReLU\\nactivation functions.\\n3.3.2 Update\\nThe function Update encodes the temporal network into the hid-\\nden state space by modeling time evolution through a discrete-\\ntime dynamical system, which generates state trajectories:\\nSk=Update (Zk,Ak,Sk‚àí1, k)k= 1, . . . , N,\\nS0=0.(2)\\nThe state matrix Sk‚ààRn√ódstate, which serves as node represen-\\ntations, is updated using the recurrent function Update , given\\nthe current structural embedding matrix Zk, current adjacency\\nmatrix Ak, previous state matrix Sk‚àí1and encoded current\\ntime step kof the system. The dimension of the hidden state\\nis denoted by dstate.\\nIn our implementation, inspired by [50] and similar to [19],\\nwe used a single-cell graphical gated recurrent unit (GGRU)\\nfor the function Update . The GGRU cell takes the concatena-\\ntion of structural and time embeddings, CONCAT (Zk, kenc),\\nas input, and along with the previous states Sk‚àí1, it com-\\nputes the updated states by applying message-passing over\\nthe current graph‚Äôs structure Ak. The time embeddings,\\nkenc=TimeEncoder (k), are computed using the proposed\\ntime encoder in [46]. The GGRU allows the model to spread\\nthe state information across time-respecting paths. The recur-\\nrentUpdate applies to all the time steps k= 1, . . . , N , with\\nthe initial state being the zero matrix S0=0. See Appendix\\n6.2 and 6.3 for further details. Algorithm 1 shows the inference\\nstep of the model, which produces the state representation ma-\\ntrixS, which can be used in link reconstruction or prediction\\nsettings.\\n53.3.3 Decoder\\nProvided the node states of the network Skat time step k,\\ntheDecoder function is responsible for reconstructing the net-\\nwork‚Äôs structure, ÀÜAk=P(Ak|Sk):\\nÀÜAk=Decoder (Sk)k= 1, . . . , N. (3)\\nIn our experiments, we implemented the Decoder function by\\napplying a linear layer to the state matrix, ÀÜYk=Linear dec(Sk),\\nand computed the link reconstruction probabilities as the\\ninner product of the resulting embeddings, i.e., P(eij=\\n1|Sk[i],Sk[j]) = œÉ(ÀÜYk[i]‚ä§ÀÜYk[j])where œÉis the sigmoid\\nfunction and eijrepresents the edge between nodes viandvj.\\n3.3.4 LinkPredictor\\nFor the dynamic link prediction task, the LinkPredictor func-\\ntion operates on the current representations of the temporal\\nnetwork to predict the structure of the temporal network at the\\nnext timestamp, ÀúAk+1=P(Ak+1|Sk):\\nÀúAk+1=LinkPredictor (Sk)k= 1, . . . , N ‚àí1.(4)\\nSimilar to the Decoder function, we first linearly transform\\nthe current state matrix, ÀúYk=Linear pred(Sk), and use the\\ninner product of the resulting embedding matrix ÀúYkas the link\\nprobabilities of the next time step: P(eij= 1|Sk[i],Sk[j]) =\\nœÉ(ÀúYk[i]‚ä§ÀúYk[j]).\\n3.3.5 PredictiveEncoder\\nWe aim to learn node representations that capture features of\\nthe temporal network that span multiple time steps into the\\nfuture. For this purpose, we employ the contrastive predictive\\ncoding (CPC) framework [40], which maximizes the mutual\\ninformation between the context sequence and future features\\nin a contrastive way. Intuitively, by leveraging the infoNCE\\nlearning objective of the CPC framework, node representations\\nare motivated to encode information that is aligned with future\\ncharacteristics of the temporal network. Thus, infoNCE can\\nbe viewed as a regularization strategy for the link prediction\\nobjective of the model. Given the complexity of temporal net-\\nwork data, we define two types of CPC objectives to guide the\\nlearning process: local and global infoNCEs. Local infoNCE\\nmaximizes the agreement between node representations and\\ntheir future structural embeddings. Additionally, to regular-\\nize the graph-level representations over time, we define the\\nglobal infoNCE objective, which encourages the graph-level\\nrepresentations to agree with their future graph-level structural\\nembeddings. This multi-scale treatment of network dynamics,\\nconsidering both local and global views of the graph, leads to\\nthe learning of more informative representations. Details on\\nthe calculation of the infoNCE loss can be found in Subsection\\n3.4.3.\\nTo implement the CPC objectives, we define the local and\\nglobal predictive encoders that act on the node and graph-level\\nrepresentations and compute the predictions of future struc-\\ntural embeddings for each iteration of processing the snapshotAlgorithm 1 Inference\\nInput: Snapshot sequence {Gk= (Xk,Ak)}N\\nk=1\\nOutput: State matrix S\\n1:S= 0\\n2:fork= 1toNdo\\n3: Encode the current graph, Zk=Encoder (Xk,Ak)\\n4: Update the states, S=Update (Zk,Ak,S, k)\\n5:end for\\nsequence. These feature predictions will be used in computing\\nthe CPC loss objective of the training.\\nFor the local node-level CPC, at the time step k, we compute\\nthe local predictive encodings for future time steps l={k+\\n1, . . . , N }using node representations Skat the time step k,\\nwhich is denoted as ÀÜZ(k)\\nl:\\nÀÜZ(k)\\nl=LocalPredictiveEncoder (Sk, lenc). (5)\\nSimilarly, for the global graph-level CPC, at the time step\\nk, we compute the global predictive encodings of future time\\nsteps l={k+ 1, . . . , N }using graph representations sk,\\nwhich is denoted by ÀÜz(k)\\nl:\\nsk=ReadOut (Sk),\\nÀÜz(k)\\nl=GlobalPredictiveEncoder (sk, lenc),(6)\\nwhere lenc=TimeEncoder (l)is the encoded time step defined\\nin [8], and the ReadOut function is defined as ReadOut (S) =\\n1\\nnP\\ni‚ààVS[i]which averages the node-level representation and\\nreturns the corresponding graph-level representations. Note\\nthat for the ReadOut function, any available graph pooling\\nmethod can be employed to account for the dynamics of hierar-\\nchical representations within the CPC framework [32, 38, 69].\\nIn the experiments, the LocalPredictiveEncoder function\\nis implemented using a 2-layer MLP with a ReLU activation\\nfunction that acts on the concatenation of node representations\\nand time step embedding, i.e.,\\nÀÜZ(k)\\nl=MLP local(CONCAT (Sk, lenc)).\\nMoreover, for the GlobalPredictiveEncoder , we apply a linear\\ntransformation to the concatenation of graph-level representa-\\ntions and time step embedding:\\nÀÜz(k)\\nl=Linear global(CONCAT (sk, lenc)).\\n3.4. Learning\\nThe model parameters are trained by minimizing the multi-\\nobjective loss function using the stochastic gradient descent\\noptimization algorithm. As mentioned in 3.2, the loss function\\nencompasses three objectives, each of which corresponds to\\na desired aspect of learning node state representations of the\\ntemporal network:\\nL=Lpred+Œ±L recon+Œ≤Lcpc, (7)\\nwhere Œ±andŒ≤are scalar hyperparameters used for weighting\\nthe relative importance of loss terms compared to the dynamic\\nlink prediction loss.\\n6Algorithm 2 Training forward pass\\nInput: Snapshot sequence {Gk= (Xk,Ak)}N\\nk=1\\nOutput: Node state matrix S, set of structural embeddings Z,\\nset of reconstructed adjacency matrices ÀÜA, set of predicted\\nadjacency matrices ÀúA, set of local and global predictive\\nencodings, ÀÜZ,ÀÜz\\n1:S= 0\\n2:Initialize Z,ÀÜA,ÀúA,ÀÜZ,ÀÜzwith empty lists\\n3:fork= 1toNdo\\n4: Encode the current graph, Zk=Encoder (Xk,Ak)\\n5: Update the states, S=Update (Zk,Ak,S, k)\\n6: Reconstruct the current graph, ÀÜAk=Decoder (Sk)\\n7: Predict the next graph, ÀúAk+1=LinkPredictor (Sk)\\n8: Get local predictive encoding for l={k+ 1, . . . , N }:\\n9: ÀÜZ(k)\\nl=LocalPredictiveEncoder (Sk, lenc)\\n10: Get global graph state, sk=ReadOut (Sk)\\n11: Get global predictive encoding for l={k+1, . . . , N }:\\n12: ÀÜz(k)\\nl=GlobalPredictiveEncoder (sk, lenc)\\n13: Append Zk,ÀÜAk,ÀúAk+1toZ,ÀÜA,ÀúA\\n14: Append local and global encodings to ÀÜZ,ÀÜz\\n15:end for\\n3.4.1 Link prediction loss\\nIn this work, the main objective of learning node state rep-\\nresentations is to extract useful information for predicting\\nthe structure of the temporal network at the next time step.\\nHence, for the link prediction loss, we compute the binary\\ncross-entropy (BCE) classification loss between the predicted\\nand ground-truth graph structures for each prediction time step:\\nLpred=1\\nN‚àí1N‚àí1X\\nk=1BCE(ÀúAk+1,Ak+1). (8)\\nWe take the average of all link prediction losses over indices\\nk= 1, . . . , N ‚àí1as the prediction loss of the model, Lpred.\\n3.4.2 Reconstruction loss\\nApart from the predictive power of the node states, our goal\\nis to learn representations that capture information about the\\ncurrent structure of the temporal network. For this purpose,\\nwe add the reconstruction loss to the training objectives. This\\nobjective is implemented as the graph autoencoder reconstruc-\\ntion loss [30] for all the snapshots in the input sequence, which\\nis essentially the BCE loss between reconstructed and ground-\\ntruth adjacency matrices:\\nLrecon=1\\nNNX\\nk=1BCE(ÀÜAk,Ak). (9)\\nWe also compute the average of all reconstruction losses as\\nLrecon.\\n3.4.3 Contrastive predictive coding loss\\nThe final term in the model‚Äôs training objective Eq.7 is the CPC\\nloss, which was originally introduced in [40]. As described\\nFigure 3. Illustration of positive and negative sample pairs for local\\nand global infoNCE losses. This example depicts positive and nega-\\ntive pairs for node the v2and graph Gk, corresponding to local and\\nglobal losses. For localNCE, different negative samples defined in\\nEq.14 are colored orange, pink, and blue; for globalNCE, negative\\nsamples are colored pink.\\nin Subsection 3.3.5, we employ local and global infoNCE\\nlosses to maximize the mutual information between node rep-\\nresentations and future features of the temporal graph at local\\nand global scales, respectively. These losses assist the model\\nin achieving a balance between learning low-level features,\\nwhich are beneficial for reconstruction and one-step prediction\\ntasks, and extracting slow-varying features that characterize\\nthe temporal network over more extended periods.\\nThe CPC loss of the model is computed as the average\\ninfoNCE loss over time steps k= 1, . . . , N ‚àí1:\\nLcpc=1\\nN‚àí1N‚àí1X\\nk=1infoNCE(k)k= 1, . . . , N ‚àí1,(10)\\nwhere for each predictive coding time step k,infoNCE(k)is\\ncomputed as the sum of local and global noise contrastive\\nlosses from current time step kto the final training time step\\nN. We have:\\ninfoNCE(k)=NX\\nl=k+1localNCE(k)\\nl+globalNCE(k)\\nl.(11)\\nLocal noise contrastive loss at time step kwith respect to\\nthe future time step l, denoted as localNCE(k)\\nl, is computed\\nat the node-level of the temporal network. By minimizing\\n7Algorithm 3 Training step\\nInput: Node state matrix S, set of structural embeddings Z,\\nset of reconstructed adjacency matrices ÀÜA, set of predicted\\nadjacency matrices ÀúA, set of local and global predictive\\nencodings, ÀÜZ,ÀÜz, loss function coefficients Œ±, Œ≤\\nOutput: Model with updated parameters\\n1:Compute LpredEq.8\\n2:Compute LreconEq.9\\n3:Compute LcpcEq.10\\n4:L=Lpred+Œ±L recon+Œ≤Lcpc\\n5:Update model‚Äôs parameters using backpropagation\\nthis term we maximize the mutual information between each\\nnode‚Äôs representation, Sk[i], with the node‚Äôs future features,\\nZl[i]forl={k+ 1, . . . , N }, which results in learning node\\nrepresentations that are more aligned with future character-\\nistics of the temporal network [40]. At the time step k, the\\nloss term localNCE(k)\\nlfor node iwith respect to time step l, is\\nimplemented as the binary cross-entropy loss between the posi-\\ntive pair (ÀÜZ(k)\\nl[i],Zl[i])and the negative pairs (ÀÜZ(k)\\nl[i],Zl‚Ä≤[i‚Ä≤]),\\nwhere (i‚Ä≤, l‚Ä≤)is drawn from the local negative sample distribu-\\ntion for node iat time step l, i.e., (i‚Ä≤, l‚Ä≤)‚àºPneg(i, l). Specifi-\\ncally, we have:\\nlocalNCE(k)\\nl=\\n‚àí1\\nnX\\ni‚ààVEPneg(i,l)\\uf8eb\\n\\uf8ec\\uf8ec\\uf8edlogexp\\x10\\nÀÜZ(k)\\nl[i]‚ä§Zl[i]\\x11\\nexp\\x10\\nÀÜZ(k)\\nl[i]‚ä§Zl[i]\\x11\\n+P\\n(i‚Ä≤,l‚Ä≤)‚àºPneg(i,l)exp\\x10\\nÀÜZ(k)\\nl[i]‚ä§Zl‚Ä≤[i‚Ä≤]\\x11\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f8.\\n(12)\\nIn a similar manner, the global noise contrastive loss at time\\nstepkwith respect to the time step l, denoted by globalNCE(k)\\nl,\\nis computed at the global graph-level scale of the temporal\\nnetwork. In particular, the loss term globalNCE(k)\\nlis computed\\nusing the graph-level representations of the temporal network,\\nsk, and the graph-level structural encodings at the future time\\nsteps, zlforl={k+ 1, . . . , N }. At the time step kand\\nwith respect to time step l, the loss term globalNCE(k)\\nlis\\nimplemented as the binary cross-entropy loss between the\\npositive pair (ÀÜz(k)\\nl,zl), and the negative pairs (ÀÜz(k)\\nl,zl‚Ä≤)where\\nl‚Ä≤is drawn from the global negative sample distribution at time\\nstepl, i.e., l‚Ä≤‚àºPneg(l):\\nglobalNCE(k)\\nl=\\n‚àíEPneg(l)\\uf8eb\\n\\uf8ec\\uf8ec\\uf8edlogexp\\x10\\nÀÜz(k)\\nl‚ä§zl\\x11\\nexp\\x10\\nÀÜz(k)\\nl‚ä§zl\\x11\\n+P\\nl‚Ä≤‚àºPneg(l)exp\\x10\\nÀÜz(k)\\nl‚ä§zl‚Ä≤\\x11\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f8.\\n(13)\\nThe local negative sample distribution, Pneg(i, l), is imple-\\nmented as the uniform probability distribution defined over\\nthe set of all negative samples, denoted as neg(i,l), that is\\nassociated to the node iat time step l:\\nneg(i,l)={(i‚Ä≤, l‚Ä≤)‚àà V √ó { 1, . . . , N }: (i‚Ä≤Ã∏=i)OR(l‚Ä≤Ã∏=l)}.\\nThe elements of negative samples set neg(i,l)consist of all\\nother nodes in the temporal network across all time steps. MoreTable 1. Dataset statistics.\\nAttribute Enron Colab Facebook\\n# Nodes 184 315 663\\n# Edges 4,784 5,104 23,394\\n# Time steps 11 10 9\\nspecifically, for node iat time l, its set of negative samples can\\nbe partitioned into three subsets:\\n\\uf8f1\\n\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f3Same node, different time: (i=i‚Ä≤, lÃ∏=l‚Ä≤),\\nDifferent node, same time: (iÃ∏=i‚Ä≤, l=l‚Ä≤),\\nDifferent node, different time: (iÃ∏=i‚Ä≤, lÃ∏=l‚Ä≤).(14)\\nSimilarly, the global negative sample distribution, Pneg(l),\\nis defined as the uniform probability distribution over the set\\nof all global negative samples corresponding to time step l:\\nneg(l)={l‚Ä≤‚àà {1, . . . , N }:l‚Ä≤Ã∏=l}, (15)\\ni.e., the set of all time steps other than the time step l. These\\nnegative samples are illustrated in Figure 3.\\n4. Experiments\\nIn this section, we present a comprehensive set of experiments\\ndemonstrating the effectiveness of the proposed teneNCE\\nmodel. All the implementations are developed in Python, using\\nthe Pytorch [42] and Pytorch Geometric [13] libraries. For all\\nthe datasets, we have used the ADAM optimizer with the learn-\\ning rate of 10‚àí3and the weight decay coefficient of 5√ó10‚àí4.\\nWe further configured the ReduceLROnPlateau learning rate\\nscheduler in training with a factor of 0.8. We also found that\\nan embedding dimension of 256performs best for all datasets.\\nA grid search was performed to find the optimal values for Œ±\\nandŒ≤in Eq 7. The best values for Œ±are 1, 2, and 4 for the\\nEnron, COLAB, and Facebook datasets, respectively, while\\nthe best values for Œ≤are 1, 4, and 2.\\n4.1. Datasets\\nWe use the following datasets to evaluate the performance of\\nthe proposed model. The summary statistics for these datasets\\nare presented in Table 1.\\nEnron. The Enron dataset captures the email communica-\\ntion network of the Enron Corporation, providing a historical\\nperspective on interactions among employees. In this dataset,\\nnodes represent employees, and edges correspond to email\\nexchanges between colleagues [44].\\nCollaboration (COLAB). This dataset consists of collab-\\noration data among 315 authors, where each author is repre-\\nsented as a node in the dynamic graph and edges correspond\\nto co-authorship relationships [45].\\nFacebook. The Facebook dataset represents the social con-\\nnections among users on the Facebook platform. It contains\\na dynamic graph of friendships and interactions, facilitating\\n8Table 2. The empirical results for the random selection of positive and negative edges. The results represent the mean and standard deviation of\\nevaluation metrics, which are obtained from five separate training runs of each model.\\nModelEnron COLAB Facebook\\nAUC AP MRR AUC AP MRR AUC AP MRR\\nVGRNN 92.68 ¬±0.5 93.04 ¬±0.3 29.02 ¬±0.3 86.56 ¬±0.4 88.52 ¬±0.5 27.20 ¬±1.5 89.44 ¬±0.1 88.82 ¬±0.3 12.06 ¬±0.2\\nDySAT 90.64 ¬±0.9 90.33 ¬±1.2 19.00 ¬±0.6 86.10 ¬±0.3 89.32 ¬±0.3 26.19 ¬±0.6 89.95 ¬±0.3 89.56 ¬±0.3 12.74 ¬±0.2\\nEvolveGCN-H 87.99 ¬±0.6 87.74 ¬±1.3 23.23 ¬±1.7 81.48 ¬±1.4 82.95 ¬±1.3 17.11 ¬±1.1 82.53 ¬±1.2 79.89 ¬±1.7 5.68 ¬±1.0\\nEvolveGCN-O 88.87 ¬±0.5 89.13 ¬±0.6 25.24 ¬±1.2 80.63 ¬±0.9 82.79 ¬±0.9 18.17 ¬±2.6 82.85 ¬±0.4 81.39 ¬±0.6 6.73 ¬±0.6\\nEuler 91.28 ¬±1.6 91.47 ¬±1.6 27.45 ¬±2.6 85.93 ¬±1.0 87.51 ¬±0.8 20.91 ¬±2.2 87.89 ¬±1.1 85.84 ¬±1.5 8.49 ¬±0.8\\nteneNCE 93.54¬±0.4 93.65 ¬±0.2 31.5 ¬±0.6 88.25 ¬±0.7 90.45 ¬±0.6 33.97 ¬±0.6 91.03 ¬±0.1 90.32 ¬±0.2 16.23 ¬±0.2\\nFigure 4. Visualization of the comparison between the temporal correlation coefficient values for the datasets in 4.1 and their corresponding\\nrandomized versions. We have used two randomization models, namely Randomly Permuted Times (RP) and Randomized Edges (RE). For each\\nnull model, we have sampled 100 randomized versions. The box plots summarize the distribution of temporal correlation coefficients of the\\nrandomized samples. The figure demonstrates that the null models have a lower temporal correlation than the original data. This suggests that\\ncapturing temporal information of dynamic graphs is essential for effectively modeling such data.\\nresearch on social network dynamics, information diffusion,\\nand community structure within an online social network [59].\\nWe evaluate our proposed method, teneNCE and the base-\\nlines on the last three snapshots of each dataset. We computed\\nthe temporal correlation coefficients of the corresponding tem-\\nporal network to provide insight into each dataset‚Äôs temporal\\nevolution. As defined in [2], the temporal correlation coeffi-\\ncient is computed as\\nC=1\\nnX\\niCi, (16)\\nwhere,\\nCi=1\\nN‚àí1N‚àí1X\\nk=1P\\njAk[i, j]Ak+1[i, j]rhP\\njAk[i, j]ihP\\njAk+1[i, j]i,(17)\\nis the temporal correlation coefficient of each node in the\\ntemporal network. The dashed lines represented in the box\\nplots of Figure 4 indicate the temporal correlation coefficient\\nof each dataset.\\nFollowing [2], we compared the temporal correlation coef-\\nficient of each dataset with its respective randomized versions.\\nFor the randomization process, we employed two null mod-\\nels: Randomized Edges (RE) and Randomly Permuted Times(RP). The randomized models RE and RP destroy the structural\\nand temporal information of the snapshot sequence, respec-\\ntively [2, 14]. The difference between the temporal correlation\\ncoefficient of the data and their corresponding randomized\\nversions demonstrates the richness of temporal information\\npresent within these networks. This supports the application\\nof temporal models to encode these networks, as opposed to\\nrelying on static models.\\nAdditionally, Figure 5 illustrates the evolution of graph den-\\nsity over time in the Enron, COLAB, and Facebook datasets.\\nGraph density is defined as the ratio of the number of existing\\nedges in a graph to the maximum possible number of edges.\\nThis ratio provides a measure of the density of the graph in\\nterms of edge connectivity. As depicted in Figure 5, the overall\\nconnectivities of the datasets vary over time, highlighting the\\nnecessity to consider temporal information in modeling.\\n4.2. Baselines\\nWe compared our method with existing GNN-based models\\nover discrete-time dynamic graphs, including VGRNN [19],\\nDySAT [49], EvolveGCN [41], and Euler [28]. A detailed\\nintroduction of these baselines can be found in Section 2.\\n9Table 3. The empirical results for evaluation subsets Rand-Pos/Hist-Neg, Hist-Pos/Rand-Neg, and Hist-pos/Hist-Neg, as defined in 4.3.\\nDataset ModelRand-Pos/Hist-Neg Hist-Pos/Rand-Neg Hist-Pos/Hist-Neg\\nAUC AP AUC AP AUC AP\\nEnronVGRNN 64.03 ¬±0.1 66.92 ¬±0.3 96.30 ¬±0.2 95.81 ¬±0.1 72.32 ¬±0.3 73.28 ¬±0.6\\nDySAT 58.30 ¬±1.4 57.54 ¬±1.0 93.73 ¬±0.7 92.88 ¬±0.9 63.43 ¬±0.4 61.00 ¬±0.7\\nEvolveGCN-H 63.77 ¬±0.4 64.04 ¬±0.6 91.96 ¬±0.8 90.95 ¬±1.5 70.41 ¬±0.2 68.86 ¬±0.2\\nEvolveGCN-O 63.82 ¬±0.4 64.65 ¬±0.7 93.56 ¬±0.7 92.61 ¬±0.5 71.33 ¬±1.3 70.30 ¬±1.5\\nEuler 62.13 ¬±1.4 64.20 ¬±2.0 95.08 ¬±1.3 94.57 ¬±1.3 70.59 ¬±1.7 70.50 ¬±2.7\\nteneNCE 65.23¬±0.2 68.32 ¬±0.7 96.81 ¬±0.1 96.20 ¬±0.2 73.95 ¬±0.5 74.29 ¬±0.8\\nCOLABVGRNN 52.44 ¬±0.8 55.38 ¬±1.2 96.52 ¬±0.5 96.08 ¬±0.8 69.04 ¬±1.2 66.25 ¬±1.8\\nDySAT 49.77 ¬±0.3 55.51 ¬±0.6 97.38 ¬±0.4 97.56 ¬±0.3 68.86 ¬±0.3 67.73 ¬±1.3\\nEvolveGCN-H 57.07 ¬±1.1 57.72 ¬±1.3 91.11 ¬±1.0 90.63 ¬±1.0 69.48 ¬±0.4 66.08 ¬±0.3\\nEvolveGCN-O 57.55¬±0.6 59.00 ¬±0.6 90.20¬±0.7 90.42 ¬±0.8 70.70 ¬±1.0 68.76 ¬±1.6\\nEuler 51.91 ¬±1.0 54.54 ¬±0.9 95.07 ¬±0.5 94.54 ¬±0.7 65.75 ¬±1.4 63.47 ¬±1.8\\nteneNCE 55.29 ¬±0.7 58.79 ¬±1.2 97.88¬±0.3 97.69 ¬±0.3 72.96 ¬±1.1 69.95 ¬±1.6\\nFacebookVGRNN 52.08 ¬±0.2 53.57 ¬±0.1 93.19 ¬±0.2 92.15 ¬±0.3 59.63 ¬±0.2 59.12 ¬±0.3\\nDySAT 52.06 ¬±0.2 53.53 ¬±0.3 93.58 ¬±0.2 92.84 ¬±0.2 59.76 ¬±0.4 59.19 ¬±0.5\\nEvolveGCN-H 53.52 ¬±0.7 52.86 ¬±0.6 85.34 ¬±1.4 82.68 ¬±1.8 58.38 ¬±0.7 56.65 ¬±0.5\\nEvolveGCN-O 53.32 ¬±0.3 52.84 ¬±0.4 86.02 ¬±0.5 84.46 ¬±0.9 58.64 ¬±0.6 56.85 ¬±0.6\\nEuler 50.65 ¬±0.5 52.19 ¬±0.4 90.45 ¬±1.2 88.65 ¬±1.6 56.58 ¬±0.6 56.81 ¬±0.5\\nteneNCE 54.65¬±0.3 55.75 ¬±0.5 94.21 ¬±0.1 93.27 ¬±0.2 62.39 ¬±0.5 61.64 ¬±0.6\\nFigure 5. The density values of snapshot sequences over time. The\\nfigure shows the variation of each graph‚Äôs structure over time.\\n4.3. Tasks and Evaluation\\nIn this study, our focus is on the graph machine learning task\\nof dynamic link prediction. Specifically, we are interested in\\nestimating the conditional probability of observing an edge\\nbetween the nodes viandvjat the time step k, given the\\nhistorical data of the dynamic graph, i.e., P(Ak+1[i, j] =\\n1|G1, . . . , G k).\\nAs it is discussed in Section 3, we employ the teneNCE\\nmodel to encode the historical interactions of each node viinto\\na state vector Sk[i]that represents the temporal-topological\\ninformation of the node. We then transform the states into\\nan embedding space where conditional probabilities of edges\\nare defined as the sigmoid function acting on the inner prod-\\nucts of embeddings, i.e., P(Ak+1[i, j] = 1|G1, . . . , G k) =\\nœÉ(ÀúYk[i]‚ä§ÀúYk[j]), where ÀúYk=Linear (Sk). Our main goal is\\nto predict the positive edges in the graph in the future time\\nsteps. For this prediction, we consider a set of negative edges\\nto train the model based on Eq. 7 Positive edges are typically\\nselected randomly from future time steps, but we define four\\nFigure 6. Visualization of teneNCE loss minimization for the Enron\\ndataset. The figure shows the evolution of loss values on a logarithmic\\nscale throughout the training process.\\nstrategies to assess the memorization and generalization capa-\\nbilities of different models, following the evaluation procedure\\nproposed in [43]. For existing positive edges at time step k+1,\\nwe define two subsets: random positive , and historical positive\\nedges. As the name implies, random positive edges are sam-\\npled at random from the edge set at the prediction time step,\\ndenoted as Ek+1. However, historical positive edges refer to\\nlinks that are observed in both the historical data and at the\\nprediction time step k+ 1:\\n{(vi, vj) : (vi, vj)‚ààEk+1,(vi, vj)‚àà ‚à™k\\nl=1El}. (18)\\nFor the prediction of the absence of an edge at time step\\nk+ 1, we further classified negative links into two subsets,\\nnamely, random negative andhistorical negative edges. Simi-\\nlar to positive samples, random negative edges are randomly\\nsampled from links that are inactive at the prediction time\\nstep. Furthermore, historical negatives refer to those links that\\nwere active in the historical data but have been removed at the\\nprediction time step:\\n{(vi, vj) : (vi, vj)/‚ààEk+1,(vi, vj)‚àà ‚à™k\\nl=1El}. (19)\\n10Table 4. Ablation study of teneNCE objective. Starting with the baseline link prediction loss, we sequentially incorporate the reconstruction,\\nlocalNCE, and globalNCE losses at each stage. The figures indicate the contribution of each loss term on the model‚Äôs performance in dynamic\\nlink prediction.\\nLossEnron COLAB Facebook\\nAUC AP AUC AP AUC AP\\nLink Prediction 91.28 ¬±0.4 92.04 ¬±0.2 85.58 ¬±0.5 88.29 ¬±0.7 87.51 ¬±0.2 87.42 ¬±0.2\\nLink Prediction + Reconstruction 92.03 ¬±0.3 92.63 ¬±0.5 85.74 ¬±0.7 88.44 ¬±0.5 87.69 ¬±0.3 88.02 ¬±0.2\\nLink Prediction + Reconstruction + localNCE 92.75 ¬±0.5 92.85 ¬±0.5 87.81 ¬±0.2 89.48 ¬±0.3 90.36 ¬±0.1 89.67 ¬±0.3\\nteneNCE loss (Eq.7) 93.54 ¬±0.4 93.65 ¬±0.2 88.25 ¬±0.7 90.45 ¬±0.6 91.03 ¬±0.1 90.32 ¬±0.2\\nThe defined positive and negative edges form four evalu-\\nation subsets that we abbreviate as follows: Rand-Pos/Rand-\\nNeg, Rand-Pos/Hist-Neg, Hist-Pos/Rand-Neg, and Hist-\\nPos/Hist-Neg. The results for the Rand-Pos/Rand-Neg subset\\nare presented in Table 2, providing a general comparison across\\nvarious models. For a more detailed comparison, the results\\nfor the other three test setups are presented in Table 3. Our\\nintuition is that a model with ample memorization capacity\\ncan more accurately predict the Hist-Pos/Rand-Neg links. On\\nthe other hand, a model with relatively more generalization\\npower will predict the Rand-Pos/Hist-Neg edges with greater\\naccuracy.\\nTo measure the performance of the teneNCE model in com-\\nparison to baseline methods, we compute standard evaluation\\nmetrics commonly used in the dynamic link prediction tasks.\\nThese metrics include Average Precision (AP), AUC score\\n(Area Under the ROC Curve), and Mean Reciprocal Rank\\n(MRR). Detailed descriptions of these evaluation metrics are\\nprovided in the Appendix 6.5.\\n4.4. Performance Comparison\\nFor an overall assessment of the performance of various models\\non each dataset, we initially computed the evaluation metrics\\nfor the Rand-Pos/Rand-Neg test subset. The results are pro-\\nvided in Table 2 demonstrating enhancements over current\\nmethods across all datasets for all evaluation metrics. Note\\nthat all experiments are conducted five times, and the mean\\nand standard deviation of results are reported. We observe that\\nthe teneNCE model achieves gains of up to 6% MRR in the\\nCOLAB dataset when compared to the second-best baseline.\\nVGRNN is typically the second-best model, which shows the\\npower of V AEs in temporal modeling. We also conducted\\npaired t-test hypothesis testing to confirm the significance of\\nnumerical improvements [62]. The calculated p-values suggest\\nthat the results hold statistical significance. For example, in\\nthe Enron dataset, the p-values for comparing the AUC and\\nAP scores of teneNCE and VGRNN are 10‚àí4and1.5√ó10‚àí3,\\nrespectively, leading to the rejection of the null hypothesis in\\nthe paired t-test.\\nAdditionally, we assessed the performance of all models\\non the remaining three specific test setups as defined in 4.3.\\nThe results are presented in Table 3. Apart from the COLAB\\ndataset, where teneNCE‚Äôs performance is the second best after\\nEvolveGCN-O, teneNCE has outperformed other models indifferent configurations and datasets. The findings highlight\\nthe effectiveness of teneNCE in modeling the historical data\\nand generalizing to unseen new links, relative to other methods.\\n4.5. Ablation study\\nIn this subsection, we aim to assess the contribution of the ad-\\nditional reconstruction and infoNCE loss terms to the teneNCE\\nmodel‚Äôs performance. To achieve this, we initially train the\\nmodel using solely the baseline dynamic link prediction loss.\\nSubsequently, we iteratively augment the loss objective with\\nadditional reconstruction, local, and global infoNCE loss func-\\ntions. At each stage, we compare the performance of the\\ntrained model against the baseline model and previous config-\\nurations.\\nThe results are presented in Table 4. We can observe the\\nimpact of each loss term on the baseline and previous mea-\\nsurements. This study validates our motivations in formulating\\nthe training objective of teneNCE. We observe a monotonic\\nincrease in the model‚Äôs performance when we enhance the\\nbaseline link prediction loss with additional loss terms. In\\nsummary, we notice an average increase of 2.8in AUCs and\\n2.2in APs across the three datasets.\\nSpecifically, we initially quantify the benefit of jointly op-\\ntimizing prediction and reconstruction loss functions. The re-\\nconstruction loss function ensures that the model captures the\\nstructural information necessary for reconstructing the graph\\nfrom node states. On average, adding the reconstruction ob-\\njective to the baseline link prediction loss has a minor positive\\nimpact across datasets. However, the reconstruction loss aids\\nthe model in extracting representations that are beneficial in\\nnumerous applications, such as data imputation for connect-\\ning missing edges or densifying the input graph for improved\\ninformation flow in subsequent machine-learning tasks.\\nThe primary enhancement in the baseline model‚Äôs perfor-\\nmance can be attributed to the incorporation of both local and\\nglobal infoNCE losses. This indicates that the features ob-\\ntained via the contrastive predictive coding technique are also\\nadvantageous for prediction in the data domain.\\nFigure 6 illustrates the progression of loss values for the\\nteneNCE model throughout the training optimization process\\non the Enron dataset. During the initial iterations of training,\\nthe link prediction and reconstruction losses are minimized.\\nHowever, the infoNCE losses continue to decrease throughout\\nthe optimization process, underscoring the efficacy of self-\\n11supervised objectives in learning representations for temporal\\nnetworks.\\n5. Conclusion\\nIn this study, we tackled the representation learning problem\\nfor temporal networks. While temporal networks are typi-\\ncally characterized as contact sequences over continuous time,\\nin this work, we modeled the discretized versions of these\\nnetworks, also known as snapshot sequences. This strategy\\nbalances the trade-off between computational complexity and\\nprecise modeling.\\nWe introduced an architecture that models information prop-\\nagation in temporal networks along time-respecting paths. Our\\nproposed architecture produces node representations that en-\\ncode current and historical snapshots of the temporal network\\nwhile maximizing the mutual information between the learned\\nrepresentations and the future network features. This is accom-\\nplished by minimizing a multi-objective training loss function\\nthat combines prediction, reconstruction, and infoNCE loss\\nterms.\\nWe demonstrated that our proposed method outperforms\\nexisting models on discrete-time dynamic graphs, using the\\nEnron, COLAB, and Facebook datasets as benchmarks. This\\nsuggests that considering the complexity of temporal networks\\nin real-world scenarios, self-supervised training with multiple\\nobjectives can assist models in deriving enhanced representa-\\ntions that capture multiple aspects of the underlying data.\\nFor future research, we plan to apply the teneNCE archi-\\ntecture to supervised tasks over temporal networks, such as\\nrecommendation systems. Furthermore, we aim to use the\\nproposed method in problems related to optimizing dynamical\\nsystems over dynamic graphs. This includes tasks such as\\nplanning and determining the optimal control over evolving\\nnetworks, with practical applications such as those found in\\ndisease transmission networks.\\nReferences\\n[1]Uri Alon and Eran Yahav. On the bottleneck of graph neu-\\nral networks and its practical implications. arXiv preprint\\narXiv:2006.05205 , 2020. 2\\n[2]Danielle S. Bassetta Ann E. Sizemorea. Dynamic graph metrics:\\nTutorial, toolbox, and tale. NeuroImage , 2018. 9\\n[3]Adri√°n Arnaiz-Rodr√≠guez, Ahmed Begga, Francisco Escolano,\\nand Nuria Oliver. Diffwire: Inductive graph rewiring via the\\nlov\\\\‚Äôasz bound. arXiv preprint arXiv:2206.07369 , 2022. 2\\n[4]Dexiong Chen, Leslie O‚ÄôBray, and Karsten Borgwardt.\\nStructure-aware transformer for graph representation learning.\\nInInternational Conference on Machine Learning , pages 3469‚Äì\\n3489. PMLR, 2022. 2\\n[5]Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: fast learning\\nwith graph convolutional networks via importance sampling.\\narXiv preprint arXiv:1801.10247 , 2018. 2\\n[6]Jinyin Chen, Xueke Wang, and Xuanheng Xu. Gc-lstm: Graph\\nconvolution embedded lstm for dynamic network link prediction.\\nApplied Intelligence , pages 1‚Äì16, 2022. 2\\n[7]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geof-\\nfrey Hinton. A simple framework for contrastive learning ofvisual representations. In International conference on machine\\nlearning , pages 1597‚Äì1607. PMLR, 2020. 3\\n[8]Weilin Cong, Si Zhang, Jian Kang, Baichuan Yuan, Hao Wu,\\nXin Zhou, Hanghang Tong, and Mehrdad Mahdavi. Do we really\\nneed complicated model architectures for temporal networks?\\narXiv preprint arXiv:2302.11636 , 2023. 3, 6, 15\\n[9]Vijay Prakash Dwivedi and Xavier Bresson. A general-\\nization of transformer networks to graphs. arXiv preprint\\narXiv:2012.09699 , 2020. 2\\n[10] Vijay Prakash Dwivedi, Anh Tuan Luu, Thomas Laurent,\\nYoshua Bengio, and Xavier Bresson. Graph neural networks\\nwith learnable structural and positional representations. arXiv\\npreprint arXiv:2110.07875 , 2021. 2\\n[11] Hongchao Fang, Sicheng Wang, Meng Zhou, Jiayuan Ding,\\nand Pengtao Xie. Cert: Contrastive self-supervised learning\\nfor language understanding. arXiv preprint arXiv:2005.12766 ,\\n2020. 3\\n[12] Oleksandr Ferludin, Arno Eigenwillig, Martin Blais, Dustin\\nZelle, Jan Pfeifer, Alvaro Sanchez-Gonzalez, Wai Lok Sibon Li,\\nSami Abu-El-Haija, Peter Battaglia, Neslihan Bulut, Jonathan\\nHalcrow, Filipe Miguel Gon√ßalves de Almeida, Pedro Gonnet,\\nLiangze Jiang, Parth Kothari, Silvio Lattanzi, Andr√© Linhares,\\nBrandon Mayer, Vahab Mirrokni, John Palowitch, Mihir Parad-\\nkar, Jennifer She, Anton Tsitsulin, Kevin Villela, Lisa Wang,\\nDavid Wong, and Bryan Perozzi. TF-GNN: graph neural net-\\nworks in tensorflow. CoRR , abs/2207.03522, 2023. 1\\n[13] Matthias Fey and Jan Eric Lenssen. Fast graph repre-\\nsentation learning with pytorch geometric. arXiv preprint\\narXiv:1903.02428 , 2019. 1, 8\\n[14] L Gauvin, M G√©nois, M Karsai, M Kivel√§, T Takaguchi, E\\nValdano, and CL Vestergaard. Randomized reference models\\nfor temporal networks. arXiv preprint arXiv:1806.04032 , 2018.\\n9\\n[15] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol\\nVinyals, and George E Dahl. Neural message passing for quan-\\ntum chemistry. In International conference on machine learning ,\\npages 1263‚Äì1272. PMLR, 2017. 2\\n[16] Jean-Bastien Grill, Florian Strub, Florent Altch√©, Corentin\\nTallec, Pierre Richemond, Elena Buchatskaya, Carl Doer-\\nsch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh-\\nlaghi Azar, et al. Bootstrap your own latent-a new approach\\nto self-supervised learning. Advances in neural information\\nprocessing systems , 33:21271‚Äì21284, 2020. 3\\n[17] Beliz Gunel, Jingfei Du, Alexis Conneau, and Ves Stoyanov.\\nSupervised contrastive learning for pre-trained language model\\nfine-tuning. arXiv preprint arXiv:2011.01403 , 2020. 3\\n[18] Michael Gutmann and Aapo Hyv√§rinen. Noise-contrastive esti-\\nmation: A new estimation principle for unnormalized statistical\\nmodels. In Proceedings of the thirteenth international confer-\\nence on artificial intelligence and statistics , pages 297‚Äì304.\\nJMLR Workshop and Conference Proceedings, 2010. 3\\n[19] Ehsan Hajiramezanali, Arman Hasanzadeh, Krishna Narayanan,\\nNick Duffield, Mingyuan Zhou, and Xiaoning Qian. Varia-\\ntional graph recurrent neural networks. Advances in neural\\ninformation processing systems , 32, 2019. 2, 5, 9\\n[20] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive\\nrepresentation learning on large graphs. Advances in neural\\ninformation processing systems , 30, 2017. 2\\n[21] Tengda Han, Weidi Xie, and Andrew Zisserman. Video repre-\\nsentation learning by dense predictive coding. In Proceedings\\nof the IEEE/CVF International Conference on Computer Vision\\nWorkshops , pages 0‚Äì0, 2019. 3\\n12[22] Kaveh Hassani and Amir Hosein Khasahmadi. Contrastive\\nmulti-view representation learning on graphs. In International\\nconference on machine learning , pages 4116‚Äì4126. PMLR,\\n2020. 3\\n[23] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\\nGirshick. Momentum contrast for unsupervised visual represen-\\ntation learning. In Proceedings of the IEEE/CVF conference\\non computer vision and pattern recognition , pages 9729‚Äì9738,\\n2020. 3\\n[24] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon,\\nKaran Grewal, Phil Bachman, Adam Trischler, and Yoshua\\nBengio. Learning deep representations by mutual information\\nestimation and maximization. arXiv preprint arXiv:1808.06670 ,\\n2018. 3\\n[25] Petter Holme and Jari Saram√§ki. Temporal networks. Physics\\nreports , 519(3):97‚Äì125, 2012. 1, 5\\n[26] Seyed Mehran Kazemi, Rishab Goel, Kshitij Jain, Ivan Kobyzev,\\nAkshay Sethi, Peter Forsyth, and Pascal Poupart. Representation\\nlearning for dynamic graphs: A survey. The Journal of Machine\\nLearning Research , 21(1):2648‚Äì2720, 2020. 3, 4, 5\\n[27] David Kempe, Jon Kleinberg, and Amit Kumar. Connectivity\\nand inference problems for temporal networks. In Proceed-\\nings of the thirty-second annual ACM symposium on Theory of\\ncomputing , pages 504‚Äì513, 2000. 5\\n[28] Isaiah J. King and H. Howie Huang. Euler: Detecting network\\nlateral movement via scalable temporal link prediction. Network\\nand Distributed Systems Security (NDSS) Symposium , 2023. 2,\\n9\\n[29] Thomas N Kipf and Max Welling. Semi-supervised classi-\\nfication with graph convolutional networks. arXiv preprint\\narXiv:1609.02907 , 2016. 2, 5, 15\\n[30] Thomas N Kipf and Max Welling. Variational graph auto-\\nencoders. arXiv preprint arXiv:1611.07308 , 2016. 2, 7\\n[31] Devin Kreuzer, Dominique Beaini, Will Hamilton, Vincent L√©-\\ntourneau, and Prudencio Tossou. Rethinking graph transformers\\nwith spectral attention. Advances in Neural Information Pro-\\ncessing Systems , 34:21618‚Äì21629, 2021. 2\\n[32] Junhyun Lee, Inyeop Lee, and Jaewoo Kang. Self-attention\\ngraph pooling. In International conference on machine learning ,\\npages 3734‚Äì3743. pmlr, 2019. 6\\n[33] Pan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec.\\nDistance encoding: Design provably more powerful neural net-\\nworks for graph representation learning. Advances in Neural\\nInformation Processing Systems , 33:4465‚Äì4478, 2020. 2\\n[34] Derek Lim, Joshua Robinson, Lingxiao Zhao, Tess Smidt, Su-\\nvrit Sra, Haggai Maron, and Stefanie Jegelka. Sign and basis\\ninvariant networks for spectral graph representation learning.\\narXiv preprint arXiv:2202.13013 , 2022. 2\\n[35] Yixin Liu, Ming Jin, Shirui Pan, Chuan Zhou, Yu Zheng, Feng\\nXia, and S Yu Philip. Graph self-supervised learning: A sur-\\nvey. IEEE Transactions on Knowledge and Data Engineering ,\\n35(6):5879‚Äì5900, 2022. 3\\n[36] Gr√©goire Mialon, Dexiong Chen, Margot Selosse, and Julien\\nMairal. Graphit: Encoding graph structure in transformers.\\narXiv preprint arXiv:2106.05667 , 2021. 2\\n[37] Daisuke Niizumi, Daiki Takeuchi, Yasunori Ohishi, Noboru\\nHarada, and Kunio Kashino. Byol for audio: Self-\\nsupervisrecently, state-of-the-art results in unsupervised graph\\nrepresenta- tion learning have been achieved by leveraging a con-\\ntrastive learn- ing loss that contrasts samples from a distribution\\nthat contains dependencies of interest and the distribution that\\ndoes not.ed learning for general-purpose audio representation.In2021 International Joint Conference on Neural Networks\\n(IJCNN) , pages 1‚Äì8. IEEE, 2021. 3\\n[38] Amirhossein Nouranizadeh, Mohammadjavad Matinkia, Mo-\\nhammad Rahmati, and Reza Safabakhsh. Maximum entropy\\nweighted independent set pooling for graph neural networks.\\narXiv preprint arXiv:2107.01410 , 2021. 6\\n[39] Kenta Oono and Taiji Suzuki. Graph neural networks expo-\\nnentially lose expressive power for node classification. arXiv\\npreprint arXiv:1905.10947 , 2019. 2\\n[40] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representa-\\ntion learning with contrastive predictive coding. arXiv preprint\\narXiv:1807.03748 , 2018. 3, 5, 6, 7, 8, 15\\n[41] Aldo Pareja, Giacomo Domeniconi, Jie Chen, Tengfei Ma, Toy-\\notaro Suzumura, Hiroki Kanezashi, Tim Kaler, Tao Schardl, and\\nCharles Leiserson. Evolvegcn: Evolving graph convolutional\\nnetworks for dynamic graphs. In Proceedings of the AAAI con-\\nference on artificial intelligence , pages 5363‚Äì5370, 2020. 2,\\n9\\n[42] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan,\\nEdward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison,\\nLuca Antiga, and Adam Lerer. Automatic differentiation in\\npytorch. Advances in neural information processing systems ,\\n2017. 8\\n[43] Farimah Poursafaei, Shenyang Huang, Kellin Pelrine, and Rei-\\nhaneh Rabbany. Towards better evaluation for dynamic link\\nprediction. Advances in Neural Information Processing Sys-\\ntems, 35:32928‚Äì32941, 2022. 10\\n[44] Carey E Priebe, John M Conroy, David J Marchette, and\\nYoungser Park. Scan statistics on enron graphs. Computa-\\ntional & Mathematical Organization Theory , 11:229‚Äì247, 2005.\\n8\\n[45] Mahmudur Rahman and Mohammad Al Hasan. Link prediction\\nin dynamic networks using graphlet. In Machine Learning\\nand Knowledge Discovery in Databases: European Conference,\\nECML PKDD 2016, Riva del Garda, Italy, September 19-23,\\n2016, Proceedings, Part I 16 , pages 394‚Äì409. Springer, 2016. 8\\n[46] Ladislav Ramp√°≈°ek, Michael Galkin, Vijay Prakash Dwivedi,\\nAnh Tuan Luu, Guy Wolf, and Dominique Beaini. Recipe for\\na general, powerful, scalable graph transformer. Advances in\\nNeural Information Processing Systems , 35:14501‚Äì14515, 2022.\\n2, 5\\n[47] Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide\\nEynard, Federico Monti, and Michael Bronstein. Temporal\\ngraph networks for deep learning on dynamic graphs. arXiv\\npreprint arXiv:2006.10637 , 2020. 3\\n[48] Aaqib Saeed, David Grangier, and Neil Zeghidour. Contrastive\\nlearning of general-purpose audio representations. In ICASSP\\n2021-2021 IEEE International Conference on Acoustics, Speech\\nand Signal Processing (ICASSP) , pages 3875‚Äì3879. IEEE, 2021.\\n3\\n[49] Aravind Sankar, Yanhong Wu, Liang Gou, Wei Zhang, and Hao\\nYang. Dysat: Deep neural representation learning on dynamic\\ngraphs via self-attention networks. In Proceedings of the 13th\\ninternational conference on web search and data mining , pages\\n519‚Äì527, 2020. 3, 9\\n[50] Youngjoo Seo, Micha√´l Defferrard, Pierre Vandergheynst, and\\nXavier Bresson. Structured sequence modeling with graph\\nconvolutional recurrent networks. In Neural Information Pro-\\ncessing: 25th International Conference, ICONIP 2018, Siem\\nReap, Cambodia, December 13-16, 2018, Proceedings, Part I\\n25, pages 362‚Äì373. Springer, 2018. 2, 5\\n13[51] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-\\nKin Wong, and Wang-chun Woo. Convolutional lstm network:\\nA machine learning approach for precipitation nowcasting. Ad-\\nvances in neural information processing systems , 28, 2015. 2\\n[52] Yu Shi, Shuxin Zheng, Guolin Ke, Yifei Shen, Jiacheng You,\\nJiyan He, Shengjie Luo, Chang Liu, Di He, and Tie-Yan Liu.\\nBenchmarking graphormer on large-scale molecular modeling\\ndatasets. arXiv preprint arXiv:2203.04810 , 2022. 2\\n[53] Janne Spijkervet and John Ashley Burgoyne. Con-\\ntrastive learning of musical representations. arXiv preprint\\narXiv:2103.09410 , 2021. 3\\n[54] Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang.\\nInfograph: Unsupervised and semi-supervised graph-level repre-\\nsentation learning via mutual information maximization. arXiv\\npreprint arXiv:1908.01000 , 2019. 3\\n[55] Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamber-\\nlain, Xiaowen Dong, and Michael M Bronstein. Understanding\\nover-squashing and bottlenecks on graphs via curvature. arXiv\\npreprint arXiv:2111.14522 , 2021. 2\\n[56] Michael Tschannen, Josip Djolonga, Paul K Rubenstein,\\nSylvain Gelly, and Mario Lucic. On mutual information\\nmaximization for representation learning. arXiv preprint\\narXiv:1907.13625 , 2019. 3\\n[57] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana\\nRomero, Pietro Lio, Yoshua Bengio, et al. Graph attention\\nnetworks. stat, 1050(20):10‚Äì48550, 2017. 2\\n[58] Petar Veli Àáckovi ¬¥c, William Fedus, William L Hamilton, Pietro\\nLi√≤, Yoshua Bengio, and R Devon Hjelm. Deep graph infomax.\\narXiv preprint arXiv:1809.10341 , 2018. 3\\n[59] Bimal Viswanath, Alan Mislove, Meeyoung Cha, and Krishna P\\nGummadi. On the evolution of user interaction in facebook.\\nInProceedings of the 2nd ACM workshop on Online social\\nnetworks , pages 37‚Äì42, 2009. 9\\n[60] Haorui Wang, Haoteng Yin, Muhan Zhang, and Pan Li. Equiv-\\nariant and stable positional encoding for more powerful graph\\nneural networks. arXiv preprint arXiv:2203.00199 , 2022. 2\\n[61] Minjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mufei Li, Xiang\\nSong, Jinjing Zhou, Chao Ma, Lingfan Yu, Yu Gai, Tianjun\\nXiao, Tong He, George Karypis, Jinyang Li, and Zheng Zhang.\\nDeep graph library: A graph-centric, highly-performant package\\nfor graph neural networks. arXiv preprint arXiv:1909.01315 ,\\n2019. 1\\n[62] Neil A Weiss. Introductory statistics . Pearson Education, Inc,\\n2005. 11\\n[63] Zhanghao Wu, Paras Jain, Matthew Wright, Azalia Mirhoseini,\\nJoseph E Gonzalez, and Ion Stoica. Representing long-range\\ncontext for graph neural networks with global attention. Ad-\\nvances in Neural Information Processing Systems , 34:13266‚Äì\\n13279, 2021. 2\\n[64] Yaochen Xie, Zhao Xu, Jingtun Zhang, Zhengyang Wang, and\\nShuiwang Ji. Self-supervised learning of graph neural networks:\\nA unified review. IEEE transactions on pattern analysis and\\nmachine intelligence , 45(2):2412‚Äì2429, 2022. 3\\n[65] Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and\\nKannan Achan. Inductive representation learning on temporal\\ngraphs. arXiv preprint arXiv:2002.07962 , 2020. 3\\n[66] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.\\nHow powerful are graph neural networks? arXiv preprint\\narXiv:1810.00826 , 2018. 2\\n[67] Yuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng Zhang, Wei\\nWu, and Weiran Xu. Consert: A contrastive framework forself-supervised sentence representation transfer. arXiv preprint\\narXiv:2105.11741 , 2021. 3\\n[68] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng,\\nGuolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do trans-\\nformers really perform badly for graph representation? Ad-\\nvances in Neural Information Processing Systems , 34:28877‚Äì\\n28888, 2021. 2\\n[69] Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will\\nHamilton, and Jure Leskovec. Hierarchical graph representa-\\ntion learning with differentiable pooling. Advances in neural\\ninformation processing systems , 31, 2018. 6\\n146. Appendix\\n6.1. Notations\\nTable 5 provides a comprehensive list of the symbols and\\nnotations used throughout this paper. Each entry includes a\\nbrief description to clarify its meaning and context within the\\nstudy.\\n6.2. Graphical Graph Recurrent Unit\\nThe graphical gated recurrent unit (GGRU), which is used to\\nimplement the function Update in Eq.2, is defined as:\\nRk=œÉ(GConv reset,z(Zk,Ak) +GConv reset,s(Sk,Ak)),\\nUk=œÉ(GConv update,z (Zk,Ak) +GConv update,s (Sk,Ak)),\\nÀúSk= tanh( GConv cand,z(Zk,Ak) +Rk‚äôGConv cand,s(Sk,Ak)),\\nSk+1= (1‚àíUk)‚äôÀúSk+Uk‚äôSk.\\n(20)\\nIn the above equation, GConv (.)denotes the graph convolu-\\ntional layer introduced in [29], and ‚äôdenotes the element-wise\\nmultiplication. Ak‚ààRn√ónis the adjacency matrix of the tem-\\nporal network at time step k.Zk‚ààRn√ódencis the graph‚Äôs\\nembedding matrix, and Sk‚ààRn√ódstateis the node state matrix\\nat time step k.Rk,Uk,ÀúSk,Sk+1‚ààRn√ódstate, are reset gates,\\nupdate gates, candidate states and updated states, respectively.\\n6.3. Time Embedding Function\\nWe used the time embedding function introduced in [8]. The\\nfunction is defined as TimeEncoder(t) = cos( tœâ), which uti-\\nlizes features œâ={Œ±‚àí(i‚àí1)\\nŒ≤}dtime\\ni=1to encode each timestamps\\ninto a d-dimensional vector. More specifically, the authors\\nfirst map each tto a vector with monotonically exponen-\\ntially decreasing values tœâ‚àà(0, t]among the feature di-\\nmension, then use the cosine function to project all values\\ntocos(tœâ)‚àà[‚àí1,+1]. The selection of Œ±, Œ≤ depends on the\\nscale of the maximum timestamp tmaxwe wish to encode.\\nIn order to distinguish all timestamps, we have to make sure\\ntmax√óŒ±‚àí(i‚àí1)\\nŒ≤‚Üí0asi‚Üídto distinguish all timestamps.\\nIn practice, d= 100 andŒ±=Œ≤=‚àö\\ndworks well for all\\ndatasets. Notice that œâis fixed and will not be updated during\\ntraining.\\n6.4. Contrastive Predictive Coding\\nThe original contrastive predictive coding technique was in-\\ntroduced in the well-known work [40] for learning context\\nrepresentation of a sequential data set Y={y1, . . . , y M}.\\nThe authors defined the infoNCE loss function to maximize\\nthe mutual information of the context representation ctwith\\nthe future features of the input data {yt+1, . . . , y M}, where\\nt= 1, . . . , M .\\nGiven a set of Msamples Y={y1, . . . , y m}, with one pos-\\nitive sample drawn from the conditional distribution p(yt+k|ct)\\nandM‚àí1negative samples drawn from the distribution\\np(yt+k), the loss function is defined as:\\nLM=‚àíEM[logfk(yt+k, ct)P\\nyj‚ààYfk(yj, ct)], (21)Table 5. Table of Notations\\nNotation Description\\nG Temporal network\\nV Set of nodes of G\\nE Set of events of G\\nn Number of nodes in G, n =|V|\\nm Number of events in G, m =|E|\\nG Sequence of graph snapshots over time\\nN Number of time steps in snapshot sequence G\\nGk Attributed snapshot graph at time step k\\nEk Set of edges of Gk\\neij Binary variable indicating the existence of an edge between\\nnodes viandvj\\nAk Adjacency matrix of Gk\\nXk Node feature matrix of Gk\\nEk Edge feature matrix of Gk\\nZk Structural embedding matrix for Gk\\nSk Node state representation matrix of Gk\\nsk Graph state representation vector of Gk\\nSk[i] Representation vector of ithnode in Gk\\nÀÜZ(k)\\nlLocal predictive encoding matrix at time step kfor future\\ntime step l\\nÀÜz(k)\\nlGlobal predictive encoding vector at time step kfor future\\ntime step l\\nÀÜAk Reconstructed adjacency matrix for time step k\\nÀúAk+1 Predicted adjacency matrix for time step k+ 1\\nÀÜZ Set of local predictive encodings\\nÀÜz Set of global predictive encodings\\nÀÜA set of reconstructed adjacency matrices\\nÀúA set of predicted adjacency matrices\\nl Dummy index for time\\nL teneNCE objective function\\nLpred Prediction loss function\\nLrecons Reconstruction loss function\\nLcpc Contrastive predictive coding (CPC) loss function\\nŒ± Weight hyperparameter for Lrecons\\nŒ≤ Weight hyperparameter for Lcpc\\nPneg(i, k) Probability distribution of local negative sample indices for\\nnodeiat time k\\nPneg(l) Probability distribution of global negative sample indices\\nat time step l\\nneg(i,l) Set of local negative samples for node iat time l\\nneg(l) Set of global negative samples at time l\\nC Temporal correlation coefficients\\nCi Temporal correlation coefficient for node i\\ndV Dimension of node feature vector\\ndE Dimension of edge feature vector\\ndenc Dimension of encoded node feature vector\\ndstate Dimension of node representation vector\\nwhere fk(yt+k, ct) = exp( z‚ä§\\nt+kWkct)assigns a real positive\\nscore to the pair of context representation ctand future fea-\\nture embedding zt+k, using a bilinear map followed by the\\nexponential function.\\nThe infoNCE loss function can be viewed as the categorical\\ncross-entropy loss of classifying the correct sample drawn from\\np(yt+k|ct)from negative samples drawn from p(yt+k). The\\nauthors show that minimizing the infoNCE loss is equivalent\\nto maximizing the mutual information between the context\\nrepresentation and future embeddings.\\n6.5. Link Prediction Evaluation Metrics\\nAverage Precision (AP score) : It measures the precision of\\na binary classification model across multiple recall decision\\n15thresholds. The AP score is determined by:\\nAPAk+1=X\\nb(Recb‚àíRecb‚àí1)Precb (22)\\nWhere RecbandPrecbare the recall and precision at the bth\\ndecision threshold, according to Eq. 23. Rec0= 0,Prec 0= 1\\nandIdenotes an indicator function.\\nRecAk+1=P\\ni,jI(ÀúAk+1[i, j] = 1) ¬∑I(Ak+1[i, j] = 1)P\\ni,jI(Ak+1[i, j] = 1),\\nPrecAk+1=P\\ni,jI(ÀúAk+1[i, j] = 1) ¬∑I(Ak+1[i, j] = 1)\\nP\\ni,jI(ÀúAk+1[i, j] = 1)\\n(23)\\nAUC score : This metric is calculated as the area under the\\nreceiver operating characteristic (ROC) curve. The ROC curve\\nplots the true positive rate (TPR) against the false positive rate\\n(FPR), and the area under this curve is a single scalar summary\\nof the classifier‚Äôs performance. It is computed as follows:\\nAUC =Z1\\n0TPR\\x00\\nFPR‚àí1(t)\\x01\\ndt (24)\\nwhere the TPR is calculated similarly to Recin Eq. 23 and the\\nFPR is given by:\\nFPRAk+1=P\\ni,jI(ÀúAk+1[i, j] = 1) ¬∑I(Ak+1[i, j] = 0)P\\ni,jI(Ak+1[i, j] = 0)\\n(25)\\nMean Reciprocal Rank (MRR) : This metric evaluates the\\nranking performance of a model by averaging the reciprocal\\nranks of the correct items in the predicted score list:\\nMRRAk+1=1\\n|Ek+1|X\\n(i,j)‚ààEk+11\\nrankij(26)\\nwhere rankijis the rank of the true link eijin the sorted list\\nof predicted scores.\\n16',\n",
       " 'Phantom: Untargeted Poisoning Attacks on Semi-Supervised\\nLearning (Full Version)*\\nJonathan Knauer\\njonathan.knauer@stud.tu-darmstadt.de\\nTechnical University of Darmstadt\\nGermanyPhillip Rieger\\nphillip.rieger@trust.tu-darmstadt.de\\nTechnical University of Darmstadt\\nGermany\\nHossein Fereidooni\\nhossein.fereidooni@kobil.com\\nKOBIL GmbH\\nGermanyAhmad-Reza Sadeghi\\nahmad.sadeghi@trust.tu-darmstadt.de\\nTechnical University of Darmstadt\\nGermany\\nABSTRACT\\nDeep Neural Networks (DNNs) can handle increasingly complex\\ntasks, albeit they require rapidly expanding training datasets. Col-\\nlecting data from platforms with user-generated content, such as\\nsocial networks, has significantly eased the acquisition of large\\ndatasets for training DNNs. Despite these advancements, the man-\\nual labeling process remains a substantial challenge in terms of\\nboth time and cost. In response, Semi-Supervised Learning (SSL) ap-\\nproaches have emerged, where only a small fraction of the dataset\\nneeds to be labeled, leaving the majority unlabeled. However, lever-\\naging data from untrusted sources like social networks also creates\\nnew security risks, as potential attackers can easily inject manipu-\\nlated samples. Previous research on the security of SSL primarily\\nfocused on injecting backdoors into trained models, while less atten-\\ntion was given to the more challenging untargeted poisoning attacks.\\nIn this paper, we introduce Phantom , the first untargeted poisoning\\nattack in SSL that disrupts the training process by injecting a small\\nnumber of manipulated images into the unlabeled dataset. Unlike\\nexisting attacks, our approach only requires adding few manipu-\\nlated samples, such as posting images on social networks, without\\nthe need to control the victim. Phantom causes SSL algorithms to\\noverlook the actual images‚Äô pixels and to rely only on maliciously\\ncrafted patterns that Phantom superimposed on the real images.\\nWe show Phantom ‚Äôs effectiveness for six different datasets and 3\\nreal-world social-media platforms (Facebook, Instagram, Pinterest).\\nAlready small fractions of manipulated samples (e.g., 5%) reduce the\\naccuracy of the resulting model by 10%, with higher percentages\\nleading to a performance comparable to a naive classifier. Our find-\\nings demonstrate the threat of poisoning user-generated content\\nplatforms, rendering them unsuitable for SSL in specific tasks.\\nKEYWORDS\\nDeep Neural Network, Semi-Supervised-Learning, Poisoning\\nPublication rights licensed to ACM. ACM acknowledges that this contribution was\\nauthored or co-authored by an employee, contractor or affiliate of a national govern-\\nment. As such, the Government retains a nonexclusive, royalty-free right to publish or\\nreproduce this article, or to allow others to do so, for Government purposes only.\\nCCS ‚Äô24, October 14‚Äì18, 2024, Salt Lake City, UT, USA\\n¬©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 979-8-4007-0636-3/24/10. . . $15.00\\nhttps://doi.org/10.1145/3658644.3690369ACM Reference Format:\\nJonathan Knauer, Phillip Rieger, Hossein Fereidooni, and Ahmad-Reza\\nSadeghi. 2024. Phantom: Untargeted Poisoning Attacks on Semi-Supervised\\nLearning (Full Version)*. In Proceedings of the 2024 ACM SIGSAC Conference\\non Computer and Communications Security (CCS ‚Äô24), October 14‚Äì18, 2024,\\nSalt Lake City, UT, USA. ACM, New York, NY, USA, 17 pages. https://doi.\\norg/10.1145/3658644.3690369\\n1 INTRODUCTION\\nDeep Neural Networks (DNNs) have been rapidly evolving with\\nspectacular abilities, such as DALL-E [46], DeepFakes [ 39], and the\\nchatbot ChatGPT [ 40], to name a few. Simultaneously, DNNs are in-\\ncreasingly used also for safety-critical domains, such as self-driving\\ncars, necessitating exceptionally high reliability and precision. How-\\never, with the growing complexity of these tasks, also the size of\\nthe DNNs and the number of trainable parameters grow signifi-\\ncantly. To effectively train these DNN models, the required amount\\nof training data is growing rapidly. One possible source of large\\namounts of training data is the Internet, where large amounts of\\nuser-generated content are available on platforms such as Insta-\\ngram (image data), Reddit (text data), and YouTube (video data).\\nThus, the data that were uploaded on these platforms became a\\nhigh-value asset, as different actors, such as companies, can use\\nthese data to train DNNs.\\nHowever, labeling the data remains a significant practical challenge\\nin terms of cost and time. To address this issue, recent research has\\nfocused on developing algorithms that either avoid the need for\\nlabeling entirely or minimize the number of samples that require\\nmanual annotation.\\nNotably, recent news about the achievements of DNNs focus on\\nLarge Language Models (LLMs) that are trained in early training\\nphases on text sequences in a self-supervised manner. However,\\nthese models still require labeled training data for later supervised\\ntraining phases [ 41,42,45]. Furthermore, other applications, such\\nas image recognition or those developed by stakeholders such as\\nsmall companies or academic institutions [ 18], typically do not rely\\nexclusively on self-supervised training methods. This shows the\\nongoing need for labeled training data, despite significant progress\\nin self-supervised learning approaches.\\n*¬©Knauer 2024. This is the author‚Äôs version of the work. It is posted here for your\\npersonal use. Not for redistribution. The definitive version was published in CCS2024,\\nhttps://doi.org/10.1145/3658644.3690369.\"arXiv:2409.01470v1  [cs.CR]  2 Sep 2024CCS ‚Äô24, October 14‚Äì18, 2024, Salt Lake City, UT, USA Jonathan Knauer, Phillip Rieger, Hossein Fereidooni, and Ahmad-Reza Sadeghi\\nSemi-Supervised Learning.\\nSemi-Supervised Learning (SSL) algorithms avoid the problem\\nof time-intensive labeling the obtained large dataset by minimizing\\nthe number of labeled samples that are required for the training al-\\ngorithm. SSL algorithms utilize two datasets, a small labeled dataset\\nand a large unlabeled dataset. The model is initially trained only on\\nthe labeled dataset, while the current model is simultaneously used\\nto predict the labels for samples from the unlabeled dataset. Once\\nthe model predicts the labels for some samples with high confidence,\\nthese samples are used with these guessed labels for further training.\\nCurrent state-of-the-art algorithms for SSL include MixMatch [ 5],\\nUDA [ 62], and FixMatch [ 55]. These algorithms are highly effective\\nat leveraging only a small number of labeled samples to achieve a\\nperformance comparable to fully supervised methods. For exam-\\nple, on the CIFAR-10 benchmark dataset, labeling a only 40 out of\\n50 000 samples is sufficient to achieve decent accuracy levels. SSL\\nalgorithms allow leveraging large unlabeled datasets, e.g., down-\\nloaded from social media platforms, while avoiding the costly and\\ntime-consuming process of manual labeling.\\nAttacks on SSL. However, despite the benefits of leveraging large\\ndatasets from platforms with user-generated content, using un-\\ntrusted data also creates new attack vectors. If a party like a com-\\npany utilizes the user-generated data of such a platform, an attacker\\ncan easily upload maliciously crafted images to the platform to\\nmanipulate DNNs that are trained on this data. Recent research\\nproposed attacks that insert manipulated samples into the unla-\\nbeled training data to inject a backdoor into the trained model. The\\nbackdoor causes the trained DNN to misbehave in a well-defined\\nmanner, such as classifying selected images as belonging to a differ-\\nent class chosen by the attacker [ 7,12]. Previous work developed\\nuntargeted poisoning attacks that reduce the trained model‚Äôs ac-\\ncuracy by manipulating the labeled dataset [ 19,32]. However, to\\nmanipulate the labeled samples, a strong control over the attacked\\nvictim is required. Thus, this would also enable the adversary to\\ninterfere directly with the training process.\\nWhile existing untargeted poisoning attacks assume access to the\\nlabeled dataset, to the best of our knowledge, no untargeted attack\\nhas been proposed that manipulates the unlabeled data to disrupt\\nthe training process. Compared to backdoor attacks that manipulate\\nthe unlabeled dataset [ 7,12], untargeted poisoning attacks pose\\nsignificant challenges. In backdoor attacks, the DNN model can\\nachieve both objectives simultaneously, a decent performance on\\nthe benign data and injecting the backdoor. Backdoor attacks only\\nadd an additional, undesired function to the DNN. Consequently,\\nthe poisoned data only need to provide examples for this additional\\nfunction such that if the DNN is trained on these data, it will also\\nlearn this function. In contrast, untargeted poisoning attacks, which\\naim to compromise the performance on benign test data, present a\\nmore challenging scenario as here both above-mentioned objectives\\nwill be in direct conflict with each other. Thus, one of the challeng-\\ning aspects when designing untargeted attacks is the necessity for\\na relatively small number of samples to overrule the impact of a\\nlarge number of benign samples.\\nAnother challenge is the lack of information about the SSL algo-\\nrithm, DNN architecture, or used model weights. As the manipu-\\nlated samples must be crafted and uploaded to the platform with\\nuser-generated content before the actual training starts, the attack\\n+  PV ‚ãÖ = (1 ‚ÄìPV)‚ãÖ\\nOriginal Image Poisoning Pattern Manipulated ImageFigure 1: Example for poisoned image construction as a com-\\nbination of the unpoisoned image and the Poisoning Pattern,\\nweighted by the Pattern Visibility (PV) parameter, here 0.1.\\nhas to be executed blindly. Moreover, generating inaccurate labels\\nfor the manipulated data remains another challenge since the labels\\nfor the unlabeled dataset are obtained by the SSL algorithm during\\nthe training.\\nOur goal and contributions. In this paper, we propose Phantom ,\\nthe first blind untargeted poisoning attack on SSL that disrupts\\nthe training by manipulating the unlabeled dataset. By adding a\\nsmall number of manipulated samples to the unlabeled dataset,\\nsuch as uploading manipulated images to a social network, Phan-\\ntomsignificantly reduces the performance of the trained model.\\nTo effectively disturb the SSL algorithm, we incorporate a hidden\\npattern (a Phantom ) into the manipulated samples, causing the al-\\ngorithms to overlook the actual content (e.g., original pixels) and\\nrely only on hidden patterns that we superimposed on the original\\ncontent for guessing the unlabeled samples‚Äô labels. This causes the\\nSSL algorithm to mislabel the manipulated samples and, in turn,\\nnegatively impacts the training and prevents the utilization of be-\\nnign samples. An example, using images as a demonstration, is\\nillustrated in Fig. 1. Here, the DNN uses a part of the poisoning\\npattern, such as the tiger or cat, to predict the label while primarily\\nusing the snake for loss calculation and parameter adaption.\\nIt is worth noting that even with a small percentage of manipulated\\ndata, such as 5%, the accuracy of the resulting model can be reduced\\nby 10%. We demonstrate that untargeted poisoning attacks can be\\nmade without making any assumptions, such as strong control or\\nthe ability to monitor the victim. Thereby, we show that untargeted\\npoisoning attacks pose a practical and realistic threat.\\nIn summary, our contributions include:\\n‚Ä¢We propose Phantom , the first untargeted poisoning attack that\\ndisrupts the training of SSL algorithms and reduces the model‚Äôs\\nutility by only adding few samples to the unlabeled dataset with-\\nout requiring any knowledge of the SSL algorithm, DNN struc-\\nture, or hyperparameters. In contrast to backdoor attacks, where\\nthe benign and backdoor objectives can be achieved in paral-\\nlel,Phantom overrules the benign majority of samples to reduce\\nthe model‚Äôs utility. Phantom poisons sources for unlabeled data,\\nmaking them unusable for SSL training in certain tasks (Sect. 4.2).\\n‚Ä¢We design a scheme for manipulating samples that takes advan-\\ntage of the model‚Äôs overfitting on the labeled dataset in the initial\\nstages of training. This causes the SSL algorithms to ignore the\\noriginal sample and entirely rely on superimposed patterns, thus\\nmislabeling the sample (Sect. 4).\\n‚Ä¢We conduct an extensive evaluation and demonstrate that al-\\nready 5% of manipulated data can lead to a significant reduction\\nin the performance of the trained model by 10%. We effectively\\nshow the efficacy of Phantom across a range of state-of-the-art\\nSSL algorithms, datasets, and parameter configurations and itsPhantom: Untargeted Poisoning Attacks on Semi-Supervised Learning (Full Version)* CCS ‚Äô24, October 14‚Äì18, 2024, Salt Lake City, UT, USA\\nresistance to data augmentation techniques and countermeasures\\nagainst adversarial examples. Additionally, we thoroughly exam-\\nine the attack‚Äôs impact on the model‚Äôs behavior in-depth using\\nexplainable AI techniques (Sect. 5).\\n‚Ä¢To demonstrate the risks posed by the Phantom attack and the\\nneed for robust SSL algorithms, we show its effectiveness through\\na real-world case study. We utilize data extracted from three major\\nsocial networks ‚Äî Facebook, Instagram, and Pinterest ‚Äî which\\nhost user-generated content and are plausible sources for SSL\\ntraining data (Sect. 5.6).\\n2 BACKGROUND\\nIn the following sections, we provide an overview of the necessary\\nbackground for the rest of the paper. In Sect. 2.1, we introduce\\nseveral state-of-the-art SSL algorithms (MixMatch [ 5], UDA [ 62],\\nand FixMatch [ 55]), which will be used in this paper. These algo-\\nrithms are chosen for their popularity and effectiveness in various\\nSSL tasks. Later, we describe and categorize different attacks that\\ntarget DNN models during the training phase (Sect. 2.2). Further, in\\nApp. A, we describe an overview of data augmentation techniques\\nthat are commonly employed in the field of SSL.\\n2.1 Semi-Supervised Learning\\nSemi-Supervised Learning (SSL) is a class of algorithms that utilize\\na combination of a small labeled dataset, denoted as ùí≥, and a much\\nlarger unlabeled dataset, denoted as ùí∞, where|ùí≥|‚â™|ùí∞|, to train\\na Machine Learning model for classification tasks, such as a Deep\\nNeural Network (DNN). In contrast, non-SSL algorithms require the\\nentire dataset to be labeled, which can be costly in terms of time and\\nexpertise. SSL algorithms only require a small portion of the dataset\\nto be labeled. The labeled part ùí≥is used for the initial training of\\nthe DNN and to make educated guesses about the labels of the\\nunlabeled data during training. In this work, we exemplary focus\\non the algorithms MixMatch [ 5], UDA [ 62], and FixMatch [ 55], as\\nthese have been shown to achieve the best performance for image\\napplications [7].\\nMixMatch generates for each unlabeled sample ùë¢‚ààùí∞, i.e., an\\nimage, multiple augmented versions and averages the predictions of\\nthe current model to obtain a guess for the probability distribution\\nfor the label of ùë¢. For a sample ùë¢‚ààùí∞the probability distribution\\nùëùùë¢is represented as a vector. For each possible label (or category)\\nùëê‚ààùíû, the respective element ùëùùë¢,ùëêindicates the probability that ùë¢\\nbelongs toùëê. This guessed probability distribution is then sharpened\\nbased on a temperature parameter ùëáas:\\nSharpen(ùëùùë¢,ùëá)ùëñ=ùëù1/ùëá\\nùë¢,ùëñ\\n√ç\\nùëê‚ààùíûùëù1/ùëá\\nùë¢,ùëê(1)\\nThe loss ‚Ñí=‚Ñíùí≥+ùúÜùí∞‚Ñíùí∞is then defined as the sum of the binary-\\ncross-entropy loss ‚Ñíùí≥for the labeled training data ùí≥, and the\\ndistance ‚Ñíùí∞between the predictions for the augmented unlabeled\\nsamplesùë¢‚ààùí∞to the sharpened guessed label. The unlabeled data‚Äôs\\nloss is in addition weighted by a hyperparameter ùúÜùí∞[5].\\nUDA\\nalso generates multiple augmented versions of each unlabeled\\nsample to guess the correct label. However, UDA uses different typesof augmentations, one for guessing the label and another for calcu-\\nlating the loss. To guess the label, UDA applies weak augmentation\\nto the sample, while for the loss calculation, it uses strong augmen-\\ntation, causing stronger deformations of the images. The loss for\\nthe unlabeled example is based on the prediction of the strongly\\naugmented version. The algorithm optimizes the parameters of the\\nDNN to make the prediction of the strong augmented sample match\\nthe label that was predicted for the weakly augmented versions.\\nAdditionally, UDA only uses samples where the confidence for the\\nprediction is higher than a configurable threshold [62].\\nFixMatch\\nbuilds on UDA by also using different augmentation types for\\nguessing the label and for minimizing the loss. However, unlike\\nUDA, FixMatch does not sharpen the probability distribution of the\\nguessed label. Instead, it selects the label with the highest score and\\nuses a one-hot encoding of this label [55].\\n2.2 Poisoning Attacks\\nAttacks that alter the training data of a DNN are often referred to as\\npoisoning attacks [ 7,16,17,19,67]. Such attacks can be classified\\ninto the following categories:\\nTargeted Poisoning:\\nIn this type of attack, the attacker aims to manipulate the DNN‚Äôs\\npredictions towards a specific target class for certain samples. Thus,\\ntargeted attacks inject a hidden function within the DNN. Some\\nliterature [ 7] subdivides this attack type into two subcategories. The\\nfirst subcategory involves attacks that cause the DNN to misclassify\\nonly a fixed set of samples. Attacks in the second subcategory cause\\nthe DNN to misclassify all samples with a specific trigger, such as\\nthe presence of a certain colored patch or object in an image.\\nUntargeted Poisoning:\\nThis category of attacks aims to degrade the overall performance\\nof the DNN on all input samples by reducing its accuracy to that of a\\nsimple or naive model. This is different from targeted attacks, where\\nthe goal is to make the DNN misclassify only specific samples. The\\nchallenge in untargeted attacks is that their objective is to negatively\\nimpact the performance of the DNN on all data, including non-\\npoisoned samples. In targeted attacks, the DNN can have a high\\naccuracy on non-poisoned data and still misclassify the targeted\\nsamples. Thus, in targeted attacks both, the objective from benign\\ndata, to learn recognizing images, as well as the objective of the\\ntargeted attacks, of misclassifying the backdoor samples, can be\\nachieved. In comparison, in untargeted attacks, either the attack\\nis successful or the model achieves a decent performance on the\\nbenign training data. Therefore, the manipulated samples here need\\nto counteract the benign data.\\n3 PROBLEM SETTING\\nIn the following, we describe the considered system (Sect. 3.1),\\ndefine the threat model (Sect. 3.2), and describe the challenges that\\nuntargeted attacks face in SSL (Sect. 3.3).\\n3.1 System Setting\\nWe consider a system where anonymous clients upload user-generated\\ncontent, such as images, to an internet platform, denoted as ùí´. Real-\\nworld examples of such platforms are social media websites, suchCCS ‚Äô24, October 14‚Äì18, 2024, Salt Lake City, UT, USA Jonathan Knauer, Phillip Rieger, Hossein Fereidooni, and Ahmad-Reza Sadeghi\\nùí≥ùí≥ùí∞ùí∞\\nùí±ùí±ictimùí´ùí´latform\\n(a) Without Attack\\nùí≥ùí≥ùí∞ùí∞\\nùí±ùí±ictimùí´ùí´latform\\n(b)Phantom\\nFigure 2: Overview of the considered system where the ùí±ictim\\ndownloads data from a ùí´latform with user-generated content\\nto obtain an unlabeled dataset ( ùí∞) that is used with a small\\nlabeled dataset ( ùí≥) to train a DNN\\nas Instagram, Facebook, or Pinterest. The platform ùí´may perform\\nmodifications on the content, such as compressing images to reduce\\ntheir size. Afterward, the victim ùí±accesses the publicly available\\ncontent on ùí´to obtain a large dataset, referred to as ùí∞, for train-\\ning a DNN using a SSL algorithm of their choice. In addition to\\nùí∞,ùí±also uses a small dataset, referred to as ùí≥, that has been la-\\nbeled manually. Since the process of labeling is time-consuming\\nand requires human effort, it is assumed that |ùí≥|‚â™|ùí∞|. Figure 2\\nillustrates the considered system.\\n3.2 Threat Model\\nWe consider a weak adversary ùíú, whose only capability is the abil-\\nity to upload data to a public platform ùí´hosting user-generated\\ncontent. ùíúaims to prevent the victim ùí±from training a model on\\ndata that is uploaded to ùí´. The purposes of such attacks might in-\\nclude disrupting ùí±, for instance, if ùíúandùí±are competing entities,\\nor preventing parties from utilizing the data that is available on ùí´.\\nIn the latter case, ùíúcould have commercial goals, e.g., revealing\\nthe poisoned samples only after ùí±pays a ransom. ùíú‚Äôs objective\\ncan be formulated as follows:\\nAdversary‚Äôs Objective is to significantly1decrease the utility of\\nDNN models that are trained in a semi-supervised manner on ùí∞.\\nAsùí´is a public platform with user-generated content, it is suffi-\\ncient for ùíúto control at least one client, such as a mobile phone or\\na computer. We, therefore, assume that ùíúcan use such a client to\\nupload content to ùí´. Also, we assume that ùíúcan make educated\\nguesses for some labeled samples from ùí≥. It is important to note\\nthatùíúdoes not need to guess the samples in ùí≥with high preci-\\nsion. If ùíúhas a set ùí≥ùíúof samples where it suspects that some of\\nthem are part of ùí≥, it is sufficient if there is some overlap, thus\\n1There exists no precise threshold which performance drop can be considered to\\nbe significant as this depends on the domain. In safety-critical applications, such as\\nself driving cars, even a drop by 1% already can make the model unusable, while in\\napplications recognizing pictures on smartphones 1% can be tolerated. We elaborate\\non this in Sect. 6.1.ùí≥ùíú‚à©ùí≥‚â†‚àÖ. Given the widely available and well-known datasets,\\ne.g., for image recognition and the fact that ùíúcan try many samples,\\nit is reasonable to assume that at least some of the guessed samples\\ninùí≥ùíúare correct (see Sect. 5.4).\\nHowever, we do not make any assumptions about the training pro-\\ncess.ùíúcan only augment the unlabeled dataset but lacks knowledge\\nof the used algorithms, DNN architecture, or training specifics. Nei-\\nther can ùíúaffect these parameters nor can ùíúmonitor the training\\nprocess. Thus ùíúperforms a black-box attack. Additionally, ùíúis\\nunable to manipulate the labeled samples in ùí≥, delete samples from\\nùí∞, or modify non-poisoned samples. The only action that ùíúcan\\ntake is adding samples to ùí∞.\\nNotably, we do not assume that the adversary ùíúhas any insight into\\nthe active training process of the victim ùí±. Consequently, ùíúcannot\\ndirectly verify the success of the attack and must instead estimate\\nits effectiveness through prior simulations, similar to existing work\\non adversarial deep learning [ 31,52,53]. This aspect will be further\\ndiscussed in Sect. 6.2.\\n3.3 Challenges\\nIn SSL, it is not practical to make strong assumptions about the\\nadversary ùíú. Instead, ùíúhas to operate blindly and perform the\\nattack, i.e., upload the images to a platform ùí´, without knowing\\nany details about the training (SSL algorithm, DNN architecture,\\ntraining parameters, etc.). In contrast to existing work [ 19,32], we\\nconsider an adversary ùíúthat does not have these details. Instead,\\nùíúaims to negatively affect the performance of the trained model\\nonly by uploading samples to a publicly accessible platform that\\nis designed for hosting user-generated content. This open threat\\nscenario poses a number of significant challenges that we describe\\nin the following:\\nC1 - Prevent utilization of non-manipulated samples:\\nGiven the assumption that ùíúcannot manipulate the samples\\nuploaded by benign users to ùí´but can only add additional samples\\nto the platform, one of the main challenges is to manipulate a small\\nnumber of samples in such a way that they negatively affect the\\nability of the training algorithm to benefit from the large number of\\nremaining benign samples. As we show in Sect. 5.4, simply prevent-\\ning the manipulated samples from contributing positively to the\\ntraining is not sufficient, as there are still enough benign samples\\nfor the DNN model to learn from.\\nC2 - Stealthiness: Even for small percentages of manipulated\\nimages, e.g., 10%, users will notice if many media files on social\\nnetworks show a certain pattern or artifacts that are caused by the\\npoisoning. Additionally, if humans can detect these manipulations,\\nit would be possible to gather a sufficient number of manipulated\\nsamples to develop an automated filtering system and thus defeat\\nthe attack. Therefore, another important challenge is to stealthily\\nmanipulate the samples to evade detection by humans while still\\neffectively disturbing the SSL training algorithm.\\nC3 - Causing SSL Algorithm to mislabel samples: Our attack\\nmakes use of wrongly labeled samples to disturb the training. How-\\never, the adversary ùíúdoes not have access to the labeled dataset\\nand the labels for the unlabeled dataset are automatically deter-\\nmined during the training. Therefore, another challenge is to make\\nthe SSL algorithm to craft wrong labels for the manipulated samplesPhantom: Untargeted Poisoning Attacks on Semi-Supervised Learning (Full Version)* CCS ‚Äô24, October 14‚Äì18, 2024, Salt Lake City, UT, USA\\n(a) PV = 0%\\n (b) PV = 10%\\n (c) PV = 25%\\n(d) PV = 50%\\n (e) PV = 75%\\n (f) PV = 100%\\nFigure 3: Effect of different Patter Visibility (PV) rates to\\nobtain a manipulated image for the ImageNet dataset.\\nwithout changing the sample (i.e., images) too much, as this would\\naffect the ability of the DNN model to learn the wrong behavior.\\nC4 - Effectiveness Against Benign Majority: Given the vast\\namount of content posted daily on social media, it is unrealistic to\\nassume that ùíúcan control a majority of the unlabeled samples by\\nuploading a large number of manipulated media files. Therefore,\\none of the key challenges for an untargeted poisoning attack is to\\neffectively decrease the utility of a trained model by only manipu-\\nlating a small fraction of the samples. For backdoor attacks, both,\\nthe benign objective of achieving a decent performance and the\\nattacker‚Äôs objective to inject a backdoor, can be fulfilled simulta-\\nneously. However, for untargeted poisoning attacks, the attacker‚Äôs\\nobjective of preventing the model from achieving a decent per-\\nformance is in contradiction with the benign objective. Only one\\nobjective can be achieved at the same time. Therefore, a key chal-\\nlenge is, how to succeed against the majority of benign samples by\\nusing only a minority of samples.\\n4PHANTOM\\nIn the following, we introduce the Phantom attack. First, we describe\\nthe motivation behind the attack. Then, we first provide a high-level\\noverview of the Phantom attack (Sect. 4.2 followed by a detailed\\nexplanation of the individual components (Sect. 4.3).\\n4.1 Motivation\\nThe primary motivation for Phantom ‚Äôs mechanism stems from\\nthe observation that SSL algorithms tend to rely heavily on the\\nlabeled data during early training epochs. Already during the early\\nepochs of the training, the DNN achieves high accuracy on the\\nlabeled dataset ùí≥(see App. C). This dependency on ùí≥makes the\\nDNN model susceptible to overfitting on the samples in the labeled\\ndataset ùí≥, which can lead to misclassification of unseen data. This\\nphenomenon becomes particularly relevant when an input ùë¢‚ààùí∞\\ncontains artifacts that resemble an image from the labeled dataset\\nùí≥with labelùëô. In such cases, the model‚Äôs increased sensitivity to\\ndetect even parts of labeled samples causes it to predict label ùëô\\nfor a new sample ùë¢, regardless of the actual class of ùë¢. Thus, the\\noverfitting of the DNN results in a high sensitivity to recognizinglabeled images, even when they are present only as weak shadows\\nwithin the manipulated image.\\nAnother consequence of the DNN‚Äôs overfitting on ùí≥is its tendency\\nto predict labels with high confidence for samples that are similar\\nto those in ùí≥. As a result, these similar samples are incorporated\\ninto the training process during early epochs, while the remaining\\nsamples are only gradually added and utilized later on.\\n4.2 High-Level Overview of Phantom\\nThe fundamental principle of the Phantom attack is to induce the\\nSSL algorithm to generate incorrect labels for samples in the un-\\nlabeled dataset. During training, the SSL algorithm‚Äôs erroneous\\nguesses for these labels cause the parameter optimizer to make in-\\ncorrect adjustments to the DNN‚Äôs parameters, thereby reducing the\\nmodel‚Äôs utility rather than improving it. To cause SSL algorithms to\\nmislabel the unlabeled samples, Phantom injects a poisoning pattern\\ninto the original samples. Once the manipulated samples are crafted,\\nthey are uploaded to a platform with user-generated content, such\\nas a social network. When the victim later crawls this platform for\\ncontent and downloads media, such as images, being posted there,\\nthe manipulated samples are unknowingly incorporated into their\\nunlabeled dataset.\\nThe manipulated samples consist of a combination of the original\\nsample and an individual poisoning pattern. This construction is\\nexemplified for images in Fig. 1. The resulting image comprises two\\nparts or layers that are placed on top of each other, the original\\nimage and the shadow, referred to as the poisoning pattern.\\nCausing Wrong Label Guesses. To influence the label guesses of\\nthe SSL algorithm with the poisoning pattern, Phantom exploits the\\ntendency of SSL algorithms to overfit on samples from the labeled\\ndataset ùí≥. This overfitting makes the model highly sensitive to\\nrecognizing samples from the labeled dataset, even when they are\\nbarely visible in the image. To leverage this effect, we construct the\\npoisoning pattern using images that resemble those in the labeled\\ndataset. This causes the SSL algorithm to use the barely visible\\npoisoning pattern as the basis for its label prediction, addressing\\nchallenge C3. By constructing the manipulated sample primarily\\nfrom a regular sample rather than solely utilizing the poisoned\\npattern, the resulting sample appears also less suspicious and barely\\nvisible to human observers, as demonstrated in Fig. 1. This approach\\naddresses C2.\\nEffect on the Training Process. Using a barely visible poisoning\\npattern and primarily the original image ensures that the original\\nsample is mainly used for training. During the training process, the\\noptimization algorithm that adapts the DNN‚Äôs weights calculates\\nthe loss and gradients based on the entire image, including its\\noriginal content. Since the original sample is more prominent, it is\\nused to train the DNN model with the wrongly guessed label. The\\noptimizer then adjusts every parameter of the DNN to increase the\\nprobability of the guessed (wrong) label. Thus, the parameters are\\nalso changed based on the image‚Äôs primary content to maximize\\nthe probability of the current label.\\nAs described in Sect. 4.1, samples similar to those in the labeled\\ndataset ùí≥, such as those containing parts of them, are incorporated\\ninto the training process already during the early phases. This early\\nintegration of manipulated samples affects the predicted labels forCCS ‚Äô24, October 14‚Äì18, 2024, Salt Lake City, UT, USA Jonathan Knauer, Phillip Rieger, Hossein Fereidooni, and Ahmad-Reza Sadeghi\\nbenign data, affecting the algorithm‚Äôs ability to benefit from these\\ndata, addressing C4. The combination of the manipulated sample\\nand the mistakenly guessed label for benign data contradicts the\\ndesired benign behavior, disrupting the training process and hinder-\\ning the DNN model‚Äôs ability to effectively utilize non-manipulated\\ndata, addressing C1.\\nExample. Fig. 1 shows exemplary the process of crafting a poi-\\nsoning image. The resulting image consists of an original image of\\na snake as well as a weak shadow of a cat and a tiger serving as\\nthe poisoning pattern. Since the superimposing of the poisoning\\npattern is only very weak, it is invisible to human eyes. However,\\nthe overfitting of the DNN model causes the DNN to focus on the\\ncat and the tiger, such that this weak shadow is sufficient to classify\\nthis image as a cat (or tiger). Especially in the early phases, the DNN\\nmodel is not capable of recognizing arbitrary objects, but rather it\\nhas learned to recognize specific images from the labeled dataset.\\nAs a result, the manipulated image is either labeled as a cat or as\\na tiger. However, once the wrong label is assigned to this image,\\nthe training process will consider the whole image. Therefore, the\\nDNN will be trained to recognize the snake image as a cat/tiger, as\\nthe poisoning pattern is barely visible in comparison to the snake.\\n4.3 Implementation\\nEach manipulated sample ùëöis created as overlay from a regular\\nsampleùëüand the poisoning pattern ùëù. To construct the poisoning\\npattern in the image domain, Phantom selects for each sample ùëütwo\\nimagesùëù1,ùëù2with respective labels ùëô1,ùëô2from the set ùí≥ùíúof images\\nthat are suspected to be part of the labeled dataset ùí≥. Notably,\\nwe leverage two samples ùëù1,ùëù2‚ààùí≥ùíúfor crafting the poisoning\\npattern. Besides causing further distraction, this also increases the\\nprobability that one of the guesses is actually part of ùí≥. We noticed\\nduring our experiments that choosing the samples ùëù1andùëù2such\\nthat they belong to different classes ( ùëô1‚â†ùëô2) increases the attack‚Äôs\\neffectiveness, as the ambiguous labeling causes further distraction.\\nAdditionally, targeting different classes increases the probability\\nof triggering the model‚Äôs overfitting, leading to incorrect label\\npredictions for the current sample.\\nTo obtain the poisoning pattern ùëùfor image applications, ùëù1and\\nùëù2are first cropped at the left and right side of the image, ensuring\\ntheir width is half that of ùëü. Then, if the concatenation of these\\ncropped images is denoted as ùëù, the colorùëêof the pixel on position\\nùë•,ùë¶of the manipulated image ùëöis then given by:\\nùëöùë•,ùë¶,ùëê=(1‚àíPV)¬∑ùëüùë•,ùë¶,ùëê+PV¬∑ùëùùë•,ùë¶,ùëê (2)\\nThe Pattern Visibility (PV) parameter controls the ratio that com-\\nbines the regular sample ùëüand the poisoned pattern ùëù. Increasing\\nthe value makes it easier for the DNN model to spot the fractions\\nof the labeled images, while a low PV value ensures that the ma-\\nnipulated images remain inconspicuous.\\nThe impact of varying PV values on the manipulated image is\\nshown in Fig. 3, its impact on Phantom ‚Äôs effectiveness is evaluated\\nin Sect. 5.3. For a PV of 100% (depicted in Fig. 3f) only the poisoning\\npattern is visible. As the PV value decreases from 100% to lower\\nvalues, the image appears increasingly less suspicious to the hu-\\nman eye, making it increasingly difficult to discern the pattern (asTable 1: Overview of the used datasets\\nDataset#train\\nsamples#test\\nsamplesModel#DNN\\nparameters\\nCIFAR-10 50 000 10 000 Wide ResNet-28-2 1 469 642\\nMNIST 60 000 10 000 Wide ResNet-28-2 1 469 354\\nSVHN 604 388 26 032 Wide ResNet-28-2 1 469 642\\nGTSRB 26 640 12 630 Wide ResNet-28-2 1 473 899\\nImageNet 1 281 167 50 000 ResNet-50 25 557 032\\nSTL-10 105 000 8 000 Wide ResNet-37-2 5 933 770\\ndemonstrated in Fig. 3b). However, even small PV values are capa-\\nble of causing a decline in the model‚Äôs performance. For instance, a\\nPV value of 10%, as seen in Fig. 3b, is sufficient to result in a 10%\\ndecrease in the accuracy of the trained model (see Sect. 5.2).\\nA challenge faced by the adversary is obtaining knowledge about\\nthe actual labeled samples. As discussed in Sect. 7, existing literature\\nassumes unrestricted access to the labeled dataset, the ability to\\nmodify it, or even knowledge of the model‚Äôs parameters, which\\nmay not be realistic. In the case of Phantom ,ùíúis required to guess\\nsamples ùí≥ùíúthat are present in the labeled dataset ùí≥. However,\\nonly some overlap between both is necessary, i.e., ùí≥ùíú‚à©ùí≥‚â†‚àÖ.\\nIn order to incorporate two labeled samples into a single sample,\\nwe employ a strategy of cropping the central portion of the labeled\\nsamples. This approach has shown to be effective as the central\\nregion of an image typically contains the most salient features. It is\\nworth noting, however, that ùíúcan manually inspect the poisoned\\npatterns before uploading the manipulated samples. As such, if ùíú\\ndetermines that the most relevant part of a labeled sample is not\\nlocated in the center, ùíúcan easily adapt the cropping accordingly.\\nAlso, if ùíúconsiders some images to be too suspicious, e.g., if the\\npoisoned pattern can be spotted in the image, it can replace the\\nrespective poisoned image with another version, e.g., by using a\\ndifferent benign image as the original image.\\n5 EVALUATION\\nIn previous sections, we introduced the Phantom attack, which\\ndisrupts SSL training through the addition of manipulated samples\\nto the unlabeled dataset. In the following, we show the effectiveness\\nofPhantom on six different datasets and conduct a real-world case\\nstudy involving three prominent social media platforms, which are\\nsuitable data sources for SSL. In addition, we analyze in App. D\\nchanges in the behavior of the model that are caused by Phantom\\nusing explainable AI, specifically saliency maps for benign and\\npoisoned input samples.\\n5.1 Experimental Setup\\nDatasets:\\nWe employed six different datasets that are regularly utilized to\\nevaluate DNNs, particularly research that aims to address SSL from\\na security perspective [ 7,12,17,19,63,64]. They are summarized\\nin Tab. 1.\\nCIFAR-10 [29] includes 50 000 training images of 32 √ó32 pixels,\\nfeaturing objects and animals from 10 distinct categories. The\\ndataset is widely utilized as a benchmark for both SSL and DNN\\nresearch [7, 12, 17, 19, 63, 64].Phantom: Untargeted Poisoning Attacks on Semi-Supervised Learning (Full Version)* CCS ‚Äô24, October 14‚Äì18, 2024, Salt Lake City, UT, USA\\nSTL-10 is tailored explicitly for SSL and consists of 100 000 unlabeled\\nsamples, 5 000 labeled images, and 8 000 test images. All images are\\ncolored and have a resolution of 96 √ó96 pixels [11].\\nSVHN comprises of 604 388 training images and 26 032 test images,\\nwith a resolution of 32 √ó32 pixels [37].\\nMNIST consists of 60 000 training images showing handwritten\\ndigits, all of which are grayscale and having a resolution of 28 √ó28\\npixels [30].\\nGTSRB (German Traffic Sign Benchmark) consists of 26 640 train\\nand 12 630 test images of different traffic signs. Each of the 43 classes\\nrepresents one traffic sign, while the images themselves show them\\nat different daytimes, perspectives, and environments (urban and\\ncountryside) [ 25]. The dataset was accessed via the official PyTorch\\nintegration [2].\\nImageNet is a large, high-resolution image dataset. We use the im-\\nages from the ImageNet Large Scale Visual Recognition Challenge\\n(ILSVRC), which consists of 1,3 million training images and 50 000\\ntest images from 1 000 different categories. The size of the images\\nis scaled to 256√ó256 due to the large variety of image sizes [3].\\nParameters:\\nThe parameters used in this study align with the setup of Zhang\\net al. [66], where the models are trained for 256 epochs, which is\\nsufficient for convergence. Only for ImageNet we used a larger\\nnumber of epochs due to the more complex training task and larger\\nnumber of trainable parameters. Since it is assumed that ùíúcan only\\nupload additional images to ùí´but cannot affect the training process,\\nno other parameters are changed from their default values. A cosine\\ndecayed learning rate is used, initialized to 0.03, a temperature ùëáof\\n0.5, and the loss weight for the unsupervised loss is set to 1.0 for\\nFixMatch and UDA, and 100.0 for MixMatch. A confidence threshold\\nof 0.8 is used for UDA and MixMatch. The simulation of adding\\npoisoned images to the unlabeled dataset is done by replacing a\\ncertain fraction of the unlabeled samples with manipulated images.\\nThe ratio of manipulated unlabeled samples to the total number of\\nunlabeled samples is referred to as the Poisoned Data Rate (PDR).\\nTo choose the number of labeled samples, we considered the number\\nof classes and ensured that the benign setting achieved reasonable\\naccuracy. For CIFAR-10 , the labeled dataset consisted of 40 images\\nfor UDA and FixMatch. Only for MixMatch 100 labeled examples\\nwere necessary to achieve a decent performance. For STL-10 we\\nused 100 from 105 000 for UDA and FixMatch and 250 for MixMatch.\\nFor SVHN, we utilized 40 labeled images out of 604 388 images in\\ntotal, from GTSRB 129 from 26 640, for MNIST 10 out of 60 000,\\nand for ImageNet 100 000 out of 1 281 167 images. Aligned with\\nestablished work on SSL [ 66], we randomly selected the labeled\\nimages.\\nExperiment Environment:\\nWe evaluate our attack on three state-of-the-art SSL algorithms:\\nMixMatch, UDA, and FixMatch as described in Sect. 2.1. To provide\\na comprehensive overview of the performance of the Phantom at-\\ntack, we evaluate the overall performance (Sect. 5.2) and perform\\nthe case study (Sect. 5.6) for all three algorithms, while the evalua-\\ntion of aspects that involve a large number of experiments, such\\nas the performance for various PDR and PV rates, consider only\\none algorithm respectively. For the implementation, we used the\\nNumPy [ 23] and PyTorch [ 1] frameworks. Further, we used theTable 2: Basic Performance of Phantom for CIFAR-10.\\nMixMatch UDA FixMatch\\nBenign Scenario 74.75% 79.64% 89.10%\\nOnly Labeled Dataset 32.23% 26.00% 26.00%\\nEmpty Images (PDR=50%) 25.52% 70.86% 74.04%\\nRemoving 50% samples 63.25% 50.93% 75.17%\\nPhantom (PDR=5%, PV=0.1) 64.85% 68.71% 83.68%\\nPhantom (PDR=50%, PV=0.6) 23.77% 46.85% 36.11%\\nTable 3: Basic Performance of Phantom for STL-10.\\nMixMatch UDA FixMatch\\nBenign Scenario 63.20% 78.90% 66.83%\\nOnly Labeled Dataset 39.53% 29.01% 29.01%\\nEmpty Images (PDR=50%) 38.10% 72.85% 64.63%\\nRemoving 50% samples 60.91% 68.11% 60.03%\\nPhantom (PDR=5%, PV=0.1) 60.34% 74.16% 61.63%\\nPhantom (PDR=50%, PV=0.6) 34.40% 49.16% 44.33%\\nSSL implementation of Zhang et al. [66]. The experiments were\\nconducted on three servers, one running Debian with 1 TB memory,\\nan AMD EPYC 7742 CPU with 64 physical cores, and 4 NVIDIA\\nQuadro RTX 8000, another server running Ubuntu with an Intel\\nXeon 5318S CPU having 24 cores, 512GB main memory, and 2\\nNvidia RTX A6000. Due to the high computational effort for Ima-\\ngeNet, we used here another server with 2 AMD EPYC 7773x CPUs,\\n2 TB memory, and 3 NVIDIA H100 GPUs. To ensure consistent\\nresults, all experiments for CIFAR-10 , and SVHN were conducted\\non the first server, experiments for the STL-10 , GTSRB, and MNIST\\ndatasets on the second server, and all ImageNet experiments were\\nconducted on the third server, avoiding any minor inconsistencies\\ndue to different library versions.\\n5.2 Performance of Phantom\\nThe tables 2 and 3 show the effectiveness of the Phantom attack\\non three state-of-the-art SSL algorithms (MixMatch, UDA, and\\nFixMatch) for CIFAR-10 and STL-10 . As Tab. 2 demonstrates, a\\nPDR of only 5% is sufficient for Phantom to decrease the accuracy\\nby 10% for MixMatch and UDA for CIFAR-10 and 4% on average for\\nSTL-10 . With higher PDR values, the accuracy reduction becomes\\neven more significant, with a drop to 23.77% for MixMatch, 46.85%\\nfor UDA, and 36.11% for FixMatch. The table also includes two\\nsimple baselines. For one baseline (denoted as \"Empty Images\" in\\nTab. 2) half of the images are replaced by empty (black) images. For\\nthe other baseline (denoted as \"Removing 50% samples\" in Tab. 2)\\nthe respective samples are removed. The results of these baselines\\ndemonstrate the superior effectiveness of the Phantom attack. For\\nexample, for FixMatch, using empty images or removing half of\\nall images reduces the accuracy only to 74% and 75% respectively,\\nwhile Phantom reduces the accuracy to 36%, which is very close to\\nthe accuracy when only the labeled dataset is used (26%). Further\\ncomparisons with baselines are provided in Sect. 5.4.\\nTo assess the stability of the obtained results, we conducted multiple\\niterations of the experiments for each Semi-Supervised Learning\\n(SSL) algorithm employing our method ( Phantom ) with varying\\nseeds. We repeated the experiments 5 times with different seedsCCS ‚Äô24, October 14‚Äì18, 2024, Salt Lake City, UT, USA Jonathan Knauer, Phillip Rieger, Hossein Fereidooni, and Ahmad-Reza Sadeghi\\n20% 40% 60% 80%\\nPoisoned Data Rate0%50%100%AccuracyNaive Class.\\nPV=10%\\nPV=20%No Attack\\nPV=50%\\nPV=100%\\nFigure 4: Impact of the Poisoned Data Rate (PDR) for the\\nPhantom attack for different Pattern Visibilities (PV) in com-\\nparison to the accuracy without attack (No Attack) and of a\\nnaive classifier (Naive Class.).\\nand compared the resulting accuracies. Notably, we found that the\\naccuracies exhibited a standard deviation of 2.6% on average across\\nthe different SSL algorithms, being negligible in comparison to the\\nnotable performance drop of almost 10% induced by Phantom and\\nunderscoring its robustness.\\nTab. 4 shows the effectiveness of the Phantom attack on various\\ndatasets that are commonly used as benchmarks for SSL algorithms\\nand attacks. As shown, Phantom effectively undermines the learning\\nprocess for all six datasets, demonstrating its general applicability.\\n5.3 Varying Attack Parameters\\nThe figures 4 and 5 show the effectiveness of the Phantom attack for\\ndifferent PDRs and PVs using MixMatch. As both figures illustrate, a\\nPDR of 5% and a PV of 0.10 suffice to reduce the performance by 10%\\ncompared to the benign scenario. Both figures also show that the\\nattack becomes more effective when using higher PDRs or higher\\nPVs. As visible in Fig. 4, by using a PDR of 50%, the attack turns\\nthe trained model into a naive one, which always predicts the same\\nclass regardless of the input. It is worth noting that, if we use only\\nthe labeled dataset, the accuracy is still 32.23%. Therefore, without\\nproviding any incorrect labels but causing the SSL algorithm to\\nmislabel the samples itself, the Phantom attack causes the resulting\\nmodel to perform worse than not using any unlabeled sample.\\n2Due to the complex task and large number of classes, for ImageNet we use the top-5\\naccuracy. Thus, a sample is considered to be predicted correctly, if the true class is\\namong the five classes with the highest predicted probabilities. Therefore, Phantom\\nmust not only make sure that the correct class does not receive the highest probability\\nbut that it is not even among the 5 classes with the highest predicted probabilities.\\nTable 4: Effectiveness of Phantom for different datasets using\\nUDA and a PDR of 5% and PV of 0.1.\\nDataset Benign Attack\\nSTL-10 78.90% 74.16%\\nMNIST 99.20% 92.97%\\nSVHN 80.78% 64.87%\\nGTSRB 83.20% 77.78%\\nImageNet268.08% 57.05%\\nCIFAR-10 79.64% 68.71%\\n0.2 0.4 0.6 0.8 1.0\\nPattern Visibility0%50%100%AccuracyNaive Class.\\nPDR= 5%\\nPDR=10%No Attack\\nPDR=30%\\nPDR=50%Figure 5: Impact of the Pattern Visibility for the Phantom\\nattack for different Poisoned Data Rates (PDR) in comparison\\nto the accuracy without attack (No Attack) and of a naive\\nclassifier (Naive Class.).\\n5.4 Ablation Study\\nTo the best of our knowledge, Phantom is the first untargeted poi-\\nsoning attack that does not require any control over the victim.\\nTherefore, no similar attacks exist for comparison. To evaluate the\\nadvantage of Phantom , we therefore defined a number of baselines\\nand also adapted two backdoor attacks [ 7,38] to utilize them for\\nan untargeted poisoning attack. Table 5 presents the performance\\nof various alternative options for the Phantom attack, in which 10%\\nof the data can be manipulated or removed. As shown in the table,\\nthe baseline performance of UDA, without any attack, is 79.64%\\naccuracy. However, when utilizing only the labeled dataset, the\\naccuracy drops to 26.00%.\\nFor two straightforward baseline attacks, we prevent the SSL train-\\ning from taking any advantage from the manipulated samples. A\\nnaive strategy would be simply not to upload these samples to the\\nplatform with user-generated content, to reduce the size of the un-\\nlabeled dataset. In this case, only the benign samples can be utilized\\nfor training. A second straightforward baseline attack to prevent\\nthe training from utilizing the manipulated images is uploading\\nempty images, e.g., showing only black pixels. However, the results\\nin Tab. 5 show that only preventing the model from utilizing the\\nmanipulated examples, therefore, not uploading them, is not an\\neffective method for undermining the model. Additionally, using\\nempty (black) images as an alternative option also demonstrates no\\nsignificant impact on performance. However, it is worth noting that\\nthis strategy seems to have a regularization effect on the training\\nprocess, thereby focusing the guessed probability distribution on\\nthe correct label and thus improving the model‚Äôs accuracy. These\\nexperiments, therefore, showed that it is not sufficient to prevent\\nthe utilization of the manipulated samples but the utilization of the\\nnon-manipulated samples must be prevented for a successful attack\\n(see challenge C1).\\nIn recent years, several backdoor attacks against SSL have been\\ndeveloped that involve poisoning the unlabeled dataset. For a more\\nsophisticated baseline, we adapted these approaches to make the\\nmodel overfit and thus reduce the model‚Äôs performance. The orig-\\ninal backdoor attacks utilize interpolation to establish a link be-\\ntween samples that are intended to be mislabeled and the target\\nsamples [ 7,12]. To adapt this attack and perform an untargeted\\npoisoning attack, we inserted a colored shape on the images, caus-\\ning the DNN to overfit and focus solely on the presence of thisPhantom: Untargeted Poisoning Attacks on Semi-Supervised Learning (Full Version)* CCS ‚Äô24, October 14‚Äì18, 2024, Salt Lake City, UT, USA\\npattern. We then utilize interpolation to create a bridge between\\nthis pattern and samples from other classes. The rationale is that\\nthe model overfits only on the pattern and unlearns other class-\\nspecific properties. Aligned with the results of Carlini [ 7], we use\\nthe density function ùëù(ùë•)=1.5‚àíùë•for interpolation. Results using\\nother density functions are provided in App. E.\\nWe evaluated various baseline attacks and compared their effective-\\nness against Phantom . First, we implemented Carlini‚Äôs backdoor-\\ning approach [ 7], which interpolates between samples of different\\nclasses to misclassify samples, effectively creating a bridge between\\nthe two classes (referred to as Sample-Interpolation in Table 5). The\\nrationale is that this interpolation might cause the SSL algorithm to\\nincorrectly label samples, hindering its ability to accurately learn\\nthe characteristics of each class. Additionally to the attack of Carlini,\\nwe adapted several attack strategies that were originally developed\\nfor centralized learning to SSL. This included a sophisticated back-\\ndoor trigger injection technique using image warping [ 38] and also\\nan adversarial-example-based attack employing the Fast Gradient\\nSign Method (FGSM) proposed by Goodfellow et al. [21]. However,\\nas shown in Tab. 5, all three attacks were ineffective. Specifically,\\nthe FGSM attack reduced the accuracy only to 76.86% (PDR=5%) or\\n74.34% (PDR=10%), while Phantom reduced the accuracy to 68.71%\\n(PDR=5%) and 67.15% (PDR=10%). The backdoor-inspired attacks\\nfailed, most likely due to the fact that in an untargeted poisoning\\nattack, it is not sufficient to add a bridge to the data for making a\\nfew, well-defined samples misclassified. For the FGSM attack, the\\ninability to prevent the utilization of the benign samples, given\\nthat the attacker controls only a small fraction of the unlabeled\\ndataset ùí∞, contributed to its failure. The attack needs to prevent\\nthe utilization of benign data to succeed (see C1), since the attack\\nobjective (preventing high accuracy) contradicts the benign objec-\\ntive (achieving high accuracy), which is given by the majority of\\nthe data. Simply connecting a few samples from specific classes or\\npreventing the utilization of a few samples is, therefore, insufficient.\\nIn comparison, Phantom exploits the model‚Äôs overfitting on the\\nlabeled dataset ùí≥and causes the model to mislabel the unlabeled\\nsamples.\\nAlso the baseline of removing the manipulated images from the\\ndataset has a negligible impact on performance. A reason might be\\nthat, even after removal, still enough samples remain that can be\\nused for training the DNN. This emphasizes that it is not sufficient\\nto prevent the poisoned samples from being used it is necessary\\nthat the manipulated samples affect the ability to utilize the other\\nsamples in the unlabeled datasets, which were not manipulated.\\nIn comparison to these baselines, Phantom reduces the performance\\nby more than 12% compared to the baseline performance, demon-\\nstrating its effectiveness in not only rendering the manipulated\\nsamples unusable but also preventing the training algorithm from\\nutilizing a significant portion of the non-manipulated images.\\nThe table also shows the accuracy when the attacker utilizes a\\npattern that is composed of 4 images, with only one of them being a\\npart of the labeled dataset. This scenario is particularly relevant in\\nsituations when the attacker must infer the labeled samples based\\non typical datasets for the targeted application. As the table shows,\\nthis attack is still effective and causes a significant drop in the\\naccuracy. In addition, we performed an experiment where the set\\nùí≥ùíú, containing the samples that the adversary believes to be usedTable 5: Effectiveness of different variations of Phantom for\\na PDR of 10%.\\nAttack UDA\\nBenign Scenario 79.64%\\nUse only Labeled Dataset 26.00%\\nTwo-unlabeled attack 79.92%\\nEmpty Images 89.06%\\nRemove 10% Samples 77.01%\\nSample-Interpolation [7] 81.03%\\nWarp Trigger [38] 78.11%\\nFGSM [21] 74.34%\\nThree-Unlabeled-One-Labeled 71.72%\\nPhantom (PV=0.2, PDR=10%) 67.15%\\nPhantom (PV=0.1, PDR=5%) 68.71%\\nTable 6: Performance of Phantom forCIFAR-10 if 10% of\\nthe labeled data are guessed correctly (precision=10% and\\nsensitivity=10%) using PDR=5% and PV=0.1.\\nMixMatch UDA FixMatch\\nBenign Scenario 74.75% 79.64% 89.10%\\nPhantom 64.85% 68.71% 83.68%\\nPhantom Reduced Overlap 70.04% 73.71% 84.60%\\nas labeled samples, overlaps only by 20% with the set ùí≥of the\\nactual labeled samples. Here, Phantom still reduced the model‚Äôs\\nutility to 71.38%.\\nIn addition, we evaluated also scenarios, where the attacker‚Äôs\\nguesses about the labeled dataset ùí≥are made with low precision\\nand sensitivity. Particularly, only 10% of the suspected samples ùí≥ùíú\\nare actually contained in ùí≥(precision=10%), while the correctly\\nguessed samples make only 10% of the actually labeled dataset ùí≥\\n(sensitivity=10%). As Tab. 6 shows, although with reduced ability\\nto guess the labeled samples, Phantom remains effective, as the\\naccuracy is still reduced by approx. 5%.\\n5.5 Potential Countermeasures\\nThe poisoning pattern of Phantom shows some similarities to ad-\\nversarial examples (see Sect. 6.4). In the following, we evaluate\\nPhantom ‚Äôs effectiveness in the presence of potential countermea-\\nsures. Zantedeschi et al. proposed applying Gaussian noise on the\\ninputs [ 65]. However, as shown in Tab. 10, the noise does not miti-\\ngate the attack. Pang et al. proposed analyzing the states of the final\\nhidden layer for a sample ùë•to determine if ùë•contains an adversarial\\npattern [ 43]. Given the predicted label ùë¶, all training samples ùëãùë¶\\nhaving the label ùë¶,ùëìùëßthe current model until the final hidden layer,\\nand the Gaussian Kernel ùëò(¬∑,¬∑), then the K-density score ùêæùê∑(ùë•)for\\nùë•is defined as:\\nùêæùê∑(ùë•)=1\\n|ùëãùë¶|¬∑‚àëÔ∏Å\\nùë•ùëñ‚ààùëãùë¶ùëò(ùëìùëß(ùë•),ùëìùëß(ùë•ùëñ)) (3)\\nSince the thresholding test was developed for scenarios where fully\\nlabeled data are available, we used the softmax label of the training\\ndata to craft ùëãùë¶. Fig. 6 shows the distribution of scores for benign\\nand poisoned images using a PDR of 5% and different PV values\\nafter the model was trained for 10 epochs. As the figure shows,CCS ‚Äô24, October 14‚Äì18, 2024, Salt Lake City, UT, USA Jonathan Knauer, Phillip Rieger, Hossein Fereidooni, and Ahmad-Reza Sadeghi\\nTable 7: Effectiveness of DNN Classifier in recognizing Phan-\\ntomattack in terms of True Negative Rate (TNR), True Pos-\\nitive Rate (TPR), Precision (PRC), F1-Score, and Accuracy\\n(ACC) for different Pattern Visibilities (PV).\\nPV TPR PRC F1-Score ACC\\n10% 44.52% 73.62% 55.49% 64.28%\\n20% 68.94% 81.20% 74.57% 76.49%\\n50% 86.23% 84.39% 85.30% 85.14%\\n75% 73.82% 82.23% 77.80% 78.93%\\n100% 8.90% 35.81% 14.26% 46.47%\\nthe distribution of scores for benign and manipulated samples are\\nindistinguishable for PV ‚â§50%, and only for larger PVs could the\\nmetric identify some values. Notably, for such a high PV value,\\nhumans can already identify the manipulated images easily (see\\nFig. 3).\\nWe also trained a dedicated CNN classifier using the VGG-11 archi-\\ntecture. We trained the classifier on benign and poisoned CIFAR-10\\nsamples and cross-evaluated them on benign and poisoned Im-\\nageNet samples (1.2M samples each). We measured the ratio of\\nmanipulated samples that are detected (True Positive Rate, TPR),\\nthe ratio of manipulated samples compared to the total number\\nof samples that are classified as manipulated (Precision, PRC), F1-\\nScore, and the overall ratio of correctly classified samples (Accuracy,\\nACC). As Tab. 7 shows, the classifier was barely able to recognize\\nsamples for low PV values (TPR=44.52%) and only for large PV val-\\nues the classifier is effective. Notably, for PVs that are too large, the\\neffectiveness is reduced again, showing that the classifier learned\\nto detect the sample overlay.\\n5.6 Case-Study\\nIn addition to the aforementioned evaluation, we also conducted a\\ncase study to assess the effectiveness of our attack in a real-world\\nscenario. Specifically, we uploaded manipulated images to three\\nreal-world social media platforms (Facebook, Instagram, and Pinter-\\nest) and evaluated how the platforms‚Äô image processing pipelines\\naffect Phantom ‚Äôs performance. To ensure that our experiments do\\nnot negatively impact other parties, we ensured that the images\\nwere only accessible to us by setting the privacy settings accord-\\ningly (see App. B). Due to the limited number of images that can\\nbe uploaded3to Instagram and Pinterest, we stacked multiple im-\\nages to form a larger image with a higher resolution, which was\\nthen split again after uploading and downloading. Since Instagram\\nonly allowed squared images, we padded the images using black\\npixels and removed the padding after the download. Additionally,\\nInstagram and Pinterest might change the image scaling if it differs\\nfrom the desired sizes. Therefore, we used a size of 612 √ó612 pixels\\nfor our images, as this size remained consistent.\\nAs Tab. 8 shows, the Phantom attack is effective for the data that\\nwere obtained from all three social networks. Phantom reduces the\\naccuracy of the trained model by 10%. Only in two cases, for UDA\\nthe accuracy is reduced only by 5% (Facebook) or 7% (Instagram).\\nOn the other side, for Instagram using FixMatch, the accuracy is\\n3Notably, as described in Sect. 6.1, an attacker could also use a script to upload each\\nimage automatically. However, as the usage of bots is clearly forbidden by these\\nplatforms, for our evaluation, we chose the batch strategy instead.\\n22.5\\n 20.0\\n 17.5\\n 15.0\\n 12.5\\n 10.0\\nlog10(K-Density)0.0%5.0%10.0%15.0%Frequency in PercentBenign\\nPoisoned(a) PV = 10%\\n22.5\\n 20.0\\n 17.5\\n 15.0\\n 12.5\\n 10.0\\nlog10(K-Density)0.0%5.0%10.0%15.0%Frequency in PercentBenign\\nPoisoned\\n(b) PV = 50%\\n22.5\\n 20.0\\n 17.5\\n 15.0\\n 12.5\\n 10.0\\nlog10(K-Density)0.0%5.0%10.0%15.0%Frequency in PercentBenign\\nPoisoned\\n(c) PV = 75%\\n22.5\\n 20.0\\n 17.5\\n 15.0\\n 12.5\\n 10.0\\nlog10(K-Density)0.0%5.0%10.0%15.0%Frequency in PercentBenign\\nPoisoned\\n(d) PV = 100%\\nFigure 6: Distribution of kernel density scores (K-\\nDensity) [ 43] for benign and poisoned images using\\ndifferent Pattern Visibilities (PVs).\\nreduced by more than 25%. Therefore, the augmentation, such as\\ncompression, that these social networks apply does not significantly\\naffect Phantom ‚Äôs impact.\\n5.7 Partial Knowledge on Output Space\\nRecently, Richards et al. described the challenge of partial knowl-\\nedge of the output space [ 47]. For crafting the poisoning pattern, it\\nis important that the patterns‚Äô labels differ from the ground truth\\nlabel of the original image. To evaluate the effectiveness of Phantom\\nunder limited knowledge of the output space, we tested Phantom\\nin two scenarios where the individual classes of CIFAR-10 werePhantom: Untargeted Poisoning Attacks on Semi-Supervised Learning (Full Version)* CCS ‚Äô24, October 14‚Äì18, 2024, Salt Lake City, UT, USA\\nTable 8: Effectiveness of Phantom in terms of accuracy for\\ntheCIFAR-10 dataset after downloading attack samples from\\ndifferent real social networks and comparison baselines.\\nScenario FixMatch MixMatch UDA\\nBenign Scenario 89.10% 74.75% 79.64%\\nSample Removal 84.58% 72.97% 77.01%\\nSimulation 77.07% 68.22% 67.15%\\nInstagram 63.30% 63.86% 72.54%\\nFacebook 76.44% 63.12% 74.67%\\nPinterest 72.90% 61.85% 69.65%\\ngrouped into three actual classes. These groups included mechan-\\nical classes (airplanes, automobiles, ships, trucks), small animals\\n(birds, cats, frogs), and large animals (deer, dogs, horses).\\nIn the first scenario, ùíúis aware of only these three classes, while\\ntheùí±uses all 10 classes. In the second scenario, ùíúuses all 10\\nclasses, while the ùí±uses only three classes. Since we observed\\na variance of 5% in the experiment results, each experiment was\\nrepeated 10 times. We observed that reducing the knowledge of the\\noutput space affects Phantom ‚Äôs effectiveness. In the first scenario,\\nPhantom reduced the accuracy in average by 4.4%, demonstrating\\nthat Phantom remains effective in this scenario. Notably, in the\\nsecond scenario, no accuracy drop was observed, and Phantom was\\nunable to reduce the accuracy.\\nThis is likely because Phantom crafts the poisoning pattern using\\nimages with labels different from the original image, which causes\\nthe SSL algorithm to mislabel the images. However, if ùíúassumes a\\nlarger output space than ùí±actually uses, it is probable that the im-\\nages of the poisoning pattern belong to the same class. For instance,\\nifùíúcreates a poisoning pattern for an airplane using images of\\nships and trucks, and ùí±uses a reduced output space where these\\nclasses are grouped together into a superclass \"mechanical items,\"\\nthe image will not be mislabeled during training. We will discuss\\nthis limitation further in Sect. 6.2.\\nIn this section, we conducted a comprehensive evaluation of the\\neffectiveness of the Phantom attack using six diverse datasets, show-\\ning its efficacy in distracting the training process diminishing the\\nutility of the trained model, even for small PDRs. We evaluated the\\neffects of various image augmentation techniques and potential\\ncounter measures, demonstrating the robustness of the Phantom\\nattack In a case study on 3 real-world social media platforms, we\\nshowed their susceptibility to Phantom , effectively demonstrating\\nits effectiveness and the associated risks it entails.\\n6 SECURITY CONSIDERATIONS\\nIn the preceding sections, we introduced the Phantom attack (Sect. 4)\\nand evaluated it in various scenarios (Sect. 5). In this section, we\\nwill discuss the potential risks that the Phantom attack presents\\n(Sect. 6.1), its limitations (Sect. 6.2), and possible future research\\ndirection (Sect. 6.3).\\n6.1 Impact of the Phantom Attack\\nThe attack allows the adversary to degrade the performance of the\\nmodel by simply uploading manipulated images to a platform withuser-generated content. The poisoned pattern introduced by the\\nattack confounds the training algorithm, causing it to mislabel the\\nmanipulated images and resulting in the DNN model being trained\\nwith incorrect data. Thus, by uploading few manipulated samples,\\nthePhantom attack affects not just a single model but all models\\nbeing trained on these data.\\nNotably, the Phantom attack does not have strong requirements or\\nassumptions. It is successful even with small Pattern Visibilities\\n(PV), making it difficult to detect the manipulated images. The Phan-\\ntomattack operates without any knowledge of the SSL algorithm,\\nhyperparameters, or DNN architecture. Instead, the attacker simply\\nuploads manipulated images to a platform with user-generated\\ncontent, prior to the victim downloading the data and beginning\\nthe training process. This ability to poison the training without any\\nknowledge of the attacked system highlights the increased attack\\nsurface created by utilizing data from untrusted sources without\\nproper mitigation strategies. By uploading manipulated data, the\\nattacker not only disrupts a specific SSL training procedure but\\nalso has the potential to prevent any SSL training for a particular\\ntask on the data from the affected platform. It is already sufficient\\nto poison a small fraction of the data to achieve an accuracy drop\\nbetween 5% and 10%. Notably, the uploading process could be also\\nautomated, as described in Sect. 6.2.\\nEven such small decreases in accuracy, such as a drop of 5% or\\n10% compared to the non-attacked model, can have serious conse-\\nquences. For instance, in the context of self-driving cars, if a model\\nfails to recognize the correct object, even in just 1% of the cases,\\nthis will cause the car to show wrong and potentially dangerous\\nbehavior in such situations, making the vehicle unusable. Similarly,\\nin competitive scenarios, if two actors both train models but one\\nmodel performs 10% better than the other model, the actor with the\\nmore accurate model will have an advantage in selling products\\nbased on their DNN models.\\nAlso, the attack is not restricted to the image domain. The only\\nrequirement is the ability to combine a labeled sample with other\\nsamples, therefore, to overlay different samples weighted. This\\nallows to combine the original samples with poisoning patterns. In\\nSect. 4.3, we described this exemplary for images. However, this\\ncan also be applied in other applications, such as applications that\\nprocess audio files. Also here, different samples can be overlayed\\nand the volume of the different samples can be used as Pattern\\nVisibility (PV). Therefore, the Phantom attack is not restricted to\\nimage applications but is generally applicable.\\nThus, the combination of low requirements and the ability to re-\\nduce the accuracy, the Phantom attack demonstrates the risk of\\nuntargeted poisoning attacks on SSL.\\n6.2 Limitations of the Phantom Attack\\nThe Phantom attack does not require any prior knowledge of the\\ntraining algorithm or the hyperparameters used, but it does necessi-\\ntate some knowledge of the samples utilized for the labeled dataset.\\nAdversaries can make educated guesses on the used labeled sam-\\nples based on commonly available and publicly accessible datasets\\nsuch as tiny images. In scenarios targeting a specific victim ùí±, the\\nattacker could also incorporate knowledge obtained through data\\nbreaches or insider threats, which are not uncommon in industry.CCS ‚Äô24, October 14‚Äì18, 2024, Salt Lake City, UT, USA Jonathan Knauer, Phillip Rieger, Hossein Fereidooni, and Ahmad-Reza Sadeghi\\nHowever, as previously discussed in Sect. 3.2 the capability to make\\neducated guesses is sufficient, as long as there is a degree of overlap\\nbetween the guessed labeled samples ùí≥ùíúand the actual labeled\\nsamples ùí≥, i.e.,ùí≥ùíú‚à©ùí≥‚â†‚àÖ. In Sect. 5.4, we evaluated scenarios\\nwere the adversary‚Äôs guesses about the labeled images showed only\\na precision of 20%, thus, 20% of the images in ùí≥ùíúwere actually in\\nùí≥. Further, we performed another experiment where the adversary\\nincreased the chance of having at least one correct guess per image\\nby leveraging poisoning patterns that consist of 4 guesses for the\\nlabeled images. These experiments demonstrated that the ability to\\nmake educated guesses about the labeled dataset with a limited pre-\\ncision is sufficient for Phantom to significantly impact the model‚Äôs\\naccuracy.\\nIn addition to the ability to make educated guesses on the labeled\\ndataset, Phantom requires the adversary to be able to upload sam-\\nples to the attacked platform. It might be a challenge here to control\\na considerable amount of samples. However, as shown in Sect. 5,\\nPhantom is effective even with very small Poisoning Data Rates\\n(PDRs). Notably, despite the social networks‚Äô significant efforts to\\nidentify and block automated content posting, bots continue to suc-\\ncessfully upload content automatically [ 28,36,56,58,60], allowing\\nan attacker to post automatically a certain fraction of manipulated\\nmedia files. Furthermore, Carlini et al. recently demonstrated that\\nsignificant portions even of established datasets can be manipu-\\nlated with minimal effort [ 8], showing the practical applicability of\\nPhantom .\\nSince the attack exploits the training algorithm‚Äôs ability to pre-\\ndict wrong labels, it is restricted to Semi-Supervised Learning\\n(SSL) settings. It is not applicable for pre-labeled scenarios like\\nself-supervised learning that are used for text processing.\\nFurther, in line with prior research on adversarial deep learning [ 4,\\n16,33] and attacks on SSL in particular [ 7,17,19], also for the\\nproposed attack, there is no formal proof to show that the attack\\nworks. To the best of our knowledge, all existing literature introduc-\\ning attacks on SSL algorithms for DNNs show their effectiveness\\nthrough an empirical evaluation. Consistent with this existing work,\\nin Sect. 5, we presented an extensive empirical evaluation show-\\ncasing the effectiveness of our proposed Phantom attack across\\ndiverse datasets, SSL algorithms, input augmentation techniques,\\nand attack parameter variations.\\nTo craft the poisoning pattern, Phantom selects images with la-\\nbels different from the original image. Therefore, Phantom requires\\nknowledge on the output space. As observed in App. 5.7, the attack\\nis ineffective if the attacker assumes a larger output space and, there-\\nfore, more labels than the victim actually uses. This occurs because\\nPhantom uses samples with different labels to create the poisoning\\npattern, causing the SSL algorithm to mislabel them. However, if\\nùíúassumes a larger output space than ùí±uses, the samples of the\\npoisoning pattern might belong to the same class in ùí±‚Äôs output\\nspace. Notably, the adversary can mitigate this risk by utilizing a\\nreduced output space during the poisoning process. Therefore, the\\nattacker needs to select images from classes that have a very high\\nprobability of belonging to different classes. For example, images\\nfrom machines can be used to poison animal images. As shown in\\nSect. 5.7, this strategy allows the attack to remain effective even in\\ncase of a reduced output space.Also, it should noted that the Phantom attack is executed in a blind\\nmanner, without any assumptions regarding access or knowledge\\nof the deployed algorithms, training process, or victims. However,\\nthis also introduces another challenge, as the adversary ùíúcannot\\nmonitor the attacked victim and, thus, cannot verify the success of\\nthe attack afterward. Analogous to other attacks against DNNs, such\\nas inference attacks [ 49,54], watermark removal strategies [ 9,59],\\nor poisoning attacks against self-supervised learning [ 31,52,53], the\\nattacker can only measure the attack‚Äôs effectiveness in a simulated\\nenvironment. Consequently, it remains infeasible for the attacker to\\nverify the attack‚Äôs success for the actual victim, therefore, whether\\nit‚Äôs preventing DNN convergence ( Phantom attack), confirming\\nthe utilization of extracted data for training (inference attacks),\\nor validating the removal of an unknown watermark (watermark\\nremoval approaches).\\n6.3 Future Work\\nIn our attack, we combined the original image with the poisoned pat-\\ntern by performing weighted averaging to obtain the manipulated\\nimage. This enables the adversary to control the suspiciousness\\nof the image using the PV parameter. Especially images that were\\ncrafted using low PV values seem to be inconspicuous to humans,\\nparticularly when they are displayed among many other benign\\nimages to the users of social media platforms. However, the ques-\\ntion of what kind of adversarial perturbations are visible to humans\\nis an active research topic [ 22,50]. In this work, we focused on\\ndeveloping a scheme that combines two images and causes the SSL\\nalgorithm to use one for guessing the labels and the other for the\\nactual training, resulting in mislabeled training samples. Therefore,\\nthe question on what kind of general patterns in images cause\\nattention by humans or which combination of colors is prone to\\nbe easily-detectable is out of the scope of this work. Future work\\nmight improve the developed scheme and reduce the number of\\npixels, e.g., by removing the background of the labeled images that\\nare used for the poisoning pattern. In this context, it should be\\nemphasized that the adversary can still inspect the images before\\nuploading them to ensure that they are inconspicuous.\\n6.4 Potential Counter Measures\\nIn this paper, we introduced the Phantom attack that disrupts SSL\\ntraining algorithms by uploading manipulated samples to the plat-\\nform that is used as data source. We demonstrated the risks posed by\\nPhantom through a case-study on three real-world social networks\\n(see Sect. 5.6). An important research direction for future research\\nis, therefore, to develop effective mitigation schemes against this\\ntype of attack.\\nWe demonstrated in App. F that Phantom is robust against standard\\nimage augmentation techniques. Other defense strategies could\\ninvolve adapting techniques that were developed against adver-\\nsarial examples. Intuitively, the poisoning pattern that Phantom\\ninjects in the manipulated images might be comparable to the noise\\nintroduced by adversarial examples. Zantedeschi et al. propose\\napplying Gaussian noise to the images to mitigate adversarial ex-\\namples [ 65]. However, as we demonstrated in App. F, although\\napplying Gaussian noise reduces the models‚Äô performance, Phan-\\ntomremains effective. Pang et al. propose a thresholding test basedPhantom: Untargeted Poisoning Attacks on Semi-Supervised Learning (Full Version)* CCS ‚Äô24, October 14‚Äì18, 2024, Salt Lake City, UT, USA\\non a kernel density function to detect adversarial examples [ 43].\\nHowever, as shown in Sect. 5.5, the obtained scores for manipulated\\nimages are in the same range as for benign images. Also, a dedicated\\nclassifier was unable to detect the manipulated images (see Sect. 5.5).\\nA key difference in an adversarial example that might be the reason\\nfor the ineffectiveness of these methods might be the fundamental\\ndifference in the patterns‚Äô structures. Adversarial examples exploit\\nspecific DNN parameter values to cause mispredictions, resulting\\nin a random-like poisoning pattern. In comparison, the pattern\\nofPhantom consists of real samples, thus showing structures and\\nedges as regular samples do. Thus, further research is required to\\ninvestigate potential countermeasures.\\nAn important aspect for countermeasures against Phantom is the\\noverlay of different samples and that the poisoning pattern is usu-\\nally only weakly embedded (using low PV values). In the following,\\nwe describe two options that future work might investigate to re-\\nmove the poisoning pattern by exploiting the overlay.\\nNoise Reduction Techniques. Signal processing methods, such\\nas noise reduction, could be utilized for the removal of the poi-\\nsoning pattern. A challenge here is to erase the pattern without\\naffecting the algorithm‚Äôs ability to process benign samples. Our\\nevaluation demonstrated that simple compression and Gaussian\\nsmoothing are insufficient (see App. F) to mitigate the Phantom at-\\ntack. Future research might, therefore, focus on more sophisticated\\nsignal-processing techniques, such as singular value decomposi-\\ntion [ 48], analyzing the samples in the frequency domain [ 24], or\\nstatistical analysis [34].\\nModeling Attack as Cocktail Party Problem. The challenge of\\nextracting the original sample from overlaid data in our scenario is\\nsimilar to the well-known cocktail party problem. This problem de-\\nscribes a situation in which multiple audio signals overlap, similar\\nto multiple conversations occurring simultaneously in a crowded\\nroom. Different approaches where proposed to separate signals\\nusing differential beamformers [ 10], transformer networks [ 57], or\\nasynchronous fully recurrent convolutional neural networks [ 26].\\nLeveraging and adapting these techniques might provide a promis-\\ning research direction to isolate and effectively separate the injected\\npoisoning patterns from the original image while preserving the\\nintegrity of the original training samples.\\n7 RELATED WORK\\nIn the following, we analyze existing work to attack SSL and untar-\\ngeted attacks against deep learning. Despite the limited research on\\nadversarial SSL, previous approaches focused on either injecting\\na backdoor into the resulting model or reducing the accuracy un-\\nder the assumption that the adversary controls the labeled dataset.\\nHowever, assuming control of the labeled dataset is not practical as\\nit requires the adversary to control the victim. First, we will discuss\\nexisting attacks on SSL (Sect. 7.1), before describing several untar-\\ngeted attacks against other, non-SSL, learning scenarios (Sect. 7.2).\\n7.1 Attacks on SSL\\nIn previous research, various targeted poisoning attacks against\\nSSL have been proposed, such as the white box backdoor attacks de-\\nveloped by Yan et al. [63,64] and Feng et al. [17], which manipulate\\nsamples in the unlabeled dataset to cause the SSL algorithm to makeincorrect label predictions. They assume that the adversary can use\\nthe parameters of an intermediate version of the DNN model to\\ndetermine a perturbation that causes the SSL algorithm to guess a\\nwrong label for this specific sample. However, these assumptions\\nare not practical, as this requires the adversary to be able to mon-\\nitor the victim and the ongoing training process. However, if the\\nadversary has access to the victim and can inspect the intermediate\\nDNN model, there is no need to poison the unlabeled dataset but it\\ncan also poison, e.g., the labeled dataset. Neither is it practical to\\nassume that the victim downloads the data a second time during the\\ntraining. In comparison, Phantom does not require the adversary\\nto have any control or any knowledge about the victim (parame-\\nters of the intermediate model, SSL algorithm, hyperparameters).\\nIt crafts the manipulated images once, before it is sufficient to up-\\nload the manipulated samples to the platform before the training is\\nstarted (see Sect. 3.2).\\nOther studies [ 7,12], focused on poisoning the unlabeled dataset\\nby adding interpolations between samples from different classes\\nto inject a backdoor into the trained DNN model. However, these\\nattacks restrict the attack objective since they can only cause mis-\\npredictions for individual samples, rather than reducing the overall\\nperformance of the model (see Sect. 5.4). In contrast, Phantom sig-\\nnificantly decreases the utility of the trained model and reduces\\nits performance to that of a naive classifier, making the poisoned\\ndataset unusable for SSL.\\nFranci et al. [19] and Liu et al. [32] perform both untargeted poi-\\nsoning attacks by altering samples from the labeled dataset. They\\naim to minimize the number of labels that need to be changed to\\ndecrease the effectiveness of the models. However, this approach\\nrequires the attacker to have access to the labeled dataset, which\\nimplies a high level of capability for the attacker. An attacker that is\\nable to control the small and manually labeled dataset of the victim\\nis highly likely to be also able to interfere with the training process\\nand directly sabotage the DNN model. In contrast, our proposed\\nmethod only requires the attacker to upload manipulated samples\\nto the source of the unlabeled dataset, such as a social network.\\nTherefore, Phantom enables even a weak attacker to carry out poi-\\nsoning attacks on semi-supervised learning and discourages the\\nuse of public data sources for SSL training.\\n7.2 Untargeted Poisoning Attacks against Other\\nLearning Scenarios\\nIn addition to SSL, untrusted data is also used in online learning\\nscenarios, such as leveraging user feedback for predictions [ 44]. Sev-\\neral approaches have been proposed to address poisoning attacks\\nin these scenarios by treating them as an optimization problem and\\ncreating samples that maximize the impact of the attack [ 27,44,67].\\nHowever, for online learning systems, the attacker can provide in-\\ncorrect labels to confuse the system, and also has knowledge about\\nthe victim models, such as hyperparameters or even the weights.\\nIn contrast, Phantom is a black-box attack, in which the attacker\\nhas no knowledge of the DNN model or training algorithm.\\nFederated Learning (FL) is a distributed training approach where\\ndifferent clients perform the training locally and only share the\\nparameters of their DNN models with a coordinating server [ 35].\\nThe decentralized structure prevents the server from inspecting theCCS ‚Äô24, October 14‚Äì18, 2024, Salt Lake City, UT, USA Jonathan Knauer, Phillip Rieger, Hossein Fereidooni, and Ahmad-Reza Sadeghi\\ntraining data, thus making it vulnerable to untargeted poisoning\\nattacks by malicious clients [ 6,16]. However, these attacks typically\\ninvolve knowledge of the current model‚Äôs state, such as the current\\nmodel parameters, and are, therefore, white-box attacks. Also, due\\nto the decentralized training structure, the adversary can manipu-\\nlate the model‚Äôs parameters directly rather than being restricted to\\nadding a few samples to the dataset as Phantom does.\\n8 CONCLUSION\\nIn this paper, we presented Phantom , a novel untargeted-poisoning\\nattack that disrupts Semi-Supervised Learning (SSL) training by\\nintroducing manipulated samples into the unlabeled dataset. While\\nexisting attacks on SSL either focus on backdoor attacks or re-\\nquire knowledge about the attacked training process, Phantom\\noperates blindly. It crafts manipulated images before the train-\\ning process starts without any knowledge about the training al-\\ngorithm or hyperparameters. Phantom employs a combination of\\ntechniques that cause SSL algorithms to overlook the sample‚Äôs\\nactual content and instead rely on maliciously crafted patterns\\nsuperimposed on real samples, leading to incorrect label guesses.\\nThrough a comprehensive evaluation, we demonstrated Phantom ‚Äôs\\neffectiveness on several state-of-the-art SSL algorithms, showing\\nthat it can be successful with small percentages of manipulated\\nexamples. Furthermore, we conducted a real-world case study to\\nillustrate the vulnerability of data obtained from social networks,\\nincluding Facebook, Instagram, and Pinterest, to Phantom .\\nOur work highlights the need for robust training algorithms that can\\neffectively counteract untargeted-poisoning attacks in the context\\nof SSL. Additionally, it emphasizes the importance of considering\\nsuch attacks in the design and deployment of DNNs, particularly\\nin sensitive domains such as social networks. Our results indicate\\nthat untargeted poisoning attacks on SSL can be effective and pose\\na serious threat to the security of DNNs. Therefore, it is crucial for\\nresearchers and practitioners to consider these types of attacks and\\ndevelop appropriate countermeasures to secure DNNs.\\nACKNOWLEDGMENTS\\nThis research received funding from the Horizon program of the\\nEuropean Union under grant agreements No. 101093126 (ACES)\\nand No. 101070537 (CROSSCON), as well as the Federal Ministry of\\nEducation and Research of Germany (BMBF) within the IoTGuard\\nproject.\\nREFERENCES\\n[1] 2019. PyTorch. https://pytorch.org.\\n[2]2019. PyTorch - GTSRB. https://pytorch.org/vision/0.17/generated/torchvision.\\ndatasets.GTSRB.html\\n[3]Wendy Kan Addison Howard, Eunbyung Park. 2018. ImageNet Object Localiza-\\ntion Challenge. https://kaggle.com/competitions/imagenet-object-localization-\\nchallenge\\n[4]Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. 2018. Synthe-\\nsizing robust adversarial examples. In ICML . PMLR.\\n[5]David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver,\\nand Colin A Raffel. 2019. Mixmatch: A holistic approach to semi-supervised\\nlearning. In NeurIPS .\\n[6]Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. 2017.\\nMachine learning with adversaries: Byzantine tolerant gradient descent. In NIPS .\\n[7]Nicholas Carlini. 2021. Poisoning the Unlabeled Dataset of Semi-Supervised\\nLearning. In USENIX Security . Usenix Association.\\n[8]Nicholas Carlini, Matthew Jagielski, Christopher A Choquette-Choo, Daniel\\nPaleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, and FlorianTram√®r. 2024. Poisoning Web-Scale Training Datasets is Practical. In IEEE S&P .\\nIEEE Computer Society.\\n[9]Xinyun Chen, Wenxiao Wang, Chris Bender, Yiming Ding, Ruoxi Jia, Bo Li,\\nand Dawn Song. 2021. Refit: a unified watermark removal framework for deep\\nlearning systems with limited data. In ACM Asia Conference on Computer and\\nCommunications Security .\\n[10] Zhuo Chen, Jinyu Li, Xiong Xiao, Takuya Yoshioka, Huaming Wang, Zhenghao\\nWang, and Yifan Gong. 2017. Cracking the cocktail party problem by multi-\\nbeam deep attractor network. In 2017 IEEE Automatic Speech Recognition and\\nUnderstanding Workshop (ASRU) . IEEE, 437‚Äì444.\\n[11] Adam Coates, Andrew Ng, and Honglak Lee. 2011. An analysis of single-layer\\nnetworks in unsupervised feature learning. In AISTATS . JMLR Workshop and\\nConference Proceedings.\\n[12] Marissa Connor and Vincent Emanuele. 2022. Rethinking Backdoor Data Poi-\\nsoning Attacks in the Context of Semi-Supervised Learning. arXiv preprint\\narXiv:2212.02582 (2022).\\n[13] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le.\\n2019. Autoaugment: Learning augmentation strategies from data. In IEEE/CVF\\nConference on Computer Vision and Pattern Recognition .\\n[14] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. 2020. Randaugment:\\nPractical automated data augmentation with a reduced search space. In IEEE/CVF\\nConference on Computer Vision and Pattern Recognition Workshops .\\n[15] Terrance DeVries and Graham W Taylor. 2017. Improved regularization of\\nconvolutional neural networks with cutout. In arXiv preprint arXiv:1708.04552 .\\n[16] Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong. 2020. Local Model Poi-\\nsoning Attacks to{Byzantine-Robust}Federated Learning. In USENIX Security .\\n[17] Le Feng, Sheng Li, Zhenxing Qian, and Xinpeng Zhang. 2022. Unlabeled Backdoor\\nPoisoning in Semi-Supervised Learning. In IEEE International Conference on\\nMultimedia and Expo (ICME) . IEEE.\\n[18] Hossein Fereidooni, Jan K√∂nig, Phillip Rieger, Marco Chilese, Bora G√∂kbakan,\\nMoritz Finke, Alexandra Dmitrienko, and Ahmad-Reza Sadeghi. 2023. Authen-\\ntiSense: A Scalable Behavioral Biometrics Authentication Scheme using Few-Shot\\nLearning for Mobile Platforms. NDSS (2023).\\n[19] Adriano Franci, Maxime Cordy, Martin Gubri, Mike Papadakis, and Yves Le Traon.\\n2022. Influence-driven data poisoning in graph-based semi-supervised classifiers.\\nInInternational Conference on AI Engineering: Software Engineering for AI .\\n[20] Jacob Gildenblat and contributors. 2021. PyTorch library for CAM methods.\\nhttps://github.com/jacobgil/pytorch-grad-cam.\\n[21] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and\\nharnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014).\\n[22] Samuel Harding, Prashanth Rajivan, Bennett I Bertenthal, and Cleotilde Gonzalez.\\n2018. Human Decisions on Targeted and Non-Targeted Adversarial Sample.. In\\nCogSci .\\n[23] Charles R Harris, K Jarrod Millman, St√©fan J Van Der Walt, Ralf Gommers,\\nPauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg,\\nNathaniel J Smith, et al .2020. Array programming with NumPy. Nature 585,\\n7825 (2020), 357‚Äì362.\\n[24] Hamid Hassanpour. 2008. A time‚Äìfrequency approach for noise reduction. Digital\\nSignal Processing 18, 5 (2008), 728‚Äì738.\\n[25] Sebastian Houben, Johannes Stallkamp, Jan Salmen, Marc Schlipsing, and Chris-\\ntian Igel. 2013. Detection of traffic signs in real-world images: The German Traffic\\nSign Detection Benchmark. In International joint conference on neural networks\\n(IJCNN) . IEEE.\\n[26] Xiaolin Hu, Kai Li, Weiyi Zhang, Yi Luo, Jean-Marie Lemercier, and Timo Gerk-\\nmann. 2021. Speech separation using an asynchronous fully recurrent convolu-\\ntional neural network. In NeurIPS .\\n[27] W Ronny Huang, Jonas Geiping, Liam Fowl, Gavin Taylor, and Tom Goldstein.\\n2020. Metapoison: Practical general-purpose clean-label data poisoning. In\\nNeurIPS .\\n[28] Stephanie Kirchgaessner, Manisha Ganguly, David Pegg, Carole Cadwalladr,\\nand Jason Burke. 2023. Revealed: The Hacking and disinformation team med-\\ndling in elections. https://www.theguardian.com/world/2023/feb/15/revealed-\\ndisinformation-team-jorge-claim-meddling-elections-tal-hanan\\n[29] Alex Krizhevsky, Geoffrey Hinton, et al .2009. Learning multiple layers of features\\nfrom tiny images. Citeseer.\\n[30] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. 1998. Gradient-based learning\\napplied to document recognition. Proc. IEEE 86, 11 (1998). https://doi.org/10.\\n1109/5.726791\\n[31] Chumeng Liang, Xiaoyu Wu, Yang Hua, Jiaru Zhang, Yiming Xue, Tao Song,\\nZhengui Xue, Ruhui Ma, and Haibing Guan. 2023. Adversarial Example Does\\nGood: Preventing Painting Imitation from Diffusion Models via Adversarial\\nExamples. In ICML . PMLR.\\n[32] Xuanqing Liu, Si Si, Xiaojin Zhu, Yang Li, and Cho-Jui Hsieh. 2019. A unified\\nframework for data poisoning attack to graph-based semi-supervised learning.\\nNeurIPS (2019).\\n[33] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang,\\nand Xiangyu Zhang. 2018. Trojaning attack on neural networks. In NDSS .Phantom: Untargeted Poisoning Attacks on Semi-Supervised Learning (Full Version)* CCS ‚Äô24, October 14‚Äì18, 2024, Salt Lake City, UT, USA\\n[34] Miguel Enrique Iglesias Mart√≠nez, Miguel √Ångel Garc√≠a March, Carles Mili√°n\\nEnrique, and Pedro Fern√°ndez de C√≥rdoba. 2022. Algorithms for Noise Reduction\\nin Signals: Theory and practical examples based on statistical and convolutional\\nanalysis . IOP Publishing.\\n[35] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and\\nBlaise Aguera y Arcas. 2017. Communication-efficient learning of deep net-\\nworks from decentralized data. In AISTATS . PMLR.\\n[36] Cade Metz. 2020. Twitter bots poised to spread disinformation before elec-\\ntion. https://www.nytimes.com/2020/10/29/technology/twitter-bots-poised-to-\\nspread-disinformation-before-election.html\\n[37] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and An-\\ndrew Y Ng. 2011. Reading digits in natural images with unsupervised feature\\nlearning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning .\\n[38] Tuan Anh Nguyen and Anh Tuan Tran. 2021. WaNet-Imperceptible Warping-\\nbased Backdoor Attack. In ICLR .\\n[39] Thanh Thi Nguyen, Quoc Viet Hung Nguyen, Dung Tien Nguyen, Duc Thanh\\nNguyen, Thien Huynh-The, Saeid Nahavandi, Thanh Tam Nguyen, Quoc-Viet\\nPham, and Cuong M Nguyen. 2022. Deep learning for deepfakes creation and\\ndetection: A survey. Computer Vision and Image Understanding 223 (2022), 103525.\\n[40] OpenAI. [n. d.]. ChatGPT: Optimizing Language Models for Dialogue. https:\\n//openai.com/blog/chatgpt/.\\n[41] OpenAI. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023).\\n[42] OpenAI. 2024. https://openai.com/index/introducing-improvements-to-the-\\nfine-tuning-api-and-expanding-our-custom-models-program\\n[43] Tianyu Pang, Chao Du, Yinpeng Dong, and Jun Zhu. 2018. Towards robust\\ndetection of adversarial examples. In NIPS .\\n[44] Tianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, and Jun Zhu. 2021. Accumula-\\ntive poisoning attacks on real-time data. In NeurIPS .\\n[45] Billy Perrigo. 2023. OpenAI used Kenyan workers on less than $2 per hour:\\nExclusive. https://time.com/6247678/openai-chatgpt-kenya-workers\\n[46] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022.\\nHierarchical text-conditional image generation with clip latents. arXiv:2204.06125\\n(2022).\\n[47] Luke E Richards, Andr√© Nguyen, Ryan Capps, Steven Forsyth, Cynthia Matuszek,\\nand Edward Raff. 2021. Adversarial transfer attacks with unknown data and class\\noverlap. In ACM workshop on artificial intelligence and security (AISec) .\\n[48] PK Sadasivan and D Narayana Dutt. 1996. SVD based technique for noise reduc-\\ntion in electroencephalographic signals. Signal Processing 55, 2 (1996), 179‚Äì189.\\n[49] Ahmed Salem, Apratim Bhattacharya, Michael Backes, Mario Fritz, and Yang\\nZhang. 2020. Updates-Leak: Data set inference and reconstruction attacks in\\nonline learning. In USENIX Security .\\n[50] Johannes Schneider and Giovanni Apruzzese. 2023. Dual adversarial attacks:\\nFooling humans and classifiers. Journal of Information Security and Applications\\n75 (2023), 103502.\\n[51] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedan-\\ntam, Devi Parikh, and Dhruv Batra. 2017. Grad-cam: Visual explanations from\\ndeep networks via gradient-based localization. In IEEE international conference\\non computer vision .\\n[52] Shawn Shan, Jenna Cryan, Emily Wenger, Haitao Zheng, Rana Hanocka, and\\nBen Y Zhao. 2023. Glaze: Protecting artists from style mimicry by Text-to-Image\\nmodels. In USENIX Security .\\n[53] Shawn Shan, Wenxin Ding, Josephine Passananti, Stanley Wu, Haitao Zheng,\\nand Ben Y Zhao. 2024. Nightshade: Prompt-Specific Poisoning Attacks on Text-\\nto-Image Generative Models. In IEEE S&P . IEEE Computer Society.\\n[54] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Mem-\\nbership inference attacks against machine learning models. In IEEE S&P . IEEE.\\n[55] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang,\\nColin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. 2020.\\nFixmatch: Simplifying semi-supervised learning with consistency and confidence.\\nInNeurIPS .\\n[56] Marianna Spring. 2024. Bot or not: Are fake accounts swaying voters towards\\nReform UK? https://www.bbc.com/news/articles/c1335nj316lo\\n[57] Cem Subakan, Mirco Ravanelli, Samuele Cornell, Mirko Bronzi, and Jianyuan\\nZhong. 2021. Attention is all you need in speech separation. In ICASSP 2021-2021\\nIEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) .\\nIEEE, 21‚Äì25.\\n[58] Paul Tassi. 2023. I Never Had Bot Problems On Twitter Until Elon Musk, Now\\nThey‚Äôre Stalking Me. https://www.forbes.com/sites/paultassi/2023/12/29/i-never-\\nhad-bot-problems-on-twitter-until-elon-musk-now-theyre-stalking-me/\\n[59] Tianhao Wang and Florian Kerschbaum. 2019. Attacks on digital watermarks for\\ndeep neural networks. In IEEE International Conference on Acoustics, Speech and\\nSignal Processing (ICASSP) . IEEE.\\n[60] Samuel C. Woolley. 2020. Bots and Computational Propaganda: Automation for\\nCommunication and Control . Cambridge University Press, 89‚Äì110.\\n[61] Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L Yuille, and Quoc V\\nLe. 2020. Adversarial examples improve image recognition. In IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition .[62] Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. 2020. Unsuper-\\nvised data augmentation for consistency training. In NeurIPS .\\n[63] Zhicong Yan, Gaolei Li, Yuan TIan, Jun Wu, Shenghong Li, Mingzhe Chen, and\\nH Vincent Poor. 2021. Dehib: Deep hidden backdoor attack on semi-supervised\\nlearning via adversarial perturbation. In AAAI conference on artificial intelligence .\\n[64] Zhicong Yan, Jun Wu, Gaolei Li, Shenghong Li, and Mohsen Guizani. 2021. Deep\\nneural backdoor in semi-supervised learning: threats and countermeasures. IEEE\\nTransactions on Information Forensics and Security 16 (2021), 4827‚Äì4842.\\n[65] Valentina Zantedeschi, Maria-Irina Nicolae, and Ambrish Rawat. 2017. Efficient\\ndefenses against adversarial attacks. In ACM workshop on artificial intelligence\\nand security .\\n[66] Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jindong Wang, Manabu\\nOkumura, and Takahiro Shinozaki. 2021. Flexmatch: Boosting semi-supervised\\nlearning with curriculum pseudo labeling. In NeurIPS .\\n[67] Xuezhou Zhang, Xiaojin Zhu, and Laurent Lessard. 2020. Online data poisoning\\nattacks. In Learning for Dynamics and Control . PMLR.\\nA IMAGE AUGMENTATION\\nData augmentation is a technique that modifies the individual sam-\\nples of training data, being used, e.g., to create a larger dataset for\\ntraining DNNs.\\nOne common example of data augmentation is the use of image\\nmirroring, where the image is flipped horizontally. This technique is\\nparticularly useful in increasing the diversity of the training dataset\\nand thereby increasing the robustness and generalization ability of\\nthe DNN. By requiring the DNN to learn features that are invariant\\nto horizontal flipping, the model is encouraged to focus on impor-\\ntant features of the image and not on the specific orientation of\\nthe image. This in turn, improves the model‚Äôs ability to generalize\\nto new, unseen images. Additionally, this technique also increases\\nthe number of training examples and can help reduce overfitting.\\nAs we will further explain in Sect. 2.1, certain algorithms for SSL\\nutilize data augmentation to improve the DNN‚Äôs generalization\\nability [ 13,15,61]. We categorize data augmentation techniques\\ninto two groups:\\nWeak augmentation\\ntechniques make minimal modifications to the training samples,\\nsuch that the colors and shapes are preserved. Examples of such\\ntechniques include horizontally flipping the images along the center\\naxis and randomly shifting the images by a small number of pixels.\\nStrong augmentation\\ninvolves significant modifications, such as altering the color or\\nobscuring parts of the image. One example of a strong data aug-\\nmentation technique used by SSL algorithms is RandAugment [ 14],\\nwhich employs techniques such as image inversion and partial im-\\nage occlusion. These modifications can make it challenging even for\\nhumans to correctly identify the class of the augmented image [ 13].\\nB ETHICAL CONSIDERATIONS\\nIn Sect. 5.6, the effectiveness of our attack on data that were ob-\\ntained from different real-world social networks is demonstrated. To\\nconduct these experiments, manipulated images were uploaded to\\nthree different social networks: Facebook, Instagram, and Pinterest.\\nThe effectiveness of our attack was evaluated using the manipulated\\ndata from these social networks. To ensure that these experiments\\ndo not have any negative impact on anyone, careful consideration\\nand adaptation of the privacy settings for the profiles on each social\\nnetwork were applied. We created on Facebook a private albumCCS ‚Äô24, October 14‚Äì18, 2024, Salt Lake City, UT, USA Jonathan Knauer, Phillip Rieger, Hossein Fereidooni, and Ahmad-Reza Sadeghi\\nthat only we could access, on Instagram a private account with no\\nfollowers, and on Pinterest a secret board was created.\\nIn the present study, particular attention was given to the privacy\\nsettings on three popular social media platforms, namely Facebook,\\nInstagram, and Pinterest. For Facebook, the \"Select audience\" op-\\ntion in the privacy settings was configured to \"Only me,\" thereby\\nensuring that the visibility of the posted content was limited to\\nthe account holder alone. For Instagram, the account policy was\\nset to \"Private Account\" and the account was not connected to any\\nother account, thereby ensuring that the content was not accessi-\\nble to others. On Pinterest, the option to \"Hide your profile from\\nsearch engines (e.g., Google)\" was enabled, and a secret board was\\ncreated to upload all images. Additionally, none of the accounts\\nwas connected to any other account, thus ensuring that the images\\nremained private.\\nAs a precautionary measure, the content and corresponding user\\naccounts were immediately deleted after downloading them. Each\\nof the social networks stated that deleting the account would also\\nresult in the deletion of any stored data. Furthermore, we carefully\\nanalyzed the usage terms of each of the social networks utilized\\nin the study and ensured that conducting the experiments did not\\nviolate them. The experiments were also aligned with the ethical\\nguidelines of the involved research institutions.\\nC FOCUS OF SSL ALGORITHMS ON LABELED\\nDATASET\\nThe focus of SSL algorithms on the labeled dataset during the\\nearly stages of training is an important factor that is exploited in\\nour proposed untargeted-poisoning attack. When training begins\\nwith randomly initialized parameters, the guessed labels have low\\naccuracy, making the labeled samples the only source of truth. This\\nresults in the DNN overfitting on these samples. Figure 7 shows\\nthe accuracy on the labeled dataset, the unlabeled dataset4, and the\\ntest dataset during training for all three SSL algorithms (MixMatch,\\nUDA, FixMatch) in a scenario without attack. As the figure shows,\\nall three algorithms achieve an accuracy of 100% on the labeled\\ndataset within 4 epochs, while the accuracy on the unlabeled dataset\\ntakes significantly longer to converge.\\n4For the accuracy calculation we used the labels for those images, which are unknown\\nto the training algorithm.\\n0 50 100 150 200 250\\nTraining Epoch0%25%50%75%100%Accuracy\\nMixMatch: Labeled accuracy\\nMixMatch: Test accuracy\\nMixMatch: Unlabeled accuracy\\nFixMatch: Labeled accuracy\\nFixMatch: Test accuracyFixMatch: Unlabeled accuracy\\nUDA: Labeled accuracy\\nUDA: Test accuracy\\nUDA: Unlabeled accuracy\\nFigure 7: Performance of the DNN for the labeled dataset ùí≥,\\nunlabeled dataset ùí∞and test dataset during training.\\n(a) PV = 0.3%\\n (b) Benign model\\n (c) Poisoned model\\nFigure 8: GradCAM of a benign and a poisoned model.\\nOur proposed attack takes advantage of this observation by hiding\\nparts of labeled images in some unlabeled images, which causes\\nthe overfitted DNN to mispredict the labels for these images based\\non the hidden labeled images. By exploiting the focus of SSL algo-\\nrithms on the labeled dataset, Phantom is able to disturb the training\\nprocess and lead to incorrect label guesses.\\nD ANALYSIS OF MODEL BEHAVIOR\\nIn order to thoroughly analyze how the Phantom attack manip-\\nulatesthe model‚Äôs behavior, we analyze the saliency map, which\\nillustrates the model‚Äôs attention. This methodology enables us to\\ncompare the model‚Äôs attention between the benign and poisoned\\nmodels, thereby providing insight into the attack‚Äôs impact on the\\nmodel. For plotting the saliency maps, we employed the widely\\nused GradCam approach [ 51] and utilized the implementation of\\nGildenblat et al. [20].\\nFigure 8 shows the saliency maps for a benign and a poisoned model.\\nThe saliency map of the benign model reveals that the model‚Äôs\\ndecision-making process is based on the analysis of the whole\\nobject, as can be observed in Fig. 8b. In contrast, the saliency map\\nof the poisoned model illustrates the model‚Äôs attention is primarily\\nfocused on the image‚Äôs right side, where one of the labeled images\\nfor the poisoning pattern is located. This can be attributed to the\\nfact that the poisoned model was trained on both the pattern and the\\ncar image, while the benign model was only trained on recognizing\\nthe car. As a result, the center of attention is shifted to the left for\\nthe poisoned image, indicating overfitting on the data. The shift\\nin attention highlights the impact of the poisoning pattern in the\\nmanipulated images and Phantom ‚Äôs effectiveness.\\nE CHOICE OF DENSITY FUNCTIONS IN\\nBACKDOOR ADAPTION\\nIn the past, different security attacks on SSL have been proposed [ 7,\\n12] that aim to inject a backdoor into the model. In comparison\\nto these attacks, does not inject additional functionality into the\\nmodel but disturbs the ability to utilize the non-manipulated data,\\nwhich creates several significant challenges, as discussed in Sect. 3.3.\\nIn Sect. 5.4, we compared the Phantom attack against different\\nbaseline attacks, including an adaption of the attack proposed by\\nCarlini [ 7]. The attack interpolates between samples of two classes\\nthat the attack aims to connect, i.e., making the model classify\\nsamples of one class as samples of the second class. In Tab. 5, we\\nshowed only the results for the density function 1.5‚àíùë•, that is,\\naccording to Carlini [ 7] the most effective density function. For\\nthe sake of completeness, we show in Tab. 9 the results for otherPhantom: Untargeted Poisoning Attacks on Semi-Supervised Learning (Full Version)* CCS ‚Äô24, October 14‚Äì18, 2024, Salt Lake City, UT, USA\\nTable 9: Effectiveness of density functions for an untargeted\\nversion of the attack proposed by Carlini [ 7] for a PDR of\\n10%.\\nDensity Function UDA\\nBenign Scenario 79.64%\\nRemove 10% of Samples 77.01%\\n1‚àíùë•2+0.5 82.47%\\n1 85.96%\\n1‚àíùë• 82.92%\\n1.5‚àíùë• 81.03%\\nPhantom (PV=0.2, PDR=10%) 67.15%\\nPhantom (PV=0.1, PDR=5%) 68.71%\\ndensity functions. As the table shows, Phantom is more effective\\nthan any of these density-function-based attacks, while the density\\nfunctions works as regularization and, in consequence, sometimes\\neven remove the model‚Äôs utility.\\nF FURTHER EVALUATION\\nIn real-world applications, typically, data preprocessing techniques\\nmight be applied to improve computation efficiency, e.g., through\\ndata compression to reduce necessary storage size. Other prepro-\\ncessing techniques can involve image augmentation to improve\\nthe performance of the trained model. To evaluate the practical\\napplicability of Phantom , we, therefore, conducted several exper-\\niments involving various data-augmentation methods (Gaussian\\nsmoothing, JPEG compression, and random image rotation) to asses\\nPhantom ‚Äôs robustness against these techniques. The evaluation is\\nconducted with a PDR of 5% and a PV of 0.1. As the results in Tab. 10\\nshow, although the data augmentation impacts model performance\\neven in the absence of an attack, Phantom is still able to reduce\\nthe model‚Äôs utility. Notably, image flipping, an augmentation tech-\\nnique that horizontally flips the original image, is already part of\\nthe weak augmentation methods applied in SSL algorithms. Conse-\\nquently, this technique was included in all our experiments, and no\\nadditional tests were conducted for it.\\nFurthermore, we assessed the effect of compression on Phantom as\\npart of our case study (see Sect. 5.6). The reason for this assessment\\nis that compression can affect the quality of the images and, thus, the\\nperformance of the algorithms that operate on them. By evaluating\\nthe effect of compression on Phantom , we aimed to investigate its\\nrobustness to this kind of degradation of the input data.\\nTable 11 shows the effectiveness of Phantom for very low PDRs.\\nAs the table shows, even for small PDRs, Phantom reduces the\\naccuracy by 10% for FixMatch and 5% for UDA and MixMatch.\\nThus MixMatch and UDA seem to be more robust against very\\nTable 10: Effectiveness of Phantom for the CIFAR-10 dataset\\nfor different data augmentation techniques.\\nFixMatch MixMatch UDA\\nAugmentation Benign Attack Benign Attack Benign Attack\\nNo Augmentation 89.10 83.68 74.75 64.85 79.64 68.71\\nGauss. Smoothing 91.98 76.21 71.45 68.43 79.53 70.53\\nRandom Rotation 82.99 72.05 63.82 61.95 83.28 77.58\\nJPEG Compression 87.75 74.12 74.25 69.92 83.05 73.48small fractions of manipulated data. Notably, for a PDR of 3%, the\\naccuracy is reduced by almost 10%, showing that Phantom is still\\neffective.\\nTable 11: Effectiveness of Phantom for very low Poisoned-\\nData-Rates (PDRs) for the CIFAR-10 dataset.\\nPDR FixMatch UDA MixMatch\\n0.0% (No-Attack) 89.10 % 79.64 % 74.75 %\\n0.1% 77.78 % 73.93 % 72.71 %\\n0.5% 79.43 % 72.41 % 69.83 %\\n3.0% 78.74 % 72.66 % 65.13 %\\n5.0% 83.68 % 68.71 % 64.85 %',\n",
       " '422Teen Talk: The Good, the Bad, and the Neutral of Adolescent\\nSocial Media Use\\nABDULMALIK ALLUHIDAN, Vanderbilt University, USA\\nMAMTAJ AKTER, New York Institute of Technology, USA\\nASHWAQ ALSOUBAI, King AbdulAziz University, KSA\\nJINKYUNG KATIE PARK, Clemson University, USA\\nPAMELA WISNIEWSKI, Vanderbilt University, USA\\nThe debate on whether social media has a net positive or negative effect on youth is ongoing. Therefore, we\\nconducted a thematic analysis on 2,061 posts made by 1,038 adolescents aged 15-17 on an online peer-support\\nplatform to investigate the ways in which these teens discussed popular social media platforms in their posts\\nand to identify differences in their experiences across platforms. Our findings revealed four main emergent\\nthemes for the ways in which social media was discussed: 1) Sharing negative experiences or outcomes of\\nsocial media use (58%, n= 1,095), 2) Attempts to connect with others (45%, n= 922), 3) Highlighting the positive\\nside of social media use (20%, n= 409), and 4) Seeking information (20%, n= 491). Overall, while sharing about\\nnegative experiences was more prominent, teens also discussed balanced perspectives of connection-seeking,\\npositive experiences, and information support on social media that should not be discounted. Moreover, we\\nfound statistical significance for how these experiences differed across social media platforms. For instance,\\nteens were most likely to seek romantic relationships on Snapchat and self-promote on YouTube. Meanwhile,\\nInstagram was mentioned most frequently for body shaming, and Facebook was the most commonly discussed\\nplatform for privacy violations (mostly from parents). The key takeaway from our study is that the benefits\\nand drawbacks of teens‚Äô social media usage can co-exist and net effects (positive or negative) can vary across\\ndifferent teens across various contexts. As such, we advocate for mitigating the negative experiences and\\noutcomes of social media use as voiced by teens, to improve, rather than limit or restrict, their overall social\\nmedia experience. We do this by taking an affordance perspective that aims to promote the digital well-being\\nand online safety of youth ‚Äúby design.‚Äù\\nCCS Concepts: ‚Ä¢Human-centered computing ‚ÜíCollaborative and social computing .\\nAdditional Key Words and Phrases: Online Safety; Social Media; Digital Youth; Peer Support; Adolescents\\nACM Reference Format:\\nAbdulmalik Alluhidan, Mamtaj Akter, Ashwaq Alsoubai, Jinkyung Katie Park, and Pamela Wisniewski. 2024.\\nTeen Talk: The Good, the Bad, and the Neutral of Adolescent Social Media Use. Proc. ACM Hum.-Comput.\\nInteract. 8, CSCW2, Article 422 (November 2024), 35 pages. https://doi.org/10.1145/3686961\\n1 INTRODUCTION\\nTeens today are unique in that they have grown up in a society that is becoming more digitized\\nand heavily relies on screens [ 96]. Having access to smartphones played an important role in this\\nAuthors‚Äô addresses: Abdulmalik Alluhidan, abdulmalik.s.alluhidan@vanderbilt.edu, Vanderbilt University, P.O. Box 1212,\\nNashville, Tennessee, USA, 37240; Mamtaj Akter, mamtaj.akter@nyit.edu, New York Institute of Technology, 1855 Broadway,\\nNew York, New York, USA, 10023; Ashwaq Alsoubai, atalsoubai@kau.edu.sa, King AbdulAziz University, Jeddah, KSA;\\nJinkyung Katie Park, jinkyup@clemson.edu, Clemson University, 105 Sikes Hall, Clemson, South Carolina, USA, 29634;\\nPamela Wisniewski, pamela.wisniewski@vanderbilt.edu, Vanderbilt University, P.O. Box 1212, Nashville, Tennessee, USA,\\n37240.\\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee\\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\\nthe full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses,\\ncontact the owner/author(s).\\n¬©2024 Copyright held by the owner/author(s).\\n2573-0142/2024/11-ART422\\nhttps://doi.org/10.1145/3686961\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.arXiv:2409.02358v1  [cs.HC]  4 Sep 2024422:2 Alluhidan et al.\\ntrend as the percentage of teens who have access to smartphones increased from 73% in 2015 to\\nreach 95% in 2022 [ 138]. By 2022, nearly half of U.S. teens were online almost constantly; more\\nthan 75% of teens are on social media; about one-in-five teens use social media almost constantly;\\nand more than half of teens said it would be hard to give up social media [ 138]. As such, social\\nmedia usage has become a fundamental and inseparable aspect of the lives of teenagers. In addition,\\nthe social media platforms teens use are becoming more varied; out of seven platform options\\nthat were presented to teens to choose from, 71% of them said they use more than one social\\nmedia platform [ 84]. Among them, YouTube, TikTok, Instagram, and Snapchat were the most\\npopular, with Facebook usage dropping significantly compared to previous years [ 138]. Teens\\noften use different social media platforms for various purposes due to a combination of factors,\\nincluding social factors such as joining popular platforms to interact with their friends, who were\\nalready members of the platform [ 3], psychological factors such as fear of missing out on trends\\nor events [ 67], and technological factors including unique features and tools that are catered for\\nteens‚Äô needs such as short videos on TikTok or temporary content on Snapchat [ 39,141]. Therefore,\\nunderstanding teens‚Äô different experiences across these social media platforms becomes of great\\nimportance for different stakeholders including parents, educators, and policymakers to better\\nsupport teens, provide appropriate guidance, and ensure their positive and safe engagement in the\\ndigital world.\\nAccording to Pew Research, teens utilize social media to talk about a variety of subjects, such as\\ntheir family life, successes, emotions, and romantic life [ 14]. Also, teens employ social media to\\nseek support because it allows them to connect with like-minded individuals without worrying\\nabout being stigmatized or judged [ 121,139]. Social media enables teens to establish networks and\\ncommunities that offer support for discussing sensitive subjects while giving them control over how\\nmuch information they disclose to others [ 121]. This is particularly true on pseudo-anonymous\\npeer support platforms, as they offer many advantages, such as emotional support, access to\\nadvice, meaningful social interactions, and the opportunity for individuals to share and articulate\\ntheir emotions and perspectives openly with strangers [ 111]. At the same time, many researchers\\n(e.g., [ 16,25,32,130,133]) and the news media [ 65,66] have surfaced serious concerns regarding\\nthe negative impact social media may have on the mental health, well-being, and online safety of\\nteens. For instance, concerns over the negative effects of social media use on adolescents spurred\\nafter The Wall Street Journal revealed internal reports from Facebook suggesting that Facebook\\nwas well aware of the adverse impacts of their platform on teens such as pushing self-harm posts\\nand spreading misinformation owing to algorithms embedded on the platform [ 66]. As a result,\\nseveral mass civil action lawsuits [ 8,71] and nationwide legal reform efforts (e.g., Kids Online Safety\\nAct [ 123]) have come to the forefront as a way to proactively shield youth from the detrimental\\neffects of social media. At the same time, other members of the research community have pushed\\nback to suggest that the fear-based narratives and ‚Äòmoral panic‚Äô [ 94,108] around social media‚Äôs\\nimpact on youth may be overblown [ 33]. For instance, Odgers et al. [ 95] poignantly urged us to\\nmove beyond arguing over the positive versus negative effects of social media use on youth; rather,\\nit is our job to maximize the benefits and mitigate risks.\\nTherefore, as we stand at the cusp of this critical and ongoing debate, it is imperative that we\\npresent compelling and balanced evidence regarding the effects of social media on our future\\ngenerations in order to inform future policies and practices that will shape how youth engage\\nwith social media for years to come. It is even more important that we do so based on the lived\\nexperiences and voices of the youth themselves. In this paper, we accomplish this goal by conducting\\na qualitative thematic analysis of 2,061 posts made by 1,038 teens (ages 15-17) on an online peer\\nsupport platform, where they mentioned one or more of the top ten most popular social media\\nplatforms used in the U.S. by teens (i.e., Twitter, Facebook, Snapchat, YouTube, TikTok, WhatsApp,\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.Adolescent Social Media Use 422:3\\nReddit, Twitch, and Tumblr [ 138]). We qualitatively coded these posts and conducted statistical\\nanalyses to determine significant differences in our codes based on the frequencies in which different\\nsocial media platforms were present in our qualitative themes. As such, we answered the following\\nhigh-level research questions:\\n‚Ä¢RQ1: In what ways do youth discuss popular social media platforms when seeking support?\\n‚Ä¢RQ2: When youth discuss their experiences on different social media platforms, are there\\nsignificant differences in the subthemes that emerge within their posts?\\nOverall, we found that teens most often posted regarding Instagram (35% of posts), Snapchat\\n(20%), and YouTube (14%), respectively. Teens primarily utilized the peer-support platform to talk\\nabout their negative experiences when engaging with social media (43% of posts). These negative\\nexperiences included social drama, cyberbullying, body shaming, harmful content viewing, and\\ninterpersonal privacy violations. Based on relative percentages of overall posts, teens called out\\nInstagram for body-shaming and cyberbullying while Facebook for interpersonal privacy violations\\n(mostly by parents). Teens also identified several negative outcomes of social media use (15% of\\nposts), including harm to their self-esteem, arousal of anger, mental health issues, and problems\\nwith time management due to addictive behaviors. Instagram appeared to be most frequently and\\nsignificantly associated with harm to one‚Äôs self-esteem, while YouTube was attributed to being the\\nmost time addictive. The next most common theme teens discussed was how they sought connection\\nthrough social media (45% of posts). YouTube was leveraged most often for self-promotion, while\\nteens sought intimate relationships significantly more frequently on Snapchat and offered or sought\\nemotional support on Facebook and WhatsApp. Additionally, teens mentioned the positive side of\\nengaging on social media (20% of posts). For instance, they identified Snapchat, Twitter and Facebook\\nas platforms that helped them more often cope with mental distress (often through distraction), and\\nInstagram was the most frequently cited platform where they encountered inspiration or supportive\\ncontent. Finally, teens sought informational support (20%) regarding social media platforms, such as\\nverifying the legitimacy of content posted, how to effectively use the features of certain platforms,\\nand seeking technical support. Content-related discussions were relatively more frequent with\\nYouTube (less so on Instagram), while how to use the platform and tech support was most common\\nwith Twitter and Instagram, not on YouTube, respectively.\\nWith the continuous increase in teens‚Äô use of social media, exploring the topic with a broad\\nperspective of both positive and negative effects is essential. Conducting a comprehensive examina-\\ntion of adolescent posts on an online peer support platform provided us with a unique opportunity\\nto discover nuances of how teens discuss their lived experiences across different social media\\nplatforms. While most of the previous works examined teens‚Äô online behaviors either irrespective\\nof social media platforms or by relying on a single platform, ours is one of the first to pinpoint\\nsome of the potential strengths and weaknesses of different social media platforms by analyzing a\\nunique dataset through both qualitative and quantitative methods. To some extent, our findings can\\nbe explained by the underlying affordances, the perceived features of social media platforms that\\nenable or constrain users‚Äô behavior on the platform [ 118], of popular social media platforms. For\\ninstance, it makes sense that Instagram, known for sharing photos, was associated most strongly\\nwith concerns about body-shaming and poor self-esteem. Alternatively, YouTube being associated\\nwith self-promotion and time management concerns speaks to its emphasis on easy-to-use content\\nsharing, compared to the other social media platforms focused more on social network connections.\\nThese results highlight how social media should not be treated as a monolith as different social\\nmedia platforms have unique affordances that shape the overall user experience of our youth.\\nTherefore, targeted risk mitigation strategies need to be designed to address the unique affordances\\nand social norms of each platform. As such, this research is valuable to the CSCW community and\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.422:4 Alluhidan et al.\\nbeyond, as researchers, designers, policymakers, and practitioners can benefit from this knowledge\\nin developing and implementing interventions that take into account the changing patterns of\\nengagement across different platforms. A key strength of our work is in demonstrating for future\\nresearch the importance of examining the lived experience of teens across various social media\\nplatforms and doing so based on digital trace data in places where they are actively seeking support\\nbased on their experiences. Overall, this research makes the following novel research contributions\\nto the online safety and digital youth literature:\\n‚Ä¢Provides an understanding of teens‚Äô lived experiences on social media platforms through the\\nanalyses of a unique dataset of 2,061 pseudo-anonymous support-seeking posts shared by\\n1,038 teens.\\n‚Ä¢Pinpoints a broad perspective of both the potential strengths and weaknesses of different\\nsocial media platforms through the lens of social media affordances. It is one of the first\\nto take an affordances perspective to better understand youths‚Äô differentiated social media\\nexperiences across different platforms.\\n‚Ä¢Provides evidence-based implications for researchers and designers to proactively develop\\ninterventions that reflect the unique affordances and social norms of each social platform.\\nIt also suggests implications for policies and practices that can shape future generations‚Äô\\npositive engagement with social media.\\n2 BACKGROUND\\nIn this section, we introduce the relevant literature regarding the debates over the net effects\\nof social media use on adolescents, analyzing support-seeking posts to understand teens‚Äô lived\\nsocial media experiences, and affordances perspectives to understand teens‚Äô differing social media\\nexperiences.\\n2.1 Debating the ‚ÄòNet Effects‚Äô of Social Media Use on Adolescents\\nPrior research has documented various benefits of teens‚Äô social media usage, such as connecting\\nand socializing to break their isolation [ 87], learning [ 15], and building support networks [ 121].\\nThese positive effects were found to be particularly salient during and after the physical isolation\\ndue to the global pandemic [ 53]. For instance, Maheux et al. [ 87] conducted a longitudinal study\\nwith 704 adolescents to examine the impact of social media use on their gratitude before and during\\nthe pandemic over 15 months. In this study, a significant positive association was found between\\nadolescents‚Äô social media usage for meaningful conversations with their friends and their gratitude\\nover time. Additionally, social media facilitated online peer support, which also benefits youth by\\nproviding users the ability to connect with similar others without fear of stigma or judgment, to\\ncreate supportive networks and communities to discuss sensitive topics while choosing how much\\nto share with others [ 121]. Online peer support platforms were found to offer benefits for teens such\\nas emotional support, the availability of advice, engagement in valuable social interactions, and the\\nopportunity to disclose and express feelings and views [ 111]. Especially, online peer support for\\nindividuals with mental health challenges experienced increased empowerment, self-efficacy, and\\nbetter management of depression, enhanced coping strategies, and reduced social isolation [ 91].\\nOverall, it is important to note that while social media provides a wide array of benefits, responsible\\nand mindful use is crucial to have a safer engagement online.\\nTherefore, over the years, a major number of prior works have extensively studied the negative\\nimpact of social media usage on adolescents, shedding light on its multifaceted consequences [ 97,\\n135]. For instance, researchers have consistently pointed to numerous concerns related to the\\nrelationship between teens‚Äô social media usage and their mental health issues, such as increased\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.Adolescent Social Media Use 422:5\\nfeelings of loneliness, anxiety, and depression [ 70]. Given the prevalence of cyberbullying among\\nadolescents due to the usage of social media platforms, several studies have primarily focused on\\nexamining the detrimental impact of cyberbullying on teens, uncovering how these experiences had\\nsevere psychological effects on teens such as increased depression, anxiety, and suicide ideation [ 43,\\n78,85]. In addition, several studies have explored the negative impact of social media through the\\nlens of social comparison, a process where individuals assess their opinions, values, achievements,\\nand abilities in relation to those of others [ 105]. For example, past research found that using\\nInstagram may heighten social comparison, subsequently leading to a decrease in self-esteem [ 63].\\nFurther studies have also indicated that the use of Instagram could lead to feelings of insufficiency\\ndue to upward social comparison [ 60]. Other researchers have looked at the negative aspects of\\nsocial comparison shedding light on other platforms. For example, Vogel et al [ 137] found that\\nfrequent Facebook use is correlated with lower self-esteem, particularly when users compare\\nthemselves to others who appear more successful or happier on the platform. In a similar vein,\\nBurke et al [ 27] found that people who often compare themselves to others on Facebook tend to be\\nhighly active on the site and have larger friend networks. However, despite being exposed to more\\npositive and socially engaging content, a substantial portion of these users experience negative\\nfeelings after encountering posts that make them feel worse about themselves. Other studies have\\nalso indicated that social comparison on social media has been linked to various negative outcomes\\nsuch as depression [ 120] and body dissatisfaction [ 64]. Pitfalls of social media-based online peer\\nsupport have also been highlighted, such as personal distress due to others‚Äô experiences [ 44],\\nunhelpful interactions with others [ 52], social exclusion [ 44], and feeling of vulnerability when\\ntalking to strangers online [ 24]. Emerging evidence also documented other adolescent online safety\\nissues including online sexual risks [ 12,58,112], exposure to explicit media content [ 31,100,119],\\nand problematic media use with mental health issues [89, 101, 102].\\nGiven the disagreement within the literature, whether social media is beneficial or detrimental\\nfor adolescents remains an open question to explore that requires a nuanced picture of its potential\\nbenefits and detriments. The review and meta-analysis also showed that the association between\\nsocial media use and teens‚Äô psychological well-being is still unclear as effects have been found to\\nexist in both (positive and negative) directions [ 96]. Yet, positive and negative effects can co-exist\\nas effects can vary across different adolescents; some adolescents experience net positive effects,\\nwhile others experience heightened risk [ 95]. For instance, through analyses of 2,155 real-time\\nassessments of the adolescents‚Äô unique susceptibility to the effects of social media, Beyens et al. [ 20]\\nillustrated that the impact of passive (but not active) social media use on their well-being differed\\nsignificantly from teen to teen, with 44.26% of teens did not feel better or worse, 45.90% felt better,\\nand a small group felt worse (9.84%). Therefore, efforts to holistically understand what adolescents\\nare going through on social media are imperative to provide appropriate support systems for\\nthem and to inform design implications to provide a safer online environment, keeping in mind to\\nmaximize the benefits while mitigating potential risks and negative outcomes. This study takes this\\nnuanced approach by analyzing teens‚Äô disclosures about their positive and negative experiences on\\nvarious social media platforms.\\n2.2 Analyzing Support Seeking Posts to Understand Teens‚Äô Lived Social Media\\nExperiences\\nHow youth seek and receive online social support has been an important area of study within the\\nCSCW and HCI community. Previous research showed that adolescents go online to seek social\\nsupport for sensitive topics such as mental health [ 129], romantic relationships [ 72], and sexual\\nexperiences [ 114]. This is because adolescents are hesitant to receive professional help or talk to their\\nparents about such sensitive topics [ 47,54] because of perceived stigma and embarrassment [ 109].\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.422:6 Alluhidan et al.\\nMost teens who face relationship difficulties do not seek help from people they know in person\\ndue to fear of judgment or concerns about confidentiality [ 72]. Therefore, they turn to online social\\nsupport to feel more comfortable interacting with people they do not know offline [ 45]. Recently,\\nresearchers utilized digital trace data from social support platforms to understand adolescents‚Äô\\nonline social support-seeking behaviors [ 54,106,114]. For instance, by conducting a thematic\\ncontent analysis of 622 initial posts shared by young people aged 11-25 years, Prescott et al. [ 106]\\nexplored how youth seek support on the online forum for emotional and mental health issues.\\nThey observed two distinctive ways of youths‚Äô online support-seeking: one was a direct request for\\nadvice, with a themed heading followed by a post with details about the specific advice they sought,\\nwhile the other was finding other young people on the forum who shared similar experiences to\\nthemselves. Another main theme they found was that in many posts, youth shared their personal\\nexperiences and offered support for others [ 106]. More recently, Razi et al. [ 114] conducted a\\nthematic analysis of over 4,000 teen posts on a peer support platform to understand how youth talk\\nabout their online sexual experiences. They found that most youths have utilized the platforms to\\nseek support (85%), while 15% were more interested in connecting with others, and 5% gave advice\\non different sexual matters. Although previous work has provided valuable insights into adolescents‚Äô\\nsupport-seeking behaviors, most have focused on a particular type of negative online experiences\\n(e.g. online sexual experiences, romantic relationship issues, digital stress) or risks experienced\\non a specific social media platform. Our work expands on previous studies by understanding\\nadolescents‚Äô experiences on various social media platforms for which they seek online peer support\\nand the differences in those experiences depending on different social media platforms. To do so,\\nwe analyzed posts that adolescents shared on an online social support platform for teens, where\\nthey explicitly mentioned the name of popular social media platforms.\\n2.3 Leveraging an Affordances Perspective to Understand Teens‚Äô Differing Social Media\\nExperiences\\nMeanwhile, adolescents use a variety of different social media platforms for different purposes in-\\ncluding learning, entertainment, networking, and communicating with others [ 13,92]. For instance,\\nteens use TikTok for entertainment, learning, and discovering trends, while, they use Snapchat\\nfor socializing, and go to Instagram for sharing pictures and checking what others are doing [ 13].\\nWithin the HCI literature, users‚Äô behaviors and experiences on social media platforms have been\\nstudied through the lens of ‚Äúaffordances,\" a relational structure between the social media platforms\\nand the users [ 46]. As a concept that captures the relationship between technology and users,\\naffordances have been applied in social media research as a key concept to examine the perceived\\nfeatures of social media platforms that enable or constrain users‚Äô behavior on the platform [ 101,118].\\nFor instance, on social media platforms that value revealing real identities (e.g., Facebook), users\\ntend to be more careful with how they present themselves, while on platforms that afford low\\nidentity affordances (e.g., Reddit), users disclose sensitive information without repercussions of\\npersonal identity [ 80]. With their low identity affordances, (pseudo) anonymous platforms provided\\nusers with online spaces to seek social support regarding sensitive topics such as mental health,\\nsexual encounters, and more. As such, affordances can aid us in understanding how adolescents\\ninteract on different social media platforms and in guiding them on the risks and benefits of each\\nplatform.\\nPrior research has revealed that user perceptions and interactions with social media platforms\\ncan vary significantly depending on the platforms‚Äô affordances. For example, affordances can\\ninfluence the selection of social media platforms [ 125]; Instagram is favored for its visual capabilities\\nin sharing and enhancing photos and videos, Facebook for connecting with friends and family\\nthrough personal interactions, and Twitter is implied to be appreciated for its real-time, concise\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.Adolescent Social Media Use 422:7\\ncommunication ideal for disseminating information and participating in public dialogues. Another\\nstudy comparing affordances between TikTok and YouTube platforms revealed that users prefer\\nYouTube for its ability to control the sharing of information and manage who sees their information,\\nactivities, or profiles; on the other hand, TikTok was favored for helping users stay informed about\\ninformation and the presence of others, allowing customization to suit personal preferences and\\ninterests, and making it easier to share content within the platform and to other platforms [ 83].\\nFurthermore, prior research suggests that individual differences affect how users interact with\\nand perceive social media. One study [ 42] found that people‚Äôs traits, such as their experience with\\nthe platform and personality affect how they present themselves online. For instance, experienced\\nusers of a platform are likely to be more skilled at navigating the platform‚Äôs features to manage\\ntheir online presence more effectively. Personality also plays a role; individuals open to exploring\\ndiverse creative tools may express themselves more freely, while those higher in neuroticism may\\nprioritize privacy controls and worry about negative feedback, affecting their online presentation\\n[42].\\nAs such, previous literature provides an understanding of the concept of affordances and how it\\ncan be applied in social media research with general populations, yet the application of affordances\\nin the adolescent online safety context remains under-studied. The motivation to explore social\\nmedia affordances in the context of teens lies in understanding how the features of social media\\nplatforms shape teenagers‚Äô behaviors, experiences, and well-being. This exploration is important\\nbecause teens may use social media differently than adults due to their unique developmental\\nneeds [ 124], social dynamics [ 62], and technological skills [ 131]. In this study, we examined teens‚Äô\\ndiffering experiences across multiple social media platforms and interpreted some of our findings\\nin light of the unique affordances of social media platforms. To do so, we explored the posts\\nteens shared on an online peer-support platform with low identity affordances (i.e., registered\\nusers use pseudo-anonymous user names) to understand in what ways teens discuss social media\\nplatforms when they seek online support. By analyzing adolescents‚Äô posts on a pseudo-anonymous\\nplatform, we provide an understanding of adolescents‚Äô experiences on social media that they may\\nnot otherwise share with their parents or peers. In doing so, we investigate whether the way teens\\ndiscuss social media platforms varies across different platforms.\\n3 METHODS\\nIn this section, we first discuss platform selection and considerations made for data ethics. Then,\\nwe outline the methods for collecting data including a description of the data set and the data\\nscoping procedure. Additionally, we present the data analysis approaches to answer our high-level\\nresearch questions.\\n3.1 Platform Selection and Considerations for Data Ethics\\nWe licensed an existing dataset from an online peer support platform that aims to offer youth and\\nyoung adults a safe environment to discuss topics such as mental health, sexuality, religion, and\\nmore. The name of this platform was anonymized to protect the teens‚Äô identities whose data we\\nanalyzed. On this platform, users can create posts, leave comments on posts, and choose whether to\\npublish the posts anonymously or using their username. Hence, users have the ability to share their\\nscreen names and profile photos with other users when they intend, making their posts pseudo-\\nanonymous; thus, safeguarding users‚Äô personal privacy. The platform is designed exclusively for\\nsharing text-based posts, hence no images were collected in our dataset. While the dataset did not\\ninclude users‚Äô country of origin, most posts were in English. The licensed dataset consisted of about\\nfive million posts from 400 thousand users, including metadata (e.g., likes, hearts, and moods). The\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.422:8 Alluhidan et al.\\ndates of the posts in the dataset ranged from 2011 to 2020. More details about the characteristics of\\nour sample can be found in section 4.1.\\nThe Institutional Review Board (IRB) of our university determined that this study was exempt\\nfrom the review for human subject research because personally identifiable information (e.g., user-\\nnames, contact information) was removed from the dataset prior to being shared with the research\\nteam. Nevertheless, we ensured that all authors completed their IRB Collaborative Institutional\\nTraining Initiative (CITI training) on human subjects research before accessing this dataset. We\\nalso took great care to remove any potentially personally identifiable information from the posts\\nthemselves (e.g., screen names for other platforms, hyperlinks) prior to sharing posts as exemplar\\nquotations in the paper.\\n3.2 Data Scoping Process\\nOur primary goal was to examine how teens (ages 13-17) discussed different social media platforms\\nin their support-seeking posts. To accomplish this, we first ran a SQL query that searched for posts\\ncreated by users whose ages were between 13 and 17 years old. Then, we narrowed our search\\ncriteria based on a keyword search of the ten most popular social media platforms among teens\\nin the U.S., which included Instagram, Twitter, Facebook, Snapchat, YouTube, TikTok, WhatsApp,\\nReddit, Twitch, and Tumblr [ 138]. Our search resulted in posts mentioning nine of the top ten\\nplatforms (Twitch did not appear in the search results). Also, to focus on more recent trends, we\\nonly included posts from the last two years of the dataset (2019 and 2020), which resulted in a total\\nof 2,465 posts. Some of these posts ( n= 205) were coded as irrelevant due to the alternative use of\\nsome of the keywords we used in our search, which can be found in Table 1. For example, the post\\n‚Äúwelligy‚Äôall don‚Äôt understand \" appeared in our search results because of the word in bold. This\\npost was deemed irrelevant because of the keyword ‚Äúig,‚Äù which in this context means ‚ÄúI guess‚Äù.\\nLastly, after removing irrelevant (8%), duplicate (7%), and non-English posts (1%), the final dataset\\ncontained 2,061 posts. We only focused on the original posts, excluding the comments on the posts,\\nsince we aimed to understand the support-seeking behavior of teens through their posts, rather\\nthan the support and advice they received as comments.\\nTable 1. Scoping Search Terms\\nKeywords\\nSocial media platforms : Instagram, Insta gram, Ig, Twitter, TW, YouTube, You Tube, YT,\\nSnapchat, Snap Chat, SC, Reddit, Twitch, WhatsApp, Whats App, WA, Tumblr, Tiktok, Tik\\nTok, TT, Facebook, Face Book, FB\\n3.3 Data Analysis Approach\\nWe first conducted a thematic analysis [ 23] of teens‚Äô posts about their experiences on social media\\nplatforms (RQ1). To do this, we familiarized ourselves with the data by reading through the posts.\\nThe first two authors then discussed the main topics presented in the posts to create initial codes.\\nSubsequently, they divided the posts equally and coded using the same set of initial codes, while\\nactively identifying new themes. Whenever the two authors identified potential new codes, other\\nco-authors deliberated on whether to include them in the codebook. If they reached an agreement,\\nthe first two authors went back and re-coded all previous posts to incorporate the new code. This\\nprocess was iterative, involving constant communication and consensus among the researchers.\\nOnce all the data coding was completed, the first two authors worked with the last author to\\nconceptually group the codes into cohesive themes that aligned with our RQ1. Finally, we reviewed\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.Adolescent Social Media Use 422:9\\nour themes alongside our dataset to confirm that they actually captured important meanings within\\nthe coded data and named the themes. Table 2 presents the themes and their corresponding codes\\nalong with illustrative quotations. Since we employed a double-coding approach for the posts,\\nthe total count of codes may exceed the overall number of posts. For example, teens sometimes\\nmentioned more than one social media platform with varying experiences across the differing\\nplatforms. Therefore, the count of all codes can be totaled to be more than 100% of our total post\\ncount.\\nAfter completing qualitative analysis (RQ1), we leveraged our codebook for statistical tests to\\nidentify patterns and trends in the data. While RQ1‚Äôs qualitative analysis provided in-depth insights\\ninto teens‚Äô complex online experiences, a limitation lies in the challenge of integrating information\\nacross observations or assessing associations between observations [ 74]. Conversely, the strength of\\nthe quantitative approach lies in its ability to compare observations and examine associations [ 29].\\nOur mixed-methods design, crucial for addressing RQ2 (relationship between platforms and teens‚Äô\\nsupport-seeking behavior), combines qualitative and quantitative data [ 38]. This approach facilitates\\naccurate insights from confirmatory statistical analysis and deep explanatory descriptions derived\\nfrom thematic analysis. For the qualitative examination of the relationship between social media\\nplatforms and teens‚Äô support-seeking behavior, we conducted ùúí2tests, a between-group test for\\ntwo or more variables [ 126]. This method ensures that observed relationships between social media\\nplatforms and codes/themes from thematic analysis are not due to random chance, enhancing\\nthe rigor of thematic analysis findings. Prior research also supported such use of between-group\\ntests in studies involving qualitative analysis of online content [ 18,55,81,134]. To demonstrate\\nsignificant differences between social media platforms, standardized residuals were used, calculated\\nby \"dividing the product of subtracting expected from observed values by the square root of the\\nexpected value\" [ 126]. Through these tests, we interpret nuanced differences between platforms\\nbased on youth disclosures of their experiences.\\nPrior to addressing our RQs, we first assessed for potential effects of the COVID-19 pandemic,\\nwhich occurred in 2020. Therefore, we conducted a chi-squared ( ùúí2) between the years 2019 and\\n2020 based on our main themes from RQ1. This analysis revealed no significant differences in\\nthe described themes in Table 2, except for seeking information about social media. Standardized\\nresiduals showed that, during the pandemic, youth in our dataset were more likely to discuss\\nseeking information related to social media platforms in 2020, compared with 2019. We highlight\\nthis difference in our results (section 4.5). Next, we conducted a Chi-squared between-group analysis\\n(ùúí2) to examine any significant differences between the online platforms based on the codes for\\neach sub-theme (RQ2). We first categorized the posts based on the different social media platforms\\nteens mentioned in their posts. Some posts ( n= 122) had multiple social media platforms mentioned,\\nso we labeled the same post for each platform cited. For two of the social media platforms, Reddit\\nand Tumblr, the code counts were notably small, with over 20% of the codes having counts below\\nfive. This rendered our data inconsistent with the assumption of the ùúí2test of having the expected\\nvalues cells to be 5 or more in at least 80% of the cells [ 90]. Therefore, we excluded these two\\nplatforms and conducted ùúí2tests among the social media platforms that were more frequently\\ndiscussed among the youth.\\n4 RESULTS\\nIn our results, we first summarize the descriptive characteristics of our sample, followed by the\\nstatistical results (RQ2) across the social media platforms and the main themes that emerged from\\nour qualitative analyses (RQ1).\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.422:10 Alluhidan et al.\\nTable 2. Codebook for RQ1\\nThemes/Subthemes Codes/Subcodes Illustrative Quotations\\nThe Negative Side of Social Media (58%, n= 1,202 posts)\\nNegative Experiences\\nfrom Social Media Use\\n(43%, n= 891)Social drama (22%, n= 461)\\n-felt cheated\\n(11%, n = 230)‚ÄòI have my boyfriend‚Äôs account on TikTok and he made a comment on another\\ngirl‚Äôs video. He also says goodnight to me and then stays online. ‚Äù\\n-relationships developed online\\n(6%, n = 118)‚ÄòI accidentally was too nice to a man on WhatsApp (so he has my number)\\nbut I‚Äôm getting cold feet now bc he‚Äôs an adult and I‚Äôm still considered a minor\\nbut I don‚Äôt want to suddenly stop talking to him and he gets angry\"\\n-failed to get attention\\n(5%, n = 113)‚ÄòI feel so worthless when i take such good photos and post them on Instagram\\nand get little likes while all my other friends get more and more and lots of\\nfollowers while i lose about 1 a day and i always feel like giving up for good\"\\nCyberbullying\\n(4%, n= 74)‚ÄòHow tf [the fuck] you gonna follow my Instagram just to message me: \" Nigga\\nyou weird\", Seriously people have nothing better to do in their meaningless\\nlives than to pick on and judge people and sit behind a phone screen and talk\\nsmack\"\\nBody-shaming\\n(4%, n= 74)‚ÄòThe only thing girls are good for is judging me for showing my body on\\nInstagram and ... calling me ugly this ugly ass ugly as fuck and hurting me\"\\nExplicit content\\n(2%, n= 32)‚ÄòCan someone tell me why random inappropriate content is always shown on\\nmy Snapchat discover feeds?\"\\nSelf-harm content (1%, n=25) ‚ÄòI just posted how to kill yourself on tumblr... I hope I dont get banned. \"\\nInterpersonal privacy\\nviolation\\n(1%, n= 27)‚ÄòMy mom always shares private stuff about me on Facebook. I really don‚Äôt\\nlike that. \"\\nOther (10%, n= 198) ‚ÄòI‚Äôm really scared to open Instagram rn but I need to‚Äù\\nNegative effects of\\nusing social media\\n(15%, n= 311)Aroused anger\\n(5%, n= 111)‚Äòif i see anyone making a vid for Tiktok in public deadass im finna commit a\\nmurder\"\\nHarm to self-esteem\\n(3%, n= 70)‚ÄòI completely abandoned Instagram for 3 months. ... Just now, when me and\\nmy cousin decided to be active again, scrolling though the posts kicked in all\\nmy insecurities again. And this time, it‚Äôs worse than before\"\\nTriggered mental health\\nissues\\n(4%, n= 83)‚ÄòMy friend just posted an ugly face of me when I was a kid on Snapchat. Like\\nhe could‚Äôve just posted other pictures. I look hideous. I‚Äôm so anxious about who\\nis gonna see it. I started having panic attacks \"\\nDisrupted time\\nmanagement\\n(2%, n= 47)‚ÄòI watched youtube until I forgot that class is now ongoing\"\\nConnecting on Social Media (45%, n= 922 posts)\\nTo boost online\\npresence\\n(26%, n= 532)Invitation to follow each\\nother\\n(15%, n= 309)‚Äòwho wanna be friends with me on tik tok my ushername is [User Name]\"\\nSelf-promotion (11%, n= 223)‚Äòfollow my instagram account [User Name] it‚Äôs about depression, anxiety, low\\nself esteem and many more!\"\\nTo connect personally\\n(19%, n= 390)Sought relationship\\n(10%, n= 216)‚ÄòAnyone who won‚Äôt leave me on seen want to Snapchat and become friends?\\n[User Name]. \"\\nSought/offered emotional\\nsupport (8%, n= 174)‚Äòmy nan passed and i feeling nothing towards the death... I feel like I need to\\ntalk to someone. Is there anyone here to dm me?\"\\nThe Positive Side of Social Media (20%, n= 409 posts)\\nHelped as coping\\nmechanism (11%, n=\\n234)‚ÄòWhen I feel low during studying I take some funny selfies in snapchat\"\\nProvided inspirational\\ncontent (8%, n= 175)‚ÄòSo I just watched at positive Memes and stuff on Instagram and THAT‚ÄôS the selfcare that works\\nfor me\"\\nSeeking Information regarding Social Media (20%, n= 421 posts)\\nDiscussing social media\\ncontent (12%, n= 247)‚Äò[YouTube link] is this actually true?.. like for real?‚Äù \"\\nSocial media features\\n(5%, n= 100)‚Äòcan you have more than one super bff on snapchat?\"\\nTechnical challenges\\n(4%, n= 74)‚Äòanyone else Instagrams is messing up or just mine\"\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.Adolescent Social Media Use 422:11\\n4.1 Descriptive Characteristics of Our Sample\\nOur study involved an analysis of 2,061 posts generated by 1,038 distinct users between the age\\nrange of 13 to 17 years. However, all the posts found were made by adolescents who fell within the\\nage group of 15 to 17 years old. Notably, a significant proportion of the posts (70%) were created by\\nteens who were 17 years old, while about a third of posts came from those aged 16 (24%), and only\\n1% of posts came from those who were 15 years old at the time of posting. With respect to gender\\ndistribution, the majority of the posts were made by adolescents who identified as female (53%),\\nfollowed by those who did not specify their gender (33%), and lastly, by those who identified as\\nmale (14%). As shown in Table 3 , the most mentioned social media platform in adolescents‚Äô posts,\\nInstagram was the highest with 35% ( n= 720) of total posts. Snapchat ranked second with 20% ( n=\\n412). YouTube was the third most mentioned platform among teens with 14% ( n= 282), followed by\\nTikTok (10%, n= 207), Facebook (7%, n= 147), WhatsApp ( n= 127) and Twitter ( n= 125) with the\\nsame frequency level (6%). Lastly, Tumblr ( n= 28) and Reddit ( n= 13) were only mentioned in 1% of\\nthe posts. Keeping in mind the relative percentages expressed by platform, there are some notable\\ntrends in Table 3 to highlight. For instance, negative experiences were most frequently shared\\nwhen teens referred to Instagram, Facebook, Twitter, Tumblr, and Reddit. Snapchat and Whatsapp\\nhad the highest relative proportion of posts related to using social media to connect with people,\\nwhile YouTube had the largest proportion of posts related to information seeking. YouTube was\\nthe only platform where the number of positive posts exceeded the negative, possibly speaking to\\nits overall popularity among youth. Tables 2 and 3 serve as the organizing structure for reporting\\nour findings below. The ùúí2results for RQ2 are presented at the beginning of each subsection and\\nhelped guide the selection of illustrative quotations presented in our qualitative results (RQ1). We\\nuse illustrative quotations to describe each theme and subtheme of our results. Each quotation is\\ncontextualized with the teen‚Äôs age and gender information.\\n‚àí1.98\\n2.17\\n6.32\\n‚àí0.69\\n1.84\\n‚àí2.79\\n‚àí2.55‚àí0.89\\n0.14\\n‚àí0.89\\n1.25\\n‚àí0.58\\n‚àí0.22\\n1.331.62\\n‚àí0.24\\n‚àí2.3\\n‚àí0.76\\n‚àí0.08\\n‚àí0.54\\n0.371.66\\n‚àí0.97\\n‚àí1.56\\n0.31\\n‚àí0.04\\n0.52\\n‚àí0.64‚àí0.58\\n‚àí1.17\\n‚àí2.9\\n‚àí0.57\\n‚àí1.46\\n7.01\\n1.281.35\\n‚àí1.9\\n‚àí1.93\\n0.56\\n0.13\\n‚àí1.13\\n1.131.61\\n‚àí0.5\\n‚àí2.16\\n0.25\\n‚àí1.09\\n‚àí1.27\\n0.68\\n‚àí2.9‚àí1.91‚àí0.920.081.072.063.054.045.036.027.01Instagram Snapchat Y outube TikTok Facebook Whatsapp Twitter\\nSocial drama\\nCyberbullying\\nBody‚àíshaming\\nExplicit content\\nSelf‚àíharm content\\nInterpersonal privacy violation\\nOther\\n(N=68)(N=5)(N=13)(N=12)(N=59)(N=41)(N=188)\\n(N=44)(N=5)(N=3)(N=9)(N=12)(N=15)(N=86)\\n(N=21)(N=2)(N=2)(N=2)(N=2)(N=7)(N=55)\\n(N=8)(N=2)(N=1)(N=2)(N=1)(N=2)(N=29)\\n(N=22)(N=13)(N=0)(N=2)(N=0)(N=4)(N=40)\\n(N=11)(N=0)(N=1)(N=2)(N=0)(N=0)(N=24)\\n(N=12)(N=0)(N=0)(N=2)(N=0)(N=3)(N=30)\\nFig. 1. Results (standardized residuals) of the between-group analysis of social media platforms based on the\\nteens‚Äô discussions about negative experiences when using social media ( ùëÅ=891). (*) indicates significant\\nassociation. Note that green denotes a positive association, while red denotes a negative one.\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.422:12 Alluhidan et al.\\nTable 3. How the ways of discussing different Social Media platforms changed across the platforms. *\\nPercentages shown in each column are standardized to the total number of posts for that column to enable\\nrelative comparisons across social media platforms.\\nSocial Media Platform Type Instagram Snapchat Youtube TikTok Facebook Whatsapp Twitter Tumblr Reddit\\nNegative experience from social media use 36%* 28% 21% 20% 42% 29% 29% 36% 38%\\nSocial drama 18% 14% 12% 13% 21% 19% 18% 14% 9%\\nCyberbullying 4% 3% 3% 1% 2% 0% 2% 2% 3%\\nBody-shaming 6% 2% 0.5% 0% 0% 0% 0% 0% 0%\\nExplicit content 1% 2% 0.5% 1% 1% 1% 1% 2% 0%\\nSelf-harm content 1% 1% 0.5% 0% 0% 1% 0% 5% 3%\\nInterpersonal privacy violation 0% 1% 0.5% 1% 7% 0% 0% 0% 0%\\nOther 6% 5% 5% 4% 11% 9% 7% 12% 22%\\nNegative effects of using social media 11% 6% 10% 28% 8% 6% 10% 19% 6%\\nAroused anger 3% 1% 3% 17% 3% 4% 4% 14% 3%\\nHarm to self-esteem 4% 2% 2% 2% 2% 1% 1% 0% 0%\\nTriggered mental health issues 3% 2% 2% 4% 2% 1% 4% 2% 3%\\nDisrupted time management 1% 1% 3% 5% 1% 0% 1% 2% 0%\\nSocial Media to boost online presence 23% 20% 15% 16% 7% 10% 23% 12% 19%\\nInvitation to follow each other 13% 15% 7% 11% 4% 6% 11% 0% 0%\\nSelf-promotion 10% 5% 8% 5% 3% 4% 12% 12% 19%\\nSocial Media to connect personally 13% 29% 3% 4% 8% 33% 6% 0% 0%\\nSought relationship 7% 20% 0% 3% 0% 16% 6% 0% 0%\\nSought/offered emotional support 6% 9% 3% 1% 8% 16% 0% 0% 0%\\nPositive sides of social media 6% 7% 24% 24% 19% 12% 21% 17% 16%\\nHelped as coping mechanism 1% 5% 11% 14% 13% 3% 17% 12% 13%\\nProvided inspirational content 5% 2% 14% 10% 6% 9% 4% 5% 3%\\nSocial Media to seek information 12% 10% 27% 8% 16% 10% 12% 17% 22%\\nDiscussing social media content 5% 4% 23% 5% 6% 7% 3% 12% 16%\\nSocial media features 3% 4% 2% 3% 6% 2% 7% 0% 0%\\nTechnical challenges 4% 2% 2% 0% 4% 1% 1% 5% 6%\\nTotal number of posts: 720 412 282 207 147 127 125 28 13\\n4.2 The Negative Side of Social Media\\nWe found that in 43% of the posts ( n= 891), teens mentioned different social media platforms\\nincluded in our dataset to share the negative experiences that they had online. The ùúí2test indicated\\nsignificant differences between these platforms based on relative expected proportions of the total\\nnumber of posts(ùúí2(ùëë ùëì=36)=120.11, ùëù<0.001). Below, we present how teens explained the\\nadverse experiences they faced on different social media platforms.\\n4.2.1 Negative Experiences from Social Media Use. Teens ( N= 401) shared many different kinds\\nof unpleasant events that took place in their online lives. Among them, social drama was the\\nmost prominent. Around a quarter of the posts (22%, n= 461) described different kinds of social\\ndrama that teens experienced from their social media feeds. Through these posts, adolescents\\noften brought up the contentions, disputes, and strife they were going through with their social\\nnetwork, which often made them more stressed, sad, or upset. One of the most frequent types of\\nsocial drama we saw is that teens often shared how their close social media connections made\\nthem feel cheated (11%, n= 230). Teens most often felt cheated when their close social relationships\\n(e.g., close friends, romantic partners) did not react or comment on their social media posts but\\nrather engaged with someone else‚Äôs posts. For instance, a teen shared her feeling of distress when\\nher romantic partner commented on other girls‚Äô content on TikTok:\\n‚ÄúI feel physically sick. I‚Äôm almost certain my bf is using TikTok to follow and talk to other\\ngirls. He‚Äôs just commented on this girl‚Äôs post and liked it even though he shouldn‚Äôt be\\nsaying those things because he goes out with me. ‚Äù - Female, 17 years old\\nTeens also often felt deceived when they saw their friends or partners staying active online but\\nnot replying to their private messages on social media platforms. This indicates that adolescents\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.Adolescent Social Media Use 422:13\\nassumed their social connections were active on social media by seeing their profile‚Äôs active status,\\ne.g., green dot on Instagram or Facebook Messenger. For example, a teen shared his disappointment\\nwith online friendships upon discovering that his friend had falsely claimed to be going to bed,\\nonly to find out that she was still active on Instagram.\\n‚ÄúFuck online friends. All they do is lie to me. This girl says she‚Äôs heading to sleep on my\\nInstagram but here I see her still online. Why do they all do this to me?\" - Male, 17 years\\nold\\nTeens often expressed a sense of betrayal from deception, particularly when close connections\\nengaged more with others‚Äô content or appeared active online but unresponsive to them, which\\ntriggered their need for validation and belonging. We saw that this need for validation, when\\nunmet on platforms designed to foster social connections, exacerbated feelings of isolation and\\ninadequacy. The digital visibility of these interactions, such as likes and comments, thus served as a\\ndouble-edged sword, offering potential for affirmation, while also exposing vulnerabilities leading\\nto feelings of neglect or rejection.\\nBesides the above type of social drama, teens also posted about the relationships that they\\ndeveloped (6%, n= 118) on social media spaces that were potentially risky. Here, teens most often\\nmentioned meeting strangers online who gave them a sense of importance by engaging with their\\nsocial media posts through likes and comments and/or by messaging. We saw that most of these\\nposts also depicted how these relationships, often romantic, instigated more emotional distress\\nfor them. For example, a teen shared her apprehensions after realizing that the individual she met\\non WhatsApp was considerably older than her and such an age difference could lead to potential\\ncomplications in their relationship.\\n‚ÄúI talk to a guy on WhatsApp, I think he is way older than I am. I now realized it might\\ncreate problems for me. This is making me worried sick. I dont want to things go messed\\nup between him and I. Any suggestion on how to breakup with him but nicely?\" - Female,\\n16 years old\\nThe example above exemplifies the broader challenges some teens in our dataset encountered,\\nwhere interactions with strangers initially seemed validating, especially in the absence of real-life\\nconnections. However, such interactions later became troubling, raising concerns about the safety\\nand intentions of those involved. These types of posts highlight the critical need for awareness and\\ncaution in these interactions.\\nIn around 5% ( n= 113) of the posts, teens shared their sadness or distress regarding their social\\nmedia follower counts. They created different digital content and shared them on their social\\nmedia profiles, which failed to get enough attention from their social media followers. They often\\nmentioned that their shared content did not receive as many reactions or comments as other digital\\ncontent creators usually would get, and they felt frustrated by this social comparison. For instance,\\nin the following post, a teen compared the view counts of her own YouTube content with others‚Äô\\nand shared how discontent she felt about it.\\n‚Äúwhy is it that i work so hard to do well on YouTube and want to make ppl happy, but i\\ncan barely hit 100 views anymore. but ppl who post the most toxic videos get millions of\\nviews :(. \" - Female, 17 years old\\nThe post above reflected a trend in the larger dataset, which illustrated how social media platforms\\ntended to make teens feel like their social worth could be quantified. This emphasis on numeric\\nmetrics often led teens in our dataset to compare themselves to others unfavorably. We observed\\nthat such comparisons skewed self-perception and fueled a continuous search for validation through\\ncontent creation, which often compromised authentic self-expression and overall well-being.\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.422:14 Alluhidan et al.\\nNext, around 4% ( n= 74) of posts depicted teens‚Äô cyberbullying experiences on social media\\nplatforms included in our dataset. As illustrated in Figure 1 , when comparing the proportions\\nof teens‚Äô cyberbullying disclosures for each platform, we found that teens pinpointed Instagram\\nsignificantly more frequently when they discussed cyberbullying experiences in their posts. Most of\\nthe teens‚Äô cyberbullying posts illustrated how teens were often harassed online regarding different\\ncontent they shared in the form of images, videos, and/or texts on social media. Teens often\\nmentioned that they received these hateful messages not just as comments on their social media\\nfeeds, but also as private messages, even from people they barely knew. For instance, a 17-year-old\\nteen posted about a message received on Instagram containing offensive language and an aggressive\\ntone:\\n‚ÄúSo this bitch somehow messaged me on Instagram that im a pathetic little bitch and asks\\nme but you wanna die you claim to be? Whoever the fuck you are Just fuck off\" - Female,\\n17 years old\\nWe also found that around 4% of the posts ( n= 74) were about different body-shaming comments\\nteens received on the social media platfroms included in our dataset. These teens mostly shared that\\ntheir social connections made inappropriate or negative comments about their appearances, espe-\\ncially their body size or shape. For the body shaming posts, Instagram also had the highest relative\\nproportion of teens disclosures about body shaming. This indicated that teens discussed their body\\nshaming experiences on Instagram in their posts more frequently than on other platforms included\\nin our dataset. The standardized residuals indicated that teens were statistically significantly more\\nlikely to discuss their body shaming experiences on Instagram and less likely to talk about these\\nexperiences on YouTube, Facebook, and Twitter (Figure 1). The body shaming experiences shared\\nby teens, though varied, were predominantly influenced by societal beauty standards. Many teens\\nin our dataset noted that individuals who conform to these norms often receive praise and positive\\nattention for their body images. However, when the teens themselves posted pictures where they\\nfelt confident but did not meet these standards, they encountered criticism and judgment.\\n‚ÄòI guess i have to look good and really attractive before any girl talks to me. To them, i\\nhave to look really fit and good looking. Most of them just unfollow me on apps like IG\\nand focus so much on my looks and body that they judge me and make me feel bad about\\nmyself saying my body is shit. Putting me down and making me insecure for showing it\\noff on instagram. I fucking hate standards and shallow people \" - Male, 17 years old\\nApart from the above negative experiences, we found some posts that showed how teens perceived\\ntheinappropriate content they got exposed to when using the different social media platforms\\nincluded in our dataset. About 2% of the posts ( n= 32) had complaints regarding different adult\\ncontent teens found while exploring social media. We noticed that most of these posts are regarding\\nother users who posted nude photos, implicating that some social media platforms allowed nudity.\\nHere, majority of these posts illustrated teens‚Äô feelings of disbelief that some social media platforms\\nwith significantly large numbers of users lacked such content moderation. In one of such posts, a\\nteenager questioned why a mirror selfie featuring someone showing their buttocks was considered\\nacceptable and had not been removed from Instagram:\\n‚ÄúHow the fuck was a mirror selfie of someone showing their ass on Instagram acceptable\\nand not taken down? I‚Äôm so confused. \" - Female, 16 years old\\nTeens also sometimes encountered self-harm content on social media platforms included in our\\ndataset and posted about it (1%, n= 25). We observed that among all other negative experiences from\\nsocial media use, teens were most distressed about this type of content. They mostly mentioned\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.Adolescent Social Media Use 422:15\\nthese contents showed someone‚Äôs suicide or self-injury/cutting stories. We also noticed that teens\\ncomplained about content that encouraged starvation or a fast diet to lose weight and explicitly\\nidentified such content as promoting self-harm. For example, a 17-year-old female expressed\\nfrustration with Instagram for not deleting a post uploaded by an internet celebrity who is well-\\nknown for extremely thin body, which she perceived as self-harm content.\\n‚ÄúFuck you instagram for not taking down [celebrity name]‚Äôs posts while it‚Äôs clearly self\\nharm and we all report it. \" - Female, 17 years old\\nWe also found a few posts (1%, n= 27) describing how using social media violated teens‚Äô sense\\nof digital privacy . Most of these posts were about privacy breaches in which they felt their\\nsocial media accounts were hacked or somehow their social connections viewed some of their\\nposts that were set private. Teens also often complained about their close circles, e.g., families\\nor friends, posting their photos or writing about their personal issues on social media without\\nasking for their prior approval. Facebook had a higher relative proportion of posts discussing\\nthese experiences than other platforms in our dataset as shown in Figure 1. This indicated that\\nteens‚Äô posted more frequently about the violations that occurred to their interpersonal privacy on\\nFacebook. When further examining these posts mentioning Facebook, teens mostly expressed that\\nthey felt their privacy was violated when their parents posted about them on Facebook without\\ntheir consent (a.k.a \"Sharenting\" [ 22,128]). For instance, the following quote describes a female\\nteen‚Äôs strong dissatisfaction with her mother sharing extensive details of her life on Facebook. She\\nshared concerns particularly about her mother‚Äôs constant updates related to herself, including her\\nactions, current activities, and remarks, accompanied by ridicule, which made her feel a violation\\nof her privacy.\\n‚ÄúIt bothers me a lot when my mother posts about every single thing that happens in our\\nlife on Facebook. Especially about me (what I do, currently do and say, she ridicules me\\nfor it), I feel like it violates my privacy. \" - Female, 17 years old\\nTo conclude, teens discussed different negative experiences they faced on social media. These\\nposts mostly included complaints about social drama, cyberbullying, and body shaming. Sometimes,\\ncriticisms about social media platforms were shared without specific reasons. Next, we discuss the\\nnegative effects adolescents had when they used different social media platforms.\\n‚àí2.51\\n3.16\\n0.53\\n‚àí1.04‚àí2.2\\n1.33\\n1.17\\n‚àí0.1‚àí1.31\\n0.11\\n‚àí0.5\\n2.214.67\\n‚àí3.51\\n‚àí2.15\\n0.6‚àí0.29\\n‚àí0.44\\n0.98\\n‚àí0.322.44\\n‚àí0.73\\n‚àí0.93\\n‚àí1.220.8\\n‚àí1.65\\n1.56\\n‚àí1.03\\n‚àí3.51‚àí2.7‚àí1.88‚àí1.06‚àí0.240.581.42.213.033.854.67Instagram Snapchat Y outube TikTok Facebook Whatsapp Twitter\\nAroused anger\\nHarm to self‚àíesteem\\nSM tiggered mental health issues\\nDisrupted time management\\n(N=15)(N=34)(N=39)(N=31)\\n(N=5)(N=12)(N=11)(N=6)\\n(N=12)(N=11)(N=11)(N=12)\\n(N=11)(N=10)(N=4)(N=37)\\n(N=2)(N=6)(N=3)(N=5)\\n(N=0)(N=1)(N=1)(N=6)\\n(N=1)(N=7)(N=1)(N=7)\\nFig. 2. Results (standardized residuals) of the between-group analysis of social media platforms based on\\nthe teens‚Äô discussions about negative effects when using social media ( ùëÅ=311). (*) indicates significant\\nassociation. Note that green denotes a positive association, while red denotes a negative one.\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.422:16 Alluhidan et al.\\n4.2.2 The Negative Effects of Using Social Media. In addition to sharing about their negative\\nexperiences, 183 teens also discussed the adverse effects they had from using social media (15%, n=\\n311). The ùúí2test resulted in differences between the online platforms based on their discussions\\nabout the adverse effects of social media platforms ( (ùúí2(ùëë ùëì=18)=51.01, ùëù<0.001). Within these\\nposts, teens discussed different negative impacts, e.g., affecting self-esteem, mental well-being, and\\nability to manage their time effectively, that social media usage had on them.\\nIn around 5% of the posts ( n= 111), teens shared their experiences of how various types of\\nonline content they encountered aroused their feelings of anger . Although the triggers for\\nanger varied among teens, in most cases, their anger stemmed from disagreements with certain\\ntrends or challenges that they perceived as trivial or even potentially harmful. TikTok had the\\nhighest relative proportion of posts by teens expressing their anger followed by Instagram. In\\naddition, the standardized residuals revealed significant positive associations between disclosures\\nof aroused anger and TikTok and WhatsApp, as well as a significant negative association between\\nthese disclosures and Instagram and Snapchat (Figure 2). This indicated that teens had the highest\\nprobability to discuss their aroused anger experiences on TikTok and WhatsApp and had the\\nleast probability to post about this anger on Instagram and Snapchat. Wihtin these posts, teens\\nexpressed frustration when confronted with content that did not align with their personal tastes,\\nvalues, or interests. This often made them question the popularity of such content, creating a\\nfeeling of disconnection and subsequently leading to anger. For example, a feeling of frustration\\nwas expressed by a teen who complained about people creating TikTok videos that baited users to\\nwatch until the end for a big reveal that never came:\\n‚ÄúEverytime I see those lame unfinished videos on TikTok, my blood boils!!\" ‚Äì Female, 16\\nyears old\\nNext, in about 3%, ( n= 70) of the posts, teens mentioned how different online content they\\nviewed harmed their self-esteem . By looking at Figure 2 and comparing the relative proportion\\nof teens‚Äô harm to self-esteem posts across the platforms, we found that Instagram had the highest\\nrelative proportion, reaching a positive statistical significance. This finding suggested that teens\\nmost frequently posted about harm to their self-esteem that occurred on Instagram, compared to\\nother social media platforms included in our dataset. Through these posts, teens often mentioned\\nhow following individuals who they perceive as having a desirable lifestyle or attractive appearance\\nled them to social comparison. Here, they mainly compared their own lives, achievements, and\\nappearance to those showcased on social media, which sometimes resulted in feelings of inadequacy\\nor a diminished sense of self-worth. Interestingly, we found some posts where adolescents shared\\nthe feelings of frustration and low self-esteem while blaming their appearance for the low user\\nengagement on their social media photos. For example, a teen shared about how insecure he felt\\nabout the appearance when others‚Äô photos on Instagram got more ‚Äôlikes‚Äô than his own.\\n‚ÄúYou know I see all these guys and beautiful girls on Instagram and it gets me so uptight\\nand mad cuz they are so good looking and get a lot of likes and I barely get anything cuz\\nI‚Äôm so unattractive in person\" - Male, 16 years old\\nIn about 4% of the posts ( n= 83), teens indicated that certain content and features on some social\\nmedia platforms were associated with triggering their mental health issues . In these posts, teens\\nexpressed how the public nature of some social media platforms amplifies personal insecurities,\\nespecially when content is shared without their consent. For instance, some teens mentioned how\\nothers posting unflattering photos of them was a potent trigger for stress and anxiety. In other cases,\\ncertain features of social media platforms contributed to retriggering pre-existing vulnerabilities\\nand past traumas. For example, features like Snapchat‚Äôs ‚Äôflashback memories,‚Äô which are designed\\nto resurface past experiences, were reported to sometimes evoke unpleasant past events:\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.Adolescent Social Media Use 422:17\\n‚ÄúThose Snapchat flashbacks that pop up in your memories really got me fucked up tho\" ‚Äì\\nFemale, 17 years old\\nThe example above, illustrates the broader issue of how teens in our dataset felt that social media\\nalgorithms are designed to maximize user engagement, often prioritizing content that elicits strong\\nemotional reactions at the detriment of the users. Unfortunately, this approach, as observed in some\\nof the teens‚Äô posts, led to the increased likelihood of teens encountering content that triggered\\nanxiety, depression, or other mental health issues.\\nWe also found that in about 2% of the posts ( n= 47), teens expressed the negative impact of social\\nmedia on their time management. Those teens highlighted their concerns about the distractions\\nand addictive nature of social media platforms, which led to excessive time spent scrolling, watching,\\nand interacting online. When comparing the relative proportions of teens‚Äô posts discussing their\\ndisrupted time management across platforms in our dataset, Instagram had the highest relative\\nproportion of posts followed by YouTube. However, a statistically significant positive association\\nwas found between disrupted time management and YouTube as shown in Figure 2, indicating that\\nteens had the highest probability to post about YouTube when they discussed their disrupted time\\nmanagement. The primary concern voiced by these adolescents centered around the challenge of\\nsuccessfully prioritizing tasks and maintaining a balanced approach to time management in the\\nface of the continual appeal of social media. Within these posts, we observed that this is mainly\\ndue to the autoplay and recommendation features. For instance, one teen expressed how YouTube\\nis distracting her from studying by continuously recommending interesting videos:\\n‚ÄúYouTube is the worst when I‚Äôm trying to concentrate on work, it keeps recommending\\nsome great videos\" ‚Äì Female, 17 years old\\nMany teens highlighted the cascading effects of poor time management and impulse control\\nleading to addictive consumption of social media on their mental and physical health. The cycle\\nof procrastination and constant distraction not only increased stress due to unfulfilled academic\\nresponsibilities but also led to a decline in academic performance. The habit of spending excessive\\ntime on screens significantly infringed upon their sleep quality, eroding both their mental and\\nphysical well-being.\\nIn sum, teens posted to vent about different unpleasant experiences they faced on social media,\\nmostly because of the interaction with their social connections. They also often shared how the\\nabove negative encounters affected their mental health such as low self-esteem and recall of\\ntraumatic experiences. Within those posts, internet addiction and disruption in time management\\nwas another concerning trends. In the next section, we explore how teens leveraged the peer\\nsupport platform to connect with others on social media.\\n4.3 Connecting on Social Media\\nOverall, we found that 570 teens mentioned different social media platforms to connect with others\\nin their posts (45%, n= 922). To this end, they invited others to connect on a variety of different\\nsocial media platforms. Based on the ùúí2test, there were significant differences between these online\\nplatforms based on the codes that emerged from teens‚Äô discussions about connecting with people\\ntheme ( ùúí2(ùëë ùëì=18)=153.53, ùëù<0.001). Below, we present how teens invited others to increase\\ntheir social connections.\\n4.3.1 To Boost Online Presence . We found that in 26% of posts ( n= 532), teens cited social media\\nplatforms included in our dataset as a way to boost their online presence. For example, in 15% of\\nthe posts ( n= 309), teens shared their accounts and invited others to follow them in return for\\nfollowing them back. As illustrated in Figure 3, Instagram had the highest relative proportion of\\nposts related to teens‚Äô following invitations to others followed by Snapchat and YouTube. However,\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.422:18 Alluhidan et al.\\n0.61\\n3.19\\n‚àí2.89\\n‚àí1.09‚àí1.28\\n‚àí5.79\\n7.11\\n0.171.25\\n4.19\\n‚àí5.23\\n‚àí0.422.93\\n0.04\\n‚àí1.65\\n‚àí1.78‚àí0.94\\n‚àí0.35\\n‚àí2.98\\n4.73‚àí3.04\\n‚àí2.7\\n2.64\\n3.730.87\\n2.78\\n‚àí0.65\\n‚àí3.37\\n‚àí5.79‚àí4.5‚àí3.21‚àí1.92‚àí0.630.661.953.244.535.827.11Instagram Snapchat Y outube TikTok Facebook Whatsapp Twitter\\nInvitation to follow each other\\nSelf‚àípromotion\\nSought relationship\\nSought/offered emotional support\\n(N=65)(N=70)(N=111)(N=129)\\n(N=55)(N=109)(N=34)(N=86)\\n(N=14)(N=0)(N=35)(N=32)\\n(N=4)(N=6)(N=11)(N=24)\\n(N=15)(N=0)(N=6)(N=7)\\n(N=21)(N=21)(N=5)(N=8)\\n(N=0)(N=9)(N=19)(N=18)\\nFig. 3. Results (standardized residuals) of the between-group analysis of social media platforms based on the\\nteens‚Äô connecting on social media ( ùëÅ=922). (*) indicates significant association. Note that green denotes a\\npositive association, while red denotes a negative one.\\nthe standardized residuals showed a significant positive association between inviting others to\\nfollow with TikTok and a significant negative association with WhatsApp. This suggested that\\nwhen teens posted to invite others to follow them, they had the highest probability to mention\\nTikTok and the least probability to mention Whatsapp. For example, some teens encouraged others\\nto follow their TikTok channels with the promise of following them back:\\n‚ÄúIf anyone has a TikTok feel free to put it in the comments and I will be sure to follow you\\nback!\" ‚Äì Unspecified, 17 years old\\nIn 11% of posts ( n= 223), teens tried to enhance their online presence through self-promotion of\\ntheir social media accounts. Within these promotional posts, teens typically emphasized the content\\nthey provide in their accounts as a way to advertise themselves. Also, they often shared the links\\nof their digital content and requested others to like, share, or comment. Based on the standardized\\nresiduals, teens more frequently posted about Instagram, YouTube, and Twitter to promote their\\nprofiles on these platforms (Figure 3). Overall, we found that teens mentioned different social media\\nplatforms to enhance their online presence, mostly by exchanging their social media links and\\nfollow each other.\\n4.3.2 To Connect Personally. Our analysis revealed that in 19% of posts ( n= 390), teens cited social\\nmedia platforms included in our dataset as a means to establish interpersonal connections with\\nothers through messaging. Unlike boosting the social media presence theme, these posts were\\nmostly intended to invite others to personally talk or chat by sharing their user identifications\\non different social media platforms. For example, 10% of the posts ( n= 216) were about seeking\\nrelationships on social media that they could not establish offline. Within these posts, teens often\\nexpressed their desire to find the right partner, often displaying a sense of desperation, particularly\\nin cases where their peers had already formed romantic relationships. As illustrated in Figure 3,\\nwhen teens posted about their seeking relationships on social media platforms, these posts had the\\nhighest probability to involve Snapchat and WhatsApp. For instance, a teen shared his Snapchat\\naccount with a specific desire to look for a romantic partner:\\n‚ÄúI need a girlfriend. When I see my friends with girlfriends, I feel so mad. Please if you are\\na girl, my Snapchat is [User Name]. \" - Male, 17 years old\\nIn addition to seeking relationships, teens also shared (8%, n= 174) their social media accounts\\nwhen they wanted to exchange emotional support with others . In these posts, teens often\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.Adolescent Social Media Use 422:19\\nshared challenges, including depression, emotional detachment, and a desire for acceptance or\\npersonal connection with others. Also, we observed that teens often shared their social media\\naccounts to offer others to listen by sharing their own experiences and offering assistance to those\\nwho may be going through similar challenges. Instagram had the highest relative proportion of\\nthese posts followed by Snapchat, while based on the standardized residuals, teens‚Äô discussions\\nabout seeking or offering emotional support had the highest probability to involve Facebook or\\nWhatsapp (Figure 3).\\n‚àí6.32\\n6.323.43\\n‚àí3.43‚àí1.16\\n1.160.98\\n‚àí0.982.35\\n‚àí2.35‚àí1.88\\n1.883.69\\n‚àí3.69\\n‚àí6.32‚àí5.06‚àí3.79‚àí2.53‚àí1.2601.262.533.795.066.32Instagram Snapchat Y outube TikTok Facebook Whatsapp Twitter\\nHelped as coping mechanism\\nProvided inspirational content\\n(N=52)(N=8)\\n(N=10)(N=31)\\n(N=57)(N=48)\\n(N=23)(N=30)\\n(N=12)(N=26)\\n(N=11)(N=4)\\n(N=7)(N=28)\\nFig. 4. Results (standardized residuals) of the between-group analysis of social media platforms based on\\nthe teens‚Äô discussions about the positive sides of social media ( ùëÅ=409). (*) indicates significant association.\\nNote that green denotes a positive association, while red denotes a negative one.\\n4.4 The Positive Side of Social Media\\nMeanwhile, 151 teens talked in a portion of their posts (20%, n= 409) about the positive aspects\\nof different social media platforms. Statistically significant differences between the social media\\nplatforms based on the codes emerged from the positive sides of social media theme ( (ùúí2(ùëë ùëì=\\n6)=65.72, ùëù<0.001). About 11% of the posts made by adolescents ( n= 234) emphasized how\\nthey utilize social media as a coping mechanism during moments of low mood or emotional\\ndistress. YouTube had the highest relative proportion of posts describing social media as a coping\\nmechanism, followed by Snapchat and then TikTok as shown at Figure 4. Yet, the standardized\\nresiduals showed that when teens posted about how they used social media as a coping mechanism,\\nthese posts had the highest probability to involve Snapchat, Twitter, and Facebook and the least\\nprobability to involve Instagram. Within these posts, some teens explained how engaging with\\nothers on social media platforms provided them with a sense of belonging and connectedness\\nduring hard times. For instance, some teens mentioned that they share funny selfies on Snapchat\\nwhen they feel stressed during studying. Other teens found Snapstreak on Snapchat (i.e., tracks how\\nmany days in row users and their friends have exchanged Snaps and provides users extra points\\nfor their Snapchat score) rewarding enough to continue doing it as an important daily routine, as\\nshown in the following post.\\n‚ÄúThis Snapstreak on Snapchat is life saving. It‚Äôs the most important daily-routine of my\\nlife right now. Gives a purpose. \" - Female, 16 years old\\nOthers shared that consuming positive content on social media helped them relax when they\\nfelt stressed. For instance, some teens manage their Twitter accounts to push funny videos so that\\nwhen they feel stressed, they can visit Twitter and ease their stress by watching those videos. For\\ninstance, in the following post, a teen shared that they filtered their Twitter content by setting\\npreferences to display only humerous videos. When feeling sad, they turned to those curated feed\\nof videos which served as a form of distraction or mood enhancement for them.\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.422:20 Alluhidan et al.\\n‚ÄúI set my Twitter settings to have all funny videos and that helps me to stay away from all\\nBS. When I am sad I just scroll through and that just works like magic. \" ‚Äì Unspecified, 17\\nyears old\\nAs such, our analysis revealed that teens turned to social media for more than just posting content\\nor engaging in Snapstreaks; they used it as a tool for emotional coping and seeking social support.\\nThese platforms provided spaces tailored to their emotional and social requirements. Moreover, it\\nallowed teens to control their social interactions, empowering them in times of stress or isolation.\\nIn some cases (8%, n= 175), teens acknowledged social media as a valuable source of inspi-\\nration , motivating them through exposure to creative content, success stories, and inspirational\\nquotes. YouTube had the highest relative proportion of posts describing social media as a source of\\ninspiration. However, Instagram reached positive statistical significance as demonstrated at Figure 4.\\nThis suggested that teens more frequently included Instagram in their discussions about how social\\nmedia platforms provided inspiration and support. In their posts, teens most often expressed how\\nthis exposure helped them gain a fresh perspective when dealing with challenges and adversity\\nin their offline or online lives. For example, some teens shared how they were motivated by the\\ninspiring content they watched on Instagram:\\n‚Äú\"Treat yourself like someone you loved. \" - some inspiring dude I saw on Instagram. \" ‚Äì\\nFemale, 16 years old\\nOverall, teens mentioned different social media platforms to appreciate the way they received\\nsupport and inspiration from others and how social media, in general, helped them with their\\nmental health. Next, we explore the ways in which teens sought information regarding different\\nsocial media platforms on a peer support platform.\\n‚àí4.34\\n0.63\\n4.87‚àí2.26\\n1.83\\n0.838.26\\n‚àí4.95\\n‚àí50.18\\n1.11\\n‚àí1.5‚àí2.18\\n1.55\\n1.040.99\\n‚àí0.89\\n‚àí0.27‚àí2.66\\n3.79\\n‚àí0.89\\n‚àí5‚àí3.68‚àí2.35‚àí1.020.31.632.954.285.616.938.26Instagram Snapchat Y outube TikTok Facebook Whatsapp Twitter\\nDiscussing SM content\\nSocial media features\\nTechnical challenges\\n(N=40)(N=35)(N=50)\\n(N=13)(N=21)(N=25)\\n(N=4)(N=11)(N=103)\\n(N=1)(N=7)(N=11)\\n(N=8)(N=12)(N=12)\\n(N=2)(N=2)(N=9)\\n(N=2)(N=12)(N=5)\\nFig. 5. Results (standardized residuals) of the between-group analysis of social media platforms based on\\nthe teens‚Äô discussions about seeking information regarding social media ( ùëÅ=421). (*) indicates significant\\nassociation. Note that green denotes a positive association, while red denotes a negative one.\\n4.5 Seeking Information regarding Social Media\\nAs we examined posts, we found that 235 teens aimed to seek information about various aspects of\\nsocial media platforms in 20% of their posts ( n= 421). Upon closer examination of our standardized\\nresiduals between 2019 and 2020, we saw that this trend has increased during the pandemic\\ncompared to 2019. Overall, the ùúí2test yielded significant differences between the platforms based\\non the emerged codes within seek information theme emerged from teens‚Äô discussions ( (ùúí2(ùëë ùëì=\\n12)=92.86, ùëù<0.001). For the most part, this was noticeable when teens discussed social media\\ncontent (12%, n= 247). As illustrated in Figure 5, YouTube had the highest relative proportion of\\nteens‚Äô posts about discussing social media content across other platforms, showing also a positive\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.Adolescent Social Media Use 422:21\\nstatistical significance. This indicated that teens most frequently discussed content found on\\nYouTube when seeking information about social media content. Within this subset of posts, teens\\nactively sought input from their peers by requesting recommendations for various social media\\ncontent, e.g., YouTube videos, ranging from makeup tutorials to mental health support. Often times,\\nthey expressed a desire to confirm the authenticity and credibility of social media content.\\n‚Äú[Youtube link] is this actually true?.. like for real?\" ‚Äì Unspecified, 17 years old\\nNext, in 5% of the posts ( n= 100), teens were seeking information on how to use different social\\nmedia features . When comparing the relative proportions of posts asking about features of social\\nmedia platforms across all platforms, we found that Instagram had the highest relative proportions\\nfollowed by Snapchat and then Instagram as shown at Figure 5. However, based on the standardized\\nresiduals, there were significant associations between posts inquiring about social media features\\nand Twitter. This indicated that teens‚Äô posts asking about social media features had the highest\\nprobability to involve Twitter. We saw that many of these posts were centered around content\\ncreation, where teens posed questions regarding methods for creating fascinating and visually\\npleasing content. They primarily sought advice on employing filters, editing tools, and creative\\nstrategies to enhance their posts to appeal to their intended audience. In some other instances,\\nteens sought assistance in understanding how to navigate privacy features (e.g., how to connect\\nwith others on specific social media platforms) to manage their online presence.\\n‚Äúim sorry, but how are you gaining an audience on an English-language twitter?\" ‚Äì\\nUnspecified, 16 years old\\nLastly, a portion of the posts made by teens (4%, n= 74) centered around asking questions about\\ndifferent technical challenges they faced on social media. Figure 5 illustrated that Instagram had\\nthe highest relative proportion of posts discussing social media technical challenges in comparison\\nwith other platforms in our dataset, reaching also a positive statistical significance. This indicated\\nthat when teens seeking technical support, they most frequently referred to Instagram. In many\\ncases, the technical challenges were about server problems or scheduled maintenance, resulting in\\ntemporary unavailability or restricted functionality on the platforms. In these instances, teens turned\\nto the peer support platform to seek solutions and reassurances to navigate through the technical\\ndifficulties they encountered. For instance, a teen posted to inquire if anyone was experiencing\\nissues with their Instagram, specifically mentioning a problem where the feed was not updating or\\nchanging as expected.\\n‚ÄúSomebody here has a problem with their Instagram? like the feed won‚Äôt change. \" ‚Äì Female,\\n17 years old\\nIn summary, when teens mentioned various social media platforms in their posts, they predomi-\\nnantly posted to vent about different unpleasant experiences they faced on social media such as\\ncyberbullying and privacy violation, while referring to Instagram, and Facebook. Furthermore,\\nteens often shared how social media adversely impacted them in terms of their self-esteem, time\\nmanagement and arousing their anger particularly when mentioning Instagram, YouTube, and\\nTikTok. In addition, teens often focused on connecting with others in their posts for self-promotion\\nand increasing their followers while referring to Instagram, YouTube, and TikTok. Also, teens ex-\\npressed a need to communicate with others for emotional support and building online relationships\\nwhen mentioning Snapchat, Whatsapp, and Facebook. On the other hand, teens also recognized\\nand appreciated its positive aspects. For instance, they highlighted how social media served as\\na coping mechanism during difficult times and how it provided them with inspiration to strive\\nfor greater success, particularly citing Snapchat, Facebook, Instagram, and Twitter. Lastly, teens\\nalso sought information and guidance on how to effectively navigate the different social media\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.422:22 Alluhidan et al.\\nplatforms, mostly pointing to YouTube, Twitter, and Instagram. In the next section, we discuss the\\nimplications of these findings.\\n5 DISCUSSION\\nIn this section, we describe the implications of our findings in relation to prior work and provide\\ndesign implications for implementing adolescent online safety features to help teens reduce their\\nnegative experiences on social media and maximize the benefits reaped.\\n5.1 A Balanced Narrative on Adolescent Social Media Use (RQ1)\\nOur qualitative findings for RQ1 unpacked negative experiences that were indexed more frequently\\nthan the positive ones, as shown in Table 2 . Yet, these seemingly overwhelming number of negative\\nexperiences might stem from teens‚Äô inclination to share during challenging times, especially since\\nthe context of the digital trace data analyzed was intended to garner peer-support. Therefore,\\nwe stress for the readers to avoid marking teens‚Äô social media usage using a false dichotomy of\\nnegative or positive. Instead, our focus should be on understanding how teens feel vulnerable to\\nwhich online harms and for what underlying reasons so that we can provide appropriate support.\\nThis recommendation is well-aligned with prior research that emphasized studying the factors that\\nmake some teens at higher risk than others online [ 96]. The impact of social media platforms on\\nteens can vary from individual to individual, impacted by factors such as personality traits [ 110],\\nfamily environment [136], and the overall digital environment [88].\\nOur research also highlights positive aspects of social media use among teens, such as coping\\nwith stress by sharing their content, connecting with others online, watching inspirational content\\nto boost their mood and self-esteem, and seeking information during the pandemic. Zooming out\\nfrom the immediate findings, positive aspects of teens‚Äô social media usage were well-documented\\nin the surveys and/or empirical studies (e.g., increased social connection, identity development,\\nand positive emotions through social support) [ 13,53]. In addition, the meta-analyses confirmed\\nthat the association between social media use and teens‚Äô negative psychological well-being is still\\nunclear, as effects have been found to exist in both (positive and negative) directions, and there has\\nbeen little work done to rule out potential confounders [ 96]. In a recent Pew Research survey, the\\nmost common way teens describe the impact of social media was neither positive nor negative [ 13].\\nNevertheless, heightened attention to the negatives of teens‚Äô social media usage led U.S. Senators\\nBlumenthal and Blackburn to propose the ‚ÄúKids Online Safety Act (KOSA)‚Äù [ 59] to protect teens\\nfrom online risks. The legislation requires social media platforms to proactively mitigate risks to\\nminors. To do so, it requires independent audits and public scrutiny from experts to ensure that\\nsocial media platforms are taking meaningful steps to address risks to kids [ 59]. Some embrace\\nthe idea of KOSA given multiple claims that social media companies fail to protect teens [ 48],\\nwhile others fear that this legislation would incentivize social media sites to collect even more\\ninformation about minors to prevent potential risks for them.\\nGiven the positive stories teens shared in our study and the trends found in existing research, we\\nneed to move away from over-emphasizing the harms that social media usage can bring to teens\\nand be overly paternalistic about protecting them from potential harm. In the same Pew Research\\nsurvey above, 22% teens perceived that their parents are extremely worried about their social media\\nlife, while 39% teens share that their online experiences are better than parents think [ 13]. Therefore,\\nwe once again highlight that social media usage is not necessarily positive or negative; rather,\\nthe two aspects co-exist without voiding one another, and net effects (positive or negative) can\\nvary across different teens in various contexts. Therefore, further research is needed to understand\\ndiffering teens‚Äô online experiences and how to amplify positive aspects at the same time tackle\\nnegative aspects to better support teens‚Äô digital well-being.\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.Adolescent Social Media Use 422:23\\n5.2 Teens Pinpoint Concerns of Different Social Media Platforms Aligned with Their\\nAffordances (RQ2)\\nWhen teens discussed their negative experiences using social media, they often mentioned specific\\nsocial media platforms. Specifically, our statistical analysis (RQ2) illustrated how teens mostly\\nmentioned Instagram whenever they discussed body-shaming and its harmful effect on their self-\\nesteem, Facebook when they talked about interpersonal privacy violations, and YouTube when they\\ndiscussed their struggles with time management (Figure 1 and Figure 2). To some extent, these\\nstatistical differences can be explained by the social media affordances, features, and social norms\\nof these platforms that facilitate how users communicate, share information, and engage with\\nothers [ 142]. For instance, Instagram is a photo-focused platform, hence, a unique culture (e.g.,\\nusers get more attention by posting physically appearing photos) was created on this platform\\nthat has strongly emphasized the significance of physical appearances [ 132], which may affect\\nteens as viewing some idealized pictures of others diminishes their self-esteem [ 28]. Moreover, we\\nsaw posts in which teens described how following individuals whom they perceive as having a\\ndesirable lifestyle or attractive look on Instagram led them to social comparison and feel lower\\nself-esteem, which has also been documented in prior research [ 63]. Meanwhile, Facebook was\\nlaunched over 20 years ago as one of the first platforms that allow users to use their real names [ 82].\\nGiven this high identity affordances, family relationships are emphasized on this platform in a\\nway that parents like to post about their teens [ 26]. These ‚Äúsharenting‚Äù [ 22,128] posts were found\\nin our qualitative analysis as a source of frustration for these teens. On the other hand, YouTube\\noffers a sheer volume of videos available for teens and employs mechanisms, such as autoplay,\\nthat make them feel less in control of the time they spend on the platform [ 86], which may result\\nin their discussions about the long time they spent on this platform. While prior research has\\nmainly focused on studying these affordances to understand the needs that would motivate people\\nto use specific social media sites [ 68], our statistical analyses comparing the platforms based on\\nteens‚Äô negative experiences discussions highlight the importance of adopting an affordances-based\\napproach to address teens‚Äô negative experiences. Given the fast and evolving landscape of social\\nmedia platforms, adopting this approach become of great importance for future researchers to\\nprovide a more nuanced understanding of how specific affordances contribute to the negative\\nexperiences of teens and/or other populations. Below, we abstract some of the key social media\\naffordances that shaped teens‚Äô social media experiences, as described in our results:\\n‚Ä¢Visibility of Status: When status visibility was exposed, this often led to teens feeling\\ncheated, overlooked, and excluded, especially when someone‚Äôs words (e.g., ‚Äú I am going to\\nbed.‚Äù) did not align with their online activity (e.g., Green - ‚ÄúActive Online‚Äù)\\n‚Ä¢Public Content Sharing: Social media platforms that facilitated the public sharing of content,\\nparticularly image-based content (e.g., Instagram) amplified opportunities for cyberbullying,\\nwhich was often perpetrated through private channels.\\n‚Ä¢Image-based Sharing: Platforms that emphasized image-based sharing (Instagram) and\\nselfies are more susceptible to negative experiences, such as body shaming.\\n‚Ä¢Attention-seeking or Status-signalling Affordances: Attention or status-seeking affor-\\ndances, such as follows, views, and likes, made teens feel undervalued or unpopular when\\ntheir counts were lower than their peers or than their expectations.\\n‚Ä¢Self-Promotion versus Interpersonal Connection Affordances: The affordances of\\nsome social media platforms (e.g., Instagram, YourTube, Twitter) were better suited for teen\\ncontent creators who wanted to self-promote and create their own personal brands, while\\nother platforms were better suited for forming interpersonal connections (e.g., Snapchat,\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.422:24 Alluhidan et al.\\nWhatsApp). Differing privacy features afforded users to manage the audiences that they\\nwould like to interact with on each platform.\\n‚Ä¢Connecting Publicly with Like-minded Peers: Social media platforms that afford social\\nfeatures (e.g., ‚ÄúFollow‚Äù and/or ‚ÄúHashtag‚Äù on Twitter) facilitated users with building social\\nconnections by following each other and forming groups/communities of users with similar\\ninterests.\\n‚Ä¢Connecting Privately with Strangers: When social media platforms allowed teens to\\nconnect with strangers privately (e.g., WhatsApp), this often led to the formation of regrettable\\nrelationships.\\n‚Ä¢Content Filters: While most platforms had filters for inappropriate content, teens expected\\nthem to work better and expressed frustration when the content in their feeds (e.g., nudity)\\nbroke with this expectation. In contrast, when social media platforms afforded teens to create\\npersonalized content filters for what they wanted to consume versus avoid (e.g., sensitivity\\nfilter on Twitter), this contributed to more positive experiences for teens by lowering the\\nchances of being exposed to unwanted content.\\n‚Ä¢Sharenting: Some platforms (e.g., Facebook), likely due to real name affordances and the\\nuser audience, facilitated practices such as ‚Äúsharenting,‚Äù parental sharing of content related\\nto their teens [77, 93], which led to interpersonal privacy violations.\\n‚Ä¢Ephemerality of Content: Social media platforms that provided features where users could\\nset how long their networks could view the content (e.g., Snapchat) had the potential of\\nlowering teens‚Äô pressure to self-present and impression management, which has also been\\nsuggested by prior works [19, 104].\\n‚Ä¢Curated Memories and Flashbacks: Similar to the findings of prior work, social media\\nplatforms that curated past memories (e.g., Snapchat) had the risk of retraumatizing youth\\nbased on negative past experiences [ 122], particularly for those who experience major changes\\nin their lives [36].\\n‚Ä¢Autoplay Content and Algorithmic Recommendations: Social media platforms that\\nused algorithms to recommend content and autoplay content (e.g., YouTube) had the potential\\nof facilitating addictive patterns of consumption and time management struggles, which has\\nbeen confirmed by prior research [86].\\n‚Ä¢Other Engagement Features: Features that promoted engagement (e.g., Snapchat streaks)\\nserved as a double-edged sword of promoting motivation and distraction from life‚Äôs problems\\nbut also addictive behaviors.\\nIn summary, we urge future research to adopt our affordances perspective when attempting to\\nunderstand the differential patterns and consequences of social media use on youth. By taking\\nthis broader perspective, we can make a marked shift in our public discourse, research, design\\npractices, as well as policies aimed at safeguarding youth from the detrimental effects of social\\nmedia by moving towards a ‚Äú safety by design ‚Äù [1,41,103] perspective that proactively considers\\nhow to shape the design-based affordances of social media in a way that is protective of youth\\nwithout restricting or over-policing their online experience.\\n5.3 Implications for Design\\nOur study provides insight into the ways how teens discuss different social media platforms when\\ninteracting with other teens on online peer support tools, suggesting features and mechanisms for\\nadolescents‚Äô online safety tools that would be needed to help them stay safe on social media.\\nDesign Affordances to Prioritize Youth Digital Well-being. Given that some of the social media\\naffordances that we identified above (Section 5.2) appeared to be problematic for teens, design-based\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.Adolescent Social Media Use 422:25\\nrecommendations can be drawn directly from our findings. For instance, as status visibility can\\nnegatively impact teens with feeling cheated or excluded, social media platforms can consider\\ndecreasing the visibility of status by providing options to manage the audience of their online status\\nor turn online status off by default and choose whether they prefer to appear online. Also, as features\\nsuch as views and likes can contribute to teens‚Äô feelings of low self-esteem and being left out,\\nsocial media platforms can consider reducing some of the attention-seeking and/or status-signaling\\naffordances. For instance, Facebook and Instagram are considering options to remove ‚ÄúLikes‚Äù for\\nthose under 18 [ 127]. Other social media platforms with similar attention-seeking affordances can\\nalso consider the same design changes to reduce teens‚Äô pressure for social presence and attention-\\nseeking on social media. Meanwhile, teens can become more susceptible to stranger danger on social\\nmedial platforms that afford teens to connect with strangers privately. Therefore, such platforms\\ncan create friction for connecting with strangers privately (e.g., WhatsApp, Instagram). For instance,\\nsocial media platforms can implement nudging systems to inform teens about the potential risks of\\nviewing messages from strangers and to let them choose whether or not to view the message at\\nall. Recently, Instagram implemented a policy in which adult users are not allowed to privately\\nmessage teens under 18 who do not follow those adult users [ 61]; proactive interventions such as\\nthis can help social media platforms to actively moderate online stranger danger.\\nAdditionally, as autoplay, recommendation algorithms, and other engagement features (e.g.,\\nSnapchat Streaks) can contribute to addictive patterns of online content consumption and time\\nmanagement struggles. In fact, addictive media use and consumption have become one of the\\nsalient social issues to the extent that a bill that requires social media companies to take measures\\nto mitigate the risks of internet addiction, the ‚ÄúSocial Media Addiction Reduction Technology\\nAct (SMART Act),‚Äù being proposed by the U.S. Senate [ 34]. The bill was to prohibit social media\\ncompanies to exploit users with addictive features such as YouTube autoplay and SnapStreaks\\nwhich make it difficult to leave a social media platform [ 117]. With these new regulations, social\\nmedia platforms are starting to introduce new safety features (e.g., features that allow users to\\ncustomize screen time limits for each day) that can help control their media use [ 69]. Along with\\nthe SMART Act bill and new safety features, we call for social media platforms to provide teens\\nwith more safety options including turning off or customizing autoplay and other engagement\\nfeatures. More ideally, we suggest design recommendations to support teens‚Äô healthy social media\\nusage by promoting self-regulation and autonomy, which we explicate in detail in the following\\nsection.\\nToward Intentional and Meaningful Social Media Use. Our statistical analyses showed that teens\\nare more likely to refer to YouTube when they discuss disrupted time management. That is, video-\\nsharing social media platforms tend to keep teens engaged to a level they feel is unhealthy, hence,\\nteens need more agency to set healthy boundaries (e.g., screen time) when on social media. In fact,\\nvideo-sharing platforms are known to intentionally employ a variety of design mechanisms (e.g.,\\nautoplay and recommendation) to maximize users‚Äô engagement with the platforms; even big-tech\\nindustry insiders warn that many of these mechanisms are designed to exploit psychological\\nvulnerabilities and negatively impact the users [ 86]. These practices that exploit human psychology\\nto substantially impede freedom of choice led the US Senate to propose the SMART Act [ 34]. One\\nway to manage screen time is through the approaches of intentional and planned use of social\\nmedia platforms (especially multimedia-based ones). Previous research documented that users can\\nmake active choices to control their screentime on YouTube when they have a specific intention in\\nmind; the more specific these intents (e.g., to watch a certain video on a certain topic), the stronger\\nagency to control their screen time [ 86]. The intentional and planned use of media is known to\\nbe especially effective in promoting self-regulation from an early childhood [ 57]. With the initial\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.422:26 Alluhidan et al.\\nprompt from parents, teens are capable of learning intentionality and making goal-directed choices\\nas planned, which is the mediating factor in developing self-regulation [ 57]. Therefore, social media\\nplatforms, particularly multimedia-based platforms such as YouTube or TikTok could consider\\nadding a screentime planning feature where teens can indicate their intention and set screentime\\nthemselves before using the platform, and reward them for achieving pre-planned screentime. This\\nwill allow teens to navigate healthy boundaries that work best for them. Also, it will ease tension\\nbetween parents and teens, especially those who struggle with negotiating conflicting boundaries.\\nDesigning Context-Aware and Intelligent Safety Features with Teens. Our results showed that teens\\nare still being exposed to risky content (i.e., explicit and/or self-harm content) even though there are\\nfiltering and/or reporting features in place on social media. This suggests that the safety features\\ndo not reflect contextualized teens‚Äô risk experience, largely because while risk is highly subjective,\\nsafety features are designed based on the risk perceptions of the third person [ 99]. Hence, we need\\nto look again at those safety features through the lens of teens‚Äô perspective. One way to do so is to\\nactively include teens in the design of safety features of social media through participatory design\\nthat considers teens as the primary stakeholders of their own online experiences. For instance,\\nthe social media platforms where teens are more likely to encounter self-harm or explicit content\\ncan have co-design sessions with them to understand how they use their existing reporting tools\\nand how to improve the usability of such tools. One potential resolution may involve improving\\nthe visibility of these reporting tools. They can also design new user interfaces with teens such\\nas real-time nudges to alert those who try to post self-harm content to think twice before they\\npost them. Yet, the alerts approach introduces an important and ongoing debate about balancing\\nthe risks associated with self-harm content online against the critical need for a supportive space\\nwhere youth can express and manage their struggles safely [ 79]. Therefore, it is essential to develop\\nnuanced policies and designs that focus on how to safeguard young users while facilitating their\\naccess to help and empathy. One way to understand the interplay between the risks of exposure\\nto harmful content and their need for supportive spaces could be conducting co-design sessions\\nwith teens who could provide valuable insights on what types of alerts, content filters, or support\\nprovisions would be most effective for them. In addition, by giving agency to the design process of\\ntheir online safety solutions, teens can reflect on their own online habits and learn how to self-\\nregulate those habits in ways that promote resilience, autonomy, and digital well-being [ 2,9,30].\\nTherefore, we encourage social media developers and designers to actively work with teens to\\ninclude their unique perspectives when designing safety features for promoting their online safety.\\nTowards Teens-Led Efforts for New Data Collection to Train Online Risk Detection Algorithms. In\\nthis paper, we identified salient negative experiences and effects that teens discussed when referring\\ntodifferent social media platforms. Our results indicate that algorithmic approaches to identify\\nonline risks on social media for youth could be more teen-centered and effective if they take into\\naccount the difference between the platforms and the types of risks teens discuss encountering\\nthe most on those platforms. In contrast, prior works on machine learning (ML) algorithms for\\ndetecting social media risks for youth have mainly centered on training models using available\\ndatasets [ 11,73,115] without considering teens‚Äô shared experiences on these platforms. Therefore,\\ninstead of focusing our efforts on collecting benchmark datasets from various social media platforms,\\nwe recommend prioritizing addressing platform-specific challenges that teens discussed when they\\ndisclosed their negative experiences on these platforms. Collecting data from platforms that align\\nwith teens‚Äô risk experiences would enhance the relevance and realism of the training data. For\\ninstance, there have been efforts to publish benchmark datasets from several social media platforms\\nsuch as Facebook, Twitter, Instagram, and Reddit to train detection models for body shaming [ 49].\\nHowever, in our study, we found that adolescents were most likely to discuss experiencing this risk\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.Adolescent Social Media Use 422:27\\non Instagram. Therefore, to make the data collection efforts more focused and teens-centered, we\\nsuggest using data from Instagram to train automated algorithms to detect teens‚Äô body shaming\\ninstances. Drawing from the Razi et al. case study [ 113], where researchers successfully gathered\\nand analyzed similar sensitive data from social media platforms, it is evident that with appropriate\\nmethodologies and ethical considerations, collecting targeted data from Instagram to detect teens‚Äô\\nbody shaming instances is indeed feasible. By doing so, the actual risks faced by teenagers in their\\nonline lives would be acknowledged, which would result in allowing the risk detection models to\\naccurately identify these risks on specific platforms (e.g., [ 10,100,112]). As such, the models would\\nlearn patterns, contextual cues, and platform-specific dynamics that would be more representative\\nof the risks faced by them. At the same time, we acknowledge the transferability of ML algorithms\\nacross different platforms is difficult and should be done with caution [ 21,35,75]. Our results show\\nfrom an empirical perspective that the teens‚Äô risk experiences across various platforms differ, hence,\\ncould inform that algorithmic approaches should be tailored to different platforms based on their\\naffordances and the types of risks teens discuss encountering the most on those platforms.\\nCollaborative Online Safety Tools for Adolescents. Our results reconfirm that adolescents expe-\\nrience online risks on social media and shed light on how such adverse experiences affect their\\nmental health to the point that they seek out support due to these experiences. At the same time,\\nour results confirmed that teens felt their privacy was violated when their parents became involved\\nin their lives via social media. One potential approach to address the problem of privacy violations\\nthat occur via sharenting may be to provide collaborative features for family members, where\\nteens could proactively review their parents‚Äô posts before they are shared via the platform, and\\nvice-versa. Through such interactive feedback, parents and teens can better understand each others‚Äô\\nprivacy boundaries. A large body of prior research on online safety domain suggested moving\\naway from restrictive and paternalistic approaches [ 50,140], as they cause more fear and paranoia\\namong families [ 98], and moving toward adopting more collaborative [ 4,7,76] and teen-centric\\napproaches [ 5,6,51,56], where adolescents can work alongside their parents to make their online\\nsafety decisions together [ 17] while having some level of personal privacy [ 37]. However, these\\nprivacy-preserving collaborative approaches may still be open to conflicts between the two parties.\\nFor example, parents could post excluding the child from viewing, or teens could intentionally\\nblock parents‚Äô benign posts. A suggested collaborative mechanism to address potential conflicts\\nis the Circle of Trust introduced by Ghosh et al. in [ 51]. This mechanism enabled parents to only\\nsee inappropriate message contents exchanged by teens within their social network and allowed\\nthem to negotiate with teens about excluding specific individuals from their trusted circle. We urge\\nfuture researchers to explore privacy-preserving yet effective ways to mitigate the potential con-\\ncerns of such parent-teen collaborative mechanisms. Thinking beyond the parent-teen relationship,\\nhowever, our research also highlights the importance of peer relationships , as teens in our study\\nsought advice and support regarding their social media experiences from strangers on an online\\npeer support platform. This is consistent with prior work that shows youth may prefer seeking\\nsupport regarding sensitive topics from peers, or even strangers [ 111], rather than their parents. As\\nsuch, it might be time to change the status quo of relying on parents as the mediators of adolescent\\nonline safety to considering ways in which we can engage peer support as a protective mechanism\\nthat teens already leverage. Thus, we urge design practitioners to consider such collaborative and\\npeer-based approaches when designing online safety features, so that teens can reach out to their\\npeers for help when navigating different social media risks.\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.422:28 Alluhidan et al.\\n5.4 Limitations and Future Work\\nThere are several limitations of our work that inform future research directions. First, we were\\nnot the first to uncover many of the themes that we identified from our analyses, as issues like\\ncyberbullying, body-shaming, and privacy violations have been well-documented by prior research\\n[40, 78, 116]. However, our research is one of the first to take an affordances perspective to better\\nunderstand youths‚Äô differentiated social media experiences across different platforms. As such,\\nour work serves to both confirm many of the findings from previous work and build upon and\\nadd nuance to those findings, as well as shift the conversation towards a more practical design-\\nbased approach to proactively address some of the problems youth encounter when using different\\nsocial media platforms. Second, our sample was skewed toward older adolescents (ages 15-17) who\\nidentified as males, females, or who did not specify their gender. Furthermore, these teens were\\nactive users on an online platform dedicated to peer and mental health support. Therefore, our\\nresults may not be generalizable to the whole teen population of different sexual orientations, ages,\\nand those not actively seeking peer support pseudo-anonymously online. Future work needs to\\ntriangulate our results across broader and more diverse demographics of youth to ensure that our\\nresults hold. Additionally, we caution readers to not assume that a higher percentage of negative\\nposts provides evidence that the drawbacks of social media use outweigh the benefits. Prior research\\nhas confirmed negative bias in online reviews [ 107]. Therefore, we assume that youth were more\\nlikely to share their negative experiences than the good ones.\\nAnother key limitation of our work is that we did not analyze the support and advice teens\\nreceived from others based on comments made on their posts. Such analyses would be helpful in\\nunderstanding potential recommendations for coping the negative experiences, amplifying positive\\nones, and generally the helpfulness of such advice. Since 10% of posts were removed before coding\\nbecause they were duplicates, this suggests that these teens may have received fewer responses\\nthan they sought. Finally, our analyses were constrained by the time period available for analyses\\n(2019 and 2020) as the social media landscape has since changed. Yet, a key strength of our work is in\\ndemonstrating for the purpose of future research the importance of examining the lived experience\\nof teens across different social media platforms and doing so based on digital trace data in places\\nwhere they are actively seeking support based on these experiences.\\n6 CONCLUSION\\nWe explored how teens discuss social media platforms in their support-seeking posts on an online\\nmental health platform. Four primary themes were identified in the discussion of social media\\nplatforms, including connecting on social media, negative sides of social media, positive sides of\\nsocial media, and seeking support/information on social media. Our analysis also showed the ways\\nteens discuss these themes vary across different platforms, along with the unique social media\\naffordances of each platform. We also provide important design and policy implications to make\\nexperiences on social media more inspiring and less detrimental for teens. The key takeaway from\\nour study is that the benefits and drawbacks of teens‚Äô social media usage can co-exist and net\\neffects (positive or negative) can vary across different teens in various contexts. Therefore, we need\\nto move away from fear-based approaches where we try to overly protect teens from potential\\nharm. Rather, the focus should be on how to empower teens with positives, at the same time, tackle\\nnegatives to better support teens‚Äô digital well-being.\\nACKNOWLEDGMENTS\\nThis research was supported by the U.S. National Science Foundation under grants IIP-2329976,\\nIIS-2333207, and the William T. Grant Foundation grant 187941. Any opinions, findings, conclusions,\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.Adolescent Social Media Use 422:29\\nor recommendations expressed in this material are those of the authors and do not necessarily\\nreflect the views of our sponsors.\\nREFERENCES\\n[1]Zainab Agha, Karla Badillo-Urquiola, and Pamela J Wisniewski. 2023. \" Strike at the Root\": Co-designing Real-Time\\nSocial Media Interventions for Adolescent Online Risk Prevention. Proceedings of the ACM on Human-Computer\\nInteraction 7, CSCW1 (2023), 1‚Äì32.\\n[2]Zainab Agha, Kelsey Miu, Sophia Piper, Jinkyung Park, and Pamela J Wisniewski. 2023. Co-designing user personas and\\nrisk scenarios for evaluating adolescent online safety interventions. In Companion Publication of the 2023 Conference\\non Computer Supported Cooperative Work and Social Computing . 249‚Äì253.\\n[3]June Ahn. 2011. The effect of social network sites on adolescents‚Äô social and academic development: Current theories\\nand controversies. Journal of the American Society for information Science and Technology 62, 8 (2011), 1435‚Äì1445.\\n[4]Mamtaj Akter, Leena Alghamdi, Dylan Gillespie, Nazmus Sakib Miazi, Jess Kropczynski, Heather Lipford, and\\nPamela J. Wisniewski. 2022. CO-OPS: A Mobile App for Community Oversight of Privacy and Security. In Companion\\nPublication of the 2022 Conference on Computer Supported Cooperative Work and Social Computing (Virtual Event,\\nTaiwan) (CSCW‚Äô22 Companion) . Association for Computing Machinery, New York, NY, USA, 179‚Äì183. https:\\n//doi.org/10.1145/3500868.3559706\\n[5]Mamtaj Akter, Leena Alghamdi, Jess Kropczynski, Heather Richter Lipford, and Pamela J. Wisniewski. 2023. It\\nTakes a Village: A Case for Including Extended Family Members in the Joint Oversight of Family-Based Privacy and\\nSecurity for Mobile Smartphones. In Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing\\nSystems (<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>) (CHI EA ‚Äô23) . Association\\nfor Computing Machinery, New York, NY, USA, Article 194, 7 pages. https://doi.org/10.1145/3544549.3585904\\n[6]Mamtaj Akter, Amy J. Godfrey, Jess Kropczynski, Heather R. Lipford, and Pamela J. Wisniewski. 2022. From Parental\\nControl to Joint Family Oversight: Can Parents and Teens Manage Mobile Online Safety and Privacy as Equals? Proc.\\nACM Hum.-Comput. Interact. 6, CSCW1, Article 57 (apr 2022), 28 pages. https://doi.org/10.1145/3512904\\n[7]Mamtaj Akter, Madiha Tabassum, Nazmus Sakib Miazi, Leena Alghamdi, Jess Kropczynski, Pamela J. Wisniewski, and\\nHeather Lipford. 2023. Evaluating the Impact of Community Oversight for Managing Mobile Privacy and Security. In\\nNineteenth Symposium on Usable Privacy and Security (SOUPS 2023) . USENIX Association, Anaheim, CA, 437‚Äì456.\\nhttps://www.usenix.org/conference/soups2023/presentation/akter\\n[8]Sharyn Alfonsi. 2022. More than 2,000 families suing social media companies over kids‚Äô mental health - CBS\\nNews. https://www.cbsnews.com/news/social-media-lawsuit-meta-tiktok-facebook-instagram-60-minutes-\\ntranscript-2023-06-04/\\n[9]Naima Samreen Ali, Zainab Agha, Neeraj Chatlani, Jinkyung Park, and Pamela J Wisniewski. 2024. A Case Study on\\nFacilitating a Long-Term Youth Advisory Board to Involve Youth in Adolescent Online Safety Research. In Extended\\nAbstracts of the CHI Conference on Human Factors in Computing Systems . 1‚Äì8.\\n[10] Shiza Ali, Afsaneh Razi, Seunghyun Kim, Ashwaq Alsoubai, Chen Ling, Munmun De Choudhury, Pamela J Wisniewski,\\nand Gianluca Stringhini. 2023. Getting meta: A multimodal approach for detecting unsafe conversations within\\ninstagram direct messages of youth. Proceedings of the ACM on Human-Computer Interaction 7, CSCW1 (2023), 1‚Äì30.\\n[11] Ashwaq Alsoubai, Jinkyung Park, Sarvech Qadir, Gianluca Stringhini, Afsaneh Razi, and Pamela J Wisniewski. 2024.\\nSystemization of Knowledge (SoK): Creating a Research Agenda for Human-Centered Real-Time Risk Detection on\\nSocial Media Platforms. In Proceedings of the CHI Conference on Human Factors in Computing Systems . 1‚Äì21.\\n[12] Ashwaq Alsoubai, Jihye Song, Afsaneh Razi, Nurun Naher, Munmun De Choudhury, and Pamela J Wisniewski. 2022.\\nFrom‚ÄôFriends with Benefits‚Äô to‚ÄôSextortion:‚ÄôA Nuanced Investigation of Adolescents‚Äô Online Sexual Risk Experiences.\\nProceedings of the ACM on Human-Computer Interaction 6, CSCW2 (2022), 1‚Äì32.\\n[13] Monica Anderson, Emily A. Vogels, Andrew Perrin, and Lee Raine. 2022. Connection, Creativity and Drama: Teen\\nLife on Social Media in 2022. https://www.pewresearch.org/internet/2022/11/16/connection-creativity-and-drama-\\nteen-life-on-social-media-in-2022/\\n[14] Monica Anderson, Emily A Vogels, Andrew Perrin, and Lee Rainie. 2022. 1. What teens post on social media.\\nhttps://www.pewresearch.org/internet/2022/11/16/1-what-teens-post-on-social-media/\\n[15] Christa SC Asterhan and Edith Bouton. 2017. Teenage peer-to-peer knowledge sharing through social network sites\\nin secondary schools. Computers & Education 110 (2017), 16‚Äì34.\\n[16] Aboluwaji D. Ayinmoro, Endurance Uzobo, Bodisere J. Teibowei, and Joyce B. Fred. [n. d.]. Sexting and other\\nrisky sexual behaviour among female students in a Nigerian academic institution. 15, 2 ([n. d.]), 116‚Äì121. https:\\n//doi.org/10.1016/j.jtumed.2020.02.007\\n[17] Karla Badillo-Urquiola, Zainab Agha, Mamtaj Akter, and Pamela Wisniewski. 2020. Towards Assets-based Approaches\\nfor Adolescent Online Safety. In Badillo-Urquiola, Agha, Z., Akter, K., Wisniewski, P.,(2020)‚ÄúTowards Assets-Based\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.422:30 Alluhidan et al.\\nApproaches for Adolescent Online Safety‚Äù Extended Abstract presented at the ACM Conference on Computer-Supported\\nCooperative Work Workshop on Operationalizing an Assets-Based Design of Technology,(CSCW 2020) .\\n[18] Corey H Basch and Sarah A MacLean. 2019. A content analysis of HPV related posts on instagram. Human vaccines\\n& immunotherapeutics 15, 7-8 (2019), 1476‚Äì1478.\\n[19] Joseph B Bayer, Nicole B Ellison, Sarita Y Schoenebeck, and Emily B Falk. 2016. Sharing the small moments: ephemeral\\nsocial interaction on Snapchat. Information, communication & society 19, 7 (2016), 956‚Äì977.\\n[20] Ine Beyens, J Loes Pouwels, Irene I van Driel, Loes Keijsers, and Patti M Valkenburg. 2020. The effect of social media\\non well-being differs from adolescent to adolescent. Scientific Reports 10, 1 (2020), 10763.\\n[21] Lindsay Blackwell, Tianying Chen, Sarita Schoenebeck, and Cliff Lampe. 2018. When online harassment is perceived\\nas justified. In Proceedings of the International AAAI Conference on Web and Social Media , Vol. 12.\\n[22] Alicia Blum-Ross and Sonia Livingstone. 2020. ‚ÄúSharenting,‚Äù parent blogging, and the boundaries of the digital self.\\nInSelf-(re) presentation now . Routledge, 70‚Äì85.\\n[23] Virginia Braun and Victoria Clarke. 2012. Thematic analysis. American Psychological Association.\\n[24] Louise Breuer and Chris Barker. 2015. Online support groups for depression: Benefits and barriers. Sage Open 5, 2\\n(2015), 2158244015574936.\\n[25] Dawn Y. Brinkley, Robert A. Ackerman, Samuel E. Ehrenreich, and Marion K. Underwood. [n. d.]. Sending and\\nreceiving text messages with sexual content: Relations with early sexual activity and borderline personality features\\nin late adolescence. 70 ([n. d.]), 119‚Äì130. https://doi.org/10.1016/j.chb.2016.12.082\\n[26] Moira Burke, Lada Adamic, and Karyn Marciniak. 2013. Families on facebook. In Proceedings of the International\\nAAAI Conference on Web and Social Media , Vol. 7. 41‚Äì50.\\n[27] Moira Burke, Justin Cheng, and Bethany de Gant. 2020. Social comparison and Facebook: Feedback, positivity, and\\nopportunities for comparison. In Proceedings of the 2020 CHI conference on human factors in computing systems . 1‚Äì13.\\n[28] Silvia Casale, Gabriele Gemelli, Chiara Calosi, Barbara Giangrasso, and Giulia Fioravanti. 2021. Multiple exposure to\\nappearance-focused real accounts on Instagram: Effects on body image among both genders. Current Psychology 40\\n(2021), 2877‚Äì2886.\\n[29] Felipe Gonz√°lez Castro, Joshua G Kellison, Stephen J Boyd, and Albert Kopak. 2010. A methodology for conducting\\nintegrative mixed methods research and data analyses. Journal of mixed methods research 4, 4 (2010), 342‚Äì360.\\n[30] Neeraj Chatlani, Arianna Davis, Karla Badillo-Urquiola, Elizabeth Bonsignore, and Pamela Wisniewski. 2023. Teen as\\nresearch-apprentice: A restorative justice approach for centering adolescents as the authority of their own online\\nsafety. International Journal of Child-Computer Interaction 35 (2023), 100549.\\n[31] Surobhi Chatterjee and Sujita Kumar Kar. 2023. Teen pornography: an emerging mental health challenge. Journal of\\npsychosexual health 5, 1 (2023), 30‚Äì34.\\n[32] Pooja Chaudhary, Melissa Peskin, Jeff Temple, Robert Addy, Elizabeth Baumler, and Ross Shegog. [n. d.]. Sexting and\\nMental Health: A School-based Longitudinal Study Among Youth in Texas. 8, 1 ([n. d.]). https://doi.org/10.58464/2155-\\n5834.1329\\n[33] Hye Jeong Choi, Camille Mori, Joris Van Ouytsel, Sheri Madigan, and Jeff R. Temple. [n. d.]. Adolescent Sexting\\nInvolvement Over 4 Years and Associations With Sexual Activity. 65, 6 ([n. d.]), 738‚Äì744. https://doi.org/10.1016/j.\\njadohealth.2019.04.026\\n[34] Congress.Gov. 2019. S.2314 - SMART Act. https://www.congress.gov/bill/116th-congress/senate-\\nbill/2314/text?q=%7B%22search%22%3A%5B%22Social+Media+Addiction+Reduction+Technology+%28SMART%29+\\nAct%22%5D%7D&r=1&s=2\\n[35] Denzil Correa, Leandro Ara√∫jo Silva, Mainack Mondal, Fabr√≠cio Benevenuto, and Krishna P Gummadi. 2015. The\\nmany shades of anonymity: Characterizing anonymous social media content. In Ninth International AAAI Conference\\non Web and Social Media .\\n[36] Shanley Corvite, Ben Zefeng Zhang, and Oliver L Haimson. 2022. Social Media‚Äôs Role During Identity Changes\\nRelated to Major Life Events. Proceedings of the ACM on Human-Computer Interaction 6, CSCW2 (2022), 1‚Äì22.\\n[37] Lorrie Faith Cranor, Adam L. Durity, Abigail Marsh, and Blase Ur. 2014. Parents‚Äô and Teens‚Äô Perspectives on Privacy In\\na Technology-Filled World. 19‚Äì35. https://www.usenix.org/conference/soups2014/proceedings/presentation/cranor\\n[38] John W Creswell, Vicki L Plano Clark, Michelle L Gutmann, and William E Hanson. 2003. Advances in mixed methods\\nresearch designs. Handbook of mixed methods in social & behavioral research (2003), 209.\\n[39] Matteo Cristofaro, Federico Giannetti, and Gianpaolo Abatecola. 2023. The initial survival of the Unicorns: a behavioral\\nperspective of Snapchat. Journal of Management History (2023).\\n[40] Ralf De Wolf. 2020. Contextualizing how teens manage personal and interpersonal privacy on social media. New\\nmedia & society 22, 6 (2020), 1058‚Äì1075.\\n[41] Terre des Hommes. 2022. Safety by design to keep children safe online. https://www.terredeshommes.nl/en/latest/\\nsafety-by-design-to-keep-children-safe-online\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.Adolescent Social Media Use 422:31\\n[42] Michael A DeVito, Jeremy Birnholtz, and Jeffery T Hancock. 2017. Platforms, people, and perception: Using affordances\\nto understand self-presentation on social media. In Proceedings of the 2017 ACM conference on computer supported\\ncooperative work and social computing . 740‚Äì754.\\n[43] Diana M Doumas and Aida Midgett. 2023. Witnessing cyberbullying and suicidal ideation among middle school\\nstudents. Psychology in the Schools 60, 4 (2023), 1149‚Äì1163.\\n[44] Katherine Easton, Jacob Diggle, Mabel Ruethi-Davis, Megan Holmes, Darian Byron-Parker, Jessica Nuttall, Chris\\nBlackmore, et al .2017. Qualitative exploration of the potential for adverse events when using an online peer support\\nnetwork for mental health: cross-sectional survey. JMIR mental health 4, 4 (2017), e8168.\\n[45] Nicole B Ellison, Lindsay Blackwell, Cliff Lampe, and Penny Trieu. 2016. ‚ÄúThe question exists, but you don‚Äôt exist\\nwith it‚Äù: Strategic anonymity in the social lives of adolescents. Social Media+ Society 2, 4 (2016), 2056305116670673.\\n[46] Samer Faraj and Bijan Azad. 2012. The materiality of technology: An affordance perspective. Materiality and\\norganizing: Social interaction in a technological world 237 (2012), 258.\\n[47] Andrea Forte, Michael Dickard, Rachel Magee, and Denise E Agosto. 2014. What do teens ask their online social\\nnetworks? Social search practices among high school students. In Proceedings of the 17th ACM conference on Computer\\nsupported cooperative work & social computing . 28‚Äì37.\\n[48] Brian Fung. 2023. Senators blast Big Tech companies over kids‚Äô safety amid renewed push for legislation. https:\\n//www.cnn.com/2023/02/14/tech/senate-online-kids-safety/index.html\\n[49] Francesca Gasparini, Giulia Rizzi, Aurora Saibene, and Elisabetta Fersini. 2022. Benchmark dataset of memes with\\ntext transcriptions for automatic detection of multi-modal misogynistic content. Data in brief 44 (2022), 108526.\\n[50] Arup Kumar Ghosh, Karla Badillo-Urquiola, Shion Guha, Joseph J. LaViola Jr, and Pamela J. Wisniewski. 2018. Safety\\nvs. Surveillance: What Children Have to Say about Mobile Apps for Parental Control. In Proceedings of the 2018 CHI\\nConference on Human Factors in Computing Systems (CHI ‚Äô18) . Association for Computing Machinery, New York, NY,\\nUSA, 1‚Äì14. https://doi.org/10.1145/3173574.3173698\\n[51] Arup Kumar Ghosh, Charles E. Hughes, and Pamela J. Wisniewski. 2020. Circle of Trust: A New Approach to Mobile\\nOnline Safety for Families. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems . ACM,\\nHonolulu HI USA, 1‚Äì14. https://doi.org/10.1145/3313831.3376747\\n[52] Kathleen Margaret Griffiths, Julia Reynolds, and Sara Vassallo. 2015. An online, moderated peer-to-peer support\\nbulletin board for depression: user-perceived advantages and disadvantages. JMIR Mental Health 2, 2 (2015), e4266.\\n[53] Aaron Haddock, Nadia Ward, Rondy Yu, and Nicole O‚ÄôDea. 2022. Positive Effects of Digital Technology Use by\\nAdolescents: A Scoping Review of the Literature. International Journal of Environmental Research and Public Health\\n19, 21 (2022), 14009.\\n[54] Heidi Hartikainen, Afsaneh Razi, and Pamela Wisniewski. 2021. Safe sexting: The advice and support adolescents\\nreceive from peers regarding online sexual risks. Proceedings of the ACM on Human-Computer Interaction 5, CSCW1\\n(2021), 1‚Äì31.\\n[55] Amanda M Harvey, Sharlynn Thompson, Andrew Lac, and Frederick L Coolidge. 2019. Fear and derision: a quantitative\\ncontent analysis of provaccine and antivaccine internet memes. Health Education & Behavior 46, 6 (2019), 1012‚Äì1023.\\n[56] Yasmeen Hashish, Andrea Bunt, and James E. Young. 2014. Involving children in content control: a collaborative\\nand education-oriented content filtering approach. In Proceedings of the SIGCHI Conference on Human Factors in\\nComputing Systems (CHI ‚Äô14) . Association for Computing Machinery, New York, NY, USA, 1797‚Äì1806. https:\\n//doi.org/10.1145/2556288.2557128\\n[57] Alexis Hiniker, Bongshin Lee, Kiley Sobel, and Eun Kyoung Choe. 2017. Plan & play: supporting intentional media\\nuse in early childhood. In Proceedings of the 2017 conference on interaction design and children . 85‚Äì95.\\n[58] Gail Hornor. 2020. Online sexual solicitation of children and adolescents. Journal of Pediatric Health Care 34, 6 (2020),\\n610‚Äì618.\\n[59] https://www.blumenthal.senate.gov/. 2022. Blumenthal & Blackburn Introduce Comprehensive Kids‚Äô Online Safety\\nLegislation . Retrieved July 11, 2023 from https://www.blumenthal.senate.gov/newsroom/press/release/blumenthal-\\nand-blackburn-introduce-comprehensive-kids-online-safety-legislation\\n[60] Ha Sung Hwnag. 2019. Why Social Comparison on Instagram Matters: Its impact on Depression. KSII Trans. Internet\\nInf. Syst. 13, 3 (2019), 1626‚Äì1638.\\n[61] Instagram. 2021. Continuing to Make Instagram Safer for the Youngest Members of Our Commu-\\nnity. https://about.instagram.com/blog/announcements/continuing-to-make-instagram-safer-for-the-youngest-\\nmembers-of-our-community\\n[62] Jin Yea Jang, Kyungsik Han, Patrick C Shih, and Dongwon Lee. 2015. Generation like: Comparative characteristics in\\ninstagram. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems . 4039‚Äì4042.\\n[63] Shaohai Jiang and Annabel Ngien. 2020. The effects of Instagram use, social comparison, and self-esteem on social\\nanxiety: A survey study in Singapore. Social Media+ Society 6, 2 (2020), 2056305120912488.\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.422:32 Alluhidan et al.\\n[64] Barbara Jiotsa, Benjamin Naccache, M√©lanie Duval, Bruno Rocher, and Marie Grall-Bronnec. 2021. Social media\\nuse and body image disorders: Association between frequency of comparing one‚Äôs own physical appearance to that\\nof people being followed on social media and body dissatisfaction and drive for thinness. International journal of\\nenvironmental research and public health 18, 6 (2021), 2880.\\n[65] Jennifer Jolly. 2023. Will TikTok be banned? Maybe it should be for kids, at least. https://www.usatoday.com/story/\\ntech/columnist/2023/03/30/tiktok-ban-social-media-kids/11554685002/\\n[66] The Wall Street Journal. 2021. the facebook files A Wall Street Journal investigation. https://www.wsj.com/articles/the-\\nfacebook-files-11631713039\\n[67] Nur Azizah Kamaruddin, Harianti Haris, et al .2022. Relationship fear of missing out with social media addiction\\nhigh school-aged teens. Comprehensive Health Care 6, 1 (2022), 24‚Äì30.\\n[68] Elena Karahanna, Sean Xin Xu, Yan Xu, and Nan Andy Zhang. 2018. The needs‚Äìaffordances‚Äìfeatures perspective for\\nthe use of social media. Mis Quarterly 42, 3 (2018), 737‚Äì756.\\n[69] Cormac Keenan. 2023. New features for teens and families on TikTok. https://newsroom.tiktok.com/en-us/new-\\nfeatures-for-teens-and-families-on-tiktok-us\\n[70] Betul Keles, Niall McCrae, and Annmarie Grealish. 2020. A systematic review: the influence of social media on\\ndepression, anxiety and psychological distress in adolescents. International journal of adolescence and youth 25, 1\\n(2020), 79‚Äì93.\\n[71] Mark Kelly. 2023. TN district joins mass action lawsuit against Big Tech . https://www.wkrn.com/special-reports/back-\\nto-school/tennessee-school-district-joins-mass-action-lawsuit-against-big-tech/\\n[72] Jung-Eun Kim, Emily C Weinstein, and Robert L Selman. 2017. Romantic relationship advice from anonymous online\\nhelpers: The peer support adolescents exchange. Youth & Society 49, 3 (2017), 369‚Äì392.\\n[73] Seunghyun Kim, Afsaneh Razi, Gianluca Stringhini, Pamela J Wisniewski, and Munmun De Choudhury. 2021. A\\nhuman-centered systematic literature review of cyberbullying detection algorithms. Proceedings of the ACM on\\nHuman-Computer Interaction 5, CSCW2 (2021), 1‚Äì34.\\n[74] Jerome Kirk and Marc L Miller. 1986. Reliability and validity in qualitative research . Vol. 1. Sage.\\n[75] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu,\\nMichihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al .2021. Wilds: A benchmark of in-the-wild distribution\\nshifts. In International Conference on Machine Learning . PMLR, 5637‚Äì5664.\\n[76] Jess Kropczynski, Reza Ghaiumy Anaraky, Mamtaj Akter, Amy J Godfrey, Heather Lipford, and Pamela J Wisniewski.\\n2021. Examining collaborative support for privacy and security in the broader context of tech caregiving. Proceedings\\nof the ACM on Human-Computer Interaction 5, CSCW2 (2021), 1‚Äì23.\\n[77] Priya Kumar. 2021. From oversharing to sharenting: How experts govern parents and their social media use . Ph. D.\\nDissertation. University of Maryland, College Park.\\n[78] Irene Kwan, Kelly Dickson, Michelle Richardson, Wendy MacDowall, Helen Burchett, Claire Stansfield, Ginny Brunton,\\nKaty Sutcliffe, and James Thomas. 2020. Cyberbullying and children and young people‚Äôs mental health: a systematic\\nmap of systematic reviews. Cyberpsychology, Behavior, and Social Networking 23, 2 (2020), 72‚Äì82.\\n[79] Anna Lavis and Rachel Winter. 2020. # Online harms or benefits? An ethnographic analysis of the positives and\\nnegatives of peer-support around self-harm on social media. Journal of child psychology and psychiatry 61, 8 (2020),\\n842‚Äì854.\\n[80] Alex Leavitt. 2015. \" This is a Throwaway Account\" Temporary Technical Identities and Perceptions of Anonymity in\\na Massive Online Community. In Proceedings of the 18th ACM conference on computer supported cooperative work &\\nsocial computing . 317‚Äì327.\\n[81] Alfred SY Lee, Martyn Standage, Martin S Hagger, and Derwin KC Chan. 2019. Sport injury prevention in-school and\\nout-of-school? A qualitative investigation of the trans-contextual model. PLoS one 14, 9 (2019), e0222015.\\n[82] Newton Lee. 2021. Facebook nation . Springer.\\n[83] Sun Kyong Lee, Juhyung Sun, and Norman Wong. [n. d.]. Comparative Analysis of Media Affordances in Tiktok and\\nYoutube. Available at SSRN 4557930 ([n. d.]).\\n[84] Amanda Lenhart. 2015. Teens, social media & technology overview 2015. (2015).\\n[85] Rebecca S Levine, Amy Vatne Bintliff, and Anita Raj. 2022. Gendered analysis of cyberbullying victimization and its\\nassociations with suicidality: findings from the 2019 Youth Risk Behavior Survey. Adolescents 2, 2 (2022), 235‚Äì251.\\n[86] Kai Lukoff, Ulrik Lyngs, Himanshu Zade, J Vera Liao, James Choi, Kaiyue Fan, Sean A Munson, and Alexis Hiniker.\\n2021. How the design of youtube influences user sense of agency. In Proceedings of the 2021 CHI Conference on Human\\nFactors in Computing Systems . 1‚Äì17.\\n[87] Anne J Maheux, Jacqueline Nesi, Brian M Galla, Savannah R Roberts, and Sophia Choukas-Bradley. 2021. # Grateful:\\nLongitudinal associations between adolescents‚Äô social media use and gratitude during the COVID-19 pandemic.\\nJournal of Research on Adolescence 31, 3 (2021), 734‚Äì747.\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.Adolescent Social Media Use 422:33\\n[88] Tim McCreanor, Antonia Lyons, Christine Griffin, Ian Goodwin, Helen Moewaka Barnes, and Fiona Hutton. 2013.\\nYouth drinking cultures, social networking and alcohol marketing: Implications for public health. Critical public\\nhealth 23, 1 (2013), 110‚Äì120.\\n[89] Bridget Christine McHugh, Pamela Wisniewski, Mary Beth Rosson, and John M Carroll. 2018. When social media\\ntraumatizes teens: The roles of online risk exposure, coping, and post-traumatic stress. Internet Research 28, 5 (2018),\\n1169‚Äì1188.\\n[90] Mary L McHugh. 2013. The chi-square test of independence. Biochemia medica 23, 2 (2013), 143‚Äì149.\\n[91] Belinda Melling and Terry Houguet-Pincham. 2011. Online peer support for individuals with depression: a summary\\nof current research and future considerations. Psychiatric rehabilitation journal 34, 3 (2011), 252.\\n[92] Megan A Moreno and Yalda T Uhls. 2019. Applying an affordances approach and a developmental lens to approach\\nadolescent social media use. Digital health 5 (2019), 2055207619826678.\\n[93] Carol Moser, Tianying Chen, and Sarita Y Schoenebeck. 2017. Parents? And Children? s preferences about parents\\nsharing about children on social media. In Proceedings of the 2017 CHI conference on human factors in computing\\nsystems . 5221‚Äì5225.\\n[94] Marijke Naezer. [n. d.]. From risky behaviour to sexy adventures: reconceptualising young people‚Äôs online sexual\\nactivities. 20, 6 ([n. d.]), 715‚Äì729. https://doi.org/10.1080/13691058.2017.1372632 Publisher: Taylor & Francis _eprint:\\nhttps://doi.org/10.1080/13691058.2017.1372632.\\n[95] Candice Odgers, Nick Allen, Jennifer Pfeifer, Ronald Dahl, Jacqueline Nesi, Stephen Schueller, Joanna Lee Williams,\\net al.2022. Engaging, safe, and evidence-based: What science tells us about how to promote positive development\\nand decrease risk in online spaces. (2022).\\n[96] Amy Orben. 2020. Teenagers, screens and social media: a narrative review of reviews and key studies. Social psychiatry\\nand psychiatric epidemiology 55, 4 (2020), 407‚Äì414.\\n[97] Yok-Fong Paat and Christine Markham. 2021. Digital crime, trauma, and abuse: Internet safety and cyber risks for\\nadolescents and emerging adults in the 21st century. Social Work in Mental Health 19, 1 (2021), 18‚Äì40.\\n[98] Rachel Pain. 2006. Paranoid parenting? Rematerializing risk and fear for children. Social & Cultural Geography 7, 2\\n(April 2006), 221‚Äì243. https://doi.org/10.1080/14649360600600585\\n[99] Jinkyung Park, Joshua Gracie, Ashwaq Alsoubai, Afsaneh Razi, and Pamela J Wisniewski. 2024. Personally Targeted\\nRisk vs. Humor: How Online Risk Perceptions of Youth vs. Third-Party Annotators Differ based on Privately Shared\\nMedia on Instagram. In IDC‚Äô24: Proceedings of the 23rd Annual ACM Interaction Design and Children Conference .\\nAssociation for Computing Machinery, 1‚Äì13.\\n[100] Jinkyung Park, Joshua Gracie, Ashwaq Alsoubai, Gianluca Stringhini, Vivek Singh, and Pamela Wisniewski. 2023.\\nTowards Automated Detection of Risky Images Shared by Youth on Social Media. In Companion Proceedings of the\\nACM Web Conference 2023 . 1348‚Äì1357.\\n[101] Jinkyung Park, Irina Lediaeva, Maria Lopez, Amy Godfrey, Kapil Chalil Madathil, Heidi Zinzow, and Pamela Wis-\\nniewski. 2023. How affordances and social norms shape the discussion of harmful social media challenges on reddit.\\nHuman Factors in Healthcare 3 (2023), 100042.\\n[102] Justin W Patchin and Sameer Hinduja. 2017. Digital self-harm among adolescents. Journal of Adolescent Health 61, 6\\n(2017), 761‚Äì766.\\n[103] John Perrino. 2022. Using ‚Äòsafety by design‚Äô to address online harms. https://www.brookings.edu/articles/using-\\nsafety-by-design-to-address-online-harms/\\n[104] Lukasz Piwek and Adam Joinson. 2016. ‚ÄúWhat do they snapchat about?‚Äù Patterns of use in time-limited instant\\nmessaging service. Computers in human behavior 54 (2016), 358‚Äì367.\\n[105] Nattavudh Powdthavee. 2014. Social Comparison Theory. Encyclopedia of Quality of Life and Well-Being Research\\n(2014), 6028‚Äì6029.\\n[106] Julie Prescott, Terry Hanley, Katalin Ujhelyi, et al .2017. Peer communication in online mental health forums for\\nyoung people: directional and nondirectional support. JMIR mental health 4, 3 (2017), e6921.\\n[107] Hamed Qahri-Saremi and Ali Reza Montazemi. 2022. Negativity bias in the diagnosticity of online review content: the\\neffects of consumers‚Äô prior experience and need for cognition. European Journal of Information Systems (2022), 1‚Äì18.\\n[108] Jenny Radesky and Alexis Hiniker. [n. d.]. From moral panic to systemic change: Making child-centered design the\\ndefault. 31 ([n. d.]), 100351. https://doi.org/10.1016/j.ijcci.2021.100351\\n[109] Jerica Radez, Tessa Reardon, Cathy Creswell, Peter J Lawrence, Georgina Evdoka-Burton, and Polly Waite. 2021. Why\\ndo children and adolescents (not) seek and access professional help for their mental health problems? A systematic\\nreview of quantitative and qualitative studies. European child & adolescent psychiatry 30 (2021), 183‚Äì211.\\n[110] Wahyu Rahardjo and Indah Mulyani. 2020. Instagram addiction in teenagers: The role of type D personality,\\nself-esteem, and fear of missing out. Psikohumaniora: Jurnal penelitian psikologi 5, 1 (2020), 29‚Äì44.\\n[111] Amy Rayland and Jacob Andrews. 2023. From Social Network to Peer Support Network: Opportunities to Explore\\nMechanisms of Online Peer Support for Mental Health. JMIR Mental Health 10 (2023), e41855.\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.422:34 Alluhidan et al.\\n[112] Afsaneh Razi, Ashwaq AlSoubai, Seunghyun Kim, Shiza Ali, Gianluca Stringhini, Munmun De Choudhury, and\\nPamela J Wisniewski. 2023. Sliding into My DMs: Detecting Uncomfortable or Unsafe Sexual Risk Experiences within\\nInstagram Direct Messages Grounded in the Perspective of Youth. Proceedings of the ACM on Human-Computer\\nInteraction 7, CSCW1 (2023), 1‚Äì29.\\n[113] Afsaneh Razi, Ashwaq AlSoubai, Seunghyun Kim, Nurun Naher, Shiza Ali, Gianluca Stringhini, Munmun De Choud-\\nhury, and Pamela J Wisniewski. 2022. Instagram Data Donation: A Case Study on Collecting Ecologically Valid Social\\nMedia Data for the Purpose of Adolescent Online Risk Detection. In CHI Conference on Human Factors in Computing\\nSystems Extended Abstracts . 1‚Äì9.\\n[114] Afsaneh Razi, Karla Badillo-Urquiola, and Pamela J Wisniewski. 2020. Let‚Äôs talk about sext: How adolescents seek\\nsupport and advice about their online sexual experiences. In Proceedings of the 2020 CHI Conference on Human Factors\\nin Computing Systems . 1‚Äì13.\\n[115] Afsaneh Razi, Seunghyun Kim, Ashwaq Alsoubai, Gianluca Stringhini, Thamar Solorio, Munmun De Choudhury, and\\nPamela J. Wisniewski. 2021. A Human-Centered Systematic Literature Review of the Computational Approaches\\nfor Online Sexual Risk Detection. Proc. ACM Hum.-Comput. Interact. 5, CSCW2, Article 465 (oct 2021), 38 pages.\\nhttps://doi.org/10.1145/3479609\\n[116] Varsha Reddy, Harika Abburi, Niyati Chhaya, Tamara Mitrovska, and Vasudeva Varma. 2022. ‚ÄòYou Are Big, S/he Is\\nSmall‚ÄôDetecting Body Shaming in Online User Content. In International Conference on Social Informatics . Springer,\\n389‚Äì397.\\n[117] Betsy Reed. 2019. US could ban ‚Äôaddictive‚Äô autoplay videos and infinite scrolling online. https://www.theguardian.\\ncom/media/2019/jul/31/us-could-ban-addictive-autoplay-videos-and-infinite-scrolling-online\\n[118] Alexander Ronzhyn, Ana Sofia Cardenal, and Albert Batlle Rubio. 2022. Defining affordances in social media research:\\nA literature review. New Media & Society (2022), 14614448221135187.\\n[119] Whitney L Rostad, Daniel Gittins-Stone, Charlie Huntington, Christie J Rizzo, Deborah Pearlman, and Lindsay\\nOrchowski. 2019. The association between exposure to violent pornography and teen dating violence in grade 10\\nhigh school students. Archives of sexual behavior 48 (2019), 2137‚Äì2147.\\n[120] Adele Samra, Wayne A Warburton, and Andrew M Collins. 2022. Social comparisons: a potential mechanism linking\\nproblematic social media use with depression. Journal of Behavioral Addictions (2022).\\n[121] E Sanger. 2022. Social networking in mental health interventions for adolescents. Perspectives in Public Health 142, 5\\n(2022), 261‚Äì262.\\n[122] Carol F Scott, Gabriela Marcu, Riana Elyse Anderson, Mark W Newman, and Sarita Schoenebeck. 2023. Trauma-\\nInformed Social Media: Towards Solutions for Reducing and Healing Online Harm. In Proceedings of the 2023 CHI\\nConference on Human Factors in Computing Systems . 1‚Äì20.\\n[123] Richard [D-CT Sen. Blumenthal. 2022. Text - S.3663 - 117th Congress (2021-2022): Kids Online Safety Act . http:\\n//www.congress.gov/bill/117th-congress/senate-bill/3663/text Archive Location: 2022-02-16.\\n[124] Janine S Senekal, Gabriella Ruth Groenewald, Lisa Wolfaardt, Cisca Jansen, and Kayla Williams. 2023. Social media\\nand adolescent psychosocial development: a systematic review. South African Journal of Psychology 53, 2 (2023),\\n157‚Äì171.\\n[125] Christina Shane-Simpson, Adriana Manago, Naomi Gaggi, and Kristen Gillespie-Lynch. 2018. Why do college\\nstudents prefer Facebook, Twitter, or Instagram? Site affordances, tensions between privacy and self-expression, and\\nimplications for social capital. Computers in human behavior 86 (2018), 276‚Äì288.\\n[126] Donald Sharpe. 2015. Chi-square test is statistically significant: Now what? Practical Assessment, Research, and\\nEvaluation 20, 1 (2015), 8.\\n[127] Matt G Southern. 2019. Facebook and Instagram May Be Forced to Remove Likes for Users Under\\n18. https://www.searchenginejournal.com/facebook-and-instagram-may-be-forced-to-remove-likes-for-users-\\nunder-18/303653/#close\\n[128] Stacey B Steinberg. 2016. Sharenting: Children‚Äôs privacy in the age of social media. Emory Lj 66 (2016), 839.\\n[129] Lalita K Suzuki and Jerel P Calzo. 2004. The search for peer advice in cyberspace: An examination of online teen\\nbulletin boards about health and sexuality. Journal of applied developmental psychology 25, 6 (2004), 685‚Äì698.\\n[130] Jeff R. Temple, Vi Donna Le, Patricia van den Berg, Yan Ling, Jonathan A. Paul, and Brian W. Temple. [n. d.]. Brief\\nreport: Teen sexting and psychosocial health. 37, 1 ([n. d.]), 33‚Äì36. https://doi.org/10.1016/j.adolescence.2013.10.008\\n[131] Melina A Throuvala, Mark D Griffiths, Mike Rennoldson, and Daria J Kuss. 2019. Motivational processes and\\ndysfunctional mechanisms of social media use among adolescents: A qualitative focus group study. Computers in\\nHuman Behavior 93 (2019), 164‚Äì175.\\n[132] Marika Tiggemann and Isabella Barbato. 2018. ‚ÄúYou look great!‚Äù: The effect of viewing appearance-related Instagram\\ncomments on women‚Äôs body image. Body image 27 (2018), 61‚Äì66.\\n[133] Sherry Turkle. 2023. Always-on/always-on-you: The tethered self. In Social Theory Re-Wired . Routledge, 485‚Äì495.\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.Adolescent Social Media Use 422:35\\n[134] Zoya Unni and Emily Weinstein. 2021. Shelter in place, connect online: Trending TikTok content during the early\\ndays of the US Covid-19 pandemic. Journal of adolescent health 68, 5 (2021), 863‚Äì868.\\n[135] Patti M Valkenburg, Adrian Meier, and Ine Beyens. 2022. Social media use and its impact on adolescent mental health:\\nAn umbrella review of the evidence. Current opinion in psychology 44 (2022), 58‚Äì68.\\n[136] Ini Vanwesenbeeck, Koen Ponnet, Michel Walrave, and Joris Van Ouytsel. 2018. Parents‚Äô role in adolescents‚Äô sexting\\nbehaviour. Sexting: Motives and risk in online sexual self-presentation (2018), 63‚Äì80.\\n[137] Erin A Vogel, Jason P Rose, Lindsay R Roberts, and Katheryn Eckles. 2014. Social comparison, social media, and\\nself-esteem. Psychology of popular media culture 3, 4 (2014), 206.\\n[138] Emily A. Vogels, Risa Gelles-Watkins, and Navid Massarat. 2022. Teens, social media & technology 2022. (2022).\\nhttps://www.pewresearch.org/internet/2022/08/10/teens-social-media-and-technology-2022/\\n[139] Emily a Vogels and Risa Gelles-Watnick. 2023. Teens and social media: Key findings from Pew Research Center\\nsurveys . https://www.pewresearch.org/short-reads/2023/04/24/teens-and-social-media-key-findings-from-pew-\\nresearch-center-surveys/\\n[140] Pamela Wisniewski, Arup Kumar Ghosh, Heng Xu, Mary Beth Rosson, and John M. Carroll. 2017. Parental Control vs.\\nTeen Self-Regulation: Is There a Middle Ground for Mobile Online Safety?. In Proceedings of the 2017 ACM Conference\\non Computer Supported Cooperative Work and Social Computing (Portland, Oregon, USA) (CSCW ‚Äô17) . Association for\\nComputing Machinery, New York, NY, USA, 51‚Äì69. https://doi.org/10.1145/2998181.2998352\\n[141] Esty Wulandari and Sri Herwindya Baskara Wijaya. 2021. Utilization of the Tiktok Video Application as a Means of\\nShowing Existence And Self-Disclosure of Teenagers on Social Media. International Journal of Social Science and\\nHuman Research 4, 9 (2021), 2610‚Äì2616.\\n[142] Yuxiang Zhao, Jiang Liu, Jian Tang, and Qinghua Zhu. 2013. Conceptualizing perceived affordances in social media\\ninteraction design. In Aslib Proceedings , Vol. 65. Emerald Group Publishing Limited, 289‚Äì303.\\nReceived July 2023; revised January 2024; accepted March 2024\\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 422. Publication date: November 2024.',\n",
       " 'Consistent Strong Triadic Closure in\\nMultilayer Networks\\nLutz Oettershagen\\nUniversity of Liverpool, UK\\nlutz.oettershagen@liverpool.ac.ukAthanasios L. Konstantinidis\\nLUISS University, Italy\\nakonstantinidis@luiss.itFariba Ranjbar\\nLUISS University, Italy\\nfariba.ranjbar@luiss.it\\nGiuseppe F. Italiano\\nLUISS University, Italy\\ngitaliano@luiss.it\\nAbstract ‚ÄîSocial network users are commonly connected to\\nhundreds or even thousands of other users. However, these ties\\nare not all of equal strength; for example, we often are connected\\nto good friends or family members as well as acquaintances.\\nInferring the tie strengths is an essential task in social network\\nanalysis. Common approaches classify the ties into strong and\\nweak edges based on the network topology using the strong triadic\\nclosure (STC) . The STC states that if for three nodes, A,B, and\\nC, there are strong ties between Aand B, as well as Aand\\nC, there has to be a (weak or strong) tie between Band C.\\nMoreover, a variant of the STC called STC+ allows adding new\\nweak edges to obtain improved solutions. Recently, the focus of\\nsocial network analysis has been shifting from single-layer to\\nmultilayer networks due to their ability to represent complex\\nsystems with multiple types of interactions or relationships in\\nmultiple social network platforms like Facebook, LinkedIn, or\\nX (formerly Twitter). However, straightforwardly applying the\\nSTC separately to each layer of multilayer networks usually\\nleads to inconsistent labelings between layers. Avoiding such\\ninconsistencies is essential as they contradict the idea that\\ntie strengths represent underlying, consistent truths about the\\nrelationships between users. Therefore, we adapt the definitions\\nof the STC and STC+ for multilayer networks and provide ILP\\nformulations to solve the problems exactly. Solving the ILPs is\\ncomputationally costly; hence, we additionally provide an efficient\\n2-approximation for the STC and a 6-approximation for the\\nSTC+ minimization variants. The experiments show that, unlike\\nstandard approaches, our new highly efficient algorithms lead to\\nconsistent strong/weak labelings of the multilayer network edges.\\nIndex Terms ‚Äîsocial networks, multilayer networks, strong\\ntriadic closure, edge strength inference\\nI. I NTRODUCTION\\nSince the pioneering work of Granovetter in 1973 [15],\\nthere has been a growing interest in the field of tie strength\\ninference, lately driven by the emergence of large-scale online\\nsocial networks and the widespread availability of digital\\ncontact and communication data. Nowadays, the inference of\\ntie strength in social networks has become a widely explored\\nsubject within the graph-mining community [1], [13], [19],\\n[26], [29], [33]. One of the fundamental ideas is that the\\nedges of a social network can be classified into strong re-\\nlationships, e.g., family members or good friends, and weak\\nrelationships, e.g., acquaintances that we met only once or\\ndistant work colleagues [15]. In the seminal work by Sintos\\nLayer 1\\nLayer 2AB\\nDC\\nAB\\nDC(a) Inconsistent labeling\\nLayer 1\\nLayer 2AB\\nDC\\nAB\\nDC (b) Consistent labeling\\nFig. 1: A toy multilayer network with four nodes in two layers.\\nThe strong edges are colored blue and weak edges red. In (a)\\nthe labeling is inconsistent in the two layers, e.g., the edges\\nbetween AandBhave different labels in the two layers. (b)\\nshows a consistent labeling, i.e., there are no two nodes that\\nare connected in both layers but with differently labeled edges.\\nand Tsaparas [34], the authors introduced the strong triadic\\nclosure (STC) property, where edges are classified as either\\nstrong orweak based solely on the network topology‚Äîfor\\nthree persons with two strong ties, there has to be a weak or\\nstrong third tie. Specifically, if person Ais strongly connected\\ntoB, and Bis strongly connected to C,AandCare at least\\nweakly connected. The intuition is that if AandBare good\\nfriends, and BandCare good friends, AandCshould at\\nleast know each other. The ability to infer the tie strength\\nbased only using the network topology is crucially important\\nas often no additional information is available, e.g., due to\\ndata collection limitations or privacy reasons.\\nWe extend the ideas of Sintos and Tsaparas [34] for mul-\\ntilayer networks which recently emerge as a powerful model\\nin social network analysis. A multilayer network consists of a\\nfinite set of vertices, for example social network users, and k\\ndistinct layers of pairwise connections (or edges) between the\\nusers. Because social structures and real-world networks are\\ncharacterized by multiple types of simultaneous interactions\\nor relationships they can be better modeled by multilayer\\nnetworks than by conventional networks [6], [8], [20]. Figure 1\\nshows a toy example of a 2-layer social network with four\\nusers. A straightforward way of applying the STC property\\nto multilayer networks is to label the edges separately forarXiv:2409.08405v1  [cs.SI]  12 Sep 2024each of the klayers. This way the edges of each layer are\\npartitioned into strong andweak edges. However, as shown in\\nFigure 1a this can lead to inconsistent labeling of the layers,\\ne.g., the edge between AandBisweak in layer one and\\nstrong in layer two. Indeed only the edge between BandD\\nstays consistently strong . However, naturally, if two users have\\nastrong connection they usually should have it in all layers\\nof the network‚Äîinconsistencies in tie strength inference pose\\na significant issue, as they contradict the expectation that tie\\nstrength represents an underlying, consistent truth about the\\nrelationship between two users. Hence, we are interested in\\ndetermining an STC labeling that is consistent over all layers.\\nIn case of the example, such a consistent solution if shown in\\nFigure 1b. To solve the issue of inconsistencies, we introduce\\na variant of the STC problem tailored to infer tie strengths\\nthat are consistent between the layers of multilayer graphs.\\nWe extend these results to a multilayer STC+ variant, where\\nSTC+ is a variant of the STC introduced in [34] which allows\\nadding a few new weak edges to obtain improved solutions.\\nOur contributions:\\n‚Ä¢We introduce variants of the STC and STC+ problems for\\nmultilayer networks. An essential aspect of our definitions\\nis that they consider consistency across different layers of\\nthe network. Our methods are based on the assumption\\nthat tie strength represents an underlying, consistent truth\\nabout user relationships.\\n‚Ä¢We provide efficient approximation algorithms with ap-\\nproximation ratios of two for the multilayer STC and six\\nfor the multilayer STC+.\\n‚Ä¢For solving the problems exactly, we introduce ILPs.\\n‚Ä¢Finally, we extensively evaluate our new algorithms us-\\ning eight real-world multilayer networks from various\\ndomains. We show that the baselines are not able to\\ncompute consistent solutions where our algorithms lead\\nto consistent labelings.\\nII. R ELATED WORK\\nThere are several recent surveys on multilayer networks,\\ne.g., [3], [5], [8], [9], [17]. Pappalardo et al. [31] measures\\nthe tie strengths in multilayer networks where a greater\\nnumber of connections on different layers results in higher\\n(continuous) tie strength between two users. However, they\\ndo not consider the STC property. There are several other\\nstudies on predicting the strength of ties given node and edge\\nfeatures in conventional networks, e.g., see [13], [18], [32],\\n[38], which also do not classify edges with respect to the STC.\\nIn contrast, our work is based on the STC and we infer binary\\ntie strengths in each layer solely based on the topology of\\nthe multilayer network. Sintos and Tsaparas [34] introduced\\nthe STC optimization problems by characterizing the edges of\\nthe network as strong orweak using only the structure of the\\nnetwork. They also proved that the problem of maximizing\\nthestrong edges is NP-hard, and provided two approximation\\nalgorithms. An extensive analysis of the STC can be found in\\nthe book of [11]. The authors of [16], [21], [22] focused on\\nrestricted networks to further explore the complexity of STCTABLE I: Commonly used notations\\nSymbol Definition\\nG= (V, E1, . . . , E k)multilayer graph\\nV finite set of nodes\\nk number of layers\\nE1, . . . , E k finite sets Eiwithi‚àà[k]of undirected edges\\nE=S\\ni‚àà[k]Ei aggregated set of edges in G\\nn=|V| number of nodes\\nmi=|Ei| number of edges in layer i\\nm=|E| number of aggregated edges\\n(v,{u, w}) wedge or open triangle with edge {u, w}missing\\nW(E) set of wedges in the set of edges E\\nW(E,{u, w}) set of wedges in Ewrt.{u, w}\\nW(G) wedge graph of G\\nSi‚äÜEi strong labeled edges in Ei\\ns(e) number of layers in which eis labeled strong\\nw(e) number of layers in which eis labeled weak\\ndk(S1, . . . , S k) number of disagreements between labelings S1, . . . , S k\\nmaximization. Rozenshtein et al. [33] discuss the STC with\\nadditional community connectivity constraints. The authors\\nof [1] proposed integer linear programming formulations and\\ncorresponding relaxations. Very recently, [28] proposed a new\\nproblem that uses the strong ties of the network to add new\\nedges and increase its connectivity. In a recent work [29],\\nthe authors considered the STC in temporal networks in\\nwhich the topology changes over time. They use a sliding\\ntime window and dynamically update the STC. The approach\\nwas then extended for the STC+ variant [30]. Veldt [37]\\ndiscusses connections between the cluster editing problem and\\nthe STC+. As far as we know, our work is the first considering\\nthe STC in multilayer networks.\\nIII. P RELIMINARIES\\nTable I gives an overview of the used notation and symbols.\\nWe use [k]withk‚ààNto denote the set {1, . . . , k }.\\nMultilayer Graphs, and (Hyper-)graphs We define a\\nmultilayer graph1G= (V, E 1, . . . , E k)consisting of a finite\\nset of nodes Vandkfinite sets Eiwithi‚àà[k]of undirected\\nedges e={u, v}withuandvinV, and uÃ∏=v. We define\\nn=|V|,mi=|Ei|, and m=Pk\\ni=1mi. We call a 1-layer\\ngraph just graph. Furthermore, given a graph G= (V, E)\\nwe define a wedge as a triplet of nodes u, v, w ‚ààVsuch\\nthat{{u, v},{v, w}} ‚äÜ Eand{u, w}/‚ààE. We denote\\nsuch a wedge by (v,{u, w}), and with W(E), the set of\\nwedges in the set of edges E. Moreover, W(E,{u, w}) =\\n{(v,{u, w})|v‚ààV} ‚äÜ W (E)denotes the set of wedges\\nwrt.{u, w}. Ahypergraph H= (V, E)consists of a finite\\nset of nodes Vand a finite set of hyperedges E‚äÜ2V\\\\ ‚àÖ,\\ni.e., each hyperedge connects a non-empty subset of V. We\\nconsider the special case of 3-uniform hypergraphs in which\\neach hyperedge consist of an unordered triple. We define the\\nunion H=H1‚à™H2of two (hyper-)graphs H1= (V1, E1)\\nandH2= (V2, E2)asH= (V1‚à™V2, E1‚à™E2).\\nStrong Triadic Closure Given a graph G= (V, E), we\\ncan assign one of the labels weak orstrong to each edge in\\n1We use the terms multilayer graph andmultilayer network interchangeably.e‚ààE. We call such a labeling a strong-weak labeling , and we\\nspecify the labeling by a subset S‚äÜE. Each edge e‚ààSis\\ncalled strong , and e‚ààE\\\\Sweak . The strong triadic closure\\n(STC) of a graph Gis a strong-weak labeling S‚äÜEsuch that\\nfor any two strong edges {u, v} ‚ààSand{v, w} ‚ààS, there\\nis a ( weak orstrong ) edge {u, w} ‚ààE. We say that such\\na labeling fulfills the strong triadic closure. In other words,\\nin a strong triadic closure, there is no pair of strong edges\\n{u, v}and{v, w}such that {u, w}/‚ààE. Consequently, a\\nlabeling S‚äÜEfulfills the STC if and only if at most one\\nedge of any wedge in W(E)is in S, i.e., there is no wedge\\nwith two strong edges [34]. The M AXSTC problem is then\\ndefined as the following optimization problem: Given a graph\\nG= (V, E), find S‚äÜEthat fulfills the strong triadic closure\\nand with |S|being maximized.\\nEquivalently, we can define an optimization problem based\\non the weak edges, denoted as M INSTC, in which given a\\ngraph G= (V, E)the goal is to find E‚Ä≤‚äÜEsuch that E\\\\E‚Ä≤\\nfulfills the strong triadic closure and |E‚Ä≤|is minimized.\\nApproximation in Single Layer Graphs As shown in [34],\\nMAXSTC is hard to approximate, while M INSTC can be\\nefficiently approximated. To approximate M INSTC in a single\\nlayer graph G= (V, E), we first construct the wedge graph\\nW(G) = ( VW, EW). Solving the vertex cover (VC) problem\\nonW(G)leads then to a solution for the STC of G, where VC\\nis defined as follows: Given a graph G= (V, E), the vertex\\ncover for a subset of the vertices C‚äÜVof minimum size\\nsuch that each edge e‚ààEis incident to a vertex v‚ààC.\\nLemma 1 ([34]) .Solving the VC on W(G)leads to a solution\\nof the STC on G.\\nNote that per definition, the wedge graph W(G)contains\\nfor each edge {i, j} ‚ààE(G)one vertex nij‚ààV(W). Two\\nvertices nuv, nuwinW(G)are only adjacent iff. there exists\\na wedge (u,{v, w})‚àà W(G). Hence, if we choose the set of\\nweak edges to be the edges {i, j} ‚ààEsuch that nij‚ààC,\\neach wedge has at least one weak edge.\\nRecently, the approximation was generalized for edge\\nweighted graphs [29] by mapping the edge weights of the input\\ngraph to node weights in the wedge graph, and solving the\\nMinimum Weighted Vertex Cover (MWVC) problem using the\\npricing algorithm, leading to a 2-approximation. The pricing\\nalgorithm is based on the primal-dual method [2]: Initially,\\neach edge e‚ààEis assigned a price p(e)of value zero. A\\nvertex is tight if the sum of the prices of its incident edges\\nequals the weight of the vertex. The pricing algorithm then\\niterates over the edges, and if for e={u, v}bothuandvare\\nnot tight, it increases the price of p(e)until at least one of u\\norvis tight. In the end, the tight vertices constitute the vertex\\ncover representing the weak edges.\\nStrong Triadic Closure with Edge Additions Here apart\\nfrom labeling the edges of the graph as strong orweak , new\\nweak edges between non-adjacent nodes can be added. We\\ndenote this problem by M INSTC+ and it is stated as follows:\\nGiven a graph G= (V, E), the goal is to find a set F‚äÜ\\x00V\\n2\\x01\\n\\\\Eand a set E‚Ä≤‚äÜEsuch that E\\\\E‚Ä≤fulfills the strong triadic\\nclosure and |E‚Ä≤‚à™F|is minimized.\\nThe motivation is the following [34]: Let Gbe an (almost)\\ncomplete graph on nvertices with exactly one edge {u, v}\\nmissing. In this case, the best strong/weak labeling for G\\ncontains n‚àí2weak edges. By adding the single edge {u, v}\\ntoG, we obtain the complete graph over nvertices for which\\nall edges can be labeled strong improving the labeling.\\nThe authors of [30] extended the approximation of the STC\\nto the STC+ problem. Given a graph G, they construct a 3-\\nuniform wedge hypergraph in which nodes represent edges\\nof input graph or edges that will be newly inserted. The\\nhyperedges correspond to the wedges in G. They obtain a 3-\\napproximation algorithm using the pricing method analogously\\nto the approximation of the weighted M INSTC problem.\\nIV. S TRONG TRIADIC CLOSURE IN MULTILAYER GRAPHS\\nWe now introduce the STC problem for multilayer networks.\\nFirst, we define a notion of disagreement as follows.\\nDefinition 1. LetSi‚äÜEifori‚àà[k]be the labelings of edges\\ninE1, . . . , E k, respectively. We say, for 1‚â§i < j ‚â§k,Si\\nandSjdisagree on edge e‚ààEi‚à©Ej, ife‚ààSi‚à™Sjand\\neÃ∏‚ààSi‚à©Sj, otherwise we say that they agree on e.\\nWe define E=S\\ni‚àà[k]Eiand for each e‚ààEwe define\\ns(e)to be the number of layers in which eis labeled strong\\nandw(e)the number of layers in which eis labeled weak .\\nFurthermore, we define the indicator function 1{e}= 1\\niff.w(e)>0and1{e}= 0 otherwise. Next, we define\\ndk(S1, . . . , S k) =P\\ne‚ààE1{e}¬∑s(e)as the number of dis-\\nagreements between the labelings S1, . . . , S k.\\nProblem 1 (MAXMULTI LAYER STC) .Given a multilayer\\ngraph G= (V, E 1, . . . , E k), find labelings Si‚äÜEifori‚àà[k]\\nsuch that (1) for each i‚àà[k],Sisatisfies STC for Ei, and (2)P\\ni‚àà[k]|Si| ‚àídk(S1, . . . , S k)is maximized.\\nSimilarly, we can define the minimization version.\\nProblem 2 (MINMULTI LAYER STC) .Given a multilayer\\ngraph G= (V, E 1, . . . , E k), find labelings Si‚äÜEifori‚àà[k]\\nsuch that (1) for each i‚àà[k],Sisatisfies STC for Ei, and (2)P\\ni‚àà[k]|Ei\\\\Si|+dk(S1, . . . , S k)is minimized.\\nThe problems are generalizations of the single layer prob-\\nlems introduced in [34] as, clearly, for a single layer graph\\nd1(S1) = 0 . It follows, that the decision version of M AX-\\nMULTI LAYER STC is NP-complete by the reduction from\\nMaximum Clique [34], which further implies that M AXMUL-\\nTILAYER STC cannot be better approximated than O(n1‚àíœµ).\\nFurthermore, solving the single layer STC optimally in each\\nlayer does not lead to an optimal solution for the multilayer\\nvariant. Figure 2 shows an example: In Figure 2a the STC is\\ncomputed in each layer separately leading to an objective value\\nof seven as there are 10 strong edges and three inconsistencies.\\nIn contrast, by solving the M AXMULTI LAYER STC problem\\noptimally, as shown in Figure 2b, we obtain an objective value\\nof eight. Moreover, note that the labeling in each layer can beLayer 1\\nLayer 2\\nLayer 3(a) Non-optimal\\nLayer 1\\nLayer 2\\nLayer 3 (b) Optimal\\nFig. 2: A multilayer graph with three layers. The blue edges\\narestrong and the red are weak . (a) shows a non-optimal\\nsolution for M AXMULTI LAYER STC by solving M AXSTC\\noptimally in each layer. (b) shows an optimal solution for\\nMAXMULTI LAYER STC. The value of the solution in (a) is\\nseven, the optimal value in (b) is eight.\\noptimal (layer 1) or non-optimal (layers 2 and 3) wrt. to the\\nstandard STC which prevents us from simply solving the STC\\nfor each layer separately.\\nIn the remaining of this section, we give a 2-approximation\\nfor M INMULTI LAYER STC and ILP formulations for solving\\nthe problems exactly in Section VI. The following lemma will\\nprovide the basis for our approximation algorithm and the ILP\\nformulations. Intuitively, it states that if there is a solution\\nwith disagreements, then there is another solution without\\ndisagreements with the same objective value.\\nLemma 2. LetS‚àó\\ni‚äÜEifor all i‚àà[k]be labelings and let\\ns‚àó=P\\ni‚àà[k]|S‚àó\\ni|‚àídk(S‚àó\\n1, . . . , S‚àó\\nk). There exist another set of\\nlabelings H‚àó\\ni‚äÜEifor all i‚àà[k]such thatP\\ni‚àà[k]|H‚àó\\nk|=s‚àó.\\nProof. The proof follows from the definition of disagreement:\\nLetF‚äÜS\\ni‚àà[k]Eibe the multiset of edges esuch that e‚àà\\nEi‚à©Ej,e‚ààS‚àó\\ni‚à™S‚àó\\njandeÃ∏‚ààS‚àó\\ni‚à©S‚àó\\njfor some 1‚â§i < j‚â§k\\n(assume the edges are distinguishable by layer). Note, that F\\ncontains all strong edges that are also weak in at least one\\nlayer. We define H‚àó\\ni=S‚àó\\ni\\\\Fifor all i‚àà[k], where Fi‚äÜF\\nis the set of edges belonging to layer i. Hence, it follows that\\nX\\ni‚àà[k]|H‚àó\\ni|=X\\ni‚àà[k]|S‚àó\\ni\\\\Fi|=X\\ni‚àà[k]|S‚àó\\ni| ‚àí |F|.\\nThe second equality holds as the multiset Fcan be partitioned\\ninto a family of sets Fifori‚àà[k]. By definition, for each\\neinE=S\\ni‚àà[k]Ei, the value of s(e)equals the number\\nof layers in which eis labeled strong anddk(S1, . . . , S k) =P\\ne‚ààE1{e}¬∑s(e), where 1{e}indicates that the edge eis in at\\nleast one layer labeled weak . Recall that Fcontains all strong\\nedges that are weak in at least one other layer. Therefore,\\ndk(S1, . . . , S k)equals the size of F.\\nAlgorithm 1 approximates the minimizing version the prob-\\nlem based on Lemma 2. The algorithm first computes aseparate wedge graph Wifor each layer i‚àà[k]of the\\nmultilayer network, and then combines these wedge graphs\\ninto a single vertex weighted wedge graph W. The weight\\nof node necorrespond to the numbers of layers in which\\nthe corresponding edge eexists. See Figure 3 for an example\\nof the construction. Using the pricing method to obtain a 2-\\napproximation algorithm for the minimum weight vertex cover\\ninW, yields a 2-approximation for multilayer STC.\\nTheorem 1. Algorithm 1 computes an 2-approximation for\\ntheMINMULTI LAYER STC inO(k¬∑n3)running time.\\nProof. LetGbe the input multilayer graph and Wbe the\\ncombined wedge graph as it is defined in Algorithm 1. Every\\nedge in Wresults from a wedge of a layer of G(the wedge\\ncan appear in more than one layer). We show that we can\\nsolve the M INMULTI LAYER STC problem in Gby solving\\nthe weighted vertex cover problem in W. Let C‚äÜV(W)\\nbe a solution for vertex cover in W. We construct a solution\\nfor M INMULTI LAYER STC by labeling weak every edge ein\\nevery layer such that nebelongs to Cand all remaining edges\\nasstrong . Thus, for every edge einW, at least one of its\\nendpoints belongs to C; therefore, for every wedge in G, one\\nof its edges is labeled as weak . Consequently, the labeling\\nfulfills the STC property in every layer i‚àà[k]. Now let\\n{S1, . . . , S k}be a solution for M INMULTI LAYER STC and\\nby Lemma 2, we know that the solution does not contain\\nany disagreement. We show that the nodes ne‚ààV(W)\\ncorresponding to the weak edges E‚Ä≤=E\\\\ ‚à™i‚àà[k]Siform\\na vertex cover for W. Since there is no disagreement in G, if\\nan edge eis weak in any layer, it is weak in every layer of G.\\nThus, for a weak edge e‚ààE‚Ä≤, we can add the corresponding\\nvertex ne‚ààV(W)to the vertex cover for W. Moreover, the\\nsolution {S1, . . . , S k}satisfies the STC property in every layer\\ni‚àà[k], which means that for every wedge, at least one of the\\nedges is weak . So, we get that for every edge in W, at least\\none of the endpoints is in the vertex cover.\\nRegarding the running time, we compute kwedge graphs\\nWi, one for each layer i‚àà[k]. Every Wican be computed in\\nO(n3)and contains at most O(n3)edges [29]. Hence, the total\\nnumber of edges of the combined graph Wis at most O(k¬∑n3).\\nThe2-approximation algorithm for the vertex cover using the\\npricing method needs linear time wrt. the edges of the wedge\\ngraph. Hence, the total running time O(k¬∑n3)follows.\\nOur algorithm is flexible towards the method used for\\nsolving the MWVC. The pricing method used in line 6 can\\nalso be replaced with other approaches, e.g., learning based\\napproaches [24] or using reduction rules [23]. For example,\\nin our experiments, we use the pricing method and a greedy\\nalgorithm (see Section VII).\\nV. M ULTILAYER STC WITH EDGE INSERTIONS\\nWe now consider the case in which we can add weak edges\\nEN\\ni‚äÜ\\x00V\\n2\\x01\\n\\\\Eiinto layer i‚àà[k]to improve the STC. To\\nthis end, we define the multilayer STC with edge additions.\\nWe first adapt the definition of disagreement accordingly toAlgorithm 1: Algorithm for M INMULTI LAYER STC\\nInput: Multilayer graph G= (V, E 1, . . . , E k)\\nOutput: Labelings S1, . . . , S k\\n1fori‚àà {1, . . . , k }do\\n2 Compute vertex weighted wedge graphs Wiof\\nGi= (V, E i)\\n3Combine wedge graphs to W=S\\ni‚àà{1,...,k}Wi\\n4forne‚ààV(W)do// edge eis inEi\\n5 w(ne)‚Üê |{i|ne‚ààV(Wi)}|\\n6Compute min. weight vertex cover ConWusing\\npricing algorithm\\n7fori‚àà[k]do\\n8 Si‚ÜêEi\\\\ {e|ne‚ààC} // strong labeled edges\\n9return Sifori‚àà[k]\\nLayer 1\\nLayer 2\\n(a)\\n11 1\\n112 2\\n11 1\\n112 2 (b)\\nFig. 3: (a) shows a multilayer graph with two layers. In each\\nlayer, the wedge graph is shown with its nodes as squares and\\nedges colored green. (b) shows the combined node-weighted\\nwedge graph as computed in Algorithm 1 and the aggregated\\nmultilayer graph in the background.\\ntake possible new weak edges into account. We now say, for\\n1‚â§i < j‚â§k, the labelings SiandSjdisagree on edge e‚àà\\n(Ei‚à™EN\\ni)‚à©(Ej‚à™EN\\nj), ife‚ààSi‚à™SjandeÃ∏‚ààSi‚à©Sj, otherwise\\nthey agree on e. As before, dk(S1, . . . , S k)denotes the number\\nof disagreements between the labelings S1, . . . , S k.\\nProblem 3 (MINMULTI LAYER STC+) .Given a multilayer\\ngraph G= (V, E 1, . . . , E k), find ksubsets of new edges\\nEN\\ni‚äÜ\\x00V\\n2\\x01\\n\\\\Eiand labelings Si‚äÜEifori‚àà[k]such\\nthat (1) for each i‚àà[k],Sisatisfies the STC for Ei‚à™EN\\ni,\\nand (2)P\\ni‚àà[k]|EN\\ni‚à™Ei\\\\Si|+dk(S1, . . . , S k)is minimized.\\nSimilar to Lemma 2, given a labeling for the M INMULTI -\\nLAYER STC+ problem, we can obtain another solution without\\ndisagreement.\\nLemma 3. LetS‚àó\\ni‚äÜEi‚à™EN\\niwithi‚àà[k]be labelings such\\nthats‚àó=P\\ni‚àà[k]|EN\\ni‚à™Ei\\\\S‚àó\\ni|+dk(S‚àó\\n1, . . . , S‚àó\\nk). Then there\\nexist another set of labelings H‚àó\\ni‚äÜEi‚à™EN\\niwithi‚àà[k]such\\nthatP\\ni‚àà[k]|EN\\ni‚à™Ei\\\\H‚àó\\ni|=s‚àó.\\nProof. LetF‚äÜS\\ni‚àà[k](Ei‚à™EN\\ni)be the multiset of all edges\\nthat are in at least one layer strong and in another layer weak ,i.e.,e‚àà(EN\\ni‚à™Ei)‚à©(EN\\nj‚à™Ej),e‚ààS‚àó\\ni‚à™S‚àó\\njandeÃ∏‚ààS‚àó\\ni‚à©S‚àó\\nj\\nfor some 1‚â§i < j ‚â§k. We define H‚àó\\ni=S‚àó\\ni\\\\Fifor\\nalli‚àà[k], where Fi‚äÜFis the set of edges belonging to\\nlayer i. Hence, it follows that\\nX\\ni‚àà[k]|EN\\ni‚à™Ei\\\\H‚àó\\ni|=X\\ni‚àà[k]|EN\\ni‚à™Ei\\\\(S‚àó\\ni\\\\Fi)|\\n=X\\ni‚àà[k]|EN\\ni‚à™Ei\\\\S‚àó\\ni|+|F|.\\nThe second equality holds as the multiset Fcan be partitioned\\ninto a family of sets Fifori‚àà[k]. We now show that |F|=\\ndk(S1, . . . , S k). By definition, for each einE=S\\ni‚àà[k](EN\\ni‚à™\\nEi), the value of s(e)equals the number of layers in which eis\\nlabeled strong anddk(S1, . . . , S k) =P\\ne‚ààE1{e}¬∑s(e), where\\n1{e}indicates that the edge eis in at least one layer labeled\\nweak . Recall that the multiset Fcontains all strong edges that\\nare in at least one other layer weak . Therefore, dk(S1, . . . , S k)\\nequals the size of F.\\nOur algorithm for the M INMULTI LAYER STC+ problem\\ncomputes an approximation without disagreement. Specifi-\\ncally, Algorithm 2 solves the following variant of Problem 3.\\nProblem 4 (MINMULTI LAYER STC+V ARIANT ).As Prob-\\nlem 3, with additionally (3) for each i‚àà[k], if there is a\\nnew edge e‚ààEN\\niand if there is another layer j‚àà[k],iÃ∏=j,\\nwith a wedge (v, e)‚àà W(Ej)then also e‚ààEN\\njholds.\\nLemma 4. AnŒ±-approximation for MINMULTI LAYER -\\nSTC+V ARIANT is a2Œ±-approximation for MINMULTI LAY-\\nERSTC+ .\\nProof. Due to Lemma 3, we know that there is a (opti-\\nmal) solution for the problem that does not contain any\\ndisagreement. Assume that there exists an Œ±-approximation\\nfor M INMULTI LAYER STC+V ARIANT . Namely, we can find a\\nsolution for M INMULTI LAYER STC+V ARIANT that is at most\\nŒ±times worse than the optimal solution. Now, for solving\\nProblem 4, if we insert a new weak edge in one layer for its\\ncorresponding wedge then we have to insert the same weak\\nedge in all the layers L ‚äÜ[k]in which a corresponding wedge\\nexists (due to (3) in Problem 4). However, it might not be\\nrequired that the corresponding wedge gets closed by the new\\nweak edge in each such layer. To be specific, adding a new\\nweak edge in all layers L, might not improve the labeling for\\nthose layers (Figure 4 shows an example). Therefore, we might\\nalready have a weak edge for the corresponding wedge, thus\\nby adding an additional weak edge, we have two weak edges,\\ni.e., in total the number of weak edges is at most doubled.\\nTherefore, the solution we get for M INMULTI LAYER STC+\\nis at most 2 Œ±times worse than the optimal solution.\\nUsing Lemma 4, we show the approximation quality of\\nAlgorithm 2 for Problem 3.\\nTheorem 2. Algorithm 2 computes a 6-approximation for\\nProblem 3 in O(k¬∑n3)running time.Layer 1\\nLayer 2C\\nBD E\\nA\\nC\\nBD E\\nAA clique with all the\\nedges labeled strong in\\nboth layersFig. 4: A multilayer graph with two layers and an optimal\\nsolution for M INMULTI LAYER STC. The blue (red) edges are\\nstrong (weak , resp.). Adding the edge {C, B}in layer one\\nreduces the number of weak edges while adding the same\\nedge in layer two does not improve the labeling.\\nAlgorithm 2: Algorithm for M INMULTI LAYER STC+\\nInput: Multilayer graph G= (V, E 1, . . . , E k)\\nOutput: Labelings S1, . . . , S k\\n1fori‚àà {1, . . . , k }do\\n2 Compute vertex weighted wedge hypergraphs Wi\\nofGi= (V, E i‚à™EN\\ni)\\n3 fore‚ààEN\\nisuch that W(EN\\ni, e)Ã∏=‚àÖdo\\n4 w(ne)‚Üê1\\n5 fore‚ààEidow(ne)‚Üê1\\n6Combine wedge hypergraphs to W=S\\ni‚àà{1,...,k}Wi\\n// sum up weights of each node over i\\n7Compute min. weight vertex cover ConWusing\\npricing algorithm\\n8fornuv‚ààCdo\\n9 if{u, v} ‚ààEN\\niand(w,{u, v})‚àà W(Ei)then\\n10 insert new weak edge {u, v}\\n11fori‚àà[k]do\\n12 Si‚ÜêEi\\\\ {e|ne‚ààC} // strong labeled edges\\n13return Sifori‚àà[k]\\nProof. LetGbe a multilayer graph and Wthe combined\\nwedge hypergraph as in Algorithm 2. Note that every hyper-\\nedge in Wresults from a wedge existing in at least one layer of\\nG. Moreover, a hyperedge cannot be created between vertices\\nwhere the corresponding edges belong to different layers. Now,\\nwe show that a minimum weighted vertex cover in Wleads\\nto a solution of M INMULTI LAYER STC+V ARIANT inG.\\nLetC‚äÜVWbe a vertex cover in W, i.e., for each\\nhyperedge e‚ààEW, there exists a vertex nuv‚ààewithnuvin\\nC, and the corresponding edge {u, v}is labeled weak in every\\nlayer that it belongs too or it is inserted as a new weak edge\\nfor closing the corresponding wedge in all layers in which the\\nwedge exists. All remaining edges of Gare labeled strong .\\nSince for every hyperedge in W, at least one of its vertices\\nbelongs to C, in the case the corresponding edge belongs to\\nEN\\nifor some i‚àà[k]then the corresponding wedge is closed\\nby the new weak edge in every layer that the wedge exists\\n(notice that if the new inserted edge already exists in some\\nother layers, it will also be labeled weak in all of them). Ifthe corresponding edge belongs to Eifor every iin which\\nthe wedge exists, then it is labeled weak in every layer that it\\nbelongs to, and we get that for every corresponding wedge\\nin layers of G, one of its edges is labeled as weak . This\\nmeans that the labeling fulfills the STC property in every layer.\\nHence, Cleads to a valid labeling for G.\\nNow let Lbe a solution for M INMULTI LAYER -\\nSTC+V ARIANT , which does not contain any disagreement (by\\nLemma 3). Since there is no disagreement in G, if an edge\\nis labeled weak in a layer, it is weak in every layer that it\\nbelongs to. Hence, we can add the corresponding vertex in\\nthe vertex cover for Wfor every weak edge. The solution L\\nsatisfies the STC property in every layer, which means every\\nwedge has at least one weak edge. Thus, every hyperedge in\\nWhas at least one of its vertices in the vertex cover. Now,\\nbecause the pricing algorithm for 3-uniform hypergraphs is a\\n3-approximation for MWVC and with Lemma 4 it follows that\\nAlgorithm 2 is a 6-approximation for Problem 3.\\nFor the time complexity, the number of hyperedges in the\\nwedge hypergraph is in O(n3)where nis the number of\\nvertices in G[29]. Since we compute the wedge hypergraphs\\nforklayers, and the pricing algorithm is linear in number of\\nhyperedges, a total running time of O(k¬∑n3)follows.\\nA. Post-Processing\\nBecause Algorithm 2 solves Problem 4 instead directly\\nProblem 3, if it inserts a new edge e, the new edge is inserted\\ninto all layers that contain a wedge that can be closed by e. It\\nmight be the case that in a layer i‚àà[k]there is no wedge that\\nis closed by econsisting out of two strong edges. Therefore,\\nremoving efrom layer jdoes not lead to a violation of the\\nSTC in this layer nor to an increase of disagreement. Our post-\\nprocessing checks for each newly inserted edge and for each\\nlayer i‚àà[k]if the edge is needed to fulfill the STC property.\\nIf this is not the case, we remove the edge from layer i.\\nVI. ILP F ORMULATIONS\\nIn order to solve the M AXMULTI LAYER STC problem ex-\\nactly, we provide an ILP formulation similar to the formulation\\nof the single layer version of the maximum STC problem [1].\\nIn the following, let E=S\\n‚Ñì‚àà[k]E‚Ñì. We use binary variables\\nxijwhich encode the if edge {i, j} ‚ààEisstrong (xij= 1) or\\nweak (xij= 0) . Furthermore, let mijbe the number of layers\\nin which the edge {i, j}exists. For M AXMULTI LAYER STC,\\nwe then define the ILP as follows:\\nmaxX\\n{i,j}‚ààExij¬∑mij (1)\\ns.t.xij+xih‚â§1‚àÄ(i,{j, h})‚àà W(E‚Ñì)and‚Ñì‚àà[k](2)\\nxij‚àà {0,1} ‚àÄ { i, j} ‚ààE. (3)\\nSimilarly, we define the minimization version for M IN-\\nMULTI LAYER STC, where we use binary variables yijwhich\\nencode the if edge {i, j} ‚ààEisstrong (yij= 0) orweak\\n(yij= 1) , as\\nminX\\n{i,j}‚ààEyij¬∑mij (4)s.t.yij+yih‚â•1‚àÄ(i,{j, h})‚àà W(E‚Ñì)and‚Ñì‚àà[k](5)\\nyij‚àà {0,1} ‚àÄ { i, j} ‚ààE. (6)\\nFor the case of the M INMULTI LAYER STC+ problem, we\\nfirst introduce a binary quadratic program, which can be\\nlinearized into an ILP. We use binary variables y‚Ñì\\nijwhich\\nencode if a (possibly new) edge {i, j} ‚ààE‚Ñì‚à™EN\\n‚Ñìisstrong\\n(y‚Ñì\\nij= 0) orweak (y‚Ñì\\nij= 1) . Additional, we use variables u‚Ñì\\nij\\nthat encode if edge {i, j}exists in layer ‚Ñì‚àà[k]withu‚Ñì\\nij= 1\\nif edge {i, j} ‚ààE‚Ñì‚à™EN\\n‚Ñì, and u‚Ñì\\nij= 0, otherwise. Clearly,\\nthe product u‚Ñì\\nijy‚Ñì\\nijis one iff. edge {i, j} ‚ààE‚Ñì‚à™EN\\n‚Ñìwill be\\na weak edge in the final result. This leads to the following\\nquadratic program:\\nminX\\n‚Ñì‚àà[k]X\\nij‚àà(V\\n2)u‚Ñì\\nij¬∑y‚Ñì\\nij (7)\\ns.t.u‚Ñì\\nijy‚Ñì\\nij+u‚Ñì\\nihy‚Ñì\\nih+u‚Ñì\\njhy‚Ñì\\njh‚â•1 (8)\\n‚àÄ(i,{j, h})‚àà W(E‚Ñì)and‚Ñì‚àà[k]\\ny‚Ñì\\nij‚àíyh\\nij= 0‚àÄ {i, j} ‚àà\\x00V\\n2\\x01\\nand1‚â§‚Ñì‚â§h‚â§k(9)\\nu‚Ñì\\nij= 1‚àÄ {i, j} ‚ààE‚Ñìand‚Ñì‚àà[k] (10)\\ny‚Ñì\\nij‚â•u‚Ñì\\nij‚àÄ {i, j} ‚àà\\x00V\\n2\\x01\\n\\\\E‚Ñìand‚Ñì‚àà[k] (11)\\ny‚Ñì\\nij‚àà {0,1} ‚àÄ { i, j} ‚àà\\x00V\\n2\\x01\\nand‚Ñì‚àà[k] (12)\\nu‚Ñì\\nij‚àà {0,1} ‚àÄ { i, j} ‚àà\\x00V\\n2\\x01\\nand‚Ñì‚àà[k]. (13)\\nEquation (8) ensures that for each wedge in any of the layers\\neither one of the wedge edges are weak , or a new edge is\\ninserted. Moreover, Equation (9) guarantees that if an edge\\nisweak in one layer, it is weak in all layers. Equation (10)\\nforces all edges that exist in the multilayer graph to exist in the\\nsolution. Finally, Equation (11) ensures that all newly inserted\\nedges are labeled weak .\\nBy linearization of the binary products, we obtain the ILP:\\nminX\\n‚Ñì‚àà[k]X\\nij‚àà(V\\n2)z‚Ñì\\nij (14)\\ns.t.z‚Ñì\\nij+z‚Ñì\\nih+z‚Ñì\\njh‚â•1 (15)\\n‚àÄ(i,{j, h})‚àà W(E‚Ñì)and‚Ñì‚àà[k]\\nz‚Ñì\\nij‚â§y‚Ñì\\nij{i, j} ‚àà\\x00V\\n2\\x01\\nand‚Ñì‚àà[k] (16)\\nz‚Ñì\\nij‚â§u‚Ñì\\nij{i, j} ‚àà\\x00V\\n2\\x01\\nand‚Ñì‚àà[k] (17)\\nz‚Ñì\\nij‚â•y‚Ñì\\nij+u‚Ñì\\nij‚àí1{i, j} ‚àà\\x00V\\n2\\x01\\nand‚Ñì‚àà[k] (18)\\ny‚Ñì\\nij‚àíyh\\nij= 0‚àÄ {i, j} ‚àà\\x00V\\n2\\x01\\nand1‚â§‚Ñì‚â§h‚â§k\\n(19)\\nu‚Ñì\\nij= 1‚àÄ {i, j} ‚ààE‚Ñìand‚Ñì‚àà[k] (20)\\ny‚Ñì\\nij‚â•u‚Ñì\\nijfor all {i, j} ‚àà\\x00V\\n2\\x01\\n\\\\E‚Ñìand‚Ñì‚àà[k](21)\\ny‚Ñì\\nij‚àà {0,1} ‚àÄ { i, j} ‚àà\\x00V\\n2\\x01\\nand‚Ñì‚àà[k] (22)\\nu‚Ñì\\nij‚àà {0,1} ‚àÄ { i, j} ‚àà\\x00V\\n2\\x01\\nand‚Ñì‚àà[k] (23)\\nz‚Ñì\\nij‚àà {0,1} ‚àÄ { i, j} ‚àà\\x00V\\n2\\x01\\nand‚Ñì‚àà[k]. (24)\\nVII. E XPERIMENTS\\nIn this section, we conduct experimental evaluations of our\\nproposed algorithms and answer the following questions:Q1. Consistency: How does the multilayer STC improve the\\nconsistency compared to the standard STC?\\nQ2. Multilayer STC+ variant: How does the M INMULTI -\\nLAYER STC+ improve the labeling?\\nQ3. Approximation quality: How is the empirical approxi-\\nmation quality of our algorithms?\\nQ4. Effect of the post-processing: How much does the post-\\nprocessing for the M INMULTI LAYER STC+ improve the\\nsolution?\\nQ5. Efficiency: How is the efficiency of our algorithms?\\nA. Data Sets\\nWe use eight real-world data sets, see Table II for an\\noverview. Specifically, (1) AUCS contains five different rela-\\ntions among the faculty within the CS department at Aarhus\\nUniversity [27]. (2) Hospital is a face-to-face contact networks\\nbetween hospital patients and health care workers [36]. The\\nnetwork spans five days representing the layers. (3) Airports\\nis an aviation transport network containing flight connections\\nbetween European airports [4], in which the layers represent\\ndifferent airline companies. (4) Rattus contains different types\\nof genetic interactions of Rattus Norvegicus [7]. (5) FfTwYt is\\na network in which users have a Twitter and a Youtube account\\nassociated with a Friendfeed account [10]. (6) Knowledge\\nis based on the FB15K-237 Knowledge Base data set [35].\\nEntities are nodes and edges in layers different kind of rela-\\ntions. (7) HomoSap is a network representing different types\\nof genetic interactions between genes in Homo Sapiens [12].\\n(8)DBLP is a subgraph of the DBLP graph [25] containing\\nonly publications from AandA‚àóranked conferences (Core\\nranking). The edges of each layer, i.e., conference, describe\\ncollaborations between authors.\\nTABLE II: Statistics of the data sets.\\nData set |V(G)| | E(G)|#Layers Domain\\nAUCS 61 353 5 Multilayer social\\nHospital 75 1 139 5 Temporal face-to-face\\nAirports 417 3 588 37 Multilayer transportation\\nRattus 2 634 3 677 6 Multilayer biological\\nFfTwYt 6 401 60 583 3 Multilayer social\\nKnowledge 14 505 210 946 30 Knowledge graph\\nHomoSap 18 190 137 659 7 Multilayer biological\\nDBLP 344 814 1 528 399 168 Multilayer collaboration\\nB. Algorithms\\nWe implemented and compared the following algorithms:\\n‚Ä¢ExactML andExactML+ are the exact computations\\nusing the ILPs (Section VI).\\n‚Ä¢BLExact andBLExact+ are the exact baseline com-\\nputations based on using the single layer graph ILP STC\\nand STC+ formulations computing the STC, or STC+,\\nresp., for each layer independently.\\n‚Ä¢Approx andApprox+ are our approximation algo-\\nrithms (Algorithm 1 and Algorithm 2).\\n‚Ä¢As baselines heuristics BLstc andBLstc+ , we first\\napproximate the solutions of the STC and STC+ in each\\nlayer independently and then fix the inconsistencies. Tothis end, we determine all strong edges that lead to\\ninconsistencies and label them weak edges to obtain\\nconsistency.\\nIn order to compute the vertex covers in the wedge graphs, we\\nuse the pricing algorithm (with unit weights in the unweighted\\ncase) and the greedy algorithm that iteratively adds the vertex\\nthat covers most uncovered edges. We append (P) for pricing\\nand(G) for greedy to the algorithm names, e.g., Approx(P)\\nfor our STC approximation using the pricing method.\\nWe implemented our algorithms in C++ using GNU CC\\nCompiler 11.4.0 with the flag --O3 . We used Gurobi 11\\nwith Python 3.11 for solving ILPs. All experiments run on\\na computer cluster. Each experiment had an exclusive node\\nwith an Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz and\\n96 GB of RAM. We used a time limit of 12 hours. The\\nsource code and data sets are available at https://gitlab.com/\\nmultilayergraphs/multilayerstc .\\nC. Results\\nIn the following, we answer the research questions.\\n1) Q1. Consistency: First, we verify that naively computing\\nthe STC labeling layer-wise leads to inconsistent STC label-\\nings. To this end, we define the consistency score\\n¬µ(S) =1\\n|F|X\\ne‚ààFs(e)\\ns(e) +w(e),\\nwhere F‚äÜEcontains all the edges ewith s(e)>0. Our\\napproach has a score value equal to one by definition.\\nTable IIIa shows the exact STC results using the ILP\\nformulations for the first four smaller data sets. The percentage\\nofweak edges is slightly higher for our algorithm compared\\nto the baseline (note that we do not relabel edges in the\\nexact baselines). However, the baseline has consistency scores\\nbetween 0.75 and 0.94, which shows that computing the STC\\nlayer-wise without considering the intricate structure of the\\nmultilayer graph leads to inconsistencies and not to an optimal\\nsolution for M INMULTI LAYER STC.\\nTable IV shows the results of approximating the STC. Here,\\nwe observe much lower consistency scores in the case of\\nthe approximation algorithms. The consistency scores of the\\nbaselines are, in most cases, very low, with averages of 0.21 for\\nBLstc(P) and 0.23 for BLstc(G) , showing that computing\\nthe labelings independent in each layer will lead to inconsistent\\nlabelings across the layers. The low consistency scores also\\nlead to low numbers of strong edges after relabeling all strong\\nedges that caused inconsistencies. The percentages of strong\\nedges are the highest for our Approx(G) algorithm for all\\ndata sets. Furthermore, the percentages of strong edges for\\nApprox(P) are higher than for BLstc(P) for all data sets.\\nFor evaluating the exact STC+, ExactML+ can only solve\\nthe smallest data set AUCS in the 12-hour time limit. The\\nreason is the large number of variables and inequalities of the\\nILPs. Similarly, ExactBL+ cannot solve Rattus in the time\\nlimit. Hence, we report the percentages of weak and strong\\nedges in an optimal solution or in a best solution found in\\nthe time limit of 12h. The BLExact+ baseline has higherTABLE III: Exact computation of label percentages and con-\\nsistency scores.\\n(a) STC\\nBLExact ExactML\\nData set Weak % Strong % ¬µ(S) Weak % Strong %\\nAUCS 55.8 44.2 0.84 64.8 35.2\\nHospital 75.2 24.8 0.75 83.6 16.4\\nAirports 85.2 14.8 0.92 86.7 13.3\\nRattus 78.0 22.0 0.94 78.7 21.3\\n(b) STC+ (*best solution found in time limit)\\nBLExact+ ExactML+\\nData set Weak % Strong % ¬µ(S) Weak % Strong %\\nAUCS 45.3 54.7 0.88 57.7 42.3\\nHospital 54.2 45.8 0.81 75.9* 24.1*\\nAirports 80.0 20.0 0.86 83.8* 16.2*\\nRattus 75.4* 24.6* 0.95* 76.5* 23.5*\\nnumbers of strong edges; however, the consistency scores are\\nonly between 0.81 and 0.95, verifying that we also, in practice,\\ncannot obtain an optimal solution for M INMULTI LAYER -\\nSTC+ by computing the STC+ layer by layer. ExactML+\\nimproves the percentage of strong edges for all data sets\\ncompared to ExactML . However, the percentages of strong\\nedges using Approx+(G) forAirports andRattus are higher\\nthan for the in the time limit of 12 hours obtained best solution\\nusing ExactML+ . The very high running times of the exact\\nalgorithms are a strong motivation for our approximations.\\n2) Q2. Multilayer STC+: Table V shows the results for\\nthe multilayer STC+ variant. In most cases, the percentages\\nofweak (strong) edges is reduced (increased) as expected.\\nOnly for BLstc+(P) the percentages of weak (strong) edges\\ncompared to BLstc(P) are slightly higher (lower). The\\nreasons are that more inconsistent strong edges are relabeled\\nweak . For BLstc+(G) , the average increase of strong edges\\nis 23.0%. The average consistency scores only slightly increase\\nfrom 0.21 to 0.22 for BLstc+(P) and from 0.23 to 0.25 for\\nBLstc+(G) . For both our algorithms, the average increase\\nofstrong edges is at 24%. Approx+(P) cannot improve the\\npercentage of strong edges for the DBLP data set but leads to\\nan increase of 75.5% in the case of Hospital .Approx+(G)\\nhas the highest increase of 71.3% for Airports and a low\\nincrease of 0.7% for the DBLP data set. The reason for the\\nlow increases for DBLP is that even the standard variants find\\nhigh numbers of strong edges (see Table IIIa) as the network\\nconsists of a union of cliques (coauthorships).\\n3) Q3. Approximation Quality: Figure 5a shows the em-\\npirical approximation ratios, i.e., the approximated objective\\nvalue divided by the optimal objective value computed by\\nthe ILPs for the multilayer STC and the four smaller data\\nsets. Even though Algorithm 1 using the pricing method\\n(Approx(P) ) is theoretically a 2-approximation, the empiri-\\ncal approximation quality is much better with values between\\n1.24 for AUCS and 1.08 for Airports . The greedy variant\\nApprox(G) , even though it is a O(logn)approximation, hasTABLE IV: Number of strong/weak edges and consistency scores for the multilayer STC.\\nBLstc(P) BLstc(G) Approx(P) Approx(G)\\nData set Weak % Strong % ¬µ(S) Weak % Strong % ¬µ(S) Weak % Strong % Weak % Strong %\\nAUCS 93.8 6.2 0.41 90.9 9.1 0.56 80.6 19.4 66.7 33.3\\nHospital 97.5 2.5 0.35 94.6 5.4 0.42 95.5 4.5 85.4 14.6\\nAirports 95.4 4.6 0.10 90.6 9.4 0.13 94.3 5.7 88.4 11.6\\nRattus 90.2 9.8 0.14 83.6 16.4 0.12 87.6 12.4 79.1 20.9\\nFfTwYt 98.7 1.3 0.32 96.1 3.9 0.31 97.4 2.6 93.0 7.0\\nKnowledge 96.9 3.1 0.11 93.1 6.9 0.10 96.1 3.9 91.7 8.3\\nHomoSap 97.4 2.6 0.17 94.0 6.0 0.16 96.5 3.5 92.2 7.8\\nDBLP 64.2 35.8 0.10 55.3 44.7 0.11 57.7 42.3 46.5 53.5\\nTABLE V: Number of strong/weak edges and consistency scores for the multilayer STC+.\\nBLstc+(P) BLstc+(G) Approx+(P) Approx+(G)\\nData set Weak % Strong % ¬µ(S) Weak % Strong % ¬µ(S) Weak % Strong % Weak % Strong %\\nAUCS 95.2 4.8 0.46 87.7 12.3 0.61 79.8 20.2 63.5 36.5\\nHospital 96.0 4.0 0.39 94.6 5.4 0.42 92.1 7.9 83.2 16.8\\nAirports 95.5 4.5 0.11 82.8 17.2 0.16 91.1 8.9 80.9 19.9\\nRattus 90.3 9.7 0.14 78.8 21.2 0.14 87.1 12.9 74.3 25.7\\nFfTwYt 98.6 1.4 0.31 89.5 10.5 0.33 96.9 3.1 91.9 8.1\\nKnowledge 96.9 3.1 0.10 88.9 11.1 0.10 95.4 4.5 89.5 10.5\\nHomoSap 97.5 2.5 0.17 91.7 8.3 0.16 95.8 4.2 89.7 10.3\\nDBLP 64.3 35.7 0.10 54.2 45.8 0.11 57.6 42.3 46.1 53.9\\nAUCS Hospital Airports Rattus1.01.11.21.3Approximation ratioApprox(P)\\nApprox(G)\\n(a) STC\\nAUCS Hospital* Airports* Rattus*1.01.11.21.3Approximation ratioApprox+(P)\\nApprox+(G) (b) STC+\\nFig. 5: The empirical approximation ratios (*using lower\\nbound of optimal solution after 12h time limit).\\neven better empirical approximation ratios between 1.005 for\\nRattus and 1.03 for AUCS . This is expected, as the greedy\\nvertex cover algorithm is known to often outperform the\\npricing method [14].\\nFigure 5b shows the results for the STC+. Here, we use\\nthe best lower bounds for the optimal solutions for the data\\nsets for which no optimal solution could be obtained within\\nthe time limit. Moreover, we report the final results after post-\\nprocessing (see Q4 for a discussion of the post-processing). We\\nsee low empirical approximation ratios for both Approx+(P)\\nandApprox+(G) with values between 1.14 for Airports and\\n1.32 for AUCS in case of Approx+(P) and 1.06 for Rattus\\nand 1.29 for Hospital in the case of Approx+(G) . Here,\\nApprox+(P) beats Approx+(G) for the Airports data set.\\nNote that as we use lower bounds of the optimal solution, the\\napproximation ratios reported are upper bounds.\\n4) Q4. Effect of the Post-Processing: Table VI shows the\\nnumbers of the newly inserted weak edges by Algorithm 2\\nwith and without the post-processing described in Section V-A.\\nThe post-processing is able to reduce the number of newTABLE VI: Number of newly inserted edges with and without\\nthe post-processing for Approx+ .\\nWithout post-processing With post-processing Reduction %\\nData set Approx+(P) Approx+(G) Approx+(P) Approx+(G) Approx+(P) Approx+(G)\\nAUCS 406 362 10 52 97.54 85.64\\nHospital 5 351 5 010 34 166 99.36 96.69\\nAirports 13 303 11 510 103 765 99.23 93.35\\nRattus 1 358 374 4 252 99.71 32.62\\nFfTwYt 197 758 177 512 64 2 111 99.97 98.81\\nKnowledge 153 771 147 403 610 13 950 99.60 90.54\\nHomoSap 230 857 185 563 483 14 714 99.79 92.07\\nDBLP 496 816 223 684 1 372 49 426 99.72 77.90\\nedges up to 99.79% in the case of the pricing method\\nand the HomoSap data set. The smallest reduction is for\\nApprox+(G) and the Rattus data set with 32.62%. On\\naverage, we observe a decrease of 99.4% for Approx+(P)\\nand 83.5% for Approx+(G) , showing a strong effectiveness\\nof the post-processing.\\n5) Q5. Efficiency: Table VII shows the running times of\\nthe algorithms. As expected, all algorithms have running\\ntimes proportional to the wedge (hyper-)graph sizes (shown\\nin Table VIII). Our algorithms achieve similar running times\\nfor most data sets as the baselines, whereas for smaller data\\nsets, the baseline algorithms are usually slightly faster than our\\nalgorithms. However, our algorithms are faster for the DBLP\\ndata set. The reason is that the vertex cover subroutine is only\\ncalled for one wedge graph, whereas for the DBLP data set,\\nit is called for each of the 168 layers. This is also the case\\nfor the other data sets, but the effect is not as strong due\\nto the lower number of layers. Similarly, in the case of the\\nSTC+ variant, our Approx+(G) beats the BLstc+(G) for\\nHomoSap . Here again, the reason is that for the baselines, the\\nvertex cover routine is called for each layer. Due to the larger\\nhypergraph sizes and because Approx+(G) only calls theTABLE VII: Running times in seconds.\\nSTC STC+\\nData set BLstc(P) BLstc(G) Approx(P) Approx(G) BLstc+(P) BLstc+(G) Approx+(P) Approx+(G)\\nAUCS 0.005 0.005 0.005 0.005 0.003 0.005 0.003 0.005\\nHospital 0.01 0.01 0.01 0.01 0.01 0.03 0.01 0.03\\nAirports 0.01 0.01 0.03 0.04 0.04 0.10 0.07 0.16\\nRattus 0.12 0.11 0.21 0.31 0.59 1.51 0.67 1.57\\nFfTwYt 7.94 9.74 9.35 16.83 29.45 103.20 42.52 118.27\\nKnowledge 22.99 26.56 32.82 58.91 90.87 340.73 102.82 366.63\\nHomoSap 41.20 45.78 54.75 100.04 231.46 905.07 272.38 731.67\\nDBLP 69.59 94.01 12.85 25.87 59.03 127.38 50.21 115.28\\nTABLE VIII: Sizes of the wedge (hyper-)graphs (the number\\nof (hyper-)edges is equal for STC and STC+).\\nData set (STC)|V(W)| (STC+) |V(W)| | E(W)|\\nAUCS 353 845 2 152\\nHospital 1 139 2 333 15 196\\nAirports 2 953 17 053 50 226\\nRattus 3 677 380 818 386 877\\nFfTwYt 60 583 6 410 083 12 037 201\\nKnowledge 210 946 19 795 949 42 820 276\\nHomoSap 137 659 50 630 405 62 634 802\\nDBLP 1 528 399 7 925 839 12 356 011\\npricing subroutine once, it achieves a lower running time.\\nVIII. C ONCLUSION\\nWe introduced variants of the popular STC and STC+ for\\nmultilayer graphs and identified the issues of inconsistent\\nSTC/STC+ labelings. To solve these issues, we proposed a set\\nof exact and approximative algorithms. Our algorithms ensure\\nconsistent STC or STC+ labeling across all layers, reflecting\\nthat tie strengths represent underlying, consistent truths about\\nthe relationships between users.\\nREFERENCES\\n[1] Adriaens, F., De Bie, T., Gionis, A., Lijffijt, J., Matakos, A., Rozen-\\nshtein, P.: Relaxing the strong triadic closure problem for edge strength\\ninference. Data Min. Knowl. Discov. pp. 1‚Äì41 (2020)\\n[2] Bar-Yehuda, R., Even, S.: A linear-time approximation algorithm for\\nthe weighted vertex cover problem. Journal of Algorithms 2(2), 198‚Äì\\n203 (1981)\\n[3] Boccaletti, S., Bianconi, G., Criado, R., Del Genio, C.I., G ¬¥omez-\\nGardenes, J., Romance, M., Sendina-Nadal, I., Wang, Z., Zanin, M.: The\\nstructure and dynamics of multilayer networks. Physics reports 544(1),\\n1‚Äì122 (2014)\\n[4] Cardillo, A., G ¬¥omez-Gardenes, J., Zanin, M., Romance, M., Papo,\\nD., Pozo, F.d., Boccaletti, S.: Emergence of network features from\\nmultiplexity. Scientific reports 3(1), 1344 (2013)\\n[5] Crainic, T.G., Gendron, B., Kazemzadeh, M.R.A.: A taxonomy of\\nmultilayer network design and a survey of transportation and telecom-\\nmunication applications. Eu. J. of Oper. Res. 303(1), 1‚Äì13 (2022)\\n[6] De Domenico, M.: More is different in real-world multilayer networks.\\nNature Physics 19(9), 1247‚Äì1262 (2023)\\n[7] De Domenico, M., Nicosia, V ., Arenas, A., Latora, V .: Structural\\nreducibility of multilayer networks. Nature comm. 6(1), 6864 (2015)\\n[8] De Domenico, M., Sol ¬¥e-Ribalta, A., Cozzo, E., Kivel ¬®a, M., Moreno,\\nY ., Porter, M.A., G ¬¥omez, S., Arenas, A.: Mathematical formulation of\\nmultilayer networks. Physical Review X 3(4), 041022 (2013)\\n[9] Dickison, M.E., Magnani, M., Rossi, L.: Multilayer social networks.\\nCambridge University Press (2016)\\n[10] Dickison, M.E., Magnani, M., Rossi, L.: Multilayer Social Networks.\\nCambridge University Press (2016)\\n[11] Easley, D.A., Kleinberg, J.M.: Networks, Crowds, and Markets - Rea-\\nsoning About a Highly Connected World (2010)[12] Galimberti, E., Bonchi, F., Gullo, F.: Core decomposition and densest\\nsubgraph in multilayer networks. In: CIKM. pp. 1807‚Äì1816 (2017)\\n[13] Gilbert, E., Karahalios, K.: Predicting tie strength with social media. In:\\nProceedings of the 27th International Conference on Human Factors in\\nComputing Systems, CHI. pp. 211‚Äì220. ACM (2009)\\n[14] Gomes, F.C., Meneses, C.N., Pardalos, P.M., Viana, G.V .R.: Experimen-\\ntal analysis of approximation algorithms for the vertex cover and set\\ncovering problems. Computers & Operations Research 33(12), 3520‚Äì\\n3534 (2006)\\n[15] Granovetter, M.S.: The strength of weak ties. A. J. of Soc. 78(6), 1360‚Äì\\n1380 (1973)\\n[16] Gr ¬®uttemeier, N., Komusiewicz, C.: On the relation of strong triadic\\nclosure and cluster deletion. Algorithmica 82, 853‚Äì880 (2020)\\n[17] Hammoud, Z., Kramer, F.: Multilayer networks: aspects, implementa-\\ntions, and application in biomedicine. Big Data Analytics 5(1), 2 (2020)\\n[18] Jones, J.J., Settle, J.E., Bond, R.M., Fariss, C.J., Marlow, C., Fowler,\\nJ.H.: Inferring tie strength from online directed behavior. PloS one 8(1),\\ne52168 (2013)\\n[19] Kahanda, I., Neville, J.: Using transactional information to predict link\\nstrength in online social networks. In: Proceedings of the International\\nAAAI Conference on Web and Social Media. vol. 3, pp. 74‚Äì81 (2009)\\n[20] Kivel ¬®a, M., Arenas, A., Barthelemy, M., Gleeson, J.P., Moreno, Y .,\\nPorter, M.A.: Multilayer networks. Journal of complex networks 2(3),\\n203‚Äì271 (2014)\\n[21] Konstantinidis, A.L., Nikolopoulos, S.D., Papadopoulos, C.: Strong\\ntriadic closure in cographs and graphs of low maximum degree. Th.\\nComp. Sci. 740, 76‚Äì84 (2018)\\n[22] Konstantinidis, A.L., Papadopoulos, C.: Maximizing the strong triadic\\nclosure in split graphs and proper interval graphs. Discret. Appl. Math.\\n285, 79‚Äì95 (2020)\\n[23] Lamm, S., Schulz, C., Strash, D., Williger, R., Zhang, H.: Exactly\\nsolving the maximum weight independent set problem on large real-\\nworld graphs. In: ALENEX. pp. 144‚Äì158. SIAM (2019)\\n[24] Langedal, K., Langguth, J., Manne, F., Schroeder, D.T.: Efficient min-\\nimum weight vertex cover heuristics using graph neural networks. In:\\nSEA (2022)\\n[25] Ley, M.: The dblp computer science bibliography: Evolution, research\\nissues, perspectives. In: International symposium on string processing\\nand information retrieval. pp. 1‚Äì10. Springer (2002)\\n[26] L ¬®u, L., Zhou, T.: Link prediction in weighted networks: The role of\\nweak ties. Europhysics Letters 89(1), 18001 (2010)\\n[27] Magnani, M., Micenkova, B., Rossi, L.: Combinatorial analysis of\\nmultiple networks. arXiv preprint arXiv:1303.4986 (2013)\\n[28] Matakos, A., Gionis, A.: Strengthening ties towards a highly-connected\\nworld. Data Min. Knowl. Discov. 36(1), 448‚Äì476 (2022)\\n[29] Oettershagen, L., Konstantinidis, A.L., Italiano, G.F.: Inferring tie\\nstrength in temporal networks. In: ECMLPKDD. pp. 69‚Äì85 (2022)\\n[30] Oettershagen, L., Konstantinidis, A.L., Italiano, G.F.: Inferring tie\\nstrength in temporal networks. arXiv preprint arXiv:2206.11705 (2024)\\n[31] Pappalardo, L., Rossetti, G., Pedreschi, D.: Measuring tie strength in\\nmultidimensional networks. In: SEBD. pp. 223‚Äì230 (2013)\\n[32] Pham, H., Shahabi, C., Liu, Y .: Inferring social strength from spatiotem-\\nporal data. ACM Trans. Database Syst. 41(1), 7:1‚Äì7:47 (2016)\\n[33] Rozenshtein, P., Tatti, N., Gionis, A.: Inferring the strength of social ties:\\nA community-driven approach. In: KDD. pp. 1017‚Äì1025. ACM (2017)\\n[34] Sintos, S., Tsaparas, P.: Using strong triadic closure to characterize ties\\nin social networks. In: KDD. pp. 1466‚Äì1475 (2014)[35] Toutanova, K., Chen, D., Pantel, P., Poon, H., Choudhury, P., Gamon,\\nM.: Representing text for joint embedding of text and knowledge bases.\\nIn: Proceedings of the 2015 conference on empirical methods in natural\\nlanguage processing. pp. 1499‚Äì1509 (2015)\\n[36] Vanhems, P., Barrat, A., Cattuto, C., Pinton, J.F., Khanafer, N., R ¬¥egis,\\nC., Kim, B.a., Comte, B., V oirin, N.: Estimating potential infection\\ntransmission routes in hospital wards using wearable proximity sensors.\\nPloS one 8(9), e73970 (2013)\\n[37] Veldt, N.: Correlation clustering via strong triadic closure labeling: Fast\\napproximation algorithms and practical lower bounds. In: International\\nConference on Machine Learning. pp. 22060‚Äì22083. PMLR (2022)\\n[38] Xiang, R., Neville, J., Rogati, M.: Modeling relationship strength in\\nonline social networks. In: Proceedings of the 19th International Con-\\nference on World Wide Web, WWW. pp. 981‚Äì990. ACM (2010)',\n",
       " 'Engagement, Content Quality and Ideology over Time on the Facebook URL\\nDataset\\nAuthors Emma Fraxanet1, Fabrizio Germano2, 3, Andreas Kaltenbrunner4,5,1, Vicenc ¬∏ G ¬¥omez1\\nAffiliations1DTIC, Pompeu Fabra University\\n2DEE, Pompeu Fabra Universtiy;3Barcelona School of Economics\\n4IN3, Universitat Oberta de Catalunya, Barcleona, Spain;5ISI Foundation, Turin, Italy\\nemma.fraxanet@upf.edu, fabrizio.germano@upf.edu, akaltenbrunner@uoc.edu, vicen.gomez@upf.edu\\nAbstract\\nUnpacking the relationship between the ideology of so-\\ncial media users and their online news consumption of-\\nfers critical insight into the feedback loop between users‚Äô\\nengagement behavior and the recommender systems‚Äô con-\\ntent provision. However, disentangling inherent user behav-\\nior from platform-induced influences poses significant chal-\\nlenges, particularly when working with datasets covering\\nlimited time periods. In this study, we conduct both aggre-\\ngate and longitudinal analyses using the Facebook Privacy-\\nProtected Full URLs Dataset, examining user engagement\\nmetrics related to news URLs in the U.S. from January 2017\\nto December 2020. By incorporating the ideological align-\\nment and quality of news sources, along with users‚Äô political\\npreferences, we construct weighted averages of ideology and\\nquality of news consumption for liberal, conservative, and\\nmoderate audiences. This allows us to track the evolution of\\n(i) the ideological gap between liberals and conservatives and\\n(ii) the average quality of each group‚Äôs news consumption.\\nThese metrics are linked to broader phenomena such as polar-\\nization and misinformation. We identify two significant shifts\\nin trends for both metrics, each coinciding with changes in\\nuser engagement. Interestingly, during both inflection points,\\nthe ideological gap widens and news quality declines; how-\\never, engagement increases after the first one and decreases\\nafter the second. Finally, we contextualize these changes by\\ndiscussing their potential relation to two major updates to\\nFacebook‚Äôs News Feed algorithm.\\nIntroduction\\nPrevious research has hinted at a correlation between the\\nquality of news sources, user ideology leanings, and online\\nengagement. Studies have suggested that individuals tend to\\ngravitate towards news sources that align with their polit-\\nical beliefs, particularly with less moderate content corre-\\nlating with lower-quality news sources and with conserva-\\ntive audiences exhibiting higher levels of ideological segre-\\ngation (Gonz ¬¥alez-Bail ¬¥on et al. 2023; Bakshy, Messing, and\\nAdamic 2015; Robertson et al. 2023; Heseltine 2023).\\nHowever, most of these results are obtained for a specific\\nmoment in the lifetime of a platform, while the platform is\\ntypically undergoing changes in its algorithmic features, UX\\ndesign, privacy settings, or moderation strategies. Therefore,\\nCopyright ¬© 2022, Association for the Advancement of Artificial\\nIntelligence (www.aaai.org). All rights reserved.while these studies are very informative on the behavior of\\ndifferent segments of the population in the online realm, they\\nusually cannot address the interplay of these behaviors and\\nthe characteristics of the platform. Understanding whether\\nthe behaviors are stable through time or, on the other hand,\\nsubject to both exogenous and endogenous changes, is of\\nrelevance for the promotion of healthier online spaces.\\nIn this paper, we approach this problem by presenting an\\naggregate as well as a longitudinal engagement analysis ex-\\nploiting the temporal characteristics of the Facebook URL\\nDataset. Our primary contributions include the characteriza-\\ntion of different types of engagement (clicks, likes, shares,\\ncomments, etc.) in relation to user ideology and domain ide-\\nology and quality. Furthermore, we develop a metric to as-\\nsess partisan bias in the consumption patterns of users, with\\na focus on the ideological gap between news consumed by\\nconservatives and liberals, which serves as a proxy for po-\\nlarization in information consumption. Additionally, we per-\\nform a detailed study of changes in the quality of news con-\\nsumption and its relation to user and news outlet ideology.\\nWe observe notable changes in engagement trends and\\nabrupt shifts in their dynamic patterns, which cannot be triv-\\nially explained by simple changes in user behavior. Lastly,\\nwe contextualize these changes within possible algorith-\\nmic adjustments (Mosseri 2018; Hagey and Horwitz 2021;\\nNarayanan 2023).\\nRelated Literature\\nWe first present the relevant literature on user behavior on-\\nline, ideological leaning, and news consumption. Then, we\\ndiscuss the dataset we used and previous research conducted\\nwith it.\\nSocial media audiences and engagement\\nA recent series of papers centered on the 2020 US Elec-\\ntion and Facebook (Guess et al. 2023a; Nyhan et al. 2023;\\nGonz ¬¥alez-Bail ¬¥on et al. 2023; Guess et al. 2023b) examine\\nsocial media engagement on Facebook and Instagram and\\npresent notable findings about the effects of algorithmic fea-\\ntures on user experience and political polarization (Garcia\\n2023).\\nWhile these studies do not offer straightforward solutions\\nto the current challenges, they pave the way for new method-\\nologies to incorporate algorithmic effects into the analysis ofarXiv:2409.13461v1  [cs.CY]  20 Sep 2024social media phenomena. For instance, Guess et al. (2023a)\\nassesses the consequences of disabling all feed ranking cal-\\nculations, revealing that chronological order increases po-\\nlitical content exposure to untrustworthy sources while de-\\ncreasing user engagement and satisfaction. Similarly, Ny-\\nhan et al. (2023) concludes that decreasing exposure to like-\\nminded sources does not seem to offer a simple solution for\\npolarization issues.\\nIn this work, we find several results that agree with these\\nstudies, for example, highly segregated and asymmetric par-\\ntisan content diets (Nyhan et al. 2023; Gonz ¬¥alez-Bail ¬¥on et al.\\n2023), and evidence that re-shares ‚Äîthe feature facilitat-\\ningvirality ‚Äî may not be the only factor generating mis-\\ninformation and polarization (Guess et al. 2023b). However,\\nGonz ¬¥alez-Bail ¬¥on et al. (2023) is performed on selected treat-\\nment groups of users who gave consent for the study. This\\nmakes it difficult to generalize the observed effects to the\\ncontent shared and posted by users who are outside of the\\nstudied samples and thus subject to the standard features of\\nthe platform. Moreover, another constraint inherent in orga-\\nnized experiments, in general, is their reliance on static data\\nor shorter time frames, which usually cover a few months in\\nextent.\\nOutside of the studies mentioned above, research sur-\\nrounding algorithmic amplification of social phenomena\\nis also relevant to our work. Chavalarias, Bouchaud, and\\nPanahi (2024) study a recommender system with an evolv-\\ning sharing network and its interaction with opinion dynam-\\nics and individual biases. They find that recommender sys-\\ntems tend to maximize user engagement, leading to overex-\\nposure to toxic content and polarization.\\nFinally, the results of Gonz ¬¥alez-Bail ¬¥on et al. (2022)\\nand Huszar et al. (2022) support the hypothesis that\\nconservative-leaning content is amplified on Twitter. Re-\\ngarding the assessment of partisan audience bias in social\\nmedia, Robertson et al. (2018, 2023) have conducted au-\\ndits and investigations on user exposure and interactions\\nwith content on Google Search, while Bakshy, Messing, and\\nAdamic (2015) used US Facebook data on interactions with\\nshared news and friend networks. Contrarily to Gonz ¬¥alez-\\nBail¬¥on et al. (2023), both concluded that individual choices\\nplayed a stronger role in increasing like-minded content,\\nas well as limiting cross-cutting exposure, than algorithmic\\nranking.\\nThe Facebook privacy protected full URLs dataset\\nTheFacebook Privacy Protected Full URLs Dataset (Mess-\\ning et al. 2020) is a large-scale collection of public posts\\ncontaining URLs and user interaction data on the Facebook\\nplatform. To be included in the dataset, a URL needs to be\\nposted (or reshared) a specific number of times1. The dataset\\ncontains information (interaction counts) at the URL-action\\nlevel, broken down by month and audience demographics\\n(country, age, and gender). For the US, these counts are also\\nstratified by the estimated ideological leaning of the users,\\n1According to Messing et al. (2020), this minimum is around\\n100 times, with some variability due to the injected noise to mini-\\nmize information leakage.called Political Page Affinity or PPA, which classifies the\\naudience into five different buckets, scaled from ‚àí2(most\\nliberal) to +2(most conservative), with an additional bin\\n(unknown). This metric, based on the model described in\\nBarber ¬¥a (2015), is computed internally at Facebook.\\nThis dataset has been protected using differential pri-\\nvacy (Messing et al. 2020; Evans and King 2023). Specif-\\nically, the counts included in each URL have been perturbed\\nwith zero-centered Gaussian noise whose variance depends\\non the type of interaction. This protects individual privacy\\nby making it hard to trace actions back to specific URLs and\\nusers.\\nEvans and King (2023) show that ignoring such differen-\\ntial privacy protections when applying inference methods to\\nthis dataset can lead to unpredictable biased results. They\\nprovide corrections in feasible scenarios, especially for lin-\\near models. For more complex models that involve, e.g.,\\nweighted average metrics, there are no available corrections\\nin closed form. For situations involving a noisy denomina-\\ntor, such as the ones in this work, Evans and King (2021)\\nand Buntain et al. (2023) show that for high enough signal-\\nto-noise ratio estimates of the denominator, it is possible to\\nensure reliable metrics.\\nResearchers have used the Facebook Privacy Protected\\nFull URLs Dataset for various applications. Our work, akin\\nto Guess et al. (2021) and Bailey, Gregersen, and Roesner\\n(2021), provides a descriptive static analysis of exposure and\\nsharing patterns of news URLs in the US. We reveal findings\\nthat agree with the aforementioned previous literature, such\\nas conservative skewed consumption of low-quality news.\\nInterestingly, Bailey, Gregersen, and Roesner (2021) also\\nfind no difference between ideological leaning when click-\\ning on a URL after exposure, while there is a difference in\\nsharing.\\nWhen it comes to studying potential platform strategies,\\nThero and Vincent (2022) and Vincent, Thero, and Shabayek\\n(2022) investigate engagement changes in posts from groups\\nand pages tagged as ‚Äúrepeat offenders‚Äù of misinformation,\\nnoting the limited long-term effectiveness of such mea-\\nsures. Bandy and Diakopoulos (2023) characterize Face-\\nbook‚Äôs news exposure during the 2020 US election. They\\nshow that Facebook amplified low-quality publishers and\\nthat their efforts to improve the news ecosystem quality by\\ntweaking their News Feed Algorithm had a generalized ef-\\nfect on most publishers, disregarding their quality.\\nFinally, other studies leverage the dataset as a tool for in-\\nformation extraction (Allen, Watts, and Rand 2024; Buntain\\net al. 2023; Barnab `o et al. 2022, 2023; Heuer and Glassman\\n2022).\\nThe work closest to ours is Buntain et al. (2023), where\\nthe authors propose a metric for obtaining robust ideology\\nscores for URLs and domains based on exposure and en-\\ngagement. Unlike Buntain et al. (2023), we consider the\\ntemporal dimension and analyze the patterns of ideological\\nsegregation in news consumption and information quality\\nfrom engagement and ideology scores.Data Description and Collection\\nDomain selection and description\\nFor our analysis, we focus on the top 1% most-viewed do-\\nmains on Facebook between 2017 and 2021 as extracted\\nby Buntain et al. (2023) from the same dataset we use here.\\nIdeology scores : To have additional information on the\\nsources, we work with the domain-level ideology scores ob-\\ntained in Robertson et al. (2018), which cover an extensive\\nsubset of our domain list. Buntain et al. (2023) also pro-\\nvide similar estimates for audience ideology scores of the\\nnews sources, but they are based on an aggregation of the\\nuser ideology estimates for each URL via PPA and thus it is\\nnot recommendable to use them within analyses on the same\\ndataset. However, in Buntain et al. (2023), it is shown that\\ntheir obtained scores are highly correlated to the ones we\\nuse, as well as the scores found in other previous work (Bak-\\nshy, Messing, and Adamic 2015; Eady et al. 2020; Budak,\\nGoel, and Rao 2016). For robustness, we also repeat our\\nanalyses on the Buntain et al. (2023) scores and obtain very\\nsimilar results (data not shown).\\nQuality scores : To complement this list with quality\\nscores of news domains, we use the open-source rating set\\nfrom Lin et al. (2023). We follow their recommendation\\nwhen setting a threshold at 0.6to separate credible/high-\\nquality news sources.\\nBy performing an inner join operation of the three lists,\\nwe end up with a total of 1,231news domains, of which\\napproximately 58% are high-quality (i.e. quality score larger\\nthan0.6). For the rest of the paper, we will only consider\\nURLs that are from this list of 1,231domains. Details about\\nthe distribution of the number of domains in the space of\\nideology versus quality are given in the Appendix, Figure\\nA1. There is a weak negative correlation between these two\\nvariables ( r=‚àí0.35, p= 1.81e‚àí37).\\nData collection and preprocessing\\nFor each URL, we extract the counts of user engagement ac-\\ntions. The data is segmented by month, covering the period\\nfrom January 2017 to December 20202. We focus on URLs\\nshared primarily in the US (i.e., the most-seen country of the\\nURL is the US) and extract engagement counts from users\\nlocated in the US only.\\nWe only extract counts for URLs that are not older than\\none year (i.e. that were not posted on Facebook for the first\\ntime more than one year previous to the measurement). This\\nis a very loosely restricting threshold to avoid extracting in-\\ntractable amounts of data, given that (we hypothesize) the\\ninclusion of URLs in the dataset is cumulative: Facebook\\nincludes new URLs in the dataset every month (if they meet\\nthe minimum shares threshold) but does not remove them.\\nThis is the case for, at least, the time interval of 2017‚Äì2019.\\nWe empirically chose the one-year threshold after observing\\nthat the engagement of a URL typically declines, on average,\\nby two orders of magnitude within three months of its initial\\nposting.\\n2Access to this data and posterior analysis is conducted on the\\nFacebook Open Research and Transparency platform and allowed\\nthrough the Social Science One (SS1) approval process.We define passive engagement orconsumption (used in-\\nterchangeably) as the absolute counts of views and clicks.\\nConversely, we define active engagement as the likes,\\nshares, comments, and emoji reactions ( angers ,hahas ,\\nwows ,loves andsorrys ). We also consider the sum of all ac-\\ntive engagements as an additional metric. We aggregate the\\ndata in terms of gender and age of users, but use the dataset\\nbucket counts for the month and user PPA, including politi-\\ncally undefined users.\\nAggregate Analysis\\nTo understand and characterize user behavior in the platform\\nconcerning their consumption and interaction with posts that\\ncontain URLs, we first provide an aggregate analysis com-\\nprising the four years of data. We want to understand how\\nthe categories of users, in terms of ideological leaning, differ\\nin their engagement patterns. Therefore, after the description\\nof the aggregate methods and before the description of other\\nmethods, we present the aggregate findings. This allows a\\nmore nuanced and comprehensive analysis in the following\\nsections.\\nAggregating engagement with news-related URLs\\nWe compute the distribution of engagement counts over each\\nuser PPA category. We do not normalize the counts of en-\\ngagement per capita because we do not have information on\\nthe unique users who performed those actions. The methods\\nto obtain confidence intervals for these sums are detailed in\\nthe Appendix, section A2.\\nTo understand the patterns in news consumption for dif-\\nferent user categories in terms of ideology and quality of the\\ndomains contained in the URLs, we discretize both dimen-\\nsions into bins and represent the number of clicks in that ide-\\nology or quality level for each category of user in the form\\nof a heatmap. We use clicks because it is a form of passive\\nengagement that is a more reliable indicator of consump-\\ntion of the information contained in the URLs, since a view\\ncould merely account for a loaded and quickly scrolled-over\\npost in a user‚Äôs feed. However, we studied the same patterns\\nexplained below with several engagement metrics (views,\\nclicks, shares, likes, and comments) and found no relevant\\ndifferences between them.\\nWe normalize the heatmaps by column, obtaining the\\nprobability of the consumption of a specific domain level\\nconditioned on a user‚Äôs membership in a given category.\\nTo assess the reliability of our results in the presence of\\nnoisy ratios, we take the following steps: (I) We compute\\nthe Signal-to-Noise Ratio (SNR) of the denominator (Bun-\\ntain et al. 2023; Evans and King 2021) to avoid problematic\\ncases. A high value of SNR ensures that the noise is suffi-\\nciently low, preventing inflation of the metric or pathological\\noutcomes such as having a zero in the denominator. Gener-\\nally, for all metrics involving a noisy denominator in our\\nanalysis, we find SNR >105. Although no universally ac-\\ncepted threshold exists, our SNRs are much larger than what\\nhas been considered sufficient in other studies. For example,\\na SNR threshold of 16was obtained by Buntain et al. (2023)\\nfor reliable ideology estimates. This suggests that our find-\\nings can also be considered reliable. (II) We determine thea\\nFigure 1: Aggregate data description. (A) Distribution of engagement counts across user political page affinity. Negative/-\\npositive values correspond to more liberal/conservative leaning. Confidence intervals (C.I.) are computed based on the number\\nof URLs and the noise injected for each metric. Note that the U-shape of the engagement patterns becomes more explicit from\\nleft to right and from top to bottom. Not defined PPA users have larger counts of passive engagement (views and clicks) and\\nthus are not represented. However, they have comparable counts for engagement metrics. (B) Distribution of clicks based\\non user and domain ideological leaning. Domain scores follow the same liberal/conservative orientation. The bin width is\\n0.09. The values shown in the heatmap are normalized by column and therefore express the probability of a domain type being\\nclicked once the user ideology is fixed. Differences between domain levels are shown on the right. We find SNR >105for\\nall denominators involved in the normalization. Noise uncertainty intervals are computed for each cell and are of the order of\\n3¬∑10‚àí4.(C) Distribution of clicks based on user leaning and domain quality. The structure and computation of this plot are\\nthe same as the previous heatmap, solely exchanging the ideology scores for quality scores. The bin width is 0.05. Again, we\\nfind SNR >105for all denominators involved in the normalization. Noise uncertainty intervals are computed for each heatmap\\ncell and are of the order of 5¬∑10‚àí5.\\nnoise uncertainty intervals of the ratios using a worst-case\\nscenario approach, as outlined in in the Appendix, section\\nA3.\\nResults of the Aggregate Analysis\\nWe here present our findings concerning the aggregate anal-\\nysis regarding the engagement of users by ideology and the\\nideology and quality of domains.\\nEngagement patterns by user ideology\\nWhen examining patterns of engagement, we observe inter-\\nesting trends across different types of interactions and ideo-\\nlogical groups. From Figure 1A, we find a slightly higher\\nnumber of views for moderate and strong liberals, whichcould imply that either (I) they are more active in passively\\nconsuming URL-containing posts or (II) there are more of\\nthem on Facebook (if we hypothesize that the passive post\\nconsumption capacity of each group is independent of their\\nideological leaning). For clicks, the other type of passive\\nengagement, the pattern shifts slightly to a U-shape, with\\ncenter users having the lowest number of clicks. As for the\\nactive engagement counts, there is a clear U-shaped pattern\\nforhahas , likes, comments, and angers (increasingly pro-\\nnounced in that same order). The distribution for shares and\\nother reactions ( loves, sorrys andwows ) has a flatter shape,\\nor at least more similar to views (where moderate liberals\\nhave larger numbers). See Figure A2 in the Appendix, sec-\\ntion A4, for the distributions of reactions of loves, sorrys,hahas andwows .\\nRegarding non-defined users (data not shown), we find\\nthat they have a significantly larger view and click count\\ncompared to other categories individually but have compara-\\nble volumes of active engagement, reaching lower values for\\nangers . If we use views as a proxy for the number of users\\nin each category, this would imply that there is a large set\\nof politically disconnected users that consume content pas-\\nsively but are much less active than politically categorized\\nusers when it comes to engaging with news-related posts.\\nThis lower activity may in part be the reason why they are\\nnot assigned to an ideology bin.\\nDomain ideology and domain quality by user\\nideology\\nIn Figure 1B we show the distribution of clicks over user\\nPPA and the ideology of the domains. We see that, while\\nusers in the liberal and center classes seem to follow a\\nsingle-mode distribution, with its center tilted towards neg-\\native values for liberals and centered for the center category,\\nconservative users follow a bimodal distribution. We can see\\none mode at the center and another mode on the far conser-\\nvative part (upper part in the heatmap). Users with undefined\\nideology have a very similar profile to center users.\\nIn Figure 1C we show the quality dimension of the do-\\nmains and find that liberal-leaning users stay within the\\nhigh-quality bins, while there is a slow progression towards\\nwider distributions for the center and conservative-leaning\\nusers that eventually reach low-quality domains for the ex-\\ntreme conservatives. The corresponding Pearson correlation\\ncoefficient is ‚àí0.28(p‚àívalue <10‚àí99). This behavior is to\\nbe expected as the distribution of domain quality vs. ideol-\\nogy of the domains (depicted in Figure A1) shows a simi-\\nlar pattern but with a slightly higher correlation coefficient\\n(‚àí0.35,p‚àívalue <10‚àí37). Again, users with undefined\\nideology have a very similar profile to center users.\\nLongitudinal Analysis\\nIn this section, we present the methods, metrics, and results\\nof our longitudinal analysis, focusing on engagement and\\nconsumption of news-related URLs over a four-year period.\\nEngagement timeline and breakpoint detection\\nFigure 2 presents examples of time series for view counts\\n(top) and aggregated active engagement counts (middle).\\nTo analyze these series, we first identify relevant inflection\\npoints by applying segmented linear regression (Muggeo\\n2003; Jekel and Venter 2019) to each engagement metric in-\\ndividually. Segmented linear regression models the data as a\\npiecewise linear trend with changes at distinct breakpoints.\\nFor most individual engagement metrics the most natural\\nnumber of breakpoints identified is two, indicating the pres-\\nence of two distinct shifts: the first around the end of year\\n2017, where engagement transitions from a steep decline\\nto a moderate increase, and the second occurring after the\\nfirst quarter of 2020, where the reverse pattern is observed.\\nThese shifts are marked by shaded regions in Figure 2 (see\\nAppendix, section A5 for more details) while the dashedlines indicate dates for which actual changes in the Face-\\nbook ranking algorithm have been reported (Mosseri 2018;\\nHagey and Horwitz 2021; Narayanan 2023).\\nAlthough all individual engagement metrics exhibit sim-\\nilar trends, we find notable differences between some of\\nthem. For example, after the second trend shift, the abso-\\nlute sum of shares declines (orange line, bottom plot in Fig-\\nure 2), aligning with the absolute sum of comments (red line\\nin the same plot). This is consistent with a reported decrease\\nin the weight for shares affecting the feed ranking in 2020.\\nWe further elaborate this in the Discussion section.\\nFigure 2: Engagement timeline. (top) Total monthly counts\\nof views (passive enagement); (middle) aggregate of active\\nengagement metrics (e.g., likes, shares, comments). (bot-\\ntom) Two specific counts: shares and comments. Text la-\\nbels indicate the maximum values of the noise uncertainty\\nconfidence intervals (CI), calculated using Eq. 4. Shaded re-\\ngions represent potential inflection points, identified through\\npiecewise linear regression. Dashed lines mark the dates of\\ntwo major documented algorithmic changes.\\nIdeology weighted averages\\nWe now introduce our proposed tentative metric for partisan\\nsegregation in news consumption, a measure that is closelyrelated to the phenomena of polarization.\\nWe first define an ideology metric for each engagement\\ntypee(shares, likes, clicks, etc) and user PPA category\\nT‚àà {‚àí 2,‚àí1,0,1,2}as the weighted average of the\\ndomain-level ideology scores over the corresponding URLs.\\nFor all domain data, ideology scores are normalized to a\\nrange between 0and1, where 0represents the most liberal-\\nleaning and 1represents the most conservative-leaning do-\\nmains in our dataset. This normalization is performed to\\nprevent complications in calculating confidence intervals for\\nthe denominators.\\nThe average ideological leaning for engagement eand cat-\\negory Tis thus computed as\\n¬µe,T=P\\niyiCi\\ne,TP\\niCi\\ne,T, (1)\\nwhere yi‚àà[0,1]is the (normalized) domain-level ideology\\nscore of URL i, and Ci\\ne,Tare the counts for engagement e\\ngiven by users in category Tto URL i. The index igoes over\\nthe set of URLs engaged by users in category T.\\nFor clarity in the analysis, we combine moderate and\\nstrong user types into a single class, resulting in four distinct\\ncategories: conservatives C(buckets +2and+1), liberals L\\n(buckets ‚àí2and‚àí1), centrists N(including only bucket 0),\\nand undefined D(when the PPA is not defined). Combin-\\ning strong and moderate buckets does not affect the results\\nobtained.\\nWe also characterize the (weighted) standard deviation for\\neach engagement type eand PPA type Taccording to3\\nœÉe,T=sP\\niCi\\ne,T(yi‚àí¬µe,T)2\\nP\\niCi\\ne,T. (2)\\nWe propose the gap between the averaged ideological lean-\\ning of the URLs engaged by groups CandLas our metric\\nof segregation in news consumption. The expression for this\\nmetric is\\nIGAP=¬µe,C‚àí¬µe,L. (3)\\nGiven that we are again working with ratios involving\\nnoisy denominators, it is important to carefully evaluate the\\nsignal-to-noise ratio (SNR) of the denominator and to de-\\ntermine the uncertainty intervals for each component of the\\nideology gap. We obtain SNR values exceeding 1√ó105and\\nuncertainty intervals for the averages of both CandLon the\\norder of 1√ó10‚àí3(see Appendix for details). We therefore\\ncan conclude that the metric is statistically significant. The\\nweighted standard deviations are substantial and vary over\\ntime, so we plot them separately when needed.\\nContent Quality metrics\\nFor our analysis, we compute the previous metrics for each\\nmonth and consider their evolution during the four years in\\nthe dataset. We then approach the study of the quality of\\nnews consumption in two separate ways.\\n3Our partisan segregation metric IGAPis always positive, e.g.,\\n¬µe,C> ¬µe,Lin all the periods considered.First, we present a combined longitudinal study of both\\ncontent quality and ideological leaning. Based on our prior\\naggregate analysis that shows differences in the news di-\\nets of conservative and liberal users in terms of quality, we\\ncalculate the average content quality for each ideological\\nclass (i.e., conservatives, liberals, centrists, and undefined),\\nweighted by engagement metrics such as shares, likes, com-\\nments, and clicks. This calculation follows the same method-\\nology as the ideology-weighted average of equation (1).\\nAdditionally, we compute the weighted standard deviation,\\nsimilarly to equation (2). The evolution of both metrics over\\ntime provides further insight into the dynamics within each\\ngroup.\\nSecond, we study the evolution of news consumption\\nthrough time in terms of quality so we can assess the distri-\\nbution of engagement for different quality levels per month.\\nIn this case, we consider all users (with defined or undefined\\nPPA).\\nResults of the Longitudinal Analysis\\nWe here present our findings for the longitudinal analysis\\nregarding the ideology gap and information quality.\\nFindings regarding the ideology gap\\nThe aggregate analysis (see Figure 1) has already shown that\\nnews consumption varies across user categories based on\\nideological leanings. Consequently, a gap between the av-\\nerage consumption across user classes is expected, and this\\nis indeed observed. However, by plotting the weighted ide-\\nology averages, we can observe the changes in the gaps over\\ntime, which can provide new information.\\nFigure 3A shows these results. While the liberal and cen-\\ntrist weighted average is stable and shows a slight increase\\nover time, the conservative counterpart shows larger varia-\\ntions with no stable trend, which include a noticeable in-\\ncrease after the second inflection point region. It is also inter-\\nesting to note that centrist and undefined users show almost\\nidentical behavior. We present these results using clicks per\\nuser category as the weight of the averages, but perform the\\nsame analysis also using different actions and find qualita-\\ntively similar results, even though conservative and liberal\\naverages are positioned more to the extremes when consid-\\nering shares and likes (data not shown).\\nExamining the Weighted Standard Deviation of the av-\\nerages (Figure 3B), we observe two noteworthy points: (i)\\nFollowing the first shift in 2018, the weighted standard de-\\nviations of center- and liberal-leaning users decreases sud-\\ndenly, and changes into a flat trend. (ii) In contrast, after\\nthe second shift in 2020, the weighted standard deviation\\nfor center-leaning users shows a sharp increase, potentially\\nindicating that some individuals within this group began en-\\ngaging with the extreme conservative cluster mentioned in\\nthe distribution of domains.\\nFocusing on the ideology gap metric between conserva-\\ntive and liberal averages, shown in Figure 3C, we observe\\nthe following relevant temporal patterns. First, a gradual de-\\ncline until the second breakpoint in 2020, where the gap in-\\ncreases sharply. This rise can be attributed to a shift in the av-\\nerage consumption and engagement of conservative-leaningFigure 3: Longitudinal results. This figure summarizes the changes in consumption and engagement (clicks) with news-related\\nposts in terms of both ideology and quality. (A)and(D)show the evolution of the domain ideology and domain quality weighted\\naverages, respectively. The averages are done for each user class (i.e. Conservative, Liberal, Centrist, or without a defined PPA)\\nusing clicks as weights. (B)and(E)show the weighted standard deviations related to the ideology and quality averages above.\\n(C)represents the ideological gap between the conservative and liberal averages and can serve as a proxy for segregation of\\nnews consumption. (F)shows the proportion of clicks directed towards low-quality domains ( <0.6quality score) for each\\nmonth. Noise uncertainty intervals are too small to be visually detected in any of the subfigures. For all shown metrics that\\ninvolve noisy denominators, we find SNR > 100000 . Relevant dates for algorithmic changes are marked with shaded lines.\\nShaded areas indicate the inflection point regions obtained through piece-wise linear regression fitting on the total engagement\\nmetrics (Figure 2).\\nusers toward more extreme-leaning domains, as illustrated\\nin Figure 3A. At the first inflection point in early 2018, a\\nslight decrease in the gap is followed by an increase, which\\nis similarly driven by conservative-leaning users moving ei-\\nther closer to or further from extreme averages.\\nFindings regarding information quality\\nIn Figure 3F we show the change in the proportion of en-\\ngagement directed at low-quality domains (quality score\\n<0.58), which fluctuates around 20% and30%. We see two\\nsudden decreases around inflection point regions, in the first\\ncase after the region (2018-03) and in the second case beforethe region (2020-03). We also see an immediate increase af-\\nter the drops in both cases, even though it is more steep in\\nthe second region, achieving values similar to the start of\\nthe plot, with 30% of the engagement directed towards low-\\nquality sources. Note that the second bounce-back effect co-\\nincides with the Covid-19 pandemic and its respective infor-\\nmation crisis.\\nFigure 3D allows us to examine the interplay of those\\nchanges with the ideological leaning of the users (i.e. we see\\nwho is involved in the changes mentioned above). Average\\nconsumed quality increases for all user classes in the same\\nmonths, where we had a sudden decrease in low-quality con-Figure 4: Changes in engagement (clicks) per quality\\nof time. Values are normalized per month (columns) with\\nSNR > 100000 , showing the probability of a quality bin\\n(bin-width=0.05) receiving a percentage of engagement for\\na specific month. Relevant dates for algorithmic changes\\nare marked with a black line marker. Time intervals marked\\nwith a red marker indicate inflection point regions obtained\\nthrough piece-wise linear regression fitting.\\ntributions (they all consumed low quality, even though they\\ndid in different proportions). However, it is interesting that\\nafter the second inflection point region liberal-leaning stays\\nat higher values, while conservative-leaning goes back to\\nlower quality. The ‚Äùbounce back‚Äù effect of low-quality con-\\ntent seems thus to be driven mainly by conservative-leaning\\nusers. Centrist and ideologically undefined users also show\\na bit of a decrease in their average quality as well as an in-\\ncrease in their weighted standard deviation (see Figure 3E),\\nand thus may also be contributing to the higher engagement\\ntoward low-quality content.\\nIn Figure 4 we show the distribution of clicks per month in\\neach domain quality bin. These bins are the same as the ones\\ndefined in the Aggregate analysis (Figure 1C). We find two\\nclear sections below and above the quality score bin of 0.58,\\nwhich agrees with the tentative threshold for high-quality\\nnews given by (Lin et al. 2023) and the threshold we used\\nin Figure 3F. The set above this bin, which corresponds to\\nmedium and high-quality domains, dominates engagement\\n(most of the clicks fall into that area of the plot), but there\\nstill are some low-quality domains that receive compara-\\nble engagement. Agreeing with the sudden drops of Fig-\\nure 3F, we find that the high engagement in the bin with\\na mean quality of 0.28decreases strongly near the inflection\\npoint region of 2018. Different bins predominate in differ-\\nent months, meaning that the popular low-quality domains\\nare not always the same. For example, the low-quality con-\\ntent that seems to be more prevalent for the steep increase\\nat the beginning of 2020 are the domains centered around amean quality score of 0.38.\\nDiscussion and Conclusions\\nSummary of findings\\nWe conducted an exploratory aggregate and longitudinal\\nanalysis of a large-scale dataset of URLs shared and ac-\\ncessed on Facebook in 2017-2021. We investigated the pat-\\nterns and shifts of news exposure and engagement of Face-\\nbook users over the period. Importantly, the data also al-\\nlowed us to track the quality of the news sources accessed\\nas well as the ideological leaning of both the sources and the\\nusers.\\nThe aggregate analysis shows U-shaped patterns of en-\\ngagement that indicate more extreme users are more likely\\nto engage actively with content, particularly through com-\\nments, likes, and expressions of anger. The correlation\\nbetween ideology strength and participation online that\\ngenerates these U-shaped patterns was already found in\\nKalogeropoulos et al. (2017) for shares and comments, hav-\\ning comments a higher association between the two vari-\\nables. Additionally, we find that news diets of users, and\\nengagement with such news, are biased toward their ideo-\\nlogical leaning. However, these biases are not symmetric.\\nAgreeing with found trends of asymmetric polarization in\\ncongressional media engagement over time, based almost\\nexclusively on a growing engagement extremity by Repub-\\nlicans in Congress (Heseltine 2023), we find a strong bi-\\nmodal pattern for conservative users comprising a moderate\\nand a more extreme set of sources. Liberals, on the other\\nhand, have a shifted unimodal pattern. In terms of qual-\\nity, we also find that conservative users, in the majority, are\\nthe users who consume content from lower-quality domains.\\nThis agrees with previous literature (Guess et al. 2021; Bai-\\nley, Gregersen, and Roesner 2021).\\nThe longitudinal analysis, on the other hand, reveals two\\ndistinct inflection points in engagement trends: an upward\\ntrend commencing in 2018 and a downward trend start-\\ning in 2020. The initial upward trend is accompanied by a\\nmoderate increase in the ideological gap and a rise in the\\nprevalence of lower-quality sources, which is followed by\\na gradual decline until early 2020. Conversely, the second\\nchange in the engagement trend is accompanied by a siz-\\nable increase in the ideological gap and prevalence of lower-\\nquality sources followed by a moderate decrease later in\\n2020. Additionally, the two previous points also exhibit sud-\\nden declines in the proportion of engagement directed to\\nlow-quality sources, which could be attributed to the down-\\ngrading of some of these sources in the posts clicked. Below,\\nwe connect these observations with their potential relation to\\nthe News Feed algorithm changes.\\nContextualization of our results with the Facebook\\nNews Feed algorithm\\nA major update in Facebook‚Äôs News Feed algorithm oc-\\ncurred at the start of 2018 (Mosseri 2018; Hagey and Hor-\\nwitz 2021; Narayanan 2023). This update had the purpose\\nof overturning an observed downward trend in user engage-\\nment. It introduced the metric of Meaningful Social Interac-tions (MSI‚Äôs) and rewarded posts that were likely to trigger\\ncertain types of engagement (especially comments and re-\\nshares, but also reactions such as haha‚Äôs ,wow‚Äôs orangers ).\\nA second update, somewhat less documented, seems to have\\noccurred at the beginning of 2020 (Hagey and Horwitz 2021;\\nNarayanan 2023). The purpose seems to have been not so\\nmuch to boost engagement as to limit the spread of partic-\\nularly toxic, divisive or low-quality posts, through limiting\\nlong chains of reshares. Among other things, it apparently\\ntook away the boost given to reshares, while keeping it high\\nfor comments (Hagey and Horwitz 2021; Narayanan 2023).\\nA natural question that arises is whether the changes in\\nengagement observed in the longitudinal analysis are related\\nto the changes in the News Feed algorithm mentioned above,\\nand, eventually, what the underlying mechanism(s) may be.\\nMatias (2023) and Narayanan (2023) emphasize the impor-\\ntance of asking this kind of question.\\nConsider the 2018 update. It increased the weights at-\\ntached to specific types of engagement (comments, shares,\\nreactions etc.) with the stated objective of increasing over-\\nall engagement (‚Äúmeaningful social interactions‚Äù) (Mosseri\\n2018; Hagey and Horwitz 2021). At the same time, how-\\never, as our longitudinal analysis suggests, the increase in\\nengagement came with increased ideological segregation of\\nnews consumed by liberals and conservatives and a decrease\\nin information quality. According to Simchon, Brady, and\\nVan Bavel (2022), whose results indicate that for political\\ncontexts polarized language is associated with higher en-\\ngagement in social media, this increase would make sense.\\nIn terms of quality, we believe that a possible explanation\\nfor this, based on the aggregate analysis, comes from the\\nfact that more extreme users consume more extreme con-\\ntent, and, particularly for conservative users, also of lower\\nquality. This, combined with the U-shaped patterns of en-\\ngagement, suggests that more extreme users may be more\\nlikely to engage with comments, likes or angers than mod-\\nerate ones. The reasoning is that an algorithmic update that\\nboosts the weights on, say, comments that are strongly U-\\nshaped, makes posts that are likely to generate comments\\nmore visible to users. But because more extreme users are\\nmore likely to comment, and such users tend to consume\\nmore extreme and (in part) also lower quality content, the\\nalgorithmic boost will tend to make more extreme and lower\\nquality content more visible. The latter will in turn be more\\nlikely to be commented on, making it even more visible and\\nso on. Related to these questions, Germano, G ¬¥omez, and\\nSobbrio (2022) study such feedback loops occurring in re-\\nsponse to algorithmic changes and Chavalarias, Bouchaud,\\nand Panahi (2024) also study associated network effects. Go-\\ning through the feedback loop between algorithm and user\\nengagement, applying the above logic to an update such as\\nthe MSI update of 2018, may well trigger increased engage-\\nment but also more polarization (because of the more op-\\npositely extreme content being consumed on both sides) and\\nmore low-quality information or misinformation (because of\\nthe lower quality content being consumed, especially on the\\nconservative side).\\nOn the other hand, the 2020 update seems more difficult to\\nreconcile with the observed patterns, since, unlike the 2018one, it did not boost the weights on engagement but rather re-\\nduced them, especially for the reshares (Hagey and Horwitz\\n2021; Narayanan 2023). We observe reduced engagement\\nbut also an increased ideology gap and a stronger prevalence\\nof low-quality sources. One point that may be worth empha-\\nsizing is that reshares have a visibly less U-shaped pattern\\nthan comments, whose weight remained high in the news\\nfeed ranking algorithm. It is therefore not inconceivable that\\na reduction in the weight of reshares (not really U-shaped)\\nwhile maintaining the weights on comments high (strongly\\nU-shaped) may make comments relatively more important\\nand may therefore boost those undesired effects in terms\\nof consumption of more extreme and low-quality sources.\\nClearly, the precise role of the differences in weights for\\ndifferent engagement patterns and their relation to engage-\\nment as well as segregation of news consumption and dif-\\nfusion of low-quality news and/or misinformation is some-\\nthing that requires access to more detailed information and\\nseems worth exploring further.\\nLimitations\\nIn our analysis, we present the findings derived from inter-\\nactions with news-related URLs on Facebook spanning from\\n2017 to 2021 by US users. However, it is crucial to note that\\nour conclusions are not intended to be universally applicable\\nto all Facebook posts or online behavior in general.\\nIt is also important to acknowledge the potential selection\\nbias inherent in the dataset, which consists only of publicly\\nshared URLs that have garnered significant interaction. This\\nbias may skew towards viral posts and a particular subset\\nof users who engage with such content. Also, since we do\\nnot have access to individual-level data we may be miss-\\ning the effects of small segments of the population that may\\ncontribute to spreading low-quality information through re-\\nshares, which was what motivated the reduction of the re-\\nshare weights.\\nThe fact that we use domain-level ideology and quality\\nscores is another limitation given that it is possible to have\\nheterogeneity of URLs within a single source. In terms of\\nideology, it has been shown that the average partisanship\\nof source audiences is a relative measure of partisanship\\n(Green et al. 2023; Gonz ¬¥alez-Bail ¬¥on et al. 2023; Buntain\\net al. 2023), not an absolute one, particularly for moderate\\nsources (i.e. there are different ways to reach a moderate av-\\nerage, which differ on what is the overlap of the consumed\\nparticular news pieces by each ideological group). If we as-\\nsume that the heterogeneity within domains remains stable\\nover time, at least for the majority of the domains, the effect\\nof these biased scores on our weighted ideology averages\\nshould remain constant over time as well. Considering that\\nthe problem is that we might be categorizing bimodal distri-\\nbutions of URLs as a moderate domain, this may result in a\\nsmaller perceived ideological gap than actually exists, which\\nwould just strengthen our conclusions. Moreover, this rea-\\nsoning likely explains the high weighted standard deviations\\nobserved in Figure 3.\\nAdditionally, three significant confounders‚Äî the\\nCOVID-19 pandemic, beginning in March 2020, the US\\n2018 Midterm elections in November 2018, and the USPresidential elections in November 2020‚Äî could poten-\\ntially influence our results. While we acknowledge their\\npresence, their exact impact remains difficult to quantify\\nwithin the scope of this study.\\nEthical considerations\\nAccess to the dataset is governed by an application pro-\\ncess and formal agreement, facilitated through Facebook‚Äôs\\nservers to ensure user privacy. While this protects user\\ndata, it poses challenges for replication by other researchers\\nand even ourselves, particularly if access is revoked or the\\ndataset undergoes changes.\\nThe dataset‚Äôs limitations prevent us from disentangling\\nthe influence of algorithmic recommendations in user news\\nfeeds from the impact of their social network, considering\\nboth friend connections and pages. Friendship networks, and\\ntheir degrees of homophily, play a role in the spreading dy-\\nnamics of reshares in contrast to comments or likes. This\\ncomplexity adds a layer of uncertainty to our analysis.\\nFinally, the opacity surrounding platform features, such\\nas alterations in the weighting of engagement metrics in the\\nrecommendation pipeline and their role in the overall News\\nFeed algorithm, poses a significant challenge. This limits\\nour study to a descriptive and exploratory analysis, leav-\\ning many questions regarding the platform‚Äôs mechanisms as\\nopen questions or mild hypotheses.\\nWe explicitly discourage the use of our research to draw\\npotentially negative conclusions about subsets of users re-\\ngarding their ideology.\\nFuture work and directions\\nThere are many more questions that this particular dataset\\nalone cannot answer, but would be of great interest to pur-\\nsue (e.g. specific information on the timing and nature of\\nkey changes in the News Feed algorithm). We hope that our\\ninitial longitudinal analysis provides a foundation for con-\\ntinued investigations.\\nIt is not within the scope of this work to disentangle the\\ncorrelation of the studied behaviors with user demographics\\nsuch as age or gender, which we believe could be a highly\\ninteresting topic for future research. Additionally, our study\\nis focused solely on US data due to availability constraints.\\nExpanding this analysis to other countries with different po-\\nlitical systems and cultural backgrounds would be a valuable\\ndirection for further research.\\nReferences\\nAllen, J.; Watts, D. J.; and Rand, D. G. 2024. Quantifying\\nthe impact of misinformation and vaccine-skeptical content\\non Facebook. Science , 384(6699): eadk3451.\\nBailey, A.; Gregersen, T.; and Roesner, F. 2021. Interac-\\ntions with potential misdisinformation urls among us users\\non facebook, 2017-2019. In Proceedings of the ACM SIG-\\nCOMM 2021 Workshop on Free and Open Communications\\non the Internet , 16‚Äì26.\\nBakshy, E.; Messing, S.; and Adamic, L. A. 2015. Expo-\\nsure to ideologically diverse news and opinion on Facebook.\\nScience , 348(6239): 1130‚Äì1132.Bandy, J.; and Diakopoulos, N. 2023. Facebook‚Äôs News\\nFeed Algorithm and the 2020 US Election. Social Media+\\nSociety , 9(3): 20563051231196898.\\nBarber ¬¥a, P. 2015. Birds of the same feather tweet together:\\nBayesian ideal point estimation using Twitter data. Political\\nanalysis , 23(1): 76‚Äì91.\\nBarnab `o, G.; Siciliano, F.; Castillo, C.; Leonardi, S.; Nakov,\\nP.; Da San Martino, G.; and Silvestri, F. 2022. FbMulti-\\nLingMisinfo: Challenging large-scale multilingual bench-\\nmark for misinformation detection. In 2022 International\\nJoint Conference on Neural Networks (IJCNN) , 1‚Äì8. IEEE.\\nBarnab `o, G.; Siciliano, F.; Castillo, C.; Leonardi, S.; Nakov,\\nP.; Da San Martino, G.; and Silvestri, F. 2023. Deep active\\nlearning for misinformation detection using geometric deep\\nlearning. Online Social Networks and Media , 33: 100244.\\nBudak, C.; Goel, S.; and Rao, J. M. 2016. Fair and balanced?\\nQuantifying media bias through crowdsourced content anal-\\nysis. Public Opinion Quarterly , 80(S1): 250‚Äì271.\\nBuntain, C.; Bonneau, R.; Nagler, J.; and Tucker, J. A. 2023.\\nMeasuring the Ideology of Audiences for Web Links and\\nDomains Using Differentially Private Engagement Data. In\\nProceedings of the International AAAI Conference on Web\\nand Social Media , volume 17, 72‚Äì83.\\nChavalarias, D.; Bouchaud, P.; and Panahi, M. 2024. Can a\\nSingle Line of Code Change Society? Optimizing Engage-\\nment in Recommender Systems Necessarily Entails Sys-\\ntemic Risks for Global Information Flows, Opinion Dynam-\\nics and Social Structures. Journal of Artificial Societies and\\nSocial Simulation , 27(1).\\nEady, G.; Bonneau, R.; Tucker, J. A.; and Nagler, J. 2020.\\nNews sharing on social media: Mapping the ideology of\\nnews media content, citizens, and politicians.\\nEvans, G.; and King, G. 2021. Statistically valid inferences\\nfrom differentially private data releases, ii: Extensions to\\nnonlinear transformations. Political Analysis , 26(B2): 1161‚Äì\\n1165.\\nEvans, G.; and King, G. 2023. Statistically valid inferences\\nfrom differentially private data releases, with application to\\nthe facebook urls dataset. Political Analysis , 31(1): 1‚Äì21.\\nGarcia, D. 2023. Influence of Facebook algorithms on polit-\\nical polarization tested. Nature , 620(7972): 39‚Äì41.\\nGermano, F.; G ¬¥omez, V .; and Sobbrio, F. 2022. Crowd-\\ning out the truth? A simple model of misinformation, po-\\nlarization and meaningful social interactions. arXiv preprint\\narXiv:2210.02248 .\\nGonz ¬¥alez-Bail ¬¥on, S.; d‚ÄôAndrea, V .; Freelon, D.; and\\nDe Domenico, M. 2022. The advantage of the right in social\\nmedia news sharing. PNAS nexus , 1(3): pgac137.\\nGonz ¬¥alez-Bail ¬¥on, S.; Lazer, D.; Barber ¬¥a, P.; Zhang, M.;\\nAllcott, H.; Brown, T.; Crespo-Tenorio, A.; Freelon, D.;\\nGentzkow, M.; Guess, A. M.; et al. 2023. Asymmetric ide-\\nological segregation in exposure to political news on Face-\\nbook. Science , 381(6656): 392‚Äì398.\\nGreen, J.; McCabe, S.; Shugars, S.; Chwe, H.; Horgan, L.;\\nCao, S.; and Lazer, D. 2023. Curation bubbles. OSF\\nPreprints. URL: https://osf. io/vbwer .Guess, A.; Aslett, K.; Tucker, J.; Bonneau, R.; and Nagler,\\nJ. 2021. Cracking open the news feed: Exploring what us\\nFacebook users see and share with large-scale platform data.\\nJournal of Quantitative Description: Digital Media , 1.\\nGuess, A. M.; Malhotra, N.; Pan, J.; Barber ¬¥a, P.; Allcott, H.;\\nBrown, T.; Crespo-Tenorio, A.; Dimmery, D.; Freelon, D.;\\nGentzkow, M.; et al. 2023a. How do social media feed algo-\\nrithms affect attitudes and behavior in an election campaign?\\nScience , 381(6656): 398‚Äì404.\\nGuess, A. M.; Malhotra, N.; Pan, J.; Barber ¬¥a, P.; Allcott, H.;\\nBrown, T.; Crespo-Tenorio, A.; Dimmery, D.; Freelon, D.;\\nGentzkow, M.; et al. 2023b. Reshares on social media am-\\nplify political news but do not detectably affect beliefs or\\nopinions. Science , 381(6656): 404‚Äì408.\\nHagey, K.; and Horwitz, J. 2021. Facebook Tried to Make\\nIts Platform a Healthier Place. It Got Angrier Instead. The\\nWall Street Journal .\\nHeseltine, M. 2023. Asymmetric Polarization in Online Me-\\ndia Engagement in the United States Congress. The Interna-\\ntional Journal of Press/Politics , 19401612231211800.\\nHeuer, H.; and Glassman, E. L. 2022. A Comparative Evalu-\\nation of Interventions Against Misinformation: Augmenting\\nthe WHO Checklist. In Proceedings of the 2022 CHI Con-\\nference on Human Factors in Computing Systems , 1‚Äì21.\\nHuszar, F.; Ktena, S. I.; O‚ÄôBrien, C.; Belli, L.; Schlaikjer, A.;\\nand Hardt, M. 2022. Algorithmic amplification of politics on\\nTwitter. Proceedings of the National Academy of Sciences ,\\n119(1): e2025334119.\\nJekel, C. F.; and Venter, G. 2019. pwlf: A Python Library for\\nFitting 1D Continuous Piecewise Linear Functions .\\nKalogeropoulos, A.; Negredo, S.; Picone, I.; and Nielsen,\\nR. K. 2017. Who shares and comments on news?:\\nA cross-national comparative analysis of online and so-\\ncial media participation. Social media+ society , 3(4):\\n2056305117735754.\\nLin, H.; Lasser, J.; Lewandowsky, S.; Cole, R.; Gully, A.;\\nRand, D. G.; and Pennycook, G. 2023. High level of corre-\\nspondence across different news domain quality rating sets.\\nPNAS nexus , 2(9): pgad286.\\nMatias, J. N. 2023. Humans and algorithms work to-\\ngether‚Äîso study them together. Nature , 617(7960): 248‚Äì\\n251.\\nMessing, S.; DeGregorio, C.; Hillenbrand, B.; King, G.; Ma-\\nhanti, S.; Mukerjee, Z.; Nayak, C.; Persily, N.; State, B.; and\\nWilkins, A. 2020. Facebook Privacy-Protected Full URLs\\nData Set.\\nMosseri, A. 2018. Bringing People Closer Together. Meta .\\nMuggeo, V . M. 2003. Estimating regression models with\\nunknown break-points. Statistics in medicine , 22(19): 3055‚Äì\\n3071.\\nNarayanan, A. 2023. Understanding social media recom-\\nmendation algorithms.\\nNyhan, B.; Settle, J.; Thorson, E.; Wojcieszak, M.; Barber ¬¥a,\\nP.; Chen, A. Y .; Allcott, H.; Brown, T.; Crespo-Tenorio, A.;\\nDimmery, D.; et al. 2023. Like-minded sources on Facebook\\nare prevalent but not polarizing. Nature , 1‚Äì8.Robertson, R. E.; Green, J.; Ruck, D. J.; Ognyanova, K.;\\nWilson, C.; and Lazer, D. 2023. Users choose to engage\\nwith more partisan news than they are exposed to on Google\\nSearch. Nature , 1‚Äì7.\\nRobertson, R. E.; Jiang, S.; Joseph, K.; Friedland, L.; Lazer,\\nD.; and Wilson, C. 2018. Auditing partisan audience bias\\nwithin google search. Proceedings of the ACM on Human-\\nComputer Interaction , 2(CSCW): 1‚Äì22.\\nSimchon, A.; Brady, W. J.; and Van Bavel, J. J. 2022. Troll\\nand divide: the language of online polarization. PNAS nexus ,\\n1(1): pgac019.\\nThero, H.; and Vincent, E. M. 2022. Investigating Face-\\nbook‚Äôs interventions against accounts that repeatedly share\\nmisinformation. Information Processing & Management ,\\n59(2): 102804.\\nVincent, E. M.; Thero, H.; and Shabayek, S. 2022. Mea-\\nsuring the effect of Facebook‚Äôs downranking interventions\\nagainst groups and websites that repeatedly share misinfor-\\nmation. Harvard Kennedy School Misinformation Review .\\nAcknowledgments\\nE. F. acknowledges funding from the Maria de Maeztu\\nUnits of Excellence Programme CEX2021-001195-M,\\nfunded by MICIU/AEI. F. G. acknowledges fund-\\ning from Grant PID2020-115044GB-I00/MICIU/AEI/\\n10.13039/501100011033 and from the Spanish Agencia\\nEstatal de Investigaci ¬¥on (AEI), through the Severo Ochoa\\nProgramme for Centres of Excellence in R&D (Barcelona\\nSchool of Economics CEX2019-000915-S).\\nAppendix\\nA1. Domain selection\\nWhen we plot the domains used in this study as news sources\\nby ideology and quality score (see Figure A1), we find a\\nweak negative correlation between the two. We can also ob-\\nserve certain clustering structures, especially two clusters at\\nhigh and medium-high quality located near the center ideol-\\nogy and a low-quality cluster at the conservative extreme.\\nA2. Uncertainty intervals for engagement counts\\nTo obtain uncertainty intervals for the sums of engagement\\nwe follow that the aggregation of two normally distributed\\nvariables such as X‚àºN(0, S2\\nx)andY‚àºN(0, S2\\ny)will\\nresult in a variable X+Ywith a standard deviation of q\\nS2x+S2y. Since the noise in the dataset is injected by\\nbucket element (defined by month, age, gender, PPA, and\\nengagement type), besides accounting for the sum over each\\nunique URL row, we also need to account for the demo-\\ngraphic and temporal rows over which we are aggregating:\\nmonth, gender, and age, with 48,3, and 7buckets respec-\\ntively. The final expression, then, will be (for a 95% confi-\\ndence interval):\\nCI= 1.96sX\\neS2e, (4)where, given œÉ2\\nefor each engagement type,\\nS2\\ne= 48¬∑3¬∑7¬∑NURLs¬∑œÉ2\\ne. (5)\\nAll values œÉecan take are stated in Messing et al. (2020).\\nFor example, views, clicks, and shares have values of 2228 ,\\n40, and 14, respectively.\\nA3. Confidence intervals of noisy ratios\\nFor URL i, the estimated volume of engagement counts\\n(shares, clicks, etc) given to this URL by a sub-set of users\\nof category T(be it an ideologically distinct sub-set or other\\nselections based on gender, age...) is given by\\nCi\\ne,T=Ci\\ne,T, TRUE+œµ, (6)\\nwhere Ci\\ne,T, TRUE is the noiseless measure and the noise in-\\njected is œµ‚àº N(0, œÉ2\\ne). We are given the variance œÉ2\\ne.\\nIf we select, for example, the subset of conservative-\\nleaning users, C, in one specific month, with an ideology\\nweighted average given by\\n¬µe,C=P\\niyiCi\\ne,CP\\niCi\\ne,C, (7)\\nthen the 95% uncertainty interval for the numerator of the\\nfirst term is\\nCIC,num= 1.96s\\n1¬∑7¬∑3¬∑2¬∑X\\ntœÉ2e, (8)\\nFigure A1: Selected domains for this study by Ideology\\nand Quality scores. Positive values of ideology denote a\\nleaning toward conservative ideology, while negative val-\\nues correspond to liberal leanings. Quality scores are higher\\nwhen domains have higher quality. The two dimensions are\\nnegatively correlated ( r=‚àí0.35, p= 1.81e‚àí37).\\nFigure A2: Distribution of engagement counts across user\\npolitical page affinity for reactions. Negative/positive val-\\nues correspond to more liberal/conservative leaning. Confi-\\ndence intervals (C.I.) are computed based on the number of\\nURLs and the noise injected for each metric.\\nwhere 1accounts for one month, 7accounts for the aggrega-\\ntion of bins of all age groups, 3for the aggregation of gen-\\nder groups, and 2for the aggregation of the two ideological\\ngroups that map to class C.\\nConsequently, the uncertainty interval for the denomina-\\ntor is\\nCIC,den= 1.96s\\n1¬∑7¬∑3¬∑2¬∑X\\niyiœÉ2e, (9)\\nbecause ytCt\\ne,C=ytCt\\ne,C,TRUE+ytœµandytœµ‚àº N(0, ytœÉ2\\ne).\\nThen, by having a ratio of these two intervals we can gen-\\nerate an interval considering the minimum and maximum\\npossible values of our metric. The maximum value of the\\nratio will be when we consider the numerator‚Äôs maximum\\nvalue and the denominator‚Äôs minimum value, and the mini-\\nmum will be the opposite case:\\n¬µe,Cmax=P\\ntytCt\\nC,e+CIC,numP\\ntCt\\nC,e‚àíCIC,den(10)\\n¬µe,Cmin=P\\ntytCt\\nC,e‚àíCIC,numP\\ntCt\\nC,e+CIC,den. (11)\\nA4. Details of engagement metrics\\nWe show the distribution of the reactions of loves, sorrys,\\nhahas andwows in Figure A2, similarly to the principal en-\\ngagement metrics shown in Figure 1. Only hahas reactions\\nexhibit a strong U-shape distribution.\\nA5. Breakpoint detection\\nTo automatically detect the breakpoints or inflection points\\nin the engagement timelines, we fit continuous piece-wiselinear functions to our data with the implementation of\\n(Jekel and Venter 2019), which combines it with global op-\\ntimization to find the best locations for the inflection points\\nwith a given number of segments to fit.\\nOur approach is the following: we fit the data of each\\nmetric for a range of number of segments and compute the\\nR-squared value of each fit. The quality of the fit increases\\nwith the number of segments since more segments will al-\\nways allow a better fit, therefore using the elbow method\\nwe heuristically choose a number that explains most of the\\nvariation without over-fitting. Then, rounding the inflection\\npoints found in this continuous space to the closest month\\nfor a discrete interval while considering the standard devia-\\ntion of the parameters, we determine which are the ‚Äùinflec-\\ntion point‚Äù periods.\\nA6. Example of change in clicks for known\\ndomains and low-quality domains\\nIn Table A1 we list 10known and popular domains as well as\\n20relevant domains from the two most salient low-quality\\nbands in Figure 4. We show their ideology score and their\\ntotal amount of received clicks in the months before and af-\\nter our detected inflection point regions. When considering\\nlow-quality domains, there is a visual decrease in clicks after\\nthe first region and an increase after the second region.Total count of clicks in the interval\\nDomainIdeology\\nscoreQuality\\nscoreQuality\\nbin mean08/2017-10/2017 CI (1) 03/2018-05/2018 CI (2) 12/2019-02/2020 CI (3) 07/2020-09/2020 CI (4)\\n0 reuters.com -0.13 1.00 0.98 1.4e+07 4.1e+05 2.2e+07 4.7e+05 3.2e+07 4.5e+05 4.7e+07 4.9e+05\\n1 wtop.com -0.08 0.90 0.93 9.3e+05 6.9e+04 7.7e+05 7.2e+04 3.8e+06 1.4e+05 1.4e+06 1.0e+05\\n2 nytimes.com -0.26 0.86 0.88 4.9e+08 8.3e+05 5.0e+08 9.2e+05 3.0e+08 9.4e+05 3.8e+08 9.3e+05\\n3 c-span.org -0.18 0.88 0.88 6.4e+05 7.8e+04 3.9e+05 7.5e+04 1.5e+06 1.3e+05 1.9e+06 1.1e+05\\n4 kqed.org -0.61 0.85 0.88 9.5e+06 1.2e+05 2.1e+06 1.4e+05 6.3e+06 1.6e+05 4.6e+06 1.3e+05\\n5 wsj.com 0.01 0.80 0.83 4.9e+07 3.6e+05 4.4e+07 3.9e+05 8.0e+07 4.3e+05 1.1e+08 4.3e+05\\n6 cnn.com -0.12 0.66 0.68 3.4e+08 7.6e+05 4.1e+08 9.0e+05 5.3e+08 1.1e+06 7.0e+08 1.1e+06\\n7 foxnews.com 0.61 0.53 0.58 4.6e+08 6.5e+05 4.2e+08 7.8e+05 4.8e+08 1.0e+06 6.4e+08 1.0e+06\\n8 huffpost.com -0.31 0.57 0.58 1.5e+05 1.3e+05 1.4e+05 1.1e+04 3.1e+08 6.7e+05 3.1e+08 6.4e+05\\n9 breitbart.com 0.74 0.30 0.33 2.4e+08 6.3e+05 1.3e+08 7.3e+05 1.4e+08 7.9e+05 2.8e+08 7.9e+05\\n10 washingtontimes.com 0.63 0.35 0.38 1.2e+07 2.1e+05 1.1e+07 2.6e+05 1.1e+08 4.4e+05 9.1e+07 4.1e+05\\n11 twitchy.com 0.88 0.35 0.38 2.0e+07 1.7e+05 6.6e+06 1.9e+05 7.4e+06 1.8e+05 2.4e+07 1.9e+05\\n12 theblaze.com 0.79 0.32 0.38 6.9e+07 2.9e+05 5.1e+07 3.0e+05 1.6e+08 4.4e+05 6.4e+08 4.7e+05\\n13 shareblue.com -0.79 0.34 0.38 2.6e+07 2.3e+05 9.2e+06 3.1e+05 5.5e+04 3.1e+05 -2.9e+04 1.7e+05\\n14 politicususa.com -0.71 0.36 0.38 2.5e+07 3.4e+05 4.4e+06 3.7e+05 1.1e+07 3.6e+05 2.3e+07 3.5e+05\\n15 palmerreport.com -0.73 0.36 0.38 2.9e+07 3.1e+05 8.3e+06 3.6e+05 4.8e+06 3.0e+05 1.5e+07 2.8e+05\\n16 lifezette.com 0.85 0.35 0.38 2.0e+07 2.3e+05 5.8e+06 2.3e+05 1.2e+07 2.5e+05 2.2e+07 2.6e+05\\n17 ladbible.com 0.22 0.36 0.38 3.1e+07 1.3e+05 1.2e+07 1.7e+05 7.1e+07 3.0e+05 7.9e+07 2.9e+05\\n18 dailywire.com 0.85 0.36 0.38 6.3e+08 4.1e+05 5.0e+08 5.6e+05 7.4e+08 6.2e+05 1.6e+09 6.4e+05\\n19 cbn.com 0.73 0.37 0.38 1.3e+07 1.9e+05 8.0e+06 2.3e+05 2.4e+07 3.3e+05 3.3e+07 2.9e+05\\n20 littlethings.com 0.19 0.25 0.28 1.5e+09 4.9e+05 3.1e+07 4.2e+05 5.4e+07 1.8e+05 1.9e+07 1.3e+05\\n21 bipartisanreport.com -0.75 0.23 0.28 5.9e+07 4.0e+05 1.3e+07 3.6e+05 1.3e+07 3.1e+05 2.2e+07 2.7e+05\\n22 pjmedia.com 0.82 0.23 0.28 4.0e+06 1.4e+05 4.1e+06 1.6e+05 5.9e+07 3.3e+05 1.3e+08 3.8e+05\\n23 higherperspectives.com -0.35 0.24 0.28 6.6e+06 7.1e+04 6.0e+04 4.5e+04 1.2e+08 1.4e+05 6.1e+06 1.5e+05\\n24 hannity.com 0.91 0.26 0.28 4.4e+07 2.4e+05 2.6e+07 2.9e+05 8.3e+06 2.7e+05 4.2e+07 2.5e+05\\n25 conservativefighters.com 0.85 0.24 0.28 1.3e+08 4.0e+05 2.9e+05 3.2e+05 0.0e+00 0.0e+00 0.0e+00 0.0e+00\\n26 theepochtimes.com 0.01 0.26 0.28 3.9e+08 2.5e+05 3.6e+08 4.0e+05 1.1e+08 5.8e+05 4.5e+07 5.1e+05\\n27 thefederalistpapers.org 0.90 0.25 0.28 1.3e+08 3.3e+05 5.1e+05 2.8e+05 1.8e+07 3.3e+05 4.6e+07 2.9e+05\\n28 collegehumor.com -0.16 0.26 0.28 7.8e+07 1.9e+05 5.9e+07 1.7e+05 -2.6e+03 7.6e+03 0.0e+00 0.0e+00\\n29 awm.com 0.23 0.26 0.28 4.0e+08 3.6e+05 4.8e+07 3.1e+05 5.0e+07 2.6e+05 2.3e+07 2.0e+05\\nTable A1: Example of domains and the changes in their engagement surrounding the two inflection point regions. We\\nshow three groups of 10news-related domains. One consists of known domains in different quality levels, and the other two are\\nthe most clicked domains in two relevant low-quality bins of Figure 4. We show their ideological leaning, their quality score\\n(as well as the quality mean of the bin to which they belong), and the absolute number of clicks in the three months before\\nand after each inflection point region. Colors in the Ideology score column indicate conservative (red) or liberal (blue) leaning.\\nColors in the click count after the inflection point regions denote an increase (green) or decrease (pink) in the number of clicks.',\n",
       " '2024 2nd International Conference on Information and Communication Technology (ICICT)\\nOctober 21-22, Dhaka, Bangladesh\\nEnglish offensive text detection using CNN based\\nBi-GRU model\\nTonmoy Roy1\\nData Analytics & Information Systems\\nUtah State University\\nUtah, United States\\ntonmoy.roy@usu.eduMd Robiul Islam2\\nComputer Science\\nWilliam & Mary\\nVirginia, United States\\nrobiul.cse.uu@gmail.comAsif Ahammad Miazee3\\nComputer Science\\nMaharishi International University\\nIowa, United States\\nasifahammad7@gmail.com\\nAnika Antara4\\nElectrical and Electronics Engineering\\nBrac University\\nDhaka, Bangladesh\\nanikaantara1000@gmail.comAl Amin5\\nComputer Science\\nUttara University\\nDhaka, Bangladesh\\nalaminbhuyan321@gmail.comSunjim Hossain6\\nComputer Science\\nNorthern University\\nDhaka, Bangladesh\\nhossainsunjim@gmail.com\\nAbstract ‚ÄîOver the years, the number of users of social\\nmedia has increased drastically. People frequently share their\\nthoughts through social platforms, and this leads to an increase\\nin hate content. In this virtual community, individuals share\\ntheir views, express their feelings, and post photos, videos, blogs,\\nand more. Social networking sites like Facebook and Twitter\\nprovide platforms to share vast amounts of content with a single\\nclick. However, these platforms do not impose restrictions on\\nthe uploaded content, which may include abusive language and\\nexplicit images unsuitable for social media. To resolve this issue,\\na new idea must be implemented to divide the inappropriate\\ncontent. Numerous studies have been done to automate the\\nprocess. In this paper, we propose a new Bi-GRU-CNN model to\\nclassify whether the text is offensive or not. The combination of\\nthe Bi-GRU and CNN models outperforms the existing models.\\nIndex Terms ‚Äîhate content, social media, CNN, Bi-GRU\\nI. I NTRODUCTION\\nHate speech is a distinct form of language that involves\\nabusive behaviour. The targets of this phenomenon are specifi-\\ncally selected based on their personal qualities or demographic\\nbackground, including race, ethnicity, religion, colour, sexual\\norientation, or other comparable criteria [1]. A multitude\\nof academics are diligently focused on addressing the issue\\nof hate speech detection using natural language processing\\ntechniques. They are developing practical frameworks and\\ncreating automatic classifiers that rely on supervised machine\\nlearning models [2]. Sharing inappropriate content on social\\nmedia platforms like Twitter and Facebook has become in-\\ncreasingly effortless, often targeting specific individuals or\\ngroups. Additionally, toxic language can manifest in various\\nforms, including cyberbullying, which has played a key role\\nin contributing to suicide. [3]. According to the United Na-\\ntions strategy and plan of action on hate speech, there is\\nno internationally agreed-upon legal definition, However, it\\ninvolves incitement, which means intentionally encouraging\\ndiscrimination, hostility, and violence [4].Similarly, offensive speech refers to writing that includes\\nabusive slurs or degrading terms [5], which are often mistaken\\nfor hate speech in various situations. Offensive language and\\nhate speech detection are two specialised areas of study\\nwithin the subject of natural language processing. The primary\\nobstacle is in the fact that the majority of unsuitable content\\nfound online is presented in the form of natural language text.\\nConsequently, it is imperative to develop efficient tools capable\\nof extracting and analysing this content from unorganised\\ntextual data.\\nThese technologies often utilise methods that are based on\\nnatural language processing (NLP), retrieval of data, machine\\nlearning, and deep learning. Research conducted by [6] focuses\\non safeguarding children from inappropriate content in mobile\\napplications. The study suggests techniques for parents to\\nassess the maturity level of smartphone apps, allowing them\\nto select apps that are appropriate for their children‚Äôs ages.\\nUnfortunately, it is not practical to manually filter out harmful\\ncontent on a large scale, and manually identifying or removing\\nanything from the internet is a laborious undertaking. This\\nserves as a driving force for researchers to develop automated\\ntechniques that can identify offensive information on social\\nmedia platforms.\\nThis study presents comprehensive experiments aimed at re-\\nsolving the problem of categorising improper content through\\nthe utilisation of machine learning and neural network models.\\nThe methodology we employ involves optimising hyperparam-\\neters and utilising word embedding features on a dataset in\\nthe English language. Our main contribution in this paper as\\nfollows:\\n‚Ä¢We proposed a fine-tuned 1D Convolutional Neural Net-\\nwork (CNN) with Bi-GRU based offensive text detection\\nmodel, which outperforms other benchmark models.\\n‚Ä¢The dataset contains more than 31 thousands tweets\\nwhich is a big data to do work.\\n979-8-3315-0822-7/24/$31.00 ¬©2024 IEEEarXiv:2409.15652v3  [cs.CL]  18 Oct 2024The paper is organized as follows: In section II, we review\\nthe relevant literature on classifying offensive speech. Section\\nIII details our proposed methodology. Section IV covers our\\nexperiments and results. In Section V , we discuss the chal-\\nlenges encountered during implementation. Finally, in section\\nVI, we conclude with a summary and outline future work.\\nII. R ELATED WORKS\\nSocial life has become part of life nowadays. According\\nto the source, on average, a user spends 3 hours and 15\\nminutes on their phone each day, and that number increases\\nif there is a holiday [7]. Individuals check their phones\\n58 times within 24 hours. Over the past few years, there\\nhas been a rapid development of social media, resulting in\\nboth benefits and drawbacks in its utilisation. Hate speech\\ncommonly targets specific races, religions, sexual orientations,\\ngenders, and castes, among other categories. This problem\\nhas prompted numerous academics to investigate an approach\\ncapable of identifying hate speech that is targeted towards\\nspecific individuals or groups [8].\\nMachine learning techniques for natural language process-\\ning (NLP) have traditionally relied on shallow models like\\nSupport Vector Machines (SVM) and Logistic Regression\\n(LR). These models are trained using features that are both\\nhigh-dimensional and sparse. Most techniques primarily con-\\ncentrate on extracting textual aspects. Some scholars have\\nemployed lexical features, such as dictionaries [9] and bag-\\nof-words [10]. Kumari et. al. propose a hybrid model for\\nidentifying aggressive posts that include both images and text\\non social media platforms [11]. In contrast, Kovacs et. al.\\nemploy various machine learning techniques and deep learning\\nmodels to automatically detect speech that is both hateful\\nand offensive [12]. Although machine learning (ML) has en-\\ncountered problems, there have been significant advancements\\nin developing ML-based systems for automatic detection,\\nwhich have yielded promising outcomes. Some noteworthy\\nexamples of classifiers are the Naive Bayes (NB) [10], Logistic\\nRegression (LR) [13], and Support Vector Machines (SVM)\\n[13]. Offensive speech detection is not only used in English; it\\nis also used in Hindi as well [14]. Also in Spanish and Italian\\n[15]. Many researchers work on several languages to detect\\noffensive text [16]. Corazza et al. [17] employ datasets in three\\ndistinct languages (English, Italian, and German) and train var-\\nious models including LSTMs, GRUs, Bidirectional LSTMs,\\nand others. Huang et al. [18] created a multilingual Twitter\\nhate speech dataset by combining data from 5 languages.\\nThey also added demographic information to investigate the\\npresence of demographic bias in hate speech classification.\\nAluru et al. [19] employed datasets from 8 different languages\\nand produced embeddings using LASER [20] and MUSE [21].\\nThese embeddings were inputted into various architectures,\\nsuch as CNNs, GRUs, and different Transformer models.\\nWhile the study achieved satisfactory performance across these\\nlanguages, it had two main shortcomings. Firstly, the claim that\\nthe models are generalizable is questionable, as they did not\\nleverage datasets from multiple other languages and lackedfine-tuning of parameters for better generalization. Conse-\\nquently, these models are unlikely to perform well outside\\nof the 8 languages studied. Secondly, the research focused\\nprimarily on models suited for low-resource settings without\\naddressing their performance in resource-abundant environ-\\nments. By aggregating datasets from 11 different languages,\\nhis study achieved superior performance [22].\\nIII. M ETHODOLOGY\\nIn this section, we present our proposed methodology\\nfor offensive text classification. Our proposed Bi-GRU and\\nCNN models take input text and output the probability of\\nit being in an inappropriate class. Our model consists of\\nnine layers (a) Input Layer, (b) Embedding Layer, (c) Con-\\nvolutional Layer(Conv1D), (d) MaxPooling Layer, (e) Bi-\\ndirectional GRU 1(Bi-GRU), (f) Bi-directional GRU 2(Bi-\\nGRU), (g) Dense Layer, (h) Dropout Layer, (i) Dense Layer.\\nA. Dataset Descriptions\\nIn this subsection, we present our dataset that we have used\\nin our experiments. The dataset contains 31,962 tweets. This\\ndataset has been split into a training set and a test set. The\\nratio is 80:20 for training and testing respectively.\\nTABLE 1 Dataset\\nClasses Total Tweet\\nOffensive 29720\\nNot offensive 2242\\nB. Pre-processing\\nIn order to enhance the efficiency of our approach, it is\\nimportant to carry out pre-processing steps to cleanse our\\ntextual data. Initially, the tweet underwent a process where\\nall numerical values, punctuation marks, URLs (starting with\\nhttp:// or www.), and symbols (like emojis, hashtags, and\\nmentions) were eliminated. This was done because these\\nelements do not contribute to the sentiment-related content of\\nthe tweet. The tweets were first broken down into individual\\nwords or phrases, a process called tokenization. This was\\ndone using a tool from the NLTK library. Then, all common\\nwords with little meaning on their own (like ‚Äùthe‚Äù, ‚Äùa‚Äù, ‚Äùis‚Äù)\\nwere removed from the tokens. These common words are also\\nprovided by the NLTK library.\\nC. Bi-directional GRU (Bi-GRU)\\nRegular GRU models only consider information from pre-\\nvious words in a sequence. But to truly grasp the meaning\\nof a word, it‚Äôs important to also understand what comes after\\nit. That‚Äôs where Bidirectional GRUs (BiGRUs) come in. Our\\nsystem uses BiGRUs, which are essentially two GRUs working\\ntogether. One reads the text forward, the other backward. This\\nallows the BiGRU to capture important details from the text\\nand analyze each word in the context of both its past and\\nfuture neighbors. This gives BiGRUs an edge over traditionalFigure 1: Proposed methodology\\nmachine learning models when it comes to understanding the\\nnuances of language and detecting hate speech.\\nD. Experimental setup\\nFor these studies, we utilise Keras and Tensorflow as the\\nunderlying frameworks. We utilise Google Colab to execute\\nthe tests with the assistance of the Graphical Processing Unit\\n(GPU). Regarding the training process, we employ a total of\\n100 epochs. Several traditional machine learning techniques\\nwere employed, including Naive Bayes, Support Vector Ma-\\nchine, Logistic Regression, Random Forest, and Adaboost.\\nFurthermore, alongside these models, various techniques for\\nword representation were employed, including count vectors\\nas features and term frequency inverse document frequency\\n(TF-IDF).\\nIV. E XPERIMENTATIONS AND RESULTS\\nIn this section, we discuss the model‚Äôs performance. We\\ndiscuss about accuracy, precision and recall.\\nA. Results\\nEvery model undergoes training using the training dataset,\\nand its performance is assessed using standard classification\\nmetrics: Accuracy (Acc), F1-score (F1), Recall (R), and Pre-\\ncision (P). In order to improve the outcomes, different hyper-\\nparameters were experimented with and integrated [23]. This\\npaper provides a comprehensive overview of all the findings,\\nwith a particular emphasis on the models that performed the\\nbest. Multiple preliminary evaluations were conducted prior to\\nsubmitting the final results.\\nFig. 3 shows the training and validation accuracy of the\\nproposed model. Fig. 4 depicts the suggested model loss graphfor 100 epochs on training and validation stage. Beside that\\nFig. 5 and Fig. 6 represents the training and validation recall\\nand AUC graph respectively. Table 2 shows the evaluation\\nresults of different machine learning models and Table 3 shows\\nthe comparative analysis of some other existing research.\\nTABLE 2 Model Evaluation table\\nNo. Algorithm Accuracy Precision Rcall F1-Score\\n1 Logistic Regres-\\nsion96.86 86.50 51.27 64.21\\n2 RandomForest\\nClassifier96.56 84.33 54.26 67.56\\n3 MultinomialNB 95.97 95.65 95.97 95.47\\n4 KNeighbors\\nClassifier95.97 81.41 81.41 81.41\\n5 Linear\\nSupportVec-\\ntorClassification94.90 98.46 82.05 89.51\\nTABLE 3 Research comparison with others\\nNo. Authors Algorithms FE method Accuracy\\n1 Vashistha\\net. al.\\n[14]LR Word embedding 76.90%\\n2 Aluru et.\\nal. [19]LR MUSE and LASER 76.90%\\n3 Deshpande\\net. al.\\n[16]TL mBERT 76.90%\\n4 Proposed\\nmodelBi-GRU and CNN CountVectorizer 96.97%Figure 2: Model Flow Chart\\nV. C ONCLUSION AND FUTURE WORK\\nThis research describes a system that combines Convolu-\\ntional Neural Networks (CNN) and Bidirectional Gated Recur-\\nrent Units (Bi-GRU). We were motivated to create contextual\\nembeddings by the use of social networks, namely by utilising\\na Twitter dataset. We use the acquired information from this\\nlanguage model designed to detect offensive language and\\nexpressions of hate in written content. We conducted an\\nassessment of various supervised machine learning classifiers\\nto detect nasty and abusive content on Twitter, using a dataset\\nspecifically collected from Twitter. Deep learning enables the\\nautomatic acquisition of multi-level feature representations,\\nFigure 3: Training and Validation Accuracy\\nFigure 4: Training and Validation Loss per epoch\\nwhile typical machine learning-based NLP systems rely ex-\\ntensively on manually engineered features. Creating these\\nartisanal elements requires a significant amount of time and\\neffort and may not always be fully developed. The deep\\nlearning models achieved outstanding results in reducing false\\npositives and showed promising outcomes in decreasing false\\nnegatives during the classification process. Our objective is\\nto do more investigation into the application of deep neural\\nnetwork architectures for the purpose of detecting hate speech.\\nWe will conduct our inquiry by employing a range of word\\nembedding approaches. In addition, we intend to broaden\\nour research by incorporating additional datasets, particularly\\nthose in the Bangla language [24].\\nREFERENCES\\n[1] C. Nobata, J. Tetreault, A. Thomas, Y . Mehdad, and Y . Chang, ‚ÄúAbusive\\nlanguage detection in online user content,‚Äù in Proceedings of the 25th\\ninternational conference on world wide web , pp. 145‚Äì153, 2016.Figure 5: Training and Validation Recall\\nFigure 6: Training and Validation AUC\\n[2] P. Fortuna and S. Nunes, ‚ÄúA survey on automatic detection of hate speech\\nin text,‚Äù ACM Computing Surveys (CSUR) , vol. 51, no. 4, pp. 1‚Äì30, 2018.\\n[3] S. Hinduja and J. W. Patchin, ‚ÄúBullying, cyberbullying, and suicide,‚Äù\\nArchives of suicide research , vol. 14, no. 3, pp. 206‚Äì221, 2010.\\n[4] ‚ÄúUnited nations.‚Äù [Online]. Available:\\nhttps://www.un.org/en/genocideprevention/documents/ [Accessed:\\n14/05/2024].\\n[5] A. Gaydhani, V . Doma, S. Kendre, and L. Bhagwat, ‚ÄúDetecting hate\\nspeech and offensive language on twitter using machine learning: An n-\\ngram and tfidf based approach,‚Äù arXiv preprint arXiv:1809.08651 , 2018.\\n[6] B. Hu, B. Liu, N. Z. Gong, D. Kong, and H. Jin, ‚ÄúProtecting your\\nchildren from inappropriate content in mobile apps: An automatic ma-\\nturity rating framework,‚Äù in Proceedings of the 24th ACM International\\non Conference on Information and Knowledge Management , pp. 1111‚Äì\\n1120, 2015.\\n[7] ‚ÄúTime spend on smartphones.‚Äù [Online]. Available:\\nhttps:https://explodingtopics.com/blog/smartphone-usage-stats\\n[Accessed: 15/05/2024].\\n[8] M. O. Ibrohim and I. Budi, ‚ÄúHate speech and abusive language detection\\nin indonesian social media: Progress and challenges,‚Äù Heliyon , 2023.\\n[9] E. M. Alshari, A. Azman, S. Doraisamy, N. Mustapha, and M. Alkeshr,\\n‚ÄúEffective method for sentiment lexical dictionary enrichment based\\non word2vec for sentiment analysis,‚Äù in 2018 fourth international con-ference on information retrieval and knowledge management (CAMP) ,\\npp. 1‚Äì5, IEEE, 2018.\\n[10] Y . Pandey, M. Sharma, M. K. Siddiqui, and S. S. Yadav, ‚ÄúHate speech\\ndetection model using bag of words and na ¬®ƒ±ve bayes,‚Äù in Advances in\\nData and Information Sciences: Proceedings of ICDIS 2021 , pp. 457‚Äì\\n470, Springer, 2022.\\n[11] K. Kumari, J. P. Singh, Y . K. Dwivedi, and N. P. Rana, ‚ÄúMulti-modal\\naggression identification using convolutional neural network and binary\\nparticle swarm optimization,‚Äù Future Generation Computer Systems ,\\nvol. 118, pp. 187‚Äì197, 2021.\\n[12] G. Kov ¬¥acs, P. Alonso, and R. Saini, ‚ÄúChallenges of hate speech detection\\nin social media: Data scarcity, and leveraging external resources,‚Äù SN\\nComputer Science , vol. 2, no. 2, p. 95, 2021.\\n[13] O. Oriola and E. Kotz ¬¥e, ‚ÄúEvaluating machine learning techniques for\\ndetecting offensive and hate speech in south african tweets,‚Äù IEEE\\nAccess , vol. 8, pp. 21496‚Äì21509, 2020.\\n[14] N. Vashistha and A. Zubiaga, ‚ÄúOnline multilingual hate speech detection:\\nexperimenting with hindi and english social media,‚Äù Information , vol. 12,\\nno. 1, p. 5, 2020.\\n[15] A. Jiang and A. Zubiaga, ‚ÄúCross-lingual capsule network for hate speech\\ndetection in social media,‚Äù in Proceedings of the 32nd ACM conference\\non hypertext and social media , pp. 217‚Äì223, 2021.\\n[16] N. Deshpande, N. Farris, and V . Kumar, ‚ÄúHighly generalizable\\nmodels for multilingual hate speech detection,‚Äù arXiv preprint\\narXiv:2201.11294 , 2022.\\n[17] M. Corazza, S. Menini, E. Cabrio, S. Tonelli, and S. Villata, ‚ÄúA multi-\\nlingual evaluation for online hate speech detection,‚Äù ACM Transactions\\non Internet Technology (TOIT) , vol. 20, no. 2, pp. 1‚Äì22, 2020.\\n[18] X. Huang, L. Xing, F. Dernoncourt, and M. J. Paul, ‚ÄúMultilingual twitter\\ncorpus and baselines for evaluating demographic bias in hate speech\\nrecognition,‚Äù arXiv preprint arXiv:2002.10361 , 2020.\\n[19] S. S. Aluru, B. Mathew, P. Saha, and A. Mukherjee, ‚ÄúDeep learn-\\ning models for multilingual hate speech detection,‚Äù arXiv preprint\\narXiv:2004.06465 , 2020.\\n[20] ‚ÄúFacebook research:laser: Language-agnostic sentence representations.‚Äù\\n[Online]. Available: https://github.com/facebookresearch/LASER [Ac-\\ncessed: 18/05/2024].\\n[21] ‚ÄúFacebook research:muse: Multilingual unsupervised\\nand supervised embeddings.‚Äù [Online]. Available:\\nhttps://github.com/facebookresearch/MUSE [Accessed: 18/05/2024].\\n[22] M. R. Islam, A. Amin, and A. N. Zereen, ‚ÄúEnhancing bangla language\\nnext word prediction and sentence completion through extended rnn with\\nbi-lstm model on n-gram language,‚Äù arXiv preprint arXiv:2405.01873 ,\\n2024.\\n[23] M. R. Islam, M. M. Islam, M. Afrin, A. Antara, N. Tabassum, A. Amin,\\net al. , ‚ÄúPhishguard: A convolutional neural network based model for\\ndetecting phishing urls with explainability analysis,‚Äù arXiv preprint\\narXiv:2404.17960 , 2024.\\n[24] A. Amin, A. Sarkar, M. M. Islam, A. A. Miazee, M. R. Islam, and M. M.\\nHoque, ‚ÄúSentiment polarity analysis of bangla food reviews using ma-\\nchine and deep learning algorithms,‚Äù arXiv preprint arXiv:2405.06667 ,\\n2024.',\n",
       " 'Social media algorithms can curb misinformation, but do they?\\nChhandak Bagchi1, Filippo Menczer2, Jennifer Lundquist1, Monideepa Tarafdar1,\\nAnthony Paik1, Przemyslaw A. Grabowicz1,3*\\n1University of Massachusetts Amherst\\n2Indiana University\\n3University College Dublin\\nA recent article in Science by Guess et al. (2023) esti-\\nmated the effect of Facebook‚Äôs news feed algorithm on ex-\\nposure to misinformation and political information among\\nFacebook users. However, its reporting and conclusions did\\nnot account for a series of temporary emergency changes\\nto Facebook‚Äôs news feed algorithm in the wake of the 2020\\nU.S. presidential election that were designed to diminish\\nthe spread of voter-fraud misinformation (U.S. House Se-\\nlect Committee 2023). This issue may have led readers to\\nmisinterpret the results of that study and to conclude that\\nthe Facebook news feed algorithm used outside of the study\\nperiod mitigates political misinformation as compared to re-\\nverse chronological feed.\\nFrom September 24th through December 23rd 2020,\\nGuess et al. performed a randomized experiment measuring\\nthe effects of social media feeds on user behaviors and atti-\\ntudes during the election campaign and its aftermath. The\\nexperiment provided a randomly assigned group of Face-\\nbook users with a news feed sorted in reverse chronological\\norder. The effects of this intervention were then compared\\nto the control condition ‚Äì the Facebook news feed algorithm\\nas it was implemented during the study period. The study\\nstated in the abstract that ‚Äúthe chronological feed affected\\nexposure to content: the amount of [...] untrustworthy con-\\ntent [users] saw increased,‚Äù concluding that ‚Äúsocial media\\nalgorithms may not be the root cause of phenomena such\\nas increasing political polarization.‚Äù Others interpreted it to\\nmean that algorithms have little effect on exposure to prob-\\nlematic content (Budak et al. 2024). These are crucial mes-\\nsages as we move into the 2024 U.S. presidential election\\nseason.\\nHowever, during the experiment, Meta introduced 63\\n‚Äúbreak-glass‚Äù changes to Facebook‚Äôs algorithmic news feed\\n‚Äì not reported by Guess et al. ‚Äì changing the control con-\\ndition of their experiment. These temporary changes were\\ndesigned to diminish the relative visibility of news content\\nfrom untrustworthy sources immediately after the 2020 U.S.\\npresidential election (Roose 2023; U.S. House Select Com-\\nmittee 2023). While Guess et al. acknowledge that their re-\\nsults may have been different ‚Äúif a different content rank-\\ning system were used as an alternative to the status quo\\n*This research was conducted when Przemyslaw A. Grabow-\\nicz was a research assistant professor at the University of Mas-\\nsachusetts Amherst. Contact: grabowicz@cs.umass.edu.feed-ranking algorithms,‚Äù the fact that the control condition\\nchanged during their experiment affects the validity and con-\\nclusion of their study.\\nThe study by Guess et al. reports that the fraction of un-\\ntrustworthy content was 40.9% lower for the algorithmic\\nnews feed (their control) than for the reverse chronolog-\\nical news feed (their treatment). Using a different dataset\\nprovided by Meta, we measured the number of times users\\nviewed news articles from trustworthy and untrustworthy\\nnews outlets in the year around the experiment. User expo-\\nsure to news from trustworthy sources increased compared\\nto untrustworthy sources from November 3, 2020, to March\\n8, 20211. The period of potential impact of the news feed\\nalgorithm change (red arrow in Figure 1a) coincides with\\nthe Guess et al. experiment (black arrow in Figure 1a). Such\\noverlap might affect the reported effects of the chronological\\nfeed compared to the algorithmic news feed on the number\\nof exposures to untrustworthy sources. Figure 1b indicates\\nthat the fraction of untrustworthy news views decreased by\\naround 24% for the algorithmic feed. Therefore, a consider-\\nable portion of the decrease reported by Guess et al. may be\\nattributable to the temporary algorithm changes.\\nThe implications of our estimated drop in the fraction of\\nuntrustworthy news views for the conclusions of Guess et\\nal. depend on various factors. First, untrustworthy content\\ncan be measured in different ways (we find similar results\\nwhen using NewsGuard scores instead of MBFC ratings).\\nSecond, the period of potential impact of the news feed al-\\ngorithm change and the Guess et al. experiment period are\\nnot perfectly aligned. Finally, there may have been a surge\\nin election-related misinformation (V osoughi, Roy, and Aral\\n2018), which might increase our estimated drop.\\nIf there is a correlation between other variables in the\\nGuess et al. study and exposure to untrustworthy sources,\\ne.g., between the consumption of both misinformation and\\npartisan news, then some other results presented by Guess\\net al. may also be explained by the temporary break-glass\\n1Our measurements are consistent with those of Bandy and Di-\\nakopoulos (2023). While they report a post-election drop in total re-\\nferrals to both high- and low-quality news sources, we focus on the\\naverage views per news, which decreased for low-quality sources.\\nThey also report no substantial changes in the number of views of\\nfive prominent media outlets, while we analyze over a thousand\\nsources.arXiv:2409.18393v1  [cs.SI]  27 Sep 20242020Jul Aug Sep Oct Nov Dec Jan\\n2021Feb Mar Apr May Jun JulScience experiment\\nPotential Facebook\\nalgorithm change\\n-24%\\n1.01.52.02.5Average #Viewsx105\\n(a)Trustworthy\\nUntrustworthy\\n0.060.080.100.12Fraction of untrust-\\nworthy news views\\n(b)Figure 1: (a) Average weekly number of views of news from trustworthy and untrustworthy sources, calculated using the\\nFacebook URLs dataset (Messing et al. 2020). Our estimates of untrustworthy news are based on links to sources rated mixed,\\nlow, or very low for factual reporting by Media Bias/Fact Check (MBFC) and shared at least 100 times, whereas Guess et al.\\nconsider any post by users with two or more reports as untrustworthy. (b) Fraction of views of untrustworthy news among all\\nviews. The horizontal dotted lines are averages of the points of the same color. We observe a drop during a period overlapping\\nwith the experiment, likely due to the changes in the news feed algorithm.\\nalgorithm changes and may not replicate if the study were\\nconducted again ‚Äî a vivid demonstration of the importance\\nof temporal validity when generalizing results from digital\\nfield experiments (Munger 2019).\\nThese results have research and societal implications.\\nFrom a research perspective, they demonstrate the chal-\\nlenges of examining the effects of social media algorithms.\\nThe Guess et al. experiment was preregistered ‚Äì that is,\\nstructured and declared ahead of its execution time. How-\\never, social media platforms do not register, let alone prereg-\\nister, significant changes to their algorithms. For example,\\nthe exact effects and timing of most of the 63 break-glass\\nnews feed changes of 2020 are not known to the public. This\\ncan lead to situations where social media companies could\\nconceivably change their algorithms to improve their pub-\\nlic image if they know they are being studied. To prevent\\nthat, there is a need for independent research of social me-\\ndia platforms and consistent, transparent disclosures about\\nmajor changes to their algorithms. Laws such as the Dig-\\nital Services Act in the European Union and the proposed\\nPlatform Accountability and Transparency Act in the U.S.,\\nif properly enforced (Carvalho 2024), could empower re-\\nsearchers to conduct independent audits of social media plat-\\nforms and better understand the potentially serious effects\\nof ever-changing social media algorithms on the public. In\\nthe last two years, however, social media transparency hasdiminished, e.g., on X and Reddit (Kupferschmidt 2023),\\nwhile Facebook has not updated the URLs dataset we an-\\nalyzed here since 2022. Without access to such data, our un-\\nderstanding of the role of algorithms in curbing misinforma-\\ntion will remain incomplete. From a societal perspective, our\\nresults suggest that news feed algorithms can mitigate misin-\\nformation. It might be valuable to keep the misinformation-\\npreventing algorithmic feed in place even outside of election\\ncampaigns and their immediate aftermath. Unfortunately,\\nthere is no guarantee that algorithms beneficial to the public\\nwill be in place again in the future. Some Facebook employ-\\nees claim in internal documents and interviews that the com-\\npany ultimately chose to revoke the break-glass safeguards\\nin the interest of market growth (Horwitz 2023).\\nAcknowledgements\\nAuthors acknowledge a data sharing agreement with Meta\\nthat enabled the study of Facebook‚Äôs URLs dataset. How-\\never, Meta was not involved in this study in any way, finan-\\ncially nor intellectually, and did not oversee, nor review, the\\nstudy. Ch.B., J.L., M.T., A.P., and P.A.G. acknowledge sup-\\nport by the Provost‚Äôs Interdisciplinary Research Grant 2023\\n(University of Massachusetts Amherst) titled ‚ÄúPolitical Mis-\\ninformation and Disinformation Through Social Media Bi-\\nases‚Äù. F.M. is supported in part by the Knight Foundation\\nand the Swiss National Science Foundation (grant 209250).References\\nBandy, J.; and Diakopoulos, N. 2023. Facebook‚Äôs news feed\\nalgorithm and the 2020 us election. Social Media+ Society ,\\n9(3): 20563051231196898.\\nBudak, C.; Nyhan, B.; Rothschild, D. M.; Thorson, E.; and\\nWatts, D. J. 2024. Misunderstanding the harms of online\\nmisinformation. Nature , 630(8015): 45‚Äì53.\\nCarvalho, M. C. d. 2024. Researcher Access to Platform\\nData and the DSA: One Step Forward, Three Steps Back.\\nTech Policy Press .\\nGuess, A. M.; Malhotra, N.; Pan, J.; Barber ¬¥a, P.; Allcott, H.;\\nBrown, T.; Crespo-Tenorio, A.; Dimmery, D.; Freelon, D.;\\nGentzkow, M.; et al. 2023. How do social media feed algo-\\nrithms affect attitudes and behavior in an election campaign?\\nScience , 381(6656): 398‚Äì404.\\nHorwitz, J. 2023. Broken Code: Inside Facebook and the\\nFight to Expose Its Harmful Secrets. Doubleday .\\nKupferschmidt, K. 2023. Twitter‚Äôs plan to cut off free data\\naccess evokes ‚Äòfair amount of panic‚Äô among scientists. Sci-\\nence (New York, NY) , 379(6633): 624‚Äì625.\\nMessing, S.; DeGregorio, C.; Hillenbrand, B.; King, G.; Ma-\\nhanti, S.; Mukerjee, Z.; Nayak, C.; Persily, N.; State, B.; and\\nWilkins, A. 2020. Facebook Privacy-Protected Full URLs\\nData Set.\\nMunger, K. 2019. The limited value of non-replicable field\\nexperiments in contexts with low temporal validity. Social\\nMedia+ Society , 5(3): 2056305119859294.\\nRoose, K. 2023. Facebook reverses postelection algorithm\\nchanges that boosted news from authoritative sources. The\\nNew York Times .\\nU.S. House Select Committee. 2023. Social Media and the\\nJanuary 6th Attack on the U.S. Capitol: Summary of Inves-\\ntigative Findings.\\nV osoughi, S.; Roy, D.; and Aral, S. 2018. The spread of true\\nand false news online. Science , 359(6380): 1146‚Äì1151.',\n",
       " 'Social Media Bot Policies:\\nEvaluating Passive and Active Enforcement\\nKristina Radivojevic\\nComputer Science and Engineering\\nUniversity of Notre Dame\\nNotre Dame, USAChristopher McAleer\\nMathematics and Statistics\\nDublin City University\\nDublin, IrelandCatrell Conley, Cormac Kennedy, Paul Brenner\\nCenter for Research Computing\\nUniversity of Notre Dame\\nNotre Dame, USA\\nAbstract ‚ÄîThe emergence of Multimodal Foundation Models\\n(MFMs) holds significant promise for transforming social media\\nplatforms. However, this advancement also introduces substantial\\nsecurity and ethical concerns, as it may facilitate malicious actors\\nin the exploitation of online users. We aim to evaluate the strength\\nof security protocols on prominent social media platforms in\\nmitigating the deployment of MFM bots. We examined the bot\\nand content policies of eight popular social media platforms:\\nX (formerly Twitter), Instagram, Facebook, Threads, TikTok,\\nMastodon, Reddit, and LinkedIn. Using Selenium, we developed a\\nweb bot to test bot deployment and AI-generated content policies\\nand their enforcement mechanisms. Our findings indicate signif-\\nicant vulnerabilities within the current enforcement mechanisms\\nof these platforms. Despite having explicit policies against bot\\nactivity, all platforms failed to detect and prevent the operation of\\nour MFM bots. This finding reveals a critical gap in the security\\nmeasures employed by these social media platforms, underscoring\\nthe potential for malicious actors to exploit these weaknesses to\\ndisseminate misinformation, commit fraud, or manipulate users.\\nIndex Terms ‚Äîsocial media, bots, multimodal foundational\\nmodels, policy.\\nI. I NTRODUCTION\\nIn the past two decades, social media platforms have\\nexperienced exponential user growth [1], becoming integral\\nto daily communication, social interaction, and information\\ndissemination. As of 2023, platforms like Facebook, Insta-\\ngram, and TikTok host billions of active users worldwide [2].\\nThe widespread adoption of social media has transformed it\\ninto a powerful tool for influencing public opinion, marketing,\\nand political engagement [3]. However, the same features that\\npromote access and ease of connectivity on these platforms\\nalso render their users more susceptible to manipulation and\\nmisinformation.\\nSeveral social media platforms provide a simple registration\\nprocess, requiring only an unvalidated name and email address\\nas the identity-related information. Various additional authen-\\ntication methods may be employed by platforms, such as one-\\ntime passwords or Turing tests to differentiate between ma-\\nchines and humans (CAPTCHA) [4]. However, these methods\\ndo not necessarily prevent fake accounts from being created\\n[5]. Accounts can be controlled by humans or automated\\nprograms with both legitimate and malicious intentions. When\\ncontrolled by a computer program, they can be referred to as\\nsocial bots [6]. Social bots play a significant role on socialmedia, ranging from benign to malicious activities. These\\nautomated accounts can perform various tasks, such as liking,\\nposting content, and following other users, at a scale and speed\\nunattainable by humans. While some bots serve legitimate\\npurposes, many are designed to manipulate public discussion,\\naccelerate and spread fake news, and create artificial trends\\n[7], [8] representing a threat to individuals, communities, and\\ngovernments.\\nThe rapid advancement of Multimodal Foundation Models\\n(MFMs) has further influenced the landscape of automated\\ncontent generation on social media. MFMs, such as OpenAI‚Äôs\\nGPT series, exhibit remarkable capabilities in understanding\\nand generating conversational text, enabling the creation of\\nsophisticated and contextually relevant content at scale [9].\\nThis technological evolution has broad applications, including\\ncustomer service, content creation, and conversational agents.\\nHowever, it also poses new challenges, as MFMs can be\\nemployed to generate realistic falsehoods and manipulate\\nonline interactions. Unlike traditional bots that operate with\\npredefined scripts, MFM-powered bots can produce dynamic\\nand contextually appropriate responses, generate arguments,\\ndraw on contextual knowledge, or perform basic reasoning\\ntasks, making them harder to detect and differentiate from\\nhuman users [10], as shown in Fig. 1. The increasing com-\\nplexity of MFMs underscores the need for advanced detec-\\ntion mechanisms and policy frameworks to address the risks\\nassociated with their misuse on social media platforms. As\\nthese technologies continue to evolve, the potential for MFMs\\nto influence public discourse becomes a pressing concern for\\nresearchers, policymakers, and platform administrators.\\nMFMs have the potential to be used in the automated\\nproduction of large volumes of harmful content, which can\\nbe disseminated widely to cause damage or influence public\\nopinion [11]. Many deep fake videos or AI-generated images\\nand text appear true to some users leading to an erosion of\\nconfidence, furthering trust problems on the Internet. In an\\ninternational survey, users have reported a decline in trust in\\nthe Internet since 2019 [12], suggesting that setting standards\\nfor the way Internet companies collect and use data could\\nimprove trust. There are proven and perceived AI risks, such\\nas bias in terms of training data, the limitations of the people\\ninvolved in the training, and even the usage context. Another\\nimportant problem related to the effects of MFMs is identityarXiv:2409.18931v1  [cs.SI]  27 Sep 2024Fig. 1. Potential Risks of Unregulated MFM-powered chatBots on Digital Platforms.\\ntheft. Some examples of such theft can be creating AI-\\ngenerated selfies to use in fake IDs or during social media\\nverification, cloning a person‚Äôs voice to engage in different\\nactivities, or creating deepfake videos to manipulate the public\\nfor different goals. There are many examples of fake accounts\\non social media platforms, many potentially being built on\\nMFMs.\\nTo mitigate threats to democracy and protect users, social\\nmedia platforms and governments often develop policies or\\nlaws that aim to reduce harm from spreading AI-generated\\ncontent (AIGC). While the U.S. currently has no laws gov-\\nerning social media automation, the European Union intro-\\nduced the Code of Practice [13] in 2018, according to which\\nplatforms committed themselves to self-regulation aimed at\\nstamping out disinformation on their sites, which includes\\nshutting down fake accounts and indicating bot accounts.\\nThere is, however, a lack of clarity surrounding when and how\\nthese platforms will implement these policies. Often, social\\nmedia companies might be reluctant to remove automated\\naccounts because a higher number of accounts leads to higher\\nadvertising revenues. Platforms often implement policies and\\nset up consequences for breaking the platform‚Äôs policies,\\ntaking the severity of the infraction and the user‚Äôs history into\\naccount. Sites reserve the right to restrict posts‚Äô viewership\\nor even ban users or accounts for violating terms of service.\\nIn addition to warnings and restrictions, they can even go so\\nfar as to sue. There is, however, a lack of clarity surrounding\\nwhen and how these platforms implement these policies.\\nOur research focuses on the occasion where a potentially\\nmalicious actor uses automated software to operate an indi-\\nvidual bot account, leveraging an MFM to generate content.\\nSpecifically, we aim to assess the enforcement mechanisms\\nof popular social media platforms against the deployment of\\nMFM bots. Our study examines the current measures of each\\nplatform in two parts. First, we collect and analyze policy\\ndetails regarding bot deployment and AIGC for eight major\\nsocial media platforms: X, Instagram, Facebook, Threads,\\nTikTok, Mastodon, Reddit, and LinkedIn. Second, we gather\\ndata on the technical measures implemented by each platformfor detecting bot accounts and AIGC. We use Selenium and\\nGPT-4 model to test these policies and technical defense\\nmeasures. Our bots achieve automated login and AI-generated\\nposts for all platforms.\\nOur findings reveal significant policy enforcement flaws\\nin these social media platforms, indicating that their policies\\nand technical measures are ineffective and often not aligned.\\nWe show that malicious actors can deploy MFM bots with\\nrelative ease, posing substantial risks to the integrity of online\\ndiscourse and the security of social media ecosystems.\\nII. R ELATED WORK\\nThe presence of bots on social media platforms has been\\nextensively researched. Bots significantly contribute to SM\\nactivity by boosting URL popularity [14], consuming and\\nproducing content, and even interacting with human users [15].\\nFurthermore, bots are likely responsible for a disproportionate\\nshare of misinformation traffic, significantly impacting the data\\nprovided by the Twitter Sample API and injecting bias into\\nthis data outlet [7]. Several bot detection tools and approaches\\nhave been developed in the research community to combat\\nthis issue [16]. Chu et al. [17] collected one month of data,\\nencompassing over 500,000 Twitter users with more than 40\\nmillion tweets, and identified features distinguishing human,\\nbot, and cyborg accounts. Alarifi et al. [18] presented the TSD\\nsystem, which utilizes supervised machine learning techniques\\nto dynamically detect Twitter fake accounts, often known as\\nSybil accounts, and warn users before they interact with these\\naccounts. Finally, the BotOrNot system employs a random\\nforest classifier to evaluate social bots [19]. To our knowledge,\\nthe platforms have not yet adopted any of these proposed\\nsolutions.\\nHowever, with the advancement of MFMs and other new\\ntechnologies, these methods require re-evaluation. Cresci et\\nal. [20] asserted that the BotOrNot service and additional\\ntraditional supervised and unsupervised classification methods\\nare outdated solutions for detecting evolving SM spam bots.\\nNasim et al. [21] discovered social bots in protest-related\\ntweets and showed existing methods are inadequate for detect-\\ning content-polluting bots. Yang and Menczer [22] determinedthat bot detection methods prove ineffective for MFM-powered\\nbots, proposing the AI text classifier provided by OpenAI\\nas a potential substitute solution. However, they identified\\nchallenges with this new tool such as unreliability for non-\\nEnglish content and short texts, considerably narrowing the\\nscope of accounts it can process [23]. Adversarial attacks and\\nevasion tactics demand a more complex detection process, as\\nbots continuously improve their ability to bypass traditional\\ndetection methods [24]. The need for scalable and real-time\\ndetection solutions presents a significant challenge, given the\\nvast scale and dynamic nature of social media platforms.\\nAyoobi, Shahriar, and Mukherjee [25] introduced a novel\\napproach for the early detection of MFM-generated profiles\\non LinkedIn. Grimme et al. [26] identified the new challenges\\nof detecting MFM-influenced campaigns in a social media\\ncontext. They found how MFMs particularly challenge algo-\\nrithms focused on the temporal analysis of topical clusters.\\nTheir results showed that campaigns can be detected despite\\nthe limited reliability of the classifiers only if they are based\\non a large amount of simultaneously spread artificial content.\\nThe increasing dangers associated with bots and MFM tech-\\nnology drive the need for further research in bot detection and\\nthe deployment of effective technical measures. Bots actively\\ngenerate election-related content [22] and have influenced\\npast social discussions on social media platforms regarding\\nelections in the U.S., France, and Brazil [3], [27], [28] and\\nthe COVID-19 vaccines [29]. Further research examined the\\nprominent political presence of bots on Reddit [30] and eval-\\nuated how vulnerable Online Social Networks (OSNs) are to\\nlarge-scale infiltration by a social bot network, using Facebook\\nas a representative OSN [31]. Moreover, generative language\\nmodels are impacting the future of online disinformation\\ncampaigns by enhancing content, promoting behaviors, and en-\\ngaging actors [32]. AIGC also shows potential for widespread\\nusage and abuse due to its ease of use and flexibility [33].\\nOur work explores the ocassion of a malicious actor deploy-\\ning a social media bot powered by an MFM with the intent to\\nmanipulate other online users. Fake news can be used by these\\nentities to manipulate people‚Äôs options and decisions on im-\\nportant daily activities, like stock markets, healthcare options,\\nonline shopping, education, and even presidential elections\\n[34]. Additionally, AI-based tools are capable of producing\\nnews and politically motivated content that readers deem as\\nequally or more credible than human-written alternatives [10].\\nFinally, researchers [35] examined the theoretical impact of\\nsocial bots, and found that a small number was sufficient\\nto influence public opinion, triggering silence from human\\ndetractors that eventually led to the acceptance of the opinion\\nof the bots as dominant. These findings demonstrate the urgent\\nneed for effective policy-driven enforcement to combat manip-\\nulative bots. However, any initiatives suggested by government\\npolicymakers and informed by research must overcome nu-\\nmerous pressing challenges: the conceptual ambiguity of bots\\nand their nature, lack of clarity over responsibility, and poor\\nmeasurement and data access.III. C URRENT PLATFORM MEASURES\\nA. Policy Details\\nEach Social Media platform we examined is governed by\\nits own respective terms and conditions. These cover a variety\\nof rules concerning issues such as account integrity and au-\\nthenticity, safety, content moderation, and security and privacy.\\nWe concentrated on gathering and analyzing policies related\\nto bot operations and AI-generated content for each platform.\\nFor ethical and legal reasons, we do not violate any policies\\nrequiring interaction with other users, spamming, or uploading\\nmisleading content. All policies, rules, and regulations are\\ncollected from official company resources.\\nX permits the use and operation of bots. However, X‚Äôs\\nautomation rules state that the use of non-API-based forms\\nof automation, such as scraping the X website, is prohibited\\nand violations may result in permanent account suspension\\n[36]. On Reddit, the majority of bot and AI content policies\\nare subreddit-specific. As our Reddit bot‚Äôs operations are con-\\nstrained to a private subreddit of our creation, we avoid many\\nof these policies. While Reddit has general rules outlining\\nbot activity and behaviour on the platform, violating these\\npolicies would require our bot to interact with other users\\nor produce inappropriate content [37], conflicting with our\\nethical guidelines. Reddit‚Äôs lack of strict platform-wide policy\\non bot activity and AI-generated content allows users to freely\\noperate bot accounts and post MFM-generated content without\\naccessing the Reddit API.\\nUnder Mastodon‚Äôs Code of Conduct, bots are mandated to\\nidentify themselves in their profile by ticking ‚ÄúThis is a bot\\naccount‚Äù, describe their purpose, and mention who their owner\\nis [38]. Furthermore, upon signing up to mastodon.social (our\\nchosen server for our bot to operate on), users must agree\\nto disclose the use of generative AI. Enforcement of these\\nrules is determined by moderators on Mastodon. Threads,\\nFacebook, and Instagram, which are all under nearly the same\\nMeta‚Äôs Transparency Center, claim they will restrict or disable\\naccounts or other entities that create or use an account by\\nscripted or other inauthentic means [39]. Moreover, in the\\ncase that Meta suspects your account to be misrepresenting\\nyour identity (Facebook only) or to be compromised they\\nwill seek further information about an account before tak-\\ning actions ranging from temporarily restricting accounts to\\npermanently disabling them. Finally, TikTok‚Äôs Community\\nGuidelines requires users to disclose AI-generated content\\nthat shows realistic-appearing scenes or people with the ‚ÄôAI\\ngenerated‚Äô label [40]. Furthermore, the Guidelines state that\\nfailure to do so may result in content removal.\\nLinkedIn prohibits users from using any bots or other\\nautomated methods to access their services, add or download\\ncontacts, or send or redirect messages. Furthermore, they do\\nnot allow users to override any security feature or scrape\\nwebpage content through any means. LinkedIn states that users\\nwho violate these policies risk having their accounts restricted\\nor shut down [41].B. Enforcement Mechanisms\\nX has recently intensified efforts to combat spam bots. The\\nplatform introduced the ‚ÄúNot a Bot‚Äù program in October 2023,\\nwith initial tests occurring in New Zealand and the Philippines\\n[42]. This program required new users to verify their phone\\nnumbers and pay a $1 USD annual fee to perform actions like\\nposting, liking, reposting, quoting, replying, and bookmarking.\\nWhile results are not yet published, this initiative may reduce\\nthe prevalence of large bot networks, though its impact on\\nindividually operated bot accounts remains uncertain. Previous\\nresearch demonstrates challenges in X‚Äôs bot detection. A 2018\\npaper found that only 153 of the 849 bots they detected in a\\nyear-old dataset were suspended [21]. Furthermore, X‚Äôs ability\\nto detect bots varies depending on the nature of the account\\n[20]. Simpler bots like fake followers face high suspension\\nrates, but more complex spambots have survival rates exceed-\\ning 95%. This indicates that X‚Äôs current detection measures\\nlag behind advancements in research-based bot detection tools.\\nThe enforcement rules of Reddit and Mastodon are influ-\\nenced by user moderation. Prior research determined that there\\nis a difference between rules regulating Mastodon instances\\nand subreddits [43]. There is a disparity between moderation\\ncommitments made by Reddit and corresponding subreddit\\nimplementation [44], and how content moderation varies by\\nsubreddit characteristics [45]. As human users struggle to\\ndistinguish modern MFM-powered bots from genuine users\\n[10], relying solely on user moderation may allow many\\nmalicious bots to evade detection.\\nBoth modern technological advancements and review teams\\npower Meta‚Äôs platform security mechanisms. Meta‚Äôs AI teams\\ndevelop machine learning models that can perform tasks such\\nas recognizing objects in a photo or understanding text [46].\\nAs the integrity teams build upon these models, they create\\nmore specific content and user behavior models. When new\\ntechnologies yield low-confidence results, human reviewers\\nare employed to make final decisions. The technology can\\nthen learn from each human decision. Meta claims to regularly\\ninvest in and improve upon a number of AI projects to\\nstrengthen their ability to detect violating content, that can\\nbe produced either by humans or bots[46].\\nBased on TikTok policy, the platform prohibits the use of\\nbots. However, despite their efforts to continually evolve their\\ndetection methods to keep pace with the evolution of AI [47],\\nresources show that the platform still does not successfully\\nban bot accounts [48], [49]. They claim to detect AI-generated\\ncontent (AIGC) through a combination of proactive technolo-\\ngies, alerts from expert and fact-checking partners, searches\\nfor clips or keywords related to known AIGC, and user\\nreports. When they identify misleading video or audio content\\nthat is spreading elsewhere online, they aim to automatically\\ncatch and take action on similar versions of that content to\\nprevent it spreading on their platform. In addition to providing\\ncreators a tool to label their own AIGC, TikTok automatically\\nlabels AIGC made with TikTok AI effects. Furthermore, they\\nlabel AIGC from certain other platforms by using ContentCredentials, a technical standard developed by the Coalition\\nfor Content Provenance and Authenticity (C2PA) [50]. Content\\nCredentials attach metadata to content, which enables TikTok‚Äôs\\nsystems to instantly recognize and label AIGC. Beyond auto-\\nmatic detection methods, TikTok is launching media literacy\\ncampaigns with guidance from organizations like Mediawise\\nand WITNESS that teach the TikTok community how to spot\\nand label AI-generated content.\\nIV. M ETHODOLOGY\\nWe developed an automated Python script with Selenium\\nas it is efficient at performing the actions needed to navigate\\nthrough web pages in a browser, such as clicking buttons and\\ntyping keys. Additionally, we preferred it to the alternative\\nof using the platforms‚Äô respective APIs, which are monitored\\nand have rate limits, due to our intention to imitate the\\nmalicious intent of a potentially malicious actor. To auto-\\nmate the browser in a new guest window, we installed a\\nChromeDriver. We used GPT-4o and DALL¬∑E 3 to generate\\ntext and images respectively. As of the time of writing, each\\nis the most advanced model OpenAI offers [51]. Therefore,\\nour bot combines Selenium script for automation of actions\\nwith MFM capabilities. We provided both models with a\\nhardcoded prompt detailing exactly how we intended them\\nto behave. We manually created each social media account,\\nbypassing the difficult task of automating CAPTCHAs and\\nemail verification. Other than the investment of a few hours,\\nit would be trivial for a malicious actor to repeat this process\\non the order of a hundred times.\\nThe bot uses the WebDriver.get() method to load the URL\\nof the website‚Äôs login page in our browser window. Using their\\nunique tags as a parameter, the bot then finds HTML elements\\non the page using the WebDriver.find element() method and\\nclicks them with the WebDriver.click() method. In this way it\\nemulates the process of a human user logging in to the website.\\nOnce logged in, the bot navigates to a page that allows it\\nto create a post on the platform. Depending on the platform,\\nthe bot may encounter a text-box for writing a caption and/or\\na button for uploading media. For text-boxes, it then uses the\\nWebDriver.send keys() method, passing as a parameter some\\nGPT-4.0-generated text. For media uploads on Instagram, it\\nalso uses the WebDriver.send keys() method, but passes the\\npath to a media file containing content previously generated by\\nDALL-E with a call to the OpenAI API. The TikTok webpage\\nonly allows users to upload video content. Without access\\nto a video-generating MFM, we manually created a library\\nof screen recordings of DALL-E-3-generated images. This\\nsimulated a scenario where a malicious actor could upload\\nMFM-generated videos from a specified folder. We utilized the\\n.moveTo() and .click() methods from PyAutoGui, a module\\ndesigned to programmatically control mouse and keyboard\\nactions, to select a video from this library to upload.\\nOn all platforms, we called the time.sleep() method between\\neach action, pausing the execution of our code and allowing\\nample time for each web page and element to load.We checked each post biweekly, commencing on June 10,\\n2024, and continuing until July 31, 2024, for signs of policy\\nenforcement from the platforms, such as flagging or removal.\\nV. R ESULTS\\nA. Account creation and Logging in\\nWe manually created accounts for our bot on all of the\\nplatforms in our experiment. We used the username ‚ÄúTest\\nPlatforms‚Äù for all accounts except when using Meta platforms,\\nwhich require accounts to bear their owners‚Äô real names.\\nFor these, we used fake but real-sounding names with the\\ninitials C-R-C. During account creation, only Facebook and\\nTikTok deployed CAPTCHA tests, with TikTok additionally\\nrequiring the use of the mobile app to create an account. All\\nplatforms required email verification, so we manually created a\\nGmail account for each bot account with a fictitious birthday,\\nname, and email address. It is important to note that many\\nemail creation options require even less user information.\\nWe successfully logged into each platform using the process\\ndescribed in Fig. 2.\\nB. Posting\\n‚Ä¢X: We automated the tweeting process and called the\\nOpenAI API to create a simple ‚Äútest‚Äù post. Although non-\\nAPI uses of automation are prohibited by X, biweekly\\ninspections found no action taken against our bot account\\nor tweet.\\n‚Ä¢Reddit: We manually created our own subreddit where\\nour bot automatically uploaded an AI-generated ‚Äútest‚Äù\\npost. While there was no Reddit bot policy violation to\\ntrigger, it demonstrated the ability to create an automated\\nscript for posting AIGC on Reddit.\\n‚Ä¢Mastodon: We had our bot automatically upload a single\\n‚Äútest‚Äù post on mastodon.social. We did not identify our\\nbot by using the ‚ÄúThis is a bot account‚Äù ticker or men-\\ntioning our bot‚Äôs purpose or owner, nor did we disclose\\nthe use of AIGC. Despite this, mastodon moderators did\\nnot flag our account or content, highlighting the difficultly\\nhuman moderators face in detecting bot accounts based\\nsolely on posts.\\n‚Ä¢Facebook: We accumulated three suspensions from Face-\\nbook while attempting to post on the platform. Each\\ntime, our bot was suspended under Account Integrity\\nand Authentic Identity, and labelled as a fake account.\\nHowever, on our fourth attempt, our bot successfully\\nuploaded a ‚Äútest‚Äù post without detection. Regular check-\\nups verified that our account and post were not picked\\nup by Meta‚Äôs detection tools.\\n‚Ä¢Instagram: We were given three account suspensions from\\nInstagram for the same reasons as Facebook. However, on\\nour fourth account, our bot successfully uploaded AIGC\\nand remained undetected. Specifically, our Instagram bot\\nuploaded an AI-generated image of the word ‚Äútest‚Äù with\\nwhite font and a simple black background.\\n‚Ä¢Threads: Each Threads account is linked to an Instagram\\naccount, so when our first three Instagram accounts weresuspended, Threads access was also removed. However,\\non our fourth account, our bot successfully uploaded\\n‚Äútest‚Äù. This attempt required phone number verification\\nto prove the account wasn‚Äôt bot-operated. After this one-\\ntime request, our bot account remained in fully automatic\\noperation on the platform.\\n‚Ä¢TikTok: TikTok imposed more CAPTCHA tests than any\\nother platform, which we mitigated by spacing logins\\n1-2 hours apart. TikTok also required longer waiting\\nperiods between uploads (5-8 seconds compared to other\\nplatforms‚Äô 2-3 seconds). We uploaded a screen recording\\nof a realistic scene, specifically the Cliffs of Moher, with\\nthe caption ‚Äútest‚Äù without labeling it as AIGC. Regular\\nchecks revealed that this post went unnoticed by TikTok‚Äôs\\nAIGC labeling systems.\\n‚Ä¢LinkedIn: We accessed LinkedIn with our bot on the\\nfirst attempt. There were no CAPTCHA tests during the\\nsign-up process, but LinkedIn did require users to enter\\nadditional information upon signing up, such as student\\nstatus, employment status, and/or school or university\\naffiliation. We also had to confirm our account with an\\nemail. We successfully created a simple post with the text\\n‚Äútest‚Äù.\\nC. Overall findings\\nOur bot faced the least difficulty on Mastodon and Reddit,\\nlikely due to their user-moderated natures. We had similar\\nease deploying the bot on X, suggesting a need for stronger\\nenforcement mechanisms there. While TikTok and the three\\nMeta platforms presented more challenges, we ultimately\\nsucceeded on each in deploying undetected bot accounts\\ncapable of uploading AIGC. We speculate that our Meta\\naccounts were banned due to the frequency of logins during\\nthe testing period of deploying our bots. Whenever the bot\\nlogged in multiple times consecutively, account suspension\\nwould swiftly follow. Thus, Meta appears to have stronger\\nbot detection mechanisms as compared to other social media\\nsites. Furthermore, we suspect another possible reason for bot\\ndetection could be attributed to the non-use of profile pictures\\nand user information.\\nThese findings indicate that current security measures on\\nmany social media platforms are insufficient to prevent the\\ndeployment and operation of MFM bots. We were able to\\nscript each platform‚Äôs HTML and CSS tags for automation\\nwith Selenium with no detection or flagging, suggesting that\\nsocial media APIs are not required to automate access to a\\nsocial media website.\\nD. Limitations and Future Work\\nAlthough we were successful at creating a post with MFM\\nincorporation on every social media platform in our experi-\\nment, there were limitations to our research. For ethical and\\nlegal reasons, we were unable to interact with other users, so\\nwe could not perform likes, comments, shares, or reposts. Due\\nto this limitation, we were unable to research whether user\\ninteraction with the bots had any effect on their probabilityFig. 2. Social Media Bot Policy Enforcement Testing Framework Leveraging Selenium and MFM Automation.\\nof detection. A more complex experiment with potentially\\nmalicious posts would require an interdisciplinary team that\\ncould mitigate ethical concerns, biases, and potential harms\\nthat could arise. For this experiment, we consulted with the\\nInstitutional Review Board, and ‚Äútest‚Äù posts were deemed\\nsufficiently benign.\\nAdditionally, we were only able to create posts with the\\ncaption and image content as the text ‚Äôtest,‚Äô meaning we could\\nnot investigate the correlation between posting complex MFM-\\ngenerated content and being detected as a bot. When posting,\\nour bot currently fills text-box fields with a string returned by\\nthe OpenAI API, which is internally indistinguishable from\\na string typed by a human. Thus, we would only expect\\nplatforms to detect MFMs when the content of the text\\ncould be recognized as having been written by AI. Since we\\nprompted our MFM to only write ‚Äútest‚Äù, the AI had no creative\\ninfluence, and the posts would not have likely been detected as\\nAIGC. This limitation meant we could not test whether these\\nplatforms have the capacity to detect MFM-generated text, but\\nit does help us show that it is possible to post some AIGC with\\nzero traces of AI influence.\\nA corollary limitation is that we were unable to post\\nmisinformation or otherwise harmful content, meaning human\\nmoderators on Reddit and Mastodon had fewer reasons to take\\nspecial notice of our posts.\\nIn our previous work, we leveraged a closed and con-\\ntrolled environment for humans and bots to interact [10].\\nThis research shows that stricter policies and enforcement\\nmechanisms are needed to provide a safe platform for allusers; therefore, we plan to continue our previous work by\\nproviding a social media testbed for peer research organiza-\\ntions to explore complex bot and human behaviors, which\\nwill require implementing validation and authentication of user\\naccounts. This work can lead to interdisciplinary collaboration\\nin understanding and mitigating potential harms produced by\\nautomated MFMs on social media platforms.\\nVI. C ONCLUSION\\nSocial media platforms host billions of users every day\\nwho interact with each other through likes, comments, shares,\\nand reposts. It is easy to assume that the general population\\nof online users are authentic humans who think and act for\\nthemselves, but this is a common misconception. In recent\\ndecades, automated bots have emerged to spread misinforma-\\ntion, scams, and sophisticated theft. With the rise of MFMs\\nand their human-like behavior, the distinction between humans\\nand bots on social media is eroding. Social media platforms do\\nnot provide sufficient protection and surveillance to keep users\\nsafe from bots. Our research shows that with simple coding\\nskills, anyone is able to deploy their own bot on social media,\\nregardless of their intent. The lack of safety from harmful\\ncontent on social media can have social, psychological, and\\nfinancial impacts on the society of online users. Identity and\\ntrust on the internet are severely compromised due to the\\nadvances of MFM technologies, potentially leading users on\\nsocial media to question everything they see on the Internet.\\nThat could represent a whole new problem where users do notbelieve anything they see online, which could potentially be\\nused as a new tool for manipulation.\\nWe acknowledge that stricter account validation policies and\\nenforcement mechanisms may reduce user anonymity since\\nsufficient information is needed to authenticate accounts. De-\\ncentralized solutions might, however, aim to mitigate privacy\\nand anonymity concerns. Fee based validation services may\\nreduce the presence of bots but will reduce access to those\\nin poorer communities. Finally, social media platforms may\\nbe reluctant to implement stricter enforcement mechanisms\\ndue to their desire to grow their user counts, to increase their\\nmarketing revenue.\\nREFERENCES\\n[1] S. Kemp. (2023) Digital 2023: Global overview\\nreport. [Online]. Available: https://datareportal.com/\\nreports/digital-2023-global-overview-report\\n[2] P. R. Centre. (2024) Social media fact sheet. [On-\\nline]. Available: https://www.pewresearch.org/internet/\\nfact-sheet/social-media/\\n[3] E. Ferrara, ‚ÄúDisinformation and social bot operations\\nin the run up to the 2017 french presidential\\nelection,‚Äù First Monday , Jul. 2017. [Online]. Available:\\nhttp://dx.doi.org/10.5210/fm.v22i8.8005\\n[4] S. Hariyal. (2024) Authentication and\\naccess control for social media. [Online].\\nAvailable: https://medium.com/@sahil.hariyal20/\\nauthentication-and-access-control-for-social-media-2eae3e31ec7b\\n[5] P. K. Roy and S. Chahar, ‚ÄúFake profile detection on\\nsocial networking websites: a comprehensive review,‚Äù\\nIEEE Transactions on Artificial Intelligence , vol. 1, no. 3,\\npp. 271‚Äì285, 2020.\\n[6] E. Ferrara, O. Varol, C. Davis, F. Menczer, and A. Flam-\\nmini, ‚ÄúThe rise of social bots,‚Äù Communications of the\\nACM , vol. 59, no. 7, pp. 96‚Äì104, 2016.\\n[7] F. Morstatter, H. Dani, J. Sampson, and H. Liu, ‚ÄúCan\\none tamper with the sample api? toward neutralizing\\nbias from spam and bot content,‚Äù in Proceedings of\\nthe 25th International Conference Companion on World\\nWide Web , 2016, pp. 81‚Äì82.\\n[8] B. S. Bello and R. Heckel, ‚ÄúAnalyzing the behaviour\\nof twitter bots in post brexit politics,‚Äù in 2019 Sixth\\nInternational Conference on Social Networks Analysis,\\nManagement and Security (SNAMS) . IEEE, 2019, pp.\\n61‚Äì66.\\n[9] S. Cresci, ‚ÄúA decade of social bot detection,‚Äù CoRR ,\\nvol. abs/2007.03604, 2020. [Online]. Available: https:\\n//arxiv.org/abs/2007.03604\\n[10] K. Radivojevic, N. Clark, and P. Brenner, ‚ÄúLlms among\\nus: Generative ai participating in digital discourse,‚Äù in\\nProceedings of the AAAI Symposium Series , vol. 3, no. 1,\\n2024, pp. 209‚Äì218.\\n[11] R. Pasupuleti, R. Vadapalli, and C. Mader, ‚ÄúCyber se-\\ncurity issues and challenges related to generative ai\\nand chatgpt,‚Äù in 2023 Tenth International Conferenceon Social Networks Analysis, Management and Security\\n(SNAMS) . IEEE, 2023, pp. 1‚Äì5.\\n[12] S. Simpson. Internet users‚Äô trust in the\\ninternet has dropped significantly since 2019.\\n[Online]. Available: https://www.ipsos.com/en-us/\\nnews-polls/trust-in-the-internet-2022\\n[13] E. Commission. (2022) 2018 code of practice on disin-\\nformation. [Online]. Available: https://digital-strategy.ec.\\neuropa.eu/en/library/2018-code-practice-disinformation\\n[14] Z. Gilani, R. Farahbakhsh, and J. Crowcroft, ‚ÄúDo\\nbots impact twitter activity?‚Äù in Proceedings of the\\n26th International Conference on World Wide Web\\nCompanion , ser. WWW ‚Äô17 Companion. Republic\\nand Canton of Geneva, CHE: International World\\nWide Web Conferences Steering Committee, 2017, p.\\n781‚Äì782. [Online]. Available: https://doi.org/10.1145/\\n3041021.3054255\\n[15] Z. Gilani, L. Wang, J. Crowcroft, M. Almeida, and\\nR. Farahbakhsh, ‚ÄúStweeler: A framework for twitter\\nbot analysis,‚Äù in Proceedings of the 25th International\\nConference Companion on World Wide Web , ser. WWW\\n‚Äô16 Companion. Republic and Canton of Geneva,\\nCHE: International World Wide Web Conferences\\nSteering Committee, 2016, p. 37‚Äì38. [Online]. Available:\\nhttps://doi.org/10.1145/2872518.2889360\\n[16] E. Ferreira Dos Santos, D. Carvalho, L. Ruback,\\nand J. Oliveira, ‚ÄúUncovering social media bots: a\\ntransparency-focused approach,‚Äù in Companion Proceed-\\nings of The 2019 World Wide Web Conference , 2019, pp.\\n545‚Äì552.\\n[17] Z. Chu, S. Gianvecchio, H. Wang, and S. Jajodia, ‚ÄúDe-\\ntecting automation of twitter accounts: Are you a human,\\nbot, or cyborg?‚Äù IEEE Transactions on dependable and\\nsecure computing , vol. 9, no. 6, pp. 811‚Äì824, 2012.\\n[18] A. Alarifi, M. Alsaleh, and A. Al-Salman, ‚ÄúTwitter turing\\ntest: Identifying social machines,‚Äù Information Sciences ,\\nvol. 372, pp. 332‚Äì346, 2016.\\n[19] C. A. Davis, O. Varol, E. Ferrara, A. Flammini, and\\nF. Menczer, ‚ÄúBotornot: A system to evaluate social\\nbots,‚Äù in Proceedings of the 25th international conference\\ncompanion on world wide web , 2016, pp. 273‚Äì274.\\n[20] S. Cresci, R. Di Pietro, M. Petrocchi, A. Spognardi,\\nand M. Tesconi, ‚ÄúThe paradigm-shift of social spambots:\\nEvidence, theories, and tools for the arms race,‚Äù in Pro-\\nceedings of the 26th international conference on world\\nwide web companion , 2017, pp. 963‚Äì972.\\n[21] M. Nasim, A. Nguyen, N. Lothian, R. Cope, and\\nL. Mitchell, ‚ÄúReal-time detection of content polluters\\nin partially observable twitter networks,‚Äù in Companion\\nProceedings of the The Web Conference 2018 , ser.\\nWWW ‚Äô18. Republic and Canton of Geneva, CHE:\\nInternational World Wide Web Conferences Steering\\nCommittee, 2018, p. 1331‚Äì1339. [Online]. Available:\\nhttps://doi.org/10.1145/3184558.3191574\\n[22] K.-C. Yang, P.-M. Hui, and F. Menczer, ‚ÄúBot election-\\neering volume: Visualizing activity during elections,‚Äùser. WWW ‚Äô19. New York, NY , USA: Association\\nfor Computing Machinery, 2019, p. 214‚Äì217. [Online].\\nAvailable: https://doi.org/10.1145/3308560.3316499\\n[23] K.-C. Yang and F. Menczer, ‚ÄúAnatomy of an ai-\\npowered malicious social botnet,‚Äù arXiv preprint\\narXiv:2307.16336 , 2023.\\n[24] E. Ferrara, ‚ÄúSocial bot detection in the age of chatgpt:\\nChallenges and opportunities,‚Äù First Monday , 2023.\\n[25] N. Ayoobi, S. Shahriar, and A. Mukherjee, ‚ÄúThe looming\\nthreat of fake and llm-generated linkedin profiles: Chal-\\nlenges and opportunities for detection and prevention,‚Äù in\\nProceedings of the 34th ACM Conference on Hypertext\\nand Social Media , 2023, pp. 1‚Äì10.\\n[26] B. Grimme, J. Pohl, H. Winkelmann, L. Stampe, and\\nC. Grimme, ‚ÄúLost in transformation: Rediscovering llm-\\ngenerated campaigns in social media,‚Äù in Multidisci-\\nplinary International Symposium on Disinformation in\\nOpen Online Media . Springer, 2023, pp. 72‚Äì87.\\n[27] A. Bessi and E. Ferrara, ‚ÄúSocial bots distort the 2016 u.s.\\npresidential election online discussion,‚Äù First Monday ,\\nvol. 21, no. 11, Nov. 2016. [Online]. Available: https:\\n//firstmonday.org/ojs/index.php/fm/article/view/7090\\n[28] D. Arnaudo, ‚ÄúComputational propaganda in brazil: Social\\nbots during elections,‚Äù 2017.\\n[29] M. Zhang, X. Qi, Z. Chen, and J. Liu, ‚ÄúSocial bots‚Äô\\ninvolvement in the covid-19 vaccine discussions on twit-\\nter,‚Äù International Journal of Environmental Research\\nand Public Health , vol. 19, no. 3, p. 1651, 2022.\\n[30] S. Hurtado, P. Ray, and R. Marculescu, ‚ÄúBot detection\\nin reddit political discussion,‚Äù in Proceedings of the\\nFourth International Workshop on Social Sensing , ser.\\nSocialSense‚Äô19. New York, NY , USA: Association\\nfor Computing Machinery, 2019, p. 30‚Äì35. [Online].\\nAvailable: https://doi.org/10.1145/3313294.3313386\\n[31] R. Kenny, B. Fischhoff, A. Davis, K. M. Carley, and\\nC. Canfield, ‚ÄúDuped by bots: why some are better than\\nothers at detecting fake social media personas,‚Äù Human\\nfactors , vol. 66, no. 1, pp. 88‚Äì102, 2024.\\n[32] Z. Guo, ‚ÄúOnline disinformation and generative language\\nmodels: Motivations, challenges, and mitigations,‚Äù in\\nCompanion Proceedings of the ACM on Web Conference\\n2024 , 2024, pp. 1174‚Äì1177.\\n[33] E.-U. Haq, Y . Zhu, P. Hui, and G. Tyson, ‚ÄúHistory\\nin making: Political campaigns in the era of artificial\\nintelligence-generated content,‚Äù in Companion Proceed-\\nings of the ACM on Web Conference 2024 , 2024, pp.\\n1115‚Äì1118.\\n[34] X. Zhang and A. A. Ghorbani, ‚ÄúAn overview of online\\nfake news: Characterization, detection, and discussion,‚Äù\\nInformation Processing & Management , vol. 57, no. 2,\\np. 102025, 2020.\\n[35] R. Gorwa and D. Guilbeault, ‚ÄúUnpacking the social\\nmedia bot: A typology to guide research and policy,‚Äù\\nPolicy & Internet , vol. 12, no. 2, pp. 225‚Äì248, 2020.\\n[Online]. Available: https://onlinelibrary.wiley.com/doi/\\nabs/10.1002/poi3.184[36] X. (2024) X‚Äôs automation development rules. [On-\\nline]. Available: https://help.x.com/en/rules-and-policies/\\nx-automation\\n[37] sodypop. (2024) Bottiquette. [Online]. Available: https:\\n//www.reddit.com/r/reddit.com/wiki/bottiquette/\\n[38] Mastodon. (2024) Code of conduct. [Online]. Available:\\nhttps://explore.mastodon.bot/rules\\n[39] M. T. Center. (2024) Account integrity and\\nauthentic identity. [Online]. Available: https:\\n//transparency.meta.com/policies/community-standards/\\naccount-integrity-and-authentic-identity/\\n[40] TikTok. (2024) Community Guidelines. [Online]. Avail-\\nable: https://www.tiktok.com/community-guidelines/en\\n[41] L. Help. (2023) Prohibited software and extensions.\\n[Online]. Available: https://www.linkedin.com/help/\\nlinkedin/answer/a1341387\\n[42] X. H. Center. Not a bot. [Online]. Available: https:\\n//help.x.com/en/using-x/not-a-bot\\n[43] M. N. Nicholson, B. C. Keegan, and C. Fiesler,\\n‚ÄúMastodon rules: Characterizing formal rules on popular\\nmastodon instances,‚Äù in Companion Publication of the\\n2023 Conference on Computer Supported Cooperative\\nWork and Social Computing , ser. CSCW ‚Äô23 Companion.\\nNew York, NY , USA: Association for Computing\\nMachinery, 2023, p. 86‚Äì90. [Online]. Available: https:\\n//doi.org/10.1145/3584931.3606970\\n[44] P. Juneja, D. Rama Subramanian, and T. Mitra,\\n‚ÄúThrough the looking glass: Study of transparency\\nin reddit‚Äôs moderation practices,‚Äù Proc. ACM Hum.-\\nComput. Interact. , vol. 4, no. GROUP, jan 2020.\\n[Online]. Available: https://doi.org/10.1145/3375197\\n[45] C. Kiene and B. M. Hill, ‚ÄúWho uses bots? a\\nstatistical analysis of bot usage in moderation teams,‚Äù\\ninExtended Abstracts of the 2020 CHI Conference\\non Human Factors in Computing Systems , ser. CHI\\nEA ‚Äô20. New York, NY , USA: Association for\\nComputing Machinery, 2020, p. 1‚Äì8. [Online]. Available:\\nhttps://doi.org/10.1145/3334480.3382960\\n[46] M. T. Center. How meta enforces its policies. [Online].\\nAvailable: https://transparency.meta.com/enforcement\\n[47] TikTok. Supporting responsible, transparent ai-generated\\ncontent. [Online]. Available: https://www.tiktok.com/\\ntransparency/en-us/\\n[48] W. S. Journal. Inside tiktok‚Äôs algo-\\nrithm: A wsj video investigation. [On-\\nline]. Available: https://www.wsj.com/articles/\\ntiktok-algorithm-video-investigation-11626877477\\n[49] J. Gabor, ‚ÄúThe tiktok algorithm is good, but is it too\\ngood? exploring the responsibility of artificial intel-\\nligence systems reinforcing harmful ideas on users,‚Äù\\nCatholic University Journal of Law and Technology ,\\nvol. 32, no. 1, pp. 109‚Äì144, 2023.\\n[50] C2PA. Overview. [Online]. Available: https://c2pa.org/\\n[51] OpenAI. (2024) Openai models. [Online]. Available:\\nhttps://platform.openai.com/docs/models',\n",
       " 'Examining the Role of Relationship Alignment in Large Language Models\\nKristen M. Altenburger‚àó1, Hongda Jiang*1, Robert E. Kraut2, Yi-Chia Wang3, Jane Dwivedi-Yu1\\n1Meta\\n2Carnegie Mellon University\\n3Stanford University\\nAbstract\\nThe rapid development and deployment of Generative AI in\\nsocial settings raise important questions about how to op-\\ntimally personalize them for users while maintaining accu-\\nracy and realism. Based on a Facebook public post-comment\\ndataset, this study evaluates the ability of Llama 3.0 (70B)\\nto predict the semantic tones across different combinations\\nof a commenter‚Äôs and poster‚Äôs gender, age, and friendship\\ncloseness and to replicate these differences in LLM-generated\\ncomments.\\nThe study consists of two parts: Part I assesses differences\\nin semantic tones across social relationship categories, and\\nPart II examines the similarity between comments generated\\nby Llama 3.0 (70B) and human comments from Part I given\\npublic Facebook posts as input. Part I results show that in-\\ncluding social relationship information improves the ability\\nof a model to predict the semantic tone of human comments.\\nHowever, Part II results show that even without including so-\\ncial context information in the prompt, LLM-generated com-\\nments and human comments are equally sensitive to social\\ncontext, suggesting that LLMs can comprehend semantics\\nfrom the original post alone. When we include all social re-\\nlationship information in the prompt, the similarity between\\nhuman comments and LLM-generated comments decreases.\\nThis inconsistency may occur because LLMs did not include\\nsocial context information as part of their training data. To-\\ngether these results demonstrate the ability of LLMs to com-\\nprehend semantics from the original post and respond sim-\\nilarly to human comments, but also highlights their limita-\\ntions in generalizing personalized comments through prompt-\\ning alone.\\nIntroduction\\nOver the past few years, large language models (LLMs)\\n(Brown 2020; Touvron et al. 2023; Zhao et al. 2023) have\\nshown remarkable capabilities across various NLP tasks\\nand enabled applications that were once thought unfeasible\\n(Chang et al. 2024). In particular, many of these applications\\nconsider LLMs as conversational agents to interact with hu-\\nmans. However, LLMs are trained to memorize vast corpora\\nin social isolation (Krishna et al. 2022) with the goal to pre-\\ndict the next token given the input context but not to con-\\nverse with humans (Brown 2020). Therefore, despite their\\n*Equal contributions.extraordinary ability to produce content mimicking human-\\ngenerated content, it does not follow that LLMs incorporate\\nsocial capabilities and can respond to humans in a way that\\nconforms to social expectations. In fact, researchers have\\ndiscovered LLMs can exhibit abnormal behaviors and gen-\\nerate improper responses, including but not limited to bias,\\nstereotypes, toxicity, misinformation, or hallucination (Wei-\\ndinger et al. 2021).\\nTo address these problems, there is an emerging research\\nfield that studies whether and to what extent LLMs can\\nmimic human behaviors and aims to align LLMs with user\\nexpectations (Shen et al. 2023). For example, some scholars\\nfocus on evaluating and improving LLM safety (Anwar et al.\\n2024; Ge et al. 2023) and the alignment with human values\\n(Hendrycks et al. 2020), such as social norms and morality\\n(Ziems et al. 2023; Xu et al. 2023). Others examine the opin-\\nions reflected by LLMs and explore techniques to align the\\nopinions they produce with those of different demographic\\npopulations (Bakker et al. 2022; Santurkar et al. 2023; Dur-\\nmus et al. 2023). Moreover, for LLMs to exhibit consistent\\nbehaviors in dialogue-based interactions, prior work has in-\\nvestigated approaches to personalize LLMs by conditioning\\ntheir responses on a certain persona, i.e., personality or pro-\\nfile information (Tseng et al. 2024; Caron and Srivastava\\n2022; Serapio-Garc ¬¥ƒ±a et al. 2023).\\nDespite recent efforts on LLM alignments for more\\nhuman-like behaviors, at least one significant research gap\\nremains ‚Äì aligning LLM-generated content with social re-\\nlationships . There is an extensive social science literature\\nfocusing on how social factors in relationships, such as age,\\ngender, and friendship tie strength, influence the language\\npeople use with each other and its semantic tone (e.g., humor\\nand emotional support) (Wellman and Wortley 1990; Dindia\\nand Allen 1992; Shriver, Nair, and Hofstetter 2013; Zeng\\nand Wei 2013; Wang, Burke, and Kraut 2016; Hall 2017;\\nGreengross, Silvia, and Nusbaum 2020). Thus, for LLMs\\nto successfully produce responses that mimic human-human\\ncommunication, it is important to incorporate social rela-\\ntionship information as context. However, existing research\\nhas not delved into the impact of social relationships on\\nLLM-generated content or how to condition LLM-generated\\ncontent on the relationship between communication part-\\nners. This paper aims to bridge the gap by proposing the\\nfirst in-depth exploration of the role of social relationshipsarXiv:2410.01708v1  [cs.CL]  2 Oct 2024in human-LLM dyadic interactions. We address one central\\nquestion: to what extent one can steer LLMs with social re-\\nlationship information so their responses are aligned with\\nthe content produced by human-human communication?\\nTo answer the research question, we conduct two stud-\\nies. We start with analyzing real-world public human-human\\ncommunication data on Facebook and then examine whether\\nLLMs can generate similar content when prompted with\\nsocial relationship information. Specifically, Study 1 repli-\\ncates and extends prior research by examining the rela-\\ntionship between social context variables (age, gender, and\\nfriendship strength) and the semantic tone of public post-\\ncomment exchanges between people on Facebook. This em-\\npirical research demonstrates that these social factors influ-\\nence not only the exchange of emotional support, humor,\\nself-disclosure, and toxic comments, which have been ex-\\namined in prior research (Wellman and Wortley 1990; Din-\\ndia and Allen 1992; Wang, Burke, and Kraut 2016; Burke\\net al. 2007; Xu, Liu, and Liu 2024; Hall 2017), but also un-\\nderstudied semantic tones, like surprise, worry, anxiety, and\\nthankfulness. Study 2 examines whether incorporating so-\\ncial relationship information in the prompt can steer LLM\\ncomments to more align with human ones and better reflect\\nthe associations between semantic tone and social relation-\\nship variables in human comments as reported in the first\\nstudy. Results suggest that LLM-generated comments are\\nquite similar to human-generated ones to the same post, but\\nthat explicitly referring to social context to tune the LLMs\\ndoes not increase their similarity to human comments.\\nLiterature Review and Hypothesis\\nPersona Alignment\\nThere has been extensive work in studying and improving\\nthe ability of large language models to act in accordance\\nwith human intentions, values, and ethical principles, termed\\nalignment in language models. Persona alignment, which in-\\nvolves tailoring the behavior and responses of AI systems to\\nmatch specific personas, has become a growing area of inter-\\nest in the development of conversational AI. Several studies\\nhave investigated the capability of language models to im-\\nitate personas through fine-tuning based on their personal\\nattributes like age, occupation, and interests (Zhang 2018;\\nLi et al. 2016; Shao et al. 2023; Lee, Oh, and Lee 2023) and\\nhave shown practical improvements when conversing with\\nchatbots when incorporating these personal attributes into\\ntheir prompts, such as increases in user engagement with\\nthem (Shuster et al. 2022) or a greater capacity for them to\\nproduce empathetic responses (Zhong et al. 2020). As LLMs\\nhave become more powerful, prompting has become a fea-\\nsible and cost-effective way of aligning responses, and stud-\\nies that evaluate persona alignment of recent state-of-the-art\\nlanguage models find high fidelity across multiple persona\\ntypes in the context of fictitious personas (Wang et al. 2024;\\nSerapio-Garc ¬¥ƒ±a et al. 2023; Caron and Srivastava 2023; Zhou\\net al. 2023; Olea et al. 2024; Njifenjou et al. 2024).Audience Alignment\\nWhile persona alignment focuses on the capacity of the\\nLLM to emulate a given persona, more recently, researchers\\nhave attempted to craft LLMs that are aligned with charac-\\nteristics of the recipient of messages (i.e., audience align-\\nment ), not just the producer. For example, in a series of ex-\\nperiments Matz et al. (2024) showed that messages crafted\\nby ChatGPT-3 that were tailored to the personality of the re-\\ncipient were significantly more influential than non-tailored\\nmessages. Choi et al. (2024) built and tested Proxona , which\\nhelps content creators match content to a target audience by\\nusing the history of audience comments as the basis for cre-\\nating audience-member personas (e.g., a practical urban gar-\\ndener).\\nRelationship Alignment\\nOur research attempts to extend the notions of persona and\\naudience alignment and introduce relationship alignment , in\\nwhich characteristics of the speaker and audience are con-\\nsidered simultaneously. The relationship between commu-\\nnication partners could include a match or mismatch in de-\\nmographic characteristics (e.g., talking to someone of the\\nsame or different gender or age) or the nature of the tie\\nstrength between them (e.g., strangers versus friends). Given\\nthe promising results from prior work in persona and audi-\\nence alignment, we expect that including social relationship\\ninformation, such as the demographic information and the\\nrelationship between two users, is likely to improve the sim-\\nilarity of LLM-generated comments to human comments.\\nHypothesis: Including social relationship information\\nin its prompt will produce LLM-generated comments\\nthat are more similar to human ones.\\nMethodological Advances This work makes two contri-\\nbutions. First, the prior work on persona and audience align-\\nment shows that social context information improves LLM-\\ngenerated content at the group level (e.g., language that\\nsounds more like a woman or is more convincing to ex-\\ntroverts). We are the first to look at the influence of social\\ncontext information at the individual level (i.e., the simi-\\nlarity between a single human comment and its matched\\nLLM-generated comment). Second, while the LLM align-\\nment literature usually evaluates model output against user\\nsurveys, we compare LLM-generated content against real\\nhuman data.\\nOperationalization of Social Relationship There is an\\nextensive literature focusing on how social context influ-\\nences people‚Äôs language, specifically its semantic tones. For\\nexample, close friends and family members use more in-\\ntimate and supportive language with each other than do\\nweak ties (Wang, Burke, and Kraut 2016); acquaintances\\nare more helpful for idea generation than strong ties (Burt\\n2004). Dindia and Allen (1992) found women self-disclose\\nmore than men when talking to friends but not when talk-\\ning to strangers. Wellman and Wortley (1990) found that\\nwomen provide more emotional support than men and the\\nexchange of support increases with tie strength. Greengross,Silvia, and Nusbaum (2020) found that given the same stim-\\nulus, men produce funnier comments than women. Hall\\n(2017) found that romantic partners with more satisfying re-\\nlationships exchanged more positive humor while those with\\nlower satisfaction exchanged more negative humor. Based\\non the prior work, we decide to focus on three types of social\\nrelationship between the Facebook poster and commenter,\\nage-age, gender-gender, and tie strength , and examine their\\ninfluence on LLM-generated content.\\nData Setup\\nWe analyze public conversations on Facebook Feed where\\nusers can make posts and share content with others1. When\\na user, referred to as the ‚Äúposter,‚Äù shares public content on\\nFeed, any Facebook user can respond to the post by leav-\\ning a public comment, which can be seen by all users. This\\ntwo-way interaction between the poster and the commenter\\nrepresents a conversation. More than one commenter can re-\\nspond to a post, which we will account for in this analysis.\\nTo protect user privacy, we de-identified all user informa-\\ntion and removed personal data such as names and phone\\nnumbers from the text before applying a comment semantic\\nclassifier and a LLM for comment generation. This ensured\\nthat no sensitive information was included in the analysis.\\nComment and Posts on Feed: Our empirical dataset\\ncomprises approximately 665k public Facebook (FB) posts\\nand comments created on May 25, 2024 by active U.S. FB\\nusers at least 18 years old. For reasons of privacy, we re-\\nstrict the sample to English posts on News Feed that any\\nuser can comment upon. We exclude from the analysis ads,\\ngroup posts, and reshared posts. Because Feed Ranking can\\ninfluence the types of posts a user views and the types of\\nconversations they participate in, we control for the likeli-\\nhood of a user to comment on a given post in this analysis.\\nSocial Context: We evaluate the following social con-\\ntexts: stated gender, stated age, and friendship closeness\\nbetween the commenter and poster. Stated gender and age\\nmeans we rely on user-provided data. Because our data can-\\nnot distinguish users who provide no gender information\\nfrom those who identify as non-binary, we exclude these two\\ngroups from the analysis and include only users who stated\\ntheir gender as male or female. For the analysis, we scale\\ncommenter/poster age to have mean 0 and standard devia-\\ntion 1. We include interaction terms between commenter‚Äôs\\ngender and poster‚Äôs gender and interaction terms between\\ncommenter‚Äôs age and poster‚Äôs age. This allows for the effect\\nof a commenter‚Äôs gender or age to vary depending on the\\nposter‚Äôs gender or age. We encode commenter/poster gender\\nas female or male and include interaction terms (i.e., women\\ntalking to a male poster and vice versa). Finally, we account\\nfor the relationship between two pairs of individuals. We en-\\ncode users as strangers or friends. Among friends, we code\\ntwo users as close friends or non-close friends. Friendship\\ncloseness is a model-based score that predicts relationship\\ncloseness between two individuals (Burke and Kraut 2014).\\nWe treat friendship as a categorical feature, where we code\\nwhether two users are close friends (1), non-close friends\\n1https://transparency.meta.com/features/ranking-and-content/GenderFemale Commenter Female Poster 50%\\nMale Commenter Male Poster 20%\\nMale Commenter Female Poster 10%\\nFemale Commenter Male Poster 20%\\nAgeAvg. Commenter Age 50\\nAvg. Poster Age 50\\nFriendshipClose Friends 45%\\nFriends 45%\\nStranger 10%\\nTable 1: Descriptive statistics for the social context features\\nin our sample of Facebook posters and comments.We intro-\\nduce random noise to the social context signals. For gender,\\nwe introduce Gaussian noise with mean 0 and standard de-\\nviation of 1. For friendship and gender, we inject noise by\\nrandomly changing the value of the feature for 10% of ob-\\nservations. We then round to the nearest 10 for gender and\\nage, and to the nearest 5 for friendship.\\n(2), or strangers (3). This coding allows for non-linear rela-\\ntionships between friendship closeness and semantic tones.\\nTable 1 presents the descriptive statistics for the social con-\\ntext variables used in the regression models predicting se-\\nmantic tone.\\nGenerated Comments: We randomly selected 10,000\\npost and comment pairs from the empirical dataset. We then\\nprompted Llama 3.0 (70B) to generate comments for each\\npost under three conditions: without social context informa-\\ntion, with a subset of social context information, and with\\nall social context information included in the prompt. The\\nprompts are detailed in the Appendix ‚ÄúLLM prompt‚Äù sec-\\ntion. Figure 1 provides examples of the prompts and cor-\\nresponding generated comments. We also tested different\\nprompt techniques such as chain-of-thoughts (Kojima et al.\\n2022), which did not change our main conclusion (see ‚ÄúPart\\nII: Prompt engineering‚Äù section in Appendix).\\nSemantic Tone of Comments: We rely on a semantic\\ntone classifier to label comments across different seman-\\ntic categories. This model relies on crowd-sourced labeled\\ntraining data. We limit our analysis to eight semantic tones‚Äì\\nSurprised, Funny, Insult, Provide Emotional Support, Self-\\nDisclosure, Worried, Thankful, Relaxed. Definitions and ex-\\namples are listed in Appendix Figure 5. We selected these\\nbecause they are likely to change the emotional state of\\nthe person being responded to, either because they them-\\nselves reflect the emotional content of the comment (e.g.,\\nsurprised, relaxed) or contain an action that is likely to af-\\nfect the poster‚Äôs emotional state (e.g., insult or provide emo-\\ntional support). They reflect a diversity of the semantic tones\\nthat are common in Facebook exchanges. We dropped tones\\nif they were highly correlated with another one (e.g., we\\nincluded Provide Emotional Support, but dropped Provide\\nInformational Support because they were highly negatively\\ncorrelated r=‚àí.80). We use the text-only version of the\\nmodel, which only takes text as input and handles process-\\ning such as removing user mentions in the comment. We ap-\\nply this model to comment text and generate semantic tone\\nscores across the eight categories described above. Through-Post \\nI‚Äôm glad I finally asked her, and \\nshe said yes! \\n[PERSON] and I got engaged. \\nHuman comment \\nCongratulations to  [PERSON] \\nand [PERSON]! - you don‚Äôt \\nknow me personally, but your \\naunt [PERSON] is my best \\nfriend and your dad was my \\nson‚Äôs favorite football coach. \\nWish you both all the best !!!\\nüéâ\\nPrompt without social context \\nYou are a Facebook user and want to comment  \\non a post from another Facebook user (the  \\nposter) in the most proper way based on the  \\nsocial relationship between you and the poster.  \\nHere is the post made by the poster on  \\nFacebook:  \\n[Post]: I‚Äôm glad I finally asked her, and she said \\nyes! [PERSON] and I got engaged. \\nGenerate a short comment right below the post,  \\nwith \"[Comment]:\" prefixing the comment, in one  \\nparagraph. No explanation needed. \\nLLM comment \\nHuge congratulations to you both! So thrilled to  \\nhear the amazing news! Wishing you a lifetime  \\nof love, happiness, and adventure together! Prompt with social context \\nYou are a Facebook user and want to comment on a  \\npost from another Facebook user (the poster) in the  \\nmost proper way based on the social relationship  \\nbetween you and the poster. Here is the post made by  \\nthe poster on Facebook:  \\n[Post]: I‚Äôm glad I finally asked her, and she said yes! \\n[PERSON] and I got engaged. \\nRemember that you and the poster are friends but not  \\ninteract with each other much. \\nAlso remember that you are a female of age 68 and the  \\nposter is a male of age 40.  \\nGenerate a short comment right below the post, with  \\n\"[Comment]:\" prefixing the comment, in one paragraph.  \\nNo explanation needed. \\nLLM comment \\nOh, congratulations to you and your fianc√©e! I\\'m  \\nbeyond thrilled to hear this wonderful news! Wishing  \\nyou both a lifetime of love, happiness, and adventure  \\ntogether! Figure 1: Example of LLM prompt without and with social context information, paraphrased human comment, and LLM\\ngenerated comments from prompts without and with social context information.\\nout this analysis, these scores will represent the outcome\\nvariable.\\nPost Topic Classifications: We finally account for the\\ntopic of the post in this analysis like whether a post is about\\n‚ÄúSports‚Äù or ‚ÄúChildren & Parenting‚Äù. We rely on a topic\\nmodeling-like approach for categorizing posts into 27 dif-\\nferent high-level categorizations. We dropped two categories\\nthat had a small number of comments.\\nBroader perspective and ethical\\nconsiderations.\\nAn internal research board reviewed the study‚Äôs research\\nethics and privacy practices prior to its start. In agreeing to\\nits terms of service, Facebook users allow the type of anal-\\nysis done in this paper to better understand how people use\\nMeta products, to improve the products, and to promote their\\nsafety, security, and integrity, including combating harmful\\nuser conduct. To preserve users‚Äô privacy, data consists of de-\\nidentified comments collected only from public posts. When\\nwriting up this paper, to preserve privacy we paraphrase ex-\\namples so that they could not be found through an internet\\nsearch (Bruckman 2002).\\nPart I: How Comment Semantic Tones on\\nFacebook Varies Across Social Context\\nThe first study aims to assess social relationships in the con-\\ntext of Facebook comments and examines a diverse set of se-\\nmantic tones. This analysis determines the significance and\\ndirectionality of various social context variables in predict-\\ning comment semantic tone. These will inform Part II, where\\nwe investigate whether LLMs can similarly leverage socialcontext to better align its semantic tone with that of the hu-\\nman comment.\\nWe fit a series of linear mixed effects regression mod-\\nels to predict the extent a comment expresses each seman-\\ntic tone (surprised, funny, insult, provide emotional sup-\\nport, self-disclosure, worried, thankful, and relaxed). We\\nlog-transformed the comment semantic tones to deal with\\nlong-tailed distributions, except for the ‚Äúrelaxed‚Äù tone where\\nthe transformation was not needed. The predictors include\\nsocial relationship information between the poster and com-\\nmenter (age, gender, and friendship strength) along with post\\ntopic category. We allow random intercepts for post IDs. Fi-\\nnally, we applied an inverse propensity weighting (IPW) cor-\\nrection to account for differences in a particular user‚Äôs like-\\nlihood to comment on a particular post, which we refer to as\\np(comment) . For IPW, due to small probability weights, we\\nset a maximum IPW weight of 10.\\nSignificance of Social Context Signals : We first eval-\\nuate whether incorporating social context information sig-\\nnificantly enhances the model‚Äôs performance compared to\\na baseline model. We test social context features individu-\\nally and together, which we compare to a baseline model\\nthat includes post topics only. This comparison is done via\\na likelihood ratio test. Appendix Table 4 shows that adding\\nsocial context signals compared to a baseline model leads to\\na statistically significant improvement in model fit for all se-\\nmantic tones. In the Appendix Table 5, we also show that a\\nfull model with all social context signals outperforms a base-\\nline model that includes just one social context signal. These\\nfindings suggest that social context signals are valuable to\\nsemantic prediction tasks. Based on these results, we antic-\\nipate that LLM tasks for comment generation will benefitFigure 2: Multi-level regression results from Part I predicting semantic tone outcomes from social context variables and post\\ntopic (not shown). Post ID is included as a random effect, since multiple comments can be nested under a single post. The\\nconditional R2represents the variation explained by both fixed and random effects, while marginal R2represents the variation\\nexplained by fixed effects only. *** p <0.001.\\nfrom knowing the social context of the conversation, which\\nwe‚Äôll explore in Part II.\\nInterpretation of Social Context Signals: Figure 2, re-\\nports the results of the regression analyses predicting seman-\\ntic tones from social relationship variables. Below we pro-\\nvide a brief interpretation of the results:\\n‚Ä¢ Gender: Male commenters and posters are less likely to\\ncreate comments that are surprised, provide emotional\\nsupport, self-disclose or are thankful compared to female\\ncommenters; male commenters and posters are more\\nlikely to create comments that are funny, have insults, are\\nworried or relaxed. When interacting, male commenters\\nand male posters have comments that have more self-\\ndisclosure, more worry, and more relaxedness.\\n‚Ä¢ Age: Older commenters are less likely to create com-\\nments that are surprised, funny, have insult, self-disclose,\\ncontain worry, or are relaxed; they are more likely to\\ncreate comments that provide emotional support and\\nare thankful compared to younger commenters. Older\\nposters are less likely to receive comments that are funny,\\nhave insult, worry, or are relaxed; they are more likely to\\nreceive comments with surprise, emotional support, and\\nthankfulness. When interacting, older commenters and\\nolder posters have comments that are more funny, more\\ninsultive, more self-disclosure, more worry, and more re-\\nlaxed.\\n‚Ä¢ Friendship Type: Compared to close friends, strangers\\nare less likely to create comments that are surprised,\\nfunny, have insults, self-disclose or are relaxed; strangers\\nare more likely to create comments with thankfulness.\\nCompared to close friends, non-close friends are less\\nlikely to create comments that are funny, have insults,\\nprovide emotional support or are relaxed; non-close\\nfriends are more likely to create comments with surprise,\\nself-disclose, worry, or are thankful.\\nMany of these associations replicate results from the prior\\nliterature (e.g., men provide less emotional support thanwomen). However, the associations are inconsistent with the\\nprior literature for some social characteristic/tone combina-\\ntions. Although not the focus of the current research, some\\ninconsistencies are worth exploring in follow-up research.\\nFor example, people self-disclose less when commenting on\\na close friend‚Äôs post than when responding to strangers. It is\\npossible that this inconsistency results from the public na-\\nture of post and comment pairs used in the current study;\\npeople may not want to reveal intimate personal informa-\\ntion even when responding to close friends because their re-\\nsponses are open to the world to see.\\nFigure 3: Mixed effect regressions predicting the log-\\ntransformed sentiment scores of LLM generated comments\\nfrom the tone of human comments with and without social\\ncontext information. The dashed line indicates perfect pre-\\ndiction, where the tone of generated comments is perfectly\\npredicted from the tone of human comments. The gray line\\nshows fitted results for LLM comments without social con-\\ntext, and the blue line shows fitted results for LLM com-\\nments with social context. The left panel with results for\\nthe ‚Äúinsult score‚Äù shows that including social context in the\\nprompt slightly increased the similarity between the human\\nand generated comments. In contrast, the right panel with\\nresults for the ‚Äúself-disclosure score‚Äù shows that including\\nthe context decreases the similarity.Tone Surprised Funny InsultEmotional\\nsupportSelf\\ndisclosureWorried Thankful Inclusion Average\\nno-context 0.309 0.518 0.436 0.312 0.425 0.585 0.382 0.465 0.429\\nall-context 0.325 0.529 0.456 0.284 0.360 0.543 0.373 0.431 0.413\\ndifference ‚àÜÀÜŒ≤0.016‚àó0.011 0.020‚àó‚àó‚àó‚àí0.028‚àó‚àó‚àó‚àí0.065‚àó‚àó‚àó‚àí0.042‚àó‚àó‚àó‚àí0.009 ‚àí0.034‚àó‚àó‚àó‚àí0.016\\nTable 2: Regression coefficients predicting the sentiment-scores of the LLM-generated comments from human-generated com-\\nments when social context was (all-context) or was not (no-context) included in the prompt. The difference row represents the\\nfitting coefficients for human-comment √óall-context interaction, ‚àÜÀÜŒ≤. *p <0.05, **p <0.01, *** p <0.001\\nPart II: Does providing social context\\ninformation help to generate human-like\\ncomments?\\nIn the second part of the study, we evaluate LLM‚Äôs ability\\nto leverage social context information to generate human-\\nlike comments. We employ two innovative approaches to as-\\nsess the similarity between the tone of human comments and\\nthe tone of generated comments. Approach 1 is the individ-\\nual comment-level analysis that directly compares the tone\\nof individual human comments and their matched LLM-\\ngenerated comments when social context was or was not\\nincluded in the LLM prompt. Approach 2 is an aggregate\\nanalysis examining whether the relationship between social\\ncontext variables and the semantic tone of LLM-generated\\ncomments more closely replicates the relationships shown in\\nStudy 1 when social context was or was not included in the\\nLLM prompt. In the Appendix, we demonstrated the con-\\nnection and difference between those two approaches using\\na simple mathematical model (see ‚ÄúPart II: Comparison be-\\ntween Approach 1 and Approach 2‚Äù section).\\nApproach 1: Individual-level comparisons\\nFirst, we employ a linear mixed effects regression model\\nto examine the relationship between the tones of generated\\ncomments and those of human comments, while controlling\\nfor the type of social context presented in the prompt. The\\nmodel specification is:\\nllm_sentiment_score\\nÀú human_sentiment_score + all_context\\n+ human_sentiment_score x all_context\\nThe fitted coefficient for ‚Äúhuman sentiment score‚Äù quan-\\ntifies how closely the tone of generated comments aligns\\nwith the tone of human comments, thereby measuring the\\nsimilarity between the two. The predictor ‚Äúall context‚Äù\\nis a binary factor indicating whether the generated com-\\nment was produced with a no-context prompt (coded 0) or\\nan all-context prompt (coded 1). The interaction between\\nthe ‚Äúall context‚Äù variable and the ‚Äúhuman sentiment score‚Äù\\ntests whether the similarity between the tone of the human\\nand generate comments changes when social context infor-\\nmation is added to the prompt,\\n‚àÜÀÜŒ≤‚â°ÀÜŒ≤human sentiment score√óallcontext . (1)\\nIn Figure 3, this is represented as the difference in the slope\\nof the grey line reflecting the similarity between the human\\nand LLM comments for the no-context condition and slopeof the blue line for the all-context condition. In Table 2, we\\nreport the fitting coefficients for all eight semantic tones.\\nAs shown in Figure 3 and Table 2, when no context infor-\\nmation is included in the prompt, the tones of the generated\\ncomments are moderately similar to the tone of the original\\nhuman comments, with the similarity coefficients ÀÜŒ≤ranging\\nfrom .309 for surprised to .585 for worried (mean ÀÜŒ≤=.429,\\nmean se = .0072, p <.001). Remember that when both the\\nindependent and dependent variables in a regression are log\\ntransformed, one interprets the coefficient as the percent in-\\ncrease in the dependent variable for every 1% increase in the\\nindependent variable. Thus these results indicate that on av-\\nerage across the eight semantic tones, when the tone of the\\nhuman comment increases by 1%, the tone of the generated\\ncomment increases by .43%.\\nThe human comment tone √óall-context interaction tests\\nwhether adding context information to the prompt changes\\nthe similarity between the human comment and the LLM\\ngenerated one. Adding context information to the prompt\\nstrengthens the similarity for surprised ( ‚àÜÀÜŒ≤=.016) and in-\\nsult (‚àÜÀÜŒ≤=.020), but reduces the similarity for four others\\n(providing emotional support ‚àÜÀÜŒ≤=‚àí.028; self-disclosure\\n‚àÜÀÜŒ≤=‚àí.065; worried ‚àÜÀÜŒ≤=‚àí.042; inclusion ‚àÜÀÜŒ≤=\\n‚àí.034) and had no reliable effect on funny ( ‚àÜÀÜŒ≤=.011)\\nor thankful ( ‚àÜÀÜŒ≤=‚àí.009). Overall, the similarity between\\nthe human and generated comments was marginally reduced\\nwhen the prompt includes the social context information\\n(difference = -.016, p <.10). This suggests that the majority\\nof the similarity between generated and human comments\\ncan be attributed to the post text itself. In contrast, adding\\nsocial context has only a small and inconsistent impact on\\nthe similarity with individual human comments.\\nApproach 2: Group-level comparison\\nWhile Approach 1 investigates whether providing social\\ncontext to the LLM enables it to generate comments that are\\nmore similar to human comments to the same posts, here\\nin Approach 2 we address whether LLM can replicate the\\ndependency of comments on social context shown in Study\\n1. That is, we expanded Study 1 by applying linear mixed\\neffects models to predict the semantic tones of LLM- com-\\nments generated in response to various prompts. We ana-\\nlyzed five different prompts: (1) a basic prompt with no so-\\ncial context (llm-no-context); (2) a prompt that includes the\\nages of both the poster and the commenter (llm-age-only);\\n(3) a prompt that specifies the genders of the poster andFigure 4: Regression coefficients of social context factors in the fitted models for log-transformed insult score (left panel)\\nand self-disclosure score (right) based on Approach 2. We compare human comments (red), generate comments when provided\\nsocial context (blue), and generated comments without provided social context (gray). The horizontal bar shows 95% confidence\\ninterval. Plots for other semantic tones are shown in Figure 6 in the Appendix.\\ncommenter (llm-gender-only); (4) a prompt that indicates\\nthe friendship status, i.e.,‚Äústranger‚Äù, ‚Äúnon-close friends‚Äù, or\\n‚Äúclose friends‚Äù (llm-friendship-only); (5) a comprehensive\\nprompt that incorporates full context (llm-all-context). Each\\nmodel uses the social relationship variables shown in Ta-\\nble 2,, but predicts the tones of the LLM-generated com-\\nments rather than the tones of human comments.\\nAs an illustration of the analysis and results, Figure\\n4 compares the fitted coefficients from models of log-\\ntransformed insult scores and self-disclosure scores for hu-\\nman comments, LLM comments with full social context,\\nand LLM comments without social context. Plots for other\\nsemantic sentiment scores can be found in Figure 6 in the\\nAppendix. We observe a consistent trend: while adding so-\\ncial context to prompts significantly alters the tone of gen-\\nerated comments, it also widens the disparity between the\\nregression coefficients of generated comments and those of\\nhuman comments. For instance, incorporating full context\\ninto the prompt significantly alters the influence of gen-\\nder and friendship status on the self-disclosure score ( p‚â§\\n0.001). However, this adjustment does not make the asso-\\nciation of gender and friendship with self-disclosure in the\\ngenerated comments more similar to their association in the\\nhuman comments. In fact, the strong association with friend-\\nship status in the generated comments causes the model to\\ndiverge further from the regression model used for human\\ncomments. Among the 64 regression coefficients (8 social\\ncontext variables √ó8 tone categories), introducing full con-\\ntext in the prompt significantly altered 21 coefficients, mak-\\ning the fitted coefficients for the LLM comments more sim-\\nilar to those in the model for human comments in only four\\nof these cases, but lesssimilar in 17 cases ( p <0.05, refer\\nto Appendix Table 7 for details).To quantify the discrepancy between human and gener-\\nated comments in terms of their association with social fac-\\ntors, we calculated the sum of absolute difference in regres-\\nsion coefficients across eight social variables. Explicitly,\\nÀÜŒ¥‚â°X\\nsocial factor|ÀÜŒ≤social factor llm‚àíÀÜŒ≤social factor human|(2)\\nAs shown in Table 3, comments generated by a no-context\\nprompt have the smallest average distance from human-\\ngenerated comments across the eight sentiment categories.\\nWhen social context information is explicitly mentioned in\\nthe prompt, the discrepancy tends to increase. The discrep-\\nancy is especially large when friendship is mentioned in the\\nprompt, suggesting that LLMs tend to over-interpret the im-\\npact of friendship when generating comments. Adding gen-\\nder information increased the distance for half of the sen-\\ntiment categories, but the change was smaller compared to\\nmentioning friendship strength. Providing only the poster‚Äôs\\nage and commenter‚Äôs age didn‚Äôt change the tone of gener-\\nated comments significantly.\\nDiscussion\\nGenerating LLM comments without social context\\nSurprisingly, we found that prompting a LLM with only the\\ninitial post produces generated comments that are similar to\\nthe human comments in terms of their tone. One explanation\\nis that the LLM is capturing the tone of the original post and\\ngenerates a comment that is similar in tone to the original\\npost (e.g., the LLM generates a funny comment in response\\nto a funny post), much as people do. Another explanation is\\nthat similarity between the tones of the human and gener-\\nated comments and the dependency of the generated com-\\nments on social variables reflects a selection bias in termsPrompt Surprised Funny InsultEmotional\\nsupportSelf\\ndisclosureWorried Thankful Inclusion Average\\nno-context 0.395 0.507 0.600 0.461 0.433 0.277 0.613 0.096 0.423\\nage-only +0.013 ‚àí0.081 +0.022 +0.032 +0.008 +0.084 +0.172 +0.063 +0.039\\ngender-only ‚àí0.091 +0.614‚àó‚àó‚àó+0.298‚àó‚àó‚àí0.098 ‚àí0.103 +0.216‚àó+0.25‚àó+0.101 +0.148\\nfriendship-only +0.465‚àó‚àó‚àó+1.100‚àó‚àó‚àó+0.521‚àó‚àó‚àó+0.109 +0.542‚àó‚àó‚àó+0.246‚àó+0.414‚àó‚àó‚àó+0.368‚àó‚àó‚àó+0.470\\nall-context +0.213‚àó‚àó‚àó+1.007‚àó‚àó‚àó+0.433‚àó‚àó‚àó+0.081 +0.386‚àó‚àó‚àó+0.174 +0.485‚àó‚àó‚àó+0.262‚àó‚àó‚àó+0.380\\nTable 3: Discrepancy in regression models between generated comments and human comments (Approach 2). The ‚Äúno-context‚Äù\\nrow shows ÀÜŒ¥nocontext , the sum of absolute differences in regression coefficients across eight social variables (Figure 4) between\\nhuman and generated comments without social context. Subsequent rows display the relative changes after including the cor-\\nresponding context in the prompt. * p <0.05, **p <0.01, *** p <0.001. p-values were obtained via error propagation and\\nt-test.\\nof who responds (e.g., women are more likely to respond\\nto a self-disclosing post or one seeking emotional support).\\nWhile we account for this in part by incorporating in our re-\\ngression models the probability that a particular viewer will\\ncomment on a given post (i.e., the p(comment) score), it‚Äôs\\npossible that p(comment) under corrects for selection effects\\nand the model is missing unobservable signals.\\nWhy providing social context does not consistently\\nmake LLM comments more like human ones?\\nWhile LLM-generated comments are quite similar to human\\ncomments in terms of tone and reflect many of the associ-\\nation between social factors and tone that exist in human-\\ncomments, explicitly including social relationship informa-\\ntion in the LLM prompt does not consistently improve these\\nmeasures of similarity averaged across the eight tones and\\noften harms it. There are several possible reasons for these\\nresults.\\nThe most fundamental reason may be that LLM training\\ndata itself does not explicitly include mention of the social\\ncharacteristics or relations among the people in a conversa-\\ntion. To the extent these social factors are reflected in the\\nlanguage that people use when responding to each other,\\ncues to their social characteristics or relations are already\\nembedded in the human posts. Relatedly, certain dyadic in-\\nteractions may be less common in the training corpus of the\\nmodel, leading to worse performance on those groups. For\\ninstance, French, D‚ÄôMello, and Wense (2024) find that the\\nLlama-2 model imitating a teacher is much worse at explain-\\ning educational information to a child than it is to an adult,\\nprobably because more of the training data captures adult-\\nadult interactions rather than adult-child interactions. Even\\nthough people are unlikely to routinely label the social char-\\nacteristics of those with whom they communicate with on-\\nline, it is possible for future work to infer some of these rela-\\ntionships from the language they use (Gumperz and Tannen\\n1979; Preot ¬∏iuc-Pietro et al. 2015), from content they follow\\n(Chamberlain, Humby, and Deisenroth 2017), from social\\nnetwork data (Tang, Lou, and Kleinberg 2012) and then in-\\ncorporate the inferred social characteristics into the training\\ndata.\\nSecond, when given explicit social relationship informa-\\ntion, LLM tends to over-react and weigh this information\\ntoo heavily compared to the social information that is im-plicit in the human post. The social relationship informa-\\ntion that people respond to may be quite subtle and extend\\nbeyond the stereotypical associations with age, gender, and\\nrelationship strength used in our LLM prompts. For exam-\\nple, the congratulations message the human commenter in\\nFigure 1 wrote refers to the relationship between the poster\\nand the commenter‚Äôs son and the commenter‚Äôs best friend.\\nThis issue of overreacting and generating a comment with\\nan extreme tone could also be related to the sycophancy is-\\nsue observed in language models, wherein models forsake\\ntruthfulness in order to be extremely helpful and even flatter\\nthe user by mirroring their own beliefs (Sharma et al. 2023;\\nWei et al. 2023).\\nThird, human commenters may subvert the original mean-\\ning of a post when trying to be creative and respond with a\\nclever retort while language models have difficulty generat-\\ning non-literal, figurative language (Lai and Nissim 2024).\\nFor example, in the following post recruiting ‚Äùkiddos‚Äù for a\\nsummer camp, the human responds with an ironic joke while\\nthe LLM treats the post literally.\\nPost: We would be thrilled to have your kiddos\\njoin us! Please consider sending them our way.\\n[http://camp url]\\nHuman: Can I send mine? He is 46 years old and has\\nanger issues, but he‚Äôs funny.\\nLLM: Aww, that sounds like an amazing experience\\nfor the kiddos! I‚Äôll definitely consider sending mine,\\nthanks for sharing this opportunity! You‚Äôre always so\\nthoughtful to think of others, love you!\\nFinally, when crafting a comment people may use additional\\nsocial context or relationship information that is invisible to\\nthe LLM. In the following example, the commenter seems\\nto allude to the poster‚Äôs and commenter‚Äôs history of playing\\ngolf.\\nPost: We finally closed on our new home last Thurs-\\nday! Now it‚Äôs time to start to move and make it ours.\\nLet the renovations begin!\\nHuman: Do I need to bring my golf clubs with me\\nwhen we come down?\\nLLM: Congrats on the new home, buddy! Exciting\\ntimes ahead with all the projects and making it your\\nown. Wishing you all the best in this new chapter!Conclusion\\nThis paper investigates social relationship alignment in\\nLLMs. We first examined real-world public post-comment\\ncommunication data from Facebook, followed by an anal-\\nysis of whether LLMs can generate comparable comments\\nwhen prompted with both post and social relationship in-\\nformation. Our findings indicate that LLMs can produce\\ncomments closely resembling human-generated responses\\nto the same post, even without being explicitly provided\\nwith social relationship details. Additionally, we observed\\nthat explicitly including social relationship information in\\nthe prompt does not consistently enhance the similarity be-\\ntween LLM-generated and human-generated comments. We\\ndiscussed possible explanations for our findings.\\nLimitations and Future Work\\nTo the best of our knowledge, this work is the first to exam-\\nine whether and how social relationships impact the seman-\\ntic tone of LLM-generated comments at the individual level,\\nproviding insights into the potential of LLMs in adapting to\\ndiverse conversational settings. It is important, however, to\\nnote that this paper is an observational study, with no modi-\\nfications made to the underlying LLM product.\\nWhile we found that LLM-generated comments and hu-\\nman comments are similar without social context informa-\\ntion in the prompt, the average similarity between human\\nand LLM comments is modest (avg ÀÜŒ≤=0.456), suggesting\\nthere‚Äôs potential room for improvement. Although we only\\nevaluated incorporating social relationships via prompt engi-\\nneering techniques, future work should consider improving\\nLLMs‚Äô ability to generate socially-realistic comments, such\\nas incorporating social context information at the training\\nstage and fine-tuning the model with conversation data.\\nAcknowledgements\\nWe thank Kaiyan Peng, Itamar Rosenn, Nicolas Stier, Aude\\nHofleitner, Shawndra Hill, Danny Ferrante, Ali Muzaffer,\\nMauricio Figueiredo, and Taylor Ebling for helpful feedback\\nand support on this project.\\nReferences\\nAnwar, U.; Saparov, A.; Rando, J.; Paleka, D.; Turpin, M.;\\nHase, P.; Lubana, E. S.; Jenner, E.; Casper, S.; Sourbut, O.;\\net al. 2024. Foundational challenges in assuring alignment\\nand safety of large language models. arXiv:2404.09932 .\\nBakker, M.; Chadwick, M.; Sheahan, H.; Tessler, M.;\\nCampbell-Gillingham, L.; Balaguer, J.; McAleese, N.;\\nGlaese, A.; Aslanides, J.; Botvinick, M.; et al. 2022. Fine-\\ntuning language models to find agreement among humans\\nwith diverse preferences. Advances in Neural Information\\nProcessing Systems , 35: 38176‚Äì38189.\\nBrown, T. B. 2020. Language models are few-shot learners.\\narXiv:2005.14165 .\\nBruckman, A. 2002. Studying the amateur artist: A perspec-\\ntive on disguising data collected in human subjects research\\non the Internet. Ethics and Information Technology , 4(3):\\n217‚Äì231.Burke, M.; Joyce, E.; Kim, T.; Anand, V .; and Kraut, R.\\n2007. Introductions and requests: Rhetorical strategies that\\nelicit response in online communities. In Communities and\\nTechnologies 2007: Proceedings of the Third Communities\\nand Technologies Conference, Michigan State University\\n2007 , 21‚Äì39. Springer.\\nBurke, M.; and Kraut, R. E. 2014. Growing closer on Face-\\nbook: Changes in tie strength through social network site\\nuse. In Proceedings of the SIGCHI Conference on Human\\nFactors in Computing Systems , 4187‚Äì4196.\\nBurt, R. S. 2004. Structural holes and good ideas. American\\nJournal of Sociology , 110(2): 349‚Äì399.\\nCaron, G.; and Srivastava, S. 2022. Identifying and\\nmanipulating the personality traits of language models.\\narXiv:2212.10276 .\\nCaron, G.; and Srivastava, S. 2023. Manipulating the Per-\\nceived Personality Traits of Language Models. In Findings\\nof the Association for Computational Linguistics: EMNLP\\n2023 , 2370‚Äì2386.\\nChamberlain, B. P.; Humby, C.; and Deisenroth, M. P. 2017.\\nProbabilistic inference of twitter users‚Äô age based on what\\nthey follow. In Machine Learning and Knowledge Discovery\\nin Databases: European Conference, ECML PKDD 2017,\\nSkopje, Macedonia, September 18‚Äì22, 2017, Proceedings,\\nPart III 10 , 191‚Äì203. Springer.\\nChang, Y .; Wang, X.; Wang, J.; Wu, Y .; Yang, L.; Zhu, K.;\\nChen, H.; Yi, X.; Wang, C.; Wang, Y .; et al. 2024. A survey\\non evaluation of large language models. ACM Transactions\\non Intelligent Systems and Technology , 15(3): 1‚Äì45.\\nChoi, Y .; Kang, E. J.; Choi, S.; Lee, M. K.; and Kim,\\nJ. 2024. Proxona: Leveraging LLM-Driven Personas\\nto Enhance Creators‚Äô Understanding of Their Audience.\\narXiv:2408.10937 .\\nDindia, K.; and Allen, M. 1992. Sex differences in self-\\ndisclosure: a meta-analysis. Psychological Bulletin , 112(1):\\n106.\\nDurmus, E.; Nguyen, K.; Liao, T. I.; Schiefer, N.; Askell,\\nA.; Bakhtin, A.; Chen, C.; Hatfield-Dodds, Z.; Hernandez,\\nD.; Joseph, N.; et al. 2023. Towards measuring the repre-\\nsentation of subjective global opinions in language models.\\narXiv:2306.16388 .\\nFrench, D.; D‚ÄôMello, S.; and Wense, K. 2024. Aligning\\nto Adults Is Easy, Aligning to Children Is Hard: A Study\\nof Linguistic Alignment in Dialogue Systems. In Proceed-\\nings of the 1st Human-Centered Large Language Modeling\\nWorkshop , 81‚Äì87.\\nGe, S.; Zhou, C.; Hou, R.; Khabsa, M.; Wang, Y .-C.; Wang,\\nQ.; Han, J.; and Mao, Y . 2023. Mart: Improving llm safety\\nwith multi-round automatic red-teaming. arXiv:2311.07689 .\\nGreengross, G.; Silvia, P. J.; and Nusbaum, E. C. 2020. Sex\\ndifferences in humor production ability: A meta-analysis.\\nJournal of Research in Personality , 84: 103886.\\nGumperz, J. J.; and Tannen, D. 1979. Individual and social\\ndifferences in language use. In Individual differences in lan-\\nguage ability and language behavior , 305‚Äì325. Elsevier.Hall, J. A. 2017. Humor in romantic relationships: A meta-\\nanalysis. Personal Relationships , 24(2): 306‚Äì322.\\nHendrycks, D.; Burns, C.; Basart, S.; Critch, A.; Li, J.; Song,\\nD.; and Steinhardt, J. 2020. Aligning ai with shared human\\nvalues. arXiv:2008.02275 .\\nKojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y .; and Iwasawa,\\nY . 2022. Large language models are zero-shot reason-\\ners.Advances in neural information processing systems , 35:\\n22199‚Äì22213.\\nKrishna, R.; Lee, D.; Fei-Fei, L.; and Bernstein, M. S. 2022.\\nSocially situated artificial intelligence enables learning from\\nhuman interaction. Proceedings of the National Academy of\\nSciences , 119(39): e2115730119.\\nLai, H.; and Nissim, M. 2024. A survey on automatic gen-\\neration of figurative language: From rule-based systems to\\nlarge language models. ACM Computing Surveys , 56(10):\\n1‚Äì34.\\nLee, J.; Oh, M.; and Lee, D. 2023. P5: Plug-and-Play\\nPersona Prompting for Personalized Response Selection.\\narXiv:2310.06390 .\\nLi, J.; Galley, M.; Brockett, C.; Spithourakis, G.; Gao, J.; and\\nDolan, W. B. 2016. A Persona-Based Neural Conversation\\nModel. In Proceedings of the 54th Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1: Long\\nPapers) , 994‚Äì1003.\\nMatz, S.; Teeny, J.; Vaid, S. S.; Peters, H.; Harari, G.; and\\nCerf, M. 2024. The potential of generative AI for personal-\\nized persuasion at scale. Scientific Reports , 14(1): 4692.\\nNjifenjou, A.; Sucal, V .; Jabaian, B.; and Lef `evre, F.\\n2024. Role-Play Zero-Shot Prompting with Large Language\\nModels for Open-Domain Human-Machine Conversation.\\narXiv:2406.18460 .\\nOlea, C.; Tucker, H.; Phelan, J.; Pattison, C.; Zhang, S.;\\nLieb, M.; Schmidt, D.; and White, J. 2024. Evaluating per-\\nsona prompting for question answering tasks. In Proceed-\\nings of the 10th International Conference on Artificial Intel-\\nligence and Soft Computing .\\nPreot ¬∏iuc-Pietro, D.; V olkova, S.; Lampos, V .; Bachrach, Y .;\\nand Aletras, N. 2015. Studying user income through lan-\\nguage, behaviour and affect in social media. PloS one , 10(9):\\ne0138717.\\nSanturkar, S.; Durmus, E.; Ladhak, F.; Lee, C.; Liang, P.; and\\nHashimoto, T. 2023. Whose opinions do language models\\nreflect? In International Conference on Machine Learning ,\\n29971‚Äì30004. PMLR.\\nSerapio-Garc ¬¥ƒ±a, G.; Safdari, M.; Crepy, C.; Sun, L.; Fitz,\\nS.; Romero, P.; Abdulhai, M.; Faust, A.; and Matari ¬¥c,\\nM. 2023. Personality traits in large language models.\\narXiv:2307.00184 .\\nShao, Y .; Li, L.; Dai, J.; and Qiu, X. 2023. Character-llm: A\\ntrainable agent for role-playing. arXiv:2310.10158 .\\nSharma, M.; Tong, M.; Korbak, T.; Duvenaud, D.; Askell,\\nA.; Bowman, S. R.; Cheng, N.; Durmus, E.; Hatfield-Dodds,\\nZ.; Johnston, S. R.; et al. 2023. Towards understanding syco-\\nphancy in language models. arXiv:2310.13548 .Shen, T.; Jin, R.; Huang, Y .; Liu, C.; Dong, W.; Guo, Z.;\\nWu, X.; Liu, Y .; and Xiong, D. 2023. Large language model\\nalignment: A survey. arXiv:2309.15025 .\\nShriver, S. K.; Nair, H. S.; and Hofstetter, R. 2013. Social\\nties and user-generated content: Evidence from an online so-\\ncial network. Management Science , 59(6): 1425‚Äì1443.\\nShuster, K.; Xu, J.; Komeili, M.; Ju, D.; Smith, E. M.; Roller,\\nS.; Ung, M.; Chen, M.; Arora, K.; Lane, J.; et al. 2022.\\nBlenderbot 3: a deployed conversational agent that contin-\\nually learns to responsibly engage. arXiv:2208.03188 .\\nTang, J.; Lou, T.; and Kleinberg, J. 2012. Inferring so-\\ncial ties across heterogenous networks. In Proceedings of\\nthe Fifth ACM International Conference on Web Search and\\nData Mining , 743‚Äì752.\\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\\nBabaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\\nS.; et al. 2023. Llama 2: Open foundation and fine-tuned\\nchat models. arXiv:2307.09288 .\\nTseng, Y .-M.; Huang, Y .-C.; Hsiao, T.-Y .; Hsu, Y .-C.; Foo,\\nJ.-Y .; Huang, C.-W.; and Chen, Y .-N. 2024. Two tales of per-\\nsona in llms: A survey of role-playing and personalization.\\narXiv:2406.01171 .\\nWang, X.; Xiao, Y .; Huang, J.-t.; Yuan, S.; Xu, R.; Guo, H.;\\nTu, Q.; Fei, Y .; Leng, Z.; Wang, W.; et al. 2024. Inchar-\\nacter: Evaluating personality fidelity in role-playing agents\\nthrough psychological interviews. In Proceedings of the\\n62nd Annual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers) , 1840‚Äì1873.\\nWang, Y .-C.; Burke, M.; and Kraut, R. 2016. Modeling self-\\ndisclosure in social networking sites. In Proceedings of the\\n19th ACM Conference on Computer-Supported Cooperative\\nWork & Social Computing , 74‚Äì85.\\nWei, J.; Huang, D.; Lu, Y .; Zhou, D.; and Le, Q. V . 2023.\\nSimple synthetic data reduces sycophancy in large language\\nmodels. arXiv:2308.03958 .\\nWeidinger, L.; Mellor, J.; Rauh, M.; Griffin, C.; Uesato, J.;\\nHuang, P.-S.; Cheng, M.; Glaese, M.; Balle, B.; Kasirzadeh,\\nA.; et al. 2021. Ethical and social risks of harm from lan-\\nguage models. arXiv:2112.04359 .\\nWellman, B.; and Wortley, S. 1990. Different strokes from\\ndifferent folks: Community ties and social support. Ameri-\\ncan Journal of Sociology , 96(3): 558‚Äì588.\\nXu, C.; Chern, S.; Chern, E.; Zhang, G.; Wang, Z.; Liu, R.;\\nLi, J.; Fu, J.; and Liu, P. 2023. Align on the fly: Adapting\\nchatbot behavior to established norms. arXiv:2312.15907 .\\nXu, X.; Liu, J.; and Liu, J. H. 2024. The effect of so-\\ncial media environments on online emotional disclosure: tie\\nstrength, network size and self-reference. Online Informa-\\ntion Review , 48(2): 390‚Äì408.\\nZeng, X.; and Wei, L. 2013. Social ties and user content\\ngeneration: Evidence from Flickr. Information Systems Re-\\nsearch , 24(1): 71‚Äì87.\\nZhang, S. 2018. Personalizing dialogue agents: I have a dog,\\ndo you have pets too. arXiv:1801.07243 .Zhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y .;\\nMin, Y .; Zhang, B.; Zhang, J.; Dong, Z.; et al. 2023. A sur-\\nvey of large language models. arXiv:2303.18223 .\\nZhong, P.; Zhang, C.; Wang, H.; Liu, Y .; and Miao, C. 2020.\\nTowards Persona-Based Empathetic Conversational Models.\\nInProceedings of the 2020 Conference on Empirical Meth-\\nods in Natural Language Processing (EMNLP) , 6556‚Äì6566.\\nZhou, J.; Chen, Z.; Wan, D.; Wen, B.; Song, Y .; Yu, J.;\\nHuang, Y .; Peng, L.; Yang, J.; Xiao, X.; et al. 2023. Char-\\nacterglm: Customizing chinese conversational ai characters\\nwith large language models. arXiv:2311.16832 .\\nZiems, C.; Dwivedi-Yu, J.; Wang, Y .-C.; Halevy, A.; and\\nYang, D. 2023. NormBank: A knowledge bank of situational\\nsocial norms. arXiv:2305.17008 .Appendix\\nComment Semantic Classifier\\nWe rely on a multi-task learning framework to predict 8 se-\\nmantic categories (e.g., insightful, emotional support). This\\nmodel was built on a combination of training data from hu-\\nman labels, user actions, and regex labels. We describe the\\nsemantic categories in Figure 5. For this analysis, we rely on\\na version of the model that relies only on comment text as\\ninput.\\nPart I\\nLikelihood Ratio Test - Full and Social Context Models\\nCompared to Baseline In this section, we show the Like-\\nlihood Ratio Test results comparing different model speci-\\nfications to a baseline model that includes only post topics\\nin Table 4. We compare individual social context features\\n(+Age, +Gender, +Friend Type) to the baseline model and\\nfind individual features lead to a better fit. When we compare\\nto a Full model, which includes all social context signals, we\\nsee further gains.\\nLikelihood Ratio Test - Full Compared to Individual So-\\ncial Context Features We next do a Likelihood Ratio test\\nwhere we compare the Full model to a model that has only\\none social context (Age, Gender, or Friend) signal. Across\\nall semantic categories, we see in Table 5 that the Full model\\noutperforms a model with just a single social context signal.\\nPart II\\nLLM prompts In this section, we describe how we\\nprompt the LLM to generate comments. Each prompt con-\\nsists of three parts: background, context, and generation in-\\nstructions. The background part provides a general descrip-\\ntion of the task and the user post, which is shared across all\\nprompts:\\nYou are a Facebook user and want to comment on a\\npost from another Facebook user (the poster) in the\\nmost proper way based on the social relationship be-\\ntween you and the poster.\\nHere is the post made by the poster on Facebook:\\n[Post]* {post text}*.\\nThe instruction part is as follows:\\nGenerate a short comment right below the post, with\\n‚Äú[Comment]:‚Äù prefixing the comment, in one para-\\ngraph. No explanation needed.\\nThe context component varies between prompt types and\\nprovides different levels of social context information. Ta-\\nble 6 presents the context part of different prompt types. The\\nfield{friendship status}can be one of the following:\\n‚Ä¢ close friends: ‚Äú you and the poster are very close friends\\nand interact with each other frequently ‚Äù\\n‚Ä¢ non-close friends: ‚Äú you and the poster are friends but not\\ninteract with each other much ‚Äù\\n‚Ä¢ strangers: ‚Äú you and the poster are strangers ‚ÄùComparison between Approach 1 and Approach 2 This\\nsection utilizes a straightforward mathematical model to\\ncontrast Approach 1 and Approach 2 from Part II. We\\ndemonstrate that Approach 1 considers both between-group\\nand within-group differences, whereas Approach 2 focuses\\nsolely on between-group differences.\\nWe frame our analysis based on the social context predic-\\ntorX, with two response variables: Yrepresenting the tone\\nof human comments, and Zindicating the tone of generated\\ncomments. Each approach employs distinct metrics to assess\\nthe similarity between YandZ. For simplicity, we consider\\na binary predictor: Xi= 1 indicates group 1 and Xi= 0\\nindicates group 0.\\nIn Approach 1, we employ the regression model\\nZi=azy+bzyYi+ei,\\nwhere the coefficient bzyis\\nbzy=Cov(Y, Z)\\nVar(Y). (3)\\nHere, Cov(Y, Z)represents the covariance between Yand\\nZ, and Var(Y)signifies the variance of Y. These statis-\\ntics are partitioned into three components: (co)variance from\\ngroup 0 ( x= 0), (co)variance from group 1 ( x= 1), and\\nbetween-group (co)variance, calculated as follows:\\nCov(Y, Z) =n0\\nnCov(Y, Z)x=0+n1\\nnCov(Y, Z)x=1\\n+n0n1\\nn2(¬ØZ1‚àí¬ØZ0)(¬ØY1‚àí¬ØY0),\\nVar(Y) =n0\\nnVar(Y)x=0+n1\\nnVar(Y)x=1+n0n1\\nn2(¬ØY1‚àí¬ØY0)2.\\n¬ØY0and¬ØY1denotes the mean values of Ywithin group 0 and\\n1, respectively. Similarly, ¬ØZ0and¬ØZ1are the mean values of\\nZin these groups. n0andn1are sample size of groups 0\\nand 1, respectively, with n=n0+n1representing the total\\nsample size.\\nTherefore, both the within-group and between-\\ngroup (co)variance influence the regression coeffi-\\ncients bzy. If the between-group (co)variance is sig-\\nnificant compared to within-group (co)variance, bzy\\napproximates the ratio of between-group differences,\\n(¬ØZ1‚àí¬ØZ0)/(¬ØY1‚àí¬ØY0). Conversely, if the within-\\ngroup (co)variance is dominant, bzy approximates\\n(n0Cov(Y, Z)x=0+n1Cov(Y, Z)x=1)/(nVar(Y)).\\nFor approach 2, we estimate two separate regressions,\\nYi=ay+byXi+eyi,\\nZi=az+bzXi+ezi.\\nWe then calculate the difference between regression coeffi-\\ncients,\\nŒ¥zy=|bz‚àíby|.\\nThis calculation is straightforward for a single variable pre-\\ndictor, resulting in\\nŒ¥zy=|¬ØZ1‚àí¬ØZ0‚àí(¬ØY1‚àí¬ØY0)|. (4)\\nComparing this with the coefficient bzyfrom Approach\\n1,Œ¥zyis solely dependent on between-group differences. In\\ncontrast, bzyincorporates both within-group differences and\\nwithin-group variability, facilitating individual-level com-\\nparisons.Figure 5: We describe the semantic categories used in this paper. For the post and comment examples, we paraphrase them.\\nSurprised Funny Insult Emo Support Self-Disclosure Worried Thankful Relaxed\\nBaseline ‚àí735416 ‚àí1142279 ‚àí1121999 ‚àí1021913 ‚àí789679 ‚àí898347 ‚àí1183726 543822\\n+Age ‚àí734805‚àó‚àó‚àó‚àí1130343‚àó‚àó‚àó‚àí1116308‚àó‚àó‚àó‚àí1017417‚àó‚àó‚àó‚àí786774‚àó‚àó‚àó‚àí895814‚àó‚àó‚àó‚àí1176533‚àó‚àó‚àó546275‚àó‚àó‚àó\\n+Gender ‚àí732373‚àó‚àó‚àó‚àí1141036‚àó‚àó‚àó‚àí1115312‚àó‚àó‚àó‚àí1014161‚àó‚àó‚àó‚àí789117‚àó‚àó‚àó‚àí897948‚àó‚àó‚àó‚àí1177248‚àó‚àó‚àó544535‚àó‚àó‚àó\\n+Friend ‚àí735340‚àó‚àó‚àó‚àí1141821‚àó‚àó‚àó‚àí1121958‚àó‚àó‚àó‚àí1021860‚àó‚àó‚àó‚àí789478‚àó‚àó‚àó‚àí898180‚àó‚àó‚àó‚àí1183699‚àó‚àó‚àó543910‚àó‚àó‚àó\\nFull ‚àí731698‚àó‚àó‚àó‚àí1128197‚àó‚àó‚àó‚àí1109017‚àó‚àó‚àó‚àí1009230‚àó‚àó‚àó‚àí786108‚àó‚àó‚àó‚àí895229‚àó‚àó‚àó‚àí1169517‚àó‚àó‚àó547143‚àó‚àó‚àó\\nTable 4: We include results for likelihood ratio tests across the different semantic tone categories when comparing to a baseline\\nmodel. We find better model fit for social context signals individually and all social context signals (Full) when compared to a\\nBaseline model. * p <0.1; ** p <0.05; *** p <0.01\\nPrompt engineering To test whether our conclusion re-\\nlies on how the social context information is presented in the\\nprompt, in this section we study how variation of the prompt\\naffects the discrepancy between generated comments and\\nhuman comments. We tested the following ideas,\\n‚Ä¢ Adjusting the order of social context information in the\\nprompt. More specifically, we switched the order of age,\\ngender, and friendship type information in the context\\npart of the prompts, as demonstrated in Table 6.\\n‚Ä¢ Zero-shot chain of thoughts (CoT). Following the idea of\\nzero-shot-CoT, we added the following sentence to the\\ninstruction part of the prompt: ‚Äú Think step by step with\\nexplanations ‚Äù.\\n‚Ä¢ Rephrasing the social context in the prompt. Our results\\nimply that the generated response is very sensitive to\\nthe friendship type presented in the social context. To\\ntest the reliability, we rephrase the friendship type from\\n‚Äústrangers‚Äù to ‚Äúnon-friends‚Äù.The results are summarized in Table 8, which shows while\\nsome prompt engineering techniques helped to reduce the\\ndiscrepancy, they did not alter the main conclusion of our\\nfindings.\\nAs illustrated in Table 8, altering the order of social con-\\ntext information in the prompt significantly reduces discrep-\\nancies. This finding aligns with previous studies indicating\\nthat the sequence of instructions in prompts affects LLM\\nperformance. Additionally, employing CoT reasoning also\\ndecreases discrepancies. However, modifying the descrip-\\ntion of friendship types has a marginal effect. Despite these\\nadjustments, the no-context prompt still exhibits the lowest\\ndiscrepancy, underscoring the LLM‚Äôs limited ability to re-\\nspond appropriately to social context.Surprised Funny Insult Emo Support Self-Disclosure Worried Thankful Relaxed\\nAge ‚àí734805 ‚àí1130343 ‚àí1116308 ‚àí1017417 ‚àí786774 ‚àí895814 ‚àí1176533 546275\\nGender ‚àí732373 ‚àí1141036 ‚àí1115312 ‚àí1014161 ‚àí789117 ‚àí897948 ‚àí1177248 544535\\nFriend ‚àí735340 ‚àí1141821 ‚àí1121958 ‚àí1021860 ‚àí789478 ‚àí898180 ‚àí1183699 543910\\n+Full ‚àí731698‚àó‚àó‚àó‚àí1128197‚àó‚àó‚àó‚àí1109017‚àó‚àó‚àó‚àí1009230‚àó‚àó‚àó‚àí786108‚àó‚àó‚àó‚àí895229‚àó‚àó‚àó‚àí1169517 ‚àó ‚àó ‚àó 547143‚àó‚àó‚àó\\nTable 5: We include results for likelihood ratio tests across the different semantic tone categories when comparing to a full\\nmodel. The comparison is done by comparing the Full model to each social context signal individually. We see that the Full\\nmodel provides better fit than a model with just a single social context signal. * p <0.1; ** p <0.05; *** p <0.01.\\nPrompt type Context\\nllm-all-contextRemember that {friendship status}. Also remember that you are a {commenter gender }of age\\n{commenter age}and the poster is a {poster gender }of age {poster age}\\nllm-age-only Remember that you are of age {commenter age}and the poster is of age {poster age}\\nllm-gender-only Remember that you are a {commenter gender }and the poster is a {poster gender }.\\nllm-friends-only Remember that {friendship status}.\\nTable 6: Social context provided in different prompts.\\nPrompt Average discrepancy\\nbaseline all context 0.803\\ngender agefriends ‚àí0.133\\nagegender friends ‚àí0.084\\nzero shot cot ‚àí0.063\\nnonfriends +0.010\\nTable 8: Tuning the prompt helps to reduce but cannot elimi-\\nnate the discrepancy¬ØÀÜŒ¥. First row shows the average discrep-\\nancy from the baseline all context prompt, followed by rela-\\ntive changes after modifying the prompt.model human llm-all-context llm-no-context Figure 6: Regression coefficients of social context factors in the fitted models for remaining scores, including surprised, funny,\\nprovide support, worried, thankful, and inclusion.\\nSocial context\\nfactorSurprised Funny InsultEmotional\\nsupportSelf\\ndisclosureWorried Thankful Inclusion\\ncommenter male ‚àí0.046 0.069 ‚àí0.023 ‚àí0.001 ‚àí0.081‚àó‚àó0.157‚àó‚àó‚àó0.006 0.069‚àó‚àó\\nposter male ‚àí0.004 0.005 ‚àí0.06 ‚àí0.026 0.023 0.011 ‚àí0.004 ‚àí0.017\\ncommenter male√ó\\nposter male0.020 0.263‚àó‚àó‚àó0.157‚àó‚àó0.050 ‚àí0.080‚àó0.022 0.234‚àó‚àó‚àó0.022\\ncommenter age 0.019‚àó‚àí0.056‚àó‚àó‚àí0.005 ‚àí0.019‚àó0.016 0.008 0.007 0.002\\nposter age 0.024‚àó0.022 ‚àí0.031 0.003 ‚àí0.013 ‚àí0.001 ‚àí0.016 ‚àí0.001\\ncommenter age√ó\\nposter age0.013 ‚àí0.01 0.010 ‚àí0.009 0 0.007 ‚àí0.01 ‚àí0.005\\nnon-close friends 0.083‚àó‚àó‚àó0.395‚àó‚àó‚àó0.255‚àó‚àó‚àó0.011 0.283‚àó‚àó‚àó‚àí0.009 0.202‚àó‚àó‚àó0.097‚àó‚àó‚àó\\nstrangers 0.104‚àó‚àó‚àó0.318‚àó‚àó‚àó0.13‚àó0.072‚àó0.238‚àó‚àó‚àó‚àí0.021 0.066 0.094‚àó‚àó\\nTable 7: The impact of providing context on the discrepancy between generated comments and human comments. The dis-\\ncrepancy is calculated as the difference in fitting coefficients between the regression models for generated comments and that\\nfor human comments. Significant changes are highlighted: red indicates that adding context increased the discrepancy, while\\ngreen shows a reduction in discrepancy due to context. * p <0.05, **p <0.01, *** p <0.001. p-values were obtained via\\ntwo-sample t-test.',\n",
       " 'Post-hoc Study of Climate Microtargeting on Social Media Ads\\nwith LLMs: Thematic Insights and Fairness Evaluation\\nTunazzina Islam\\nDepartment of Computer Science, Purdue University\\nWest Lafayette, Indiana 47907, USA\\nislam32@purdue.eduDan Goldwasser\\nDepartment of Computer Science, Purdue University\\nWest Lafayette, Indiana 47907, USA\\ndgoldwas@purdue.edu\\nAbstract\\nClimate change communication on social media increasingly em-\\nploys microtargeting strategies to effectively reach and influence\\nspecific demographic groups. This study presents a post-hoc analysis\\nof microtargeting practices within climate campaigns by leveraging\\nlarge language models (LLMs) to examine Facebook advertisements.\\nOur analysis focuses on two key aspects: demographic targeting\\nandfairness . We evaluate the ability of LLMs to accurately predict\\nthe intended demographic targets, such as gender and age group,\\nachieving an overall accuracy of 88.55%. Furthermore, we instruct\\nthe LLMs to generate explanations for their classifications, provid-\\ning transparent reasoning behind each decision. These explanations\\nreveal the specific thematic elements used to engage different demo-\\ngraphic segments, highlighting distinct strategies tailored to various\\naudiences. Our findings show that young adults are primarily tar-\\ngeted through messages emphasizing activism and environmental\\nconsciousness , while women are engaged through themes related\\ntocaregiving roles and social advocacy . In addition to evaluating\\nthe effectiveness of LLMs in detecting microtargeted messaging,\\nwe conduct a comprehensive fairness analysis to identify potential\\nbiases in model predictions. We assess disparities in accuracy and\\nerror rates across demographic groups using established fairness\\nmetrics such as Demographic Parity, Equal Opportunity, and Pre-\\ndictive Equality. Our findings indicate that while LLMs perform\\nwell overall, certain biases exist, particularly in the classification\\nofsenior citizens andmale audiences. The analysis of thematic\\nexplanations uncovers recurring patterns in messaging strategies\\ntailored to various demographic groups, while the fairness analysis\\nunderscores the need for more inclusive and unbiased targeting\\nmethods. By showcasing the efficacy of LLMs in dissecting and\\nexplaining targeted communication strategies and by highlighting\\nfairness concerns, this study provides a valuable framework for\\nfuture research aimed at enhancing transparency, accountability,\\nand inclusivity in social media-driven climate campaigns.\\nCCS Concepts\\n‚Ä¢Information systems ‚ÜíOnline advertising ;‚Ä¢Computing\\nmethodologies‚ÜíNatural language processing ;‚Ä¢Applied\\ncomputing‚ÜíLaw, social and behavioral sciences .\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for third-party components of this work must be honored.\\nFor all other uses, contact the owner/author(s).\\nACM, 2024, N, NN, NNN\\n¬©2024 Copyright held by the owner/author(s).\\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM\\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnnKeywords\\nmicrotargeting, climate campaigns, social media, demographic tar-\\ngeting, fairness, thematic analysis, large language models\\nACM Reference Format:\\nTunazzina Islam and Dan Goldwasser. 2024. Post-hoc Study of Climate\\nMicrotargeting on Social Media Ads with LLMs: Thematic Insights and\\nFairness Evaluation. In ACM Conference, 2024, N, NN, NNN . ACM, New\\nYork, NY, USA, 14 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\\n1 Introduction\\nClimate change represents one of the most pressing global chal-\\nlenges of the 21st century, necessitating widespread public aware-\\nness and engagement to drive meaningful environmental action\\n[26,62]. As traditional media channels evolve, social media has\\nemerged as a pivotal platform for climate communication, enabling\\norganizations, activists, and policymakers to disseminate informa-\\ntion, mobilize support, and influence public discourse on environ-\\nmental issues [ 2,35,63]. The interactive and targeted nature of\\nsocial media advertising allows for the customization of messages\\nto resonate with specific audiences, thereby enhancing the effec-\\ntiveness of communication strategies aimed at fostering climate\\nawareness and behavioral change [12, 74, 80].\\nIn recent years, the utilization of microtargeting strategies in\\nsocial media campaigns has gained significant traction. Microtar-\\ngeting involves the precise tailoring of messages to distinct demo-\\ngraphic segments based on factors such as age, gender, location,\\nand interests [ 7,34,64]. This approach leverages vast amounts of\\nuser data to craft personalized content that is more likely to engage\\nand persuade individual users. In the context of climate commu-\\nnication, microtargeting can enhance the relevance and impact of\\nmessages, potentially leading to greater public engagement and\\nsupport for environmental initiatives. However, the sophistication\\nof these strategies also raises critical questions about the trans-\\nparency, ethical implications, and overall effectiveness of targeted\\nclimate messaging [40, 43].\\nFigure 1: Example of climate microtargeting.arXiv:2410.05401v1  [cs.CL]  7 Oct 2024ACM, 2024, N, NN, NNN Islam and Goldwasser\\nFigure 1 illustrates the targeted climate advertisements on social\\nmedia, with a specific focus on demographic targeting. The first ad\\nsource targets young adults aged 18-24 , with a message encour-\\naging action against climate change and inviting them to become TCI\\nAmbassadors . The second ad source is tailored to a female audience\\nwith a message emphasizing the importance of reducing carbon pol-\\nlution from home electricity and making a pledge for their children‚Äôs\\nfuture . The third ad source targets a male audience, focusing on\\nthebenefits of using clean fuel like Ohio Propane for heating rural\\nhomes and fueling appliances .\\nDespite the growing prevalence of microtargeting in climate cam-\\npaigns, there remains a limited understanding of the specific tech-\\nniques and linguistic patterns employed to engage different demo-\\ngraphic groups. Traditionally, thematic analysis (TA) has been the\\npreferred method for this task, functioning as a qualitative research\\ntechnique that focuses on identifying patterns, where themes that\\nemerge from the data drive further analysis [ 13,14,66,76,78,79].\\nHowever, with the surge of textual data in the digital era, there is\\nan increasing trend towards using computational methods for text\\nanalysis [ 29,44,73]. TA method often falls short in capturing the\\nnuanced and context-dependent nature of targeted communication.\\nThis gap highlights the need for advanced analytical tools that\\ncan dissect and interpret the complex language and strategies used\\nin microtargeted climate advertisements. Large language models\\n(LLMs) [ 15], with their robust natural language processing (NLP)\\ncapabilities, offer a promising solution to this challenge. In this\\npaper, we investigate whether the newly emerged paradigm in\\nNLP; zero-shot prompting of LLMs [ 15] and the practice of provid-\\ning explanations of answers ‚Äî is better equipped to address those\\nchallenges.\\nExplanations are fundamental to human learning [ 3], as they un-\\nderscore task principles that facilitate broad generalizations [ 56,57].\\nConsider the example text ( ‚ÄúThe turbines that provide clean wind energy\\nalso create new habitats for fish, keeping the ecosystem healthy. \" ) from Fig-\\nure 2(a) for gender identification task. An explanation can elaborate\\non a brief answer (e.g., male) by connecting it to the broader rea-\\nsoning process necessary to solve the problem (e.g., ‚Äú Emphasizing\\nfish habitats may appeal more to men interested in fishing and\\nenvironmental conservation. ‚Äù). Thus, explanations enhance under-\\nstanding by demonstrating how task principles connect questions\\nto their answers.\\nWhile LLMs offer powerful capabilities for analyzing and gener-\\nating text, their widespread use has also highlighted significant chal-\\nlenges related to fairness [ 51,53,54,68] and bias [ 28,30,50,55,77].\\nResearch has shown that LLMs, like other machine learning (ML)\\nmodels, can inherit and even amplify biases present in the data\\nthey are trained on [ 10,11]. These biases can manifest in various\\nforms, such as differential accuracy across demographic groups\\n[22,27,36,58], harmful stereotypes [ 4,9,59], and discriminatory\\nlanguage patterns [ 69]. The implications are particularly concern-\\ning in high-stakes applications such as healthcare, finance, and\\nsocial media, where biased outcomes can perpetuate inequities and\\nundermine trust in artificial intelligence (AI) systems [ 16,60]. Ad-\\ndressing these issues requires fairness evaluations using metrics\\nlike Demographic Parity, Equal Opportunity, and Predictive Equal-\\nity to ensure that models perform equitably across all user groups\\n[33,87]. In the context of this study, examining the fairness of LLMspredictions in demographic targeting is crucial for understanding\\nthe microtargeted climate messaging.\\nIn this study, we conduct a post-hoc analysis of climate micro-\\ntargeting practices on social media by leveraging the power of\\nLLMs ( OpenAI‚Äôs o1-preview model1). Post-hoc analysis, typi-\\ncally performed after the main experiment or event, allows us to\\nretrospectively evaluate how effective these campaigns are in tar-\\ngeting specific demographics. Building upon data from previous\\nresearch by Islam et al . [43] , we investigate the ability of LLMs to ac-\\ncurately detect targeted messaging based on specific demographic\\nvariables, including gender and age group. Additionally, LLMs pro-\\nvide explanations for their classification decisions, offering insights\\ninto the thematic and linguistic elements used to engage differ-\\nent audiences. Furthermore, we conduct a comprehensive fairness\\nanalysis to identify potential biases in model predictions.\\nOur exploration leads us to the following research questions (RQ),\\nwhich are crucial for assessing the potential of LLMs to understand\\nmicrotargeting patterns and provide deeper and more nuanced\\ninsights:\\n‚Ä¢RQ1: Given a text, can LLMs predict the targeted demo-\\ngraphic of the corresponding text accurately and provide the\\nexplanation for the reasoning behind the prediction?\\n‚Ä¢RQ2: Which words or phrases are most commonly associ-\\nated with certain demographics?\\n‚Ä¢RQ3: What are the recurrent themes and aspects of expla-\\nnations provided by LLMs?\\n‚Ä¢RQ4: How fair are the LLMs predictions in terms of demo-\\ngraphic targeting, and what are the disparities in predic-\\ntion accuracy and error rates across different demographic\\ngroups?\\nThe implications of this research are multifaceted. From an aca-\\ndemic perspective, it contributes to the burgeoning field of com-\\nputational social science (CSS) by showcasing the application of\\nadvanced language models in dissecting complex communication\\nstrategies. Practically, the findings offer valuable insights for policy-\\nmakers, environmental organizations, and social media platforms\\nseeking to enhance the transparency, accountability, and inclusivity\\nof their climate communication efforts. By illuminating the specific\\nmethods used to tailor messages to different demographics and\\nby highlighting the need for fairer and more inclusive targeting\\nmethods, this study lays the groundwork for future investigations\\ninto the role of AI in enhancing the efficacy and ethical standards\\nof digital climate advocacy.\\n2 Related Work\\nThe intersection of microtargeting, social media, and climate com-\\nmunication has drawn considerable interest in recent years, with\\nstudies examining how digital platforms influence public opinion\\nand engagement [ 37,40,41,43]. The growing capabilities of social\\nmedia platforms to deliver personalized messages based on user data\\nhave sparked a significant body of research into computational ad-\\nvertising [ 1,38,84,85], demographic targeting [ 5,45,46,48,65,67],\\nand the broader implications of these practices for societal dis-\\ncourse.\\n1https://openai.com/index/learning-to-reason-with-llms/Post-hoc Study of Climate Microtargeting with LLMs: Thematic Insights and Fairness Evaluation ACM, 2024, N, NN, NNN\\nFigure 2: Prompt examples for gender prediction (shown as zero-shot). (a) male, (b) female. Inputs are shown in blue, and\\noutputs are shown in red.\\nMicrotargeting on social media platforms such as Facebook has\\nbeen widely studied in the context of political campaigns [ 17,18,\\n42,70,71] and public health messaging [ 39,61,72], revealing both\\nthe potential benefits and ethical concerns associated with this\\npractice. In the realm of climate communication, microtargeting\\ncan be a powerful tool for enhancing message relevance and impact\\nby tailoring content to the specific values and interests of diverse\\ndemographic groups [ 43]. However, the effectiveness and ethical\\nimplications of such targeted messaging remain under-explored,\\nparticularly in terms of how different demographic groups are\\nengaged and whether biases are present in the targeting strategies.\\nRecent advances in LLMs have demonstrated their capability for\\nin-context learning (ICL), significantly enhancing their ability to\\nperform tasks traditionally handled by humans [ 15,19,20,24,25,32,\\n49,52,86]. This progress suggests a strong potential for effectively\\napplying LLMs to our specific task. In the realms of qualitative\\nresearch (QR) and NLP, innovative methods are being explored\\nto integrate LLMs into TA. Researchers have proposed various\\nframeworks, including an LLM-in-the-loop approach [ 24,40,41],\\nintegrating GPT-3 with expert-designed codebooks [ 83], and devel-\\noping collaborative interfaces that utilize LLMs for code generation\\nand support in decision-making processes [ 31]. Other recent work\\nhas shown that LLMs can benefit from examples that decompose\\nthe reasoning process (can be seen as an explanation), leading to\\nan answer [ 81]. Despite the impressive capabilities of LLMs, there\\nare concerns about fairness, accountability, and transparency inLLMs predictions which have been highlighted in recent literatures\\n[6,8,23,51,54,82], emphasizing the need for rigorous evaluation of\\nbiases and disparities in model performance across different groups.\\nIn this paper, we leverage zero-shot capabilities of LLMs to\\nidentify targeted demographics and provide explanations of de-\\nmographic targeting regarding climate-related advertisements on\\nFacebook. Besides, we develop a new set of themes and aspects\\nbased on those explanations specifically tailored for analyzing mes-\\nsaging. Additionally, we extend previous research by not only fo-\\ncusing on the accuracy of these predictions but also conducting a\\ncomprehensive fairness analysis to identify and address potential\\nbiases in the model‚Äôs performance.\\n3 Dataset\\nWe investigate the climate campaigns case study for this work. We\\nwork on the corpus of 21372 ads released by Islam et al . [43] . This\\ndataset includes climate-related English ads on Facebook from the\\nUS, spanning from January 2021 to January 2022. Each ad includes\\nthe following attributes: ad ID, title, ad description, ad body, funding\\nentity, spend, impressions, and distribution of impressions broken\\ndown by gender (male, female, unknown), age (seven groups), and\\nlocation down to the state level in the USA. Additional details about\\nthe dataset can be found in the original publications.\\nFor this work, we consider two demographic indicators, i.e.,\\ngender andage group . We consider two gender categories, i.e.,\\nmale vsfemale . Regarding age group, we consider four age groupACM, 2024, N, NN, NNN Islam and Goldwasser\\ncategories, i.e., young adults whose age range is 18-24, early working\\nage group (25-44), late working age group (45-64), and senior citizens\\n(65+). We found approximately 227targeted unique ads; among\\nthem, 59ads targeting only females and 47ads targeting only males.\\nHowever, we find 25ads target only young adults, 82ads target\\nonly the early working age group, 8ads target only the late working\\nage group, and 6ads target only senior citizens.\\nFigure 3: Prompt template for targeted demographic predic-\\ntion (shown as zero-shot). (a) gender, (b) age group. Inputs\\nare shown in blue, and outputs are shown in red.\\n4 Task Definition\\nThe identification of the targeted demographic (with explanation)\\nin a text involves the following steps:\\nGender prediction with Explanation: For a given text ùë°, the\\ntask involves identifying the targeted gender and explaining the\\nrationale behind its selection.\\nAge group prediction with Explanation: Subsequently, the task\\nrequires predicting the targeted age group and providing an expla-\\nnation for the specific choice.\\nTo predict the targeted demographics, we employ zero-shot\\nprompting using the most recently2released by OpenAI, o1-preview\\nmodel3. This is a new large language model that uses reinforcement\\nlearning (RL) and chain of thought (COT) techniques for complex\\nreasoning, allowing it to think through a detailed internal process\\nbefore responding to users.\\nThe prompt template for the demographic prediction task with\\nan explanation using LLMs can be found in Figure 3. Figure 2 shows\\nthe example prompts for gender prediction. Figure 5 (in App. A)\\nshows the example prompts for age group prediction from the\\nclimate campaign dataset.\\n5 Experimental Setup\\nIn this work, we use OpenAI Playground API to run o1-preview\\nby keeping the default parameters.\\n5.1 Results\\nTable 1 provides the overall accuracy of the targeted demographic\\nprediction task by LLMs as well as a detailed breakdown of correct\\nand incorrect predictions across each demographic category. LLMs\\n2September 12, 2024\\n3https://openai.com/index/introducing-openai-o1-preview/Category Total\\nAdsCorrect\\nPred.Acc.\\n(%)Misclass.\\nAll 227 201 88.55 -\\nFemale 59 56 94.92 3 (Male)\\nMale 47 40 85.10 7 (Female)\\nYoung\\nadults25 22 88.00 2 (Early Work-\\ning), 1 (Late\\nWorking)\\nEarly\\nWorking82 75 91.46 4 (Young), 4 (Late\\nWorking)\\nLate\\nWorking8 6 75.00 2 (Early Work-\\ning)\\nSenior 6 2 33.33 3 (Young), 1 (Late\\nWorking)\\nTable 1: Accuracy and Misclassifications for Demographics.\\ncan predict the targeted demographics with an accuracy of 88.55%\\nanswering RQ1 . Figure 6 in App. B.1 shows confusion matrices\\nfor targeted gender (Figure 6a) and age (Figure 6b) prediction. This\\nhelps identify specific demographics where the model performs\\nwell or struggles. LLMs achieve high accuracy in predicting both\\nfemales ( 94.92%) and males ( 85.10%) (Figure 6a). A small number of\\nfemales are misclassified as males, and a few males are misclassi-\\nfied as females. Figure 6b shows high accuracy for young ( 88.00%)\\nand early working ( 91.46%) categories. Performance drops for the\\nlate working ( 75%) age group and significantly for senior ( 33.33%)\\ncategories.\\nFor baseline comparison, we use open sourced LLMs Llama 3\\n(llama3-70b-81924) [75] and Mistral Large 2 (mistral-large-24075)\\n[47]. OpenAI‚Äôs o1-preview model outperforms the baselines both\\nin gender and age group predictions (Table 2).\\nModel Demo. Acc. (%) Macro Avg. F1 (%)\\no1-preview gender 90.57 90.35\\no1-preview age 85.95 71.00\\nllama 3 gender 80.19 79.67\\nllama 3 age 58.68 36.84\\nMistral Large 2 gender 82.08 82.07\\nMistral Large 2 age 74.38 48.68\\nTable 2: Baseline comparisons.\\nWe evaluate the o1-preview‚Äôs gender classification model on\\n106samples, with 59labeled as female and 47as male. The model\\nachieved an overall accuracy of 91%, correctly classifying 96out\\nof106instances. The classification report in Table 8 in App. B.2\\nprovides detailed performance metrics for each gender class. The\\nmodel demonstrates strong performance across both gender\\nclasses . For the female class, it achieved a precision of 0.89and a\\nrecall of 0.95, indicating high correctness in positive predictions\\nand a high rate of identifying actual positives, respectively. For the\\nmale class, precision is 0.93, and recall is 0.85, showing a slightly\\nhigher precision but lower recall compared to the female class.\\nThe model exhibits strong performance for the early working\\nandyoung groups but struggles with the senior and late Working\\n4https://github.com/meta-llama/llama3\\n5https://mistral.ai/news/mistral-large-2407/Post-hoc Study of Climate Microtargeting with LLMs: Thematic Insights and Fairness Evaluation ACM, 2024, N, NN, NNN\\ngroups. The model achieves an overall accuracy of 86%, correctly\\nclassifying 104out of 121age-group instances (Table 9 in App. B.2).\\nTable 9 suggests that the performance varies across different age\\ngroups:\\nEarly Working: High precision ( 0.95) and recall ( 0.90), indicating\\nstrong performance in identifying individuals in this age group.\\nLate Working: Moderate precision ( 0.50) and high recall ( 0.75),\\nsuggesting that while the model captures most of the actual in-\\nstances, it has a higher rate of false positives.\\nSenior: Perfect precision ( 1.00) but low recall ( 0.33), meaning the\\nmodel is very accurate when it predicts the Senior class but misses\\na significant number of actual senior instances.\\nYoung: Good precision ( 0.76) and high recall ( 0.88), showing reli-\\nable performance in identifying younger individuals.\\n5.2 Error Analysis\\nTable 10 in the App. B.3 presents an analysis of ad misclassifications\\nbased on gender and age group predictions. Each entry includes\\nthe actual demographic, the predicted demographic, and a brief\\nexplanation generated by LLMs. Table 10 and explanations high-\\nlight how specific patterns and themes within an ad can lead to\\ndemographic misclassifications. In some cases, traditional gender\\nroles andage-related interests played a significant role in these\\nmisclassifications. Understanding these nuances can help in refin-\\ning predictive models and improving the accuracy of demographic\\ntargeting in future ad campaigns.\\nFigure 4: Prompt template for generating theme and aspects\\nfrom predictions and explanations (shown as zero-shot). In-\\nputs are shown in blue, and outputs are shown in red.\\n6 Ad Content Analysis\\nTo determine which words or phrases are most commonly asso-\\nciated with certain demographics (answering RQ2 ), we identify\\nthe top-5 most frequent bigrams (two-word pairs) and trigrams\\n(three-word pairs) of the ad content for male, female, young adults,\\nand early working age group. Due to the small sample size, we do\\nnot show this analysis for the late working age group and senior\\ncitizens. Table 11 in App. C details the results.\\nThese frequent bigrams and trigrams highlight key themes in the\\nads, particularly focusing on the phrases related to climate action,\\nurgency (emergency), and leadership for young adults . On the\\nother hand, frequent bigrams and trigrams of early working age\\ngroup highlight the central themes of climate change, clean energy,\\nand calls to action in the ads. Moreover, for male , these frequent\\nbigrams and trigrams highlight the recurring themes related to\\nclimate change, clean energy, and specific campaigns or initiatives\\nin the ads. In contrast, for female , these frequent bigrams andtrigrams emphasize recurring themes around climate change, clean\\nenergy, and the ‚ÄòBuild Back Better‚Äô agenda in the ads (Table 11 in\\nApp. C ).\\nTo calculate the statistical significance, we perform Chi-Square\\nTest [ 21] for Independence between male and female demographics\\nand their bigrams/trigrams. The p-values for both the bigrams and\\ntrigrams are greater than the common significance level of 0.05. This\\nsuggests that there is no statistically significant association between\\ngender (male and female) and the frequency of the bigrams and\\ntrigrams analyzed. Therefore, we fail to reject the null hypothesis\\nand conclude that the observed differences in bigram and trigram\\nfrequencies between males and females could be due to chance.\\nThe same trend is noticed while performing a Chi-Square Test for\\nIndependence between the young adults (18-24) and early working\\n(25-44) age groups and their bigrams/trigram.\\n7 Thematic Insights of Explanations\\nAs LLMs provide explanations to provide reasoning behind their\\nprediction, we use those explanations for thematic analysis to an-\\nswer RQ3 . In this analysis, we only include the correct predictions\\nand their explanations.\\n7.1 Themes and Aspects of Gender Explanations\\nWe prompt LLMs in a zero-shot manner to provide the common\\ntheme and aspects under specific theme of the explanations from\\n40correct male predictions and 56correct female predictions. The\\nprompt template is shown in Figure 4. We detail the theme of the\\ngender explanation and aspects of the explanation in Table 3.\\nFrom 1ùë†ùë°row of Table 3, we notice that the overall theme re-\\nvolves around targeting men by aligning ads with their perceived\\ninterests and roles in technology, finance, property, traditional\\nmale activities, and political or economic discourse. The explana-\\ntions consistently emphasize the following aspects:\\nInterest in Technology and Innovation: Men are often depicted\\nas being more engaged with technology, engineering, and renew-\\nable energy solutions. Ads related to technical aspects of engines,\\nenergy efficiency, and infrastructure are considered more likely to\\nappeal to men.\\nFocus on Economic and Financial Issues: Many explanations\\nsuggest that men are more concerned with economic benefits, in-\\nvestment opportunities, and financial savings, making them the\\nlikely target for ads that emphasize these aspects.\\nProperty and Land Management: The theme of land ownership,\\nproperty value improvement, and land management is frequently\\nmentioned, with the assumption that men are more interested in\\nthese areas.\\nTraditional Male Activities: Ads that involve traditionally male-\\noriented activities, such as beer consumption, vehicle-related sav-\\nings, home maintenance, and physical strength, are seen as more\\nlikely to target men.\\nEngagement in Political and Infrastructure Topics: Men are\\noften portrayed as more engaged in political discourse, infrastruc-\\nture initiatives, and discussions around energy and policy, making\\nthem the primary audience for ads focused on these themes.\\nConservative Views and Skepticism: Some explanations sug-\\ngest that men are more likely to resonate with conservative views,ACM, 2024, N, NN, NNN Islam and Goldwasser\\nGender Theme of Explanation Aspects of Explanation\\nMale Perceived Interests and Roles\\n‚Ä¢Interest in Technology and Innovation\\n‚Ä¢Focus on Economic and Financial Issues\\n‚Ä¢Property and Land Management\\n‚Ä¢Traditional Male Activities\\n‚Ä¢Engagement in Political and Infrastructure Topics\\n‚Ä¢Conservative Views and Skepticism\\nFemale Roles as Caregivers, Environmental\\nAdvocates, and Socially Conscious\\nIndividuals‚Ä¢Parental and Caregiving Roles\\n‚Ä¢Environmental Consciousness\\n‚Ä¢Social Welfare and Community Involvement\\n‚Ä¢Empathy and Emotional Appeal\\n‚Ä¢Female Empowerment and Leadership\\n‚Ä¢Health and Safety Concerns\\nTable 3: Gender based Themes and Aspects of Explanations.\\nskepticism about environmental claims, and anti-establishment\\nsentiments.\\nFrom 2ùëõùëërow of Table 3, we observe that the overall theme re-\\nvolves around targeting women by aligning ads with their roles\\nas caregivers, environmental advocates, and socially con-\\nscious individuals who prioritize the well-being of their families,\\ncommunities, and the environment. The explanations consistently\\nemphasize the following aspects:\\nParental and Caregiving Roles: Many explanations highlight\\nthat women, particularly mothers, are more likely to resonate with\\nmessages about protecting children‚Äôs futures, parental responsibil-\\nities, and family well-being. These ads often appeal to maternal\\ninstincts and the role of women as primary caregivers.\\nEnvironmental Consciousness: Women are frequently depicted\\nas being more engaged with environmental issues, sustainability,\\nand community health. The explanations suggest that women are\\nmore proactive and vocal about climate change, conservation, and\\neco-friendly initiatives.\\nSocial Welfare and Community Involvement: The explanations\\nnote that women are more likely to be concerned with social issues\\nsuch as paid leave, affordable childcare, healthcare, and community\\nwell-being. Ads that emphasize these themes are seen as more likely\\nto appeal to women.\\nEmpathy and Emotional Appeal: The explanations often men-\\ntion that women are more responsive to ads that evoke empathy,\\nemotional concerns, and collective action. This includes ads that\\nfocus on protecting the environment for future generations and\\nsupporting social safety nets.\\nFemale Empowerment and Leadership: Some explanations\\nspecifically mention themes of women‚Äôs empowerment, leader-\\nship, and support for female scientists or leaders. These themes are\\nlikely to resonate more with female audiences who identify with\\nor support gender equality and empowerment.\\nHealth and Safety Concerns: Women are portrayed as being\\nmore attentive to issues related to health, safety, and the well-being\\nof their families and communities. This includes a strong focus on\\nenvironmental health and sustainability.7.2 Themes and Aspects of Age Explanations\\nWe prompt LLMs in a zero-shot manner to provide the common\\ntheme and aspects under specific theme of the explanations from\\n22correct young adult predictions, 75correct early working age\\ngroup predictions, 6correct late working age group predictions,\\nand 2correct senior citizen predictions. We detail the theme of\\nthe age group explanation and aspects of that explanation in Table\\n12 in App. D.\\nFrom 1ùë†ùë°row of Table 12 (App. D), we observe that the over-\\nall theme revolves around the activism and the environmental\\nconsciousness ofyoung adults , positioning them as a key demo-\\ngraphic for campaigns and initiatives focused on climate change\\nand sustainability. The explanations consistently highlight the fol-\\nlowing aspects:\\nPassion for Climate Action: Young adults are described as be-\\ning particularly passionate about addressing climate change, often\\nleading or participating in environmental activism and campaigns.\\nSupport for Bold Environmental Leadership: This age group is\\nlikely to support bold and urgent actions related to environmental\\nprotection and sustainability.\\nEngagement with Activism: The explanations emphasize that\\nyoung adults are more likely to be engaged in climate-related ac-\\ntivism and are motivated to take meaningful actions.\\nDesire for Immediate Change: There is a recurring mention of\\nthe desire for immediate and meaningful change, reflecting the\\nurgency with which young adults approach environmental issues.\\nParticipation in Training and Advocacy: The group is also char-\\nacterized as eager to participate in training programs and initiatives\\nthat allow them to contribute actively to environmental causes.\\nFrom 2ùëõùëërow of Table 12 in App. D, we can see that the overall\\ntheme revolves around the proactive and responsible mindset\\nofearly working-age adults, who are not only financially capable\\nbut also motivated by a strong sense of social and environmental re-\\nsponsibility. They are seen as key targets for initiatives that combine\\nsustainability with practical, career-oriented, and family-focused\\nbenefits. The explanations consistently emphasize the following\\naspects:Post-hoc Study of Climate Microtargeting with LLMs: Thematic Insights and Fairness Evaluation ACM, 2024, N, NN, NNN\\nEnvironmental Consciousness: This age group is described as\\nbeing highly engaged with environmental issues, such as climate\\nchange, sustainability, and clean energy. They are likely to support\\ninitiatives and products that align with eco-friendly values.\\nFinancial Stability and Disposable Income: Many explanations\\nnote that individuals in this group have disposable income, making\\nthem financially capable of supporting and investing in sustainable\\nproducts, services, and causes.\\nParental and Future Concerns: This demographic is often por-\\ntrayed as parents or future-focused individuals who are concerned\\nabout the impact of environmental issues on their children and\\nfuture generations.\\nCareer Engagement and Professional Roles: The explanations\\nfrequently mention that this age group is active in their careers,\\noften holding decision-making roles that influence corporate and\\nhousehold sustainability practices.\\nInterest in Innovation and Technology: Individuals in this age\\ngroup are also depicted as being interested in innovative industries,\\nclean energy solutions, and sustainability technologies, which align\\nwith their professional and personal goals.\\nSocial and Political Engagement: The group is characterized\\nas being engaged in socio-political issues, particularly those re-\\nlated to corporate accountability, sustainability, and environmental\\nadvocacy.\\nFrom 3ùëüùëërow of Table 12 in App. D, we can notice that the overall\\ntheme revolves around the responsibilities and concerns of indi-\\nviduals in the late working (45-64) age group, focusing on their\\nroles as homeowners, voters, and economically engaged citizens\\nwho are likely to be influenced by environmental, economic, and\\npolicy-related messaging. The explanations specifically emphasize\\nthe following aspects:\\nEconomic and Environmental Responsibility: Many of the ex-\\nplanations mention that individuals in this age group are concerned\\nwith sustainability, home energy efficiency, and environmental im-\\npact. They are likely to invest in public resources and adopt changes\\nthat contribute to economic and environmental sustainability.\\nHomeownership and Financial Stability: This demographic\\nis characterized as established homeowners who are financially\\nsecure. They are seen as key targets for changes related to home\\nenergy efficiency, such as adopting solar power, due to their finan-\\ncial means and homeownership status.\\nVoter and Policy Engagement: The explanations suggest that\\nthis age group is politically active, particularly concerned with pub-\\nlic safety, and likely to support policy changes by voting on local\\nmeasures.\\nEconomic Concerns: There is an emphasis on economic factors\\nsuch as unemployment, inflation, and gas prices, with concerns\\nabout current economic policies affecting their businesses and fi-\\nnancial stability.\\nFrom 4ùë°‚Ñérow of Table 12 (App. D), we notice that the overall\\ntheme centers on health and safety concerns that are particularly\\nimportant to senior citizens , with a focus on programs that cater\\nto their specific needs and the heightened risks they face in certain\\nsituations. The key aspects highlighted in the explanations are:\\nHealth and Wellness Programs: The first explanation mentions\\nprograms like SilverSneakers and Silver&Fit, which are specificallydesigned for senior citizens to support their physical health and\\nwell-being.\\nVulnerability and Safety: The second explanation focuses on the\\nincreased vulnerability of seniors to COVID-19 and emphasizes the\\nrisks they face, particularly in the context of political decisions or\\npublic health issues.\\n8 Fairness and Bias Analysis\\nIn this section, we present a comprehensive fairness analysis of\\nthe model for gender and age group classifications to answer RQ4 .\\nWe evaluate the models using established fairness metrics such as\\nDemographic Parity, Equal Opportunity, and Predictive Equality\\nto assess their performance across different groups. By analyzing\\nconfusion matrices and classification reports, we identify any dis-\\nparities in prediction accuracy and error rates between groups. Our\\nanalysis aims to identify biases, investigate the underlying reasons\\nfor any observed biases. The insights gained from this analysis are\\ncritical for guiding future research in developing fair and inclusive\\nalgorithms.\\nPredicted Gender Demographic Parity Ratio\\nFemale 1.0678\\nMale 0.9149\\nTable 4: Demographic Parity for Gender.\\nPredicted Age Group Demographic Parity Ratio\\nEarly Working 0.95\\nLate Working 1.50\\nSenior 0.33\\nYoung 1.16\\nTable 5: Demographic Parity for Age Group.\\nAge Group Equal Opportu-\\nnity (TPR)Predictive Equal-\\nity (FPR)\\nEarly Working 0.90 0.10\\nLate Working 0.75 0.05\\nSenior 0.33 0.00\\nYoung 0.88 0.07\\nTable 6: Equal Opportunity (True Positive Rate) and Predic-\\ntive Equality (False Positive Rate) for Age Groups.\\n8.1 Fairness Analysis on Gender Prediction\\nTo assess the fairness of the model on gender prediction, we com-\\npute several fairness metrics, including Demographic Parity, Equal\\nOpportunity, and Predictive Equality.\\nDemographic parity examines whether each gender group re-\\nceives positive predictions at equal rates. Table 4 presents the Demo-\\ngraphic Parity ratios for each gender. A ratio of 1indicates perfect\\nparity. The results show that females have a slightly higher like-\\nlihood of receiving positive predictions compared to males ,\\nsuggesting a minor imbalance favoring the female class.\\nEqual Opportunity (True Positive Rate) focuses on the True\\nPositive Rates (TPR) across gender groups, measuring the model‚ÄôsACM, 2024, N, NN, NNN Islam and Goldwasser\\nAd Text Prediction Explanations by LLMs\\nThe Hebrew University Center for Climate Science was\\nestablished in Israel to fight climate change worldwide.Young Targets university-age students interested in climate sci-\\nence to study and combat climate change.\\nTell Rep. Schrader: Now is the time to go big on climate.\\nVOTE YES on the Build Back Better Act.Young Targets young adults passionate about climate action and\\neager to influence political decisions for their future.\\nGet clean water and pollution-free electricity to all of\\nAmerica.Young Targets environmentally conscious young adults con-\\ncerned about sustainability and future impact of clean\\nwater and energy.\\nLet‚Äôs make one thing clear: Pennsylvania will be the single\\nmost competitive Senate race of 2022... It‚Äôs one of the rea-\\nsons I am running for the U.S. Senate seat in Pennsylvania.Late working Targets 45-64-year-olds by focusing on working families,\\nanti-celebrity politics, and referencing the Trump Era.\\nTable 7: Misclassified Senior Instances.\\nability to correctly identify positive instances within each group.\\nWe have Female TPR: 0.95andMale TPR: 0.85. The TPR for\\nfemales is higher by 0.10, indicating that the model is more effec-\\ntive at correctly identifying females than males .\\nPredictive Equality (False Positive Rate) assesses the False Pos-\\nitive Rates (FPR) across gender groups, reflecting the rate at which\\nnegative instances are incorrectly labeled as positive. We achieve\\nFemale FPR: 0.07andMale FPR: 0.05. The slightly higher FPR for\\nfemales suggests that females are more likely to be incorrectly\\npredicted as positive compared to males .\\n8.2 Fairness Analysis on Age Group Prediction\\nResults in Table 5 show that the late working group has a higher\\nlikelihood of receiving positive predictions , while the senior\\ngroup has a significantly lower rate, indicating potential bias\\nagainst seniors . The senior group has a Demographic Parity ratio\\nof0.33, significantly lower than the ideal value of 1, suggesting\\nunder representation in positive predictions .\\nTable 6 shows that the TPR for the senior group is notably lower,\\nsuggesting the model is less effective at correctly identifying individ-\\nuals in this age group. The senior group has an FPR of 0, indicating\\nno false positives, while the early working group has the highest\\nFPR among the groups.\\nTo understand the observed bias in the misclassified senior age\\ngroup, we conduct an analysis of the misclassified instances (Table\\n7). The goal is to identify patterns and underlying reasons why the\\nmodel predicts seniors as belonging to the young and late working\\nage group. The misclassification of the Senior age group as Young\\nor Late Working can be attributed to several factors:\\nThematic Content and Topic Association: The first three mis-\\nclassified ads (Table 7) focus on climate change, environmental\\nactivism, and sustainability. These topics are often associated with\\nyounger demographics, particularly young adults (18-24), who are\\nperceived to be more engaged in activism and environmental causes.\\nThe model appears to have learned an association between these\\ntopics and the Young age group, leading to misclassification when\\nseniors engage with similar content.\\nIn Table 7, the fourth instance shows a misclassification where\\nthe model predicts it as a late working (45-64 years) age group.\\nThe ad mentions ‚Äòworking families‚Äô, which is a term commonly\\nassociated with individuals in the Late Working age group who\\nare actively engaged in the workforce and supporting families. The\\ncontent revolves around a political campaign emphasizing the needfor change and active participation, themes often associated with\\nthe 45‚àí64age demographic who are typically more politically\\nactive and influential.\\nLack of Age-Specific Cues: The misclassified ads do not contain\\nexplicit references to seniors or age-specific language that would\\nsignal the content is intended for the senior demographic. The lan-\\nguage is broad and does not mention age-related concerns, such\\nas retirement, health issues prevalent among seniors, or senior-\\nspecific programs.\\nReliance on Stereotypical Associations: The explanations gen-\\nerated by LLMs indicate that the model relies on stereotypes, as-\\nsociating certain topics exclusively with specific age groups. By\\nassuming that environmental activism is primarily of interest to\\nyoung adults, the model overgeneralizes by overlooking the possi-\\nbility that seniors are also engaged in these issues.\\nFeature Representation Limitations: The model may lack fea-\\ntures that capture subtle cues indicating the ad‚Äôs target age group\\nwhen explicit age markers are absent. The model may not effec-\\ntively utilize contextual information that could hint at the intended\\naudience beyond topic associations. For example, the interests and\\nconcerns of the late working and senior age groups can overlap,\\nespecially in areas like politics and social change.\\n9 Conclusion\\nOur work advances the discourse on climate microtargeting by\\ndemonstrating the utility of LLMs in accurately detecting and ex-\\nplaining targeted messaging strategies on social media. We provide\\na broader theme and various aspects under each theme based on\\nthe explanations from LLMs, which we hope will be an important\\ncontribution to the CSS community. Besides, the fairness analysis\\nconducted in our study underscores the importance of evaluat-\\ning and addressing the biases. Disparities in prediction accuracy\\nand error rates, particularly in underrepresented groups, highlight\\nthe need for more inclusive and equitable targeting methods. Ul-\\ntimately, this study lays the groundwork for future investigations\\ninto the role of AI in enhancing the efficacy, transparency, and\\naccountability of digital climate advocacy.\\n10 Limitations\\nOur analysis relies on OpenAI o1-preview model. We chose o1-\\npreview instead of the open-source counterparts due to compu-\\ntational resource constraints. We show our analysis on climate\\ncampaigns dataset, but our approach can easily be adapted in any\\ndataset. It is designed to be scalable without any modifications.Post-hoc Study of Climate Microtargeting with LLMs: Thematic Insights and Fairness Evaluation ACM, 2024, N, NN, NNN\\n11 Ethics Statement\\nTo the best of our knowledge, we did not violate any ethical code\\nwhile conducting the research work described in this paper. We\\nreport the technical details for the reproducibility of results. In\\nthis paper, we did not introduce any new dataset; instead, we ex-\\nperimented using existing dataset that are adequately cited. The\\nauthor‚Äôs personal views are not represented in any qualitative re-\\nsults we report, as it is solely an outcome derived from a machine\\nlearning model.\\nReferences\\n[1]Adam Abrams, Melanie Beckerleg, and Maryia Shpak. 2022. Targeting Adver-\\ntisements and inferring demographics in the Hospitality Industry. (2022).\\n[2]W Neil Adger, Saleemul Huq, Katrina Brown, Declan Conway, and Mike Hulme.\\n2003. Adaptation to climate change in the developing world. Progress in develop-\\nment studies 3, 3 (2003), 179‚Äì195.\\n[3]Woo-kyoung Ahn, William F Brewer, and Raymond J Mooney. 1992. Schema\\nacquisition from a single example. Journal of Experimental Psychology: Learning,\\nMemory, and Cognition 18, 2 (1992), 391.\\n[4]Laura Alonso Alemany, Luciana Benotti, Hern√°n Maina, Luc√≠a Gonz√°lez, Mariela\\nRajngewerc, Lautaro Mart√≠nez, Jorge S√°nchez, Mauro Schilman, Guido Ivetta,\\nAlexia Halvorsen, et al .2022. A methodology to characterize bias and harmful\\nstereotypes in natural language processing in Latin America. arXiv preprint\\narXiv:2207.06591 (2022).\\n[5]Gabrielle Marie Allen. 2022. Targeted: How Relevant Parties Position the Ethics\\nof Online Demographic-Based Targeted Advertising. (2022).\\n[6]Jacy Anthis, Kristian Lum, Michael Ekstrand, Avi Feller, Alexander D‚ÄôAmour,\\nand Chenhao Tan. 2024. The Impossibility of Fair LLMs. arXiv preprint\\narXiv:2406.03198 (2024).\\n[7]Oana Barbu. 2014. Advertising, microtargeting and social media. Procedia-Social\\nand Behavioral Sciences 163 (2014), 44‚Äì49.\\n[8]Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret\\nShmitchell. 2021. On the dangers of stochastic parrots: Can language models\\nbe too big?. In Proceedings of the 2021 ACM conference on fairness, accountability,\\nand transparency . 610‚Äì623.\\n[9]Federico Bianchi, Pratyusha Kalluri, Esin Durmus, Faisal Ladhak, Myra Cheng,\\nDebora Nozza, Tatsunori Hashimoto, Dan Jurafsky, James Zou, and Aylin\\nCaliskan. 2023. Easily accessible text-to-image generation amplifies demographic\\nstereotypes at large scale. In Proceedings of the 2023 ACM Conference on Fairness,\\nAccountability, and Transparency . 1493‚Äì1504.\\n[10] Reuben Binns. 2018. Fairness in machine learning: Lessons from political philos-\\nophy. In Conference on fairness, accountability and transparency . PMLR, 149‚Äì159.\\n[11] Su Lin Blodgett, Solon Barocas, Hal Daum√© III, and Hanna Wallach. 2020. Lan-\\nguage (Technology) is Power: A Critical Survey of ‚ÄúBias‚Äù in NLP. In Proceedings\\nof the 58th Annual Meeting of the Association for Computational Linguistics . 5454‚Äì\\n5476.\\n[12] Emma Frances Bloomfield and Denise Tillery. 2019. The circulation of climate\\nchange denial online: Rhetorical and networking strategies on Facebook. Envi-\\nronmental Communication 13, 1 (2019), 23‚Äì34.\\n[13] Virginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology.\\nQualitative research in psychology 3, 2 (2006), 77‚Äì101.\\n[14] Virginia Braun and Victoria Clarke. 2012. Thematic analysis. American Psycho-\\nlogical Association.\\n[15] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al .2020. Language models are few-shot learners. Advances in neural\\ninformation processing systems 33 (2020), 1877‚Äì1901.\\n[16] Joy Buolamwini and Timnit Gebru. 2018. Gender shades: Intersectional accu-\\nracy disparities in commercial gender classification. In Conference on fairness,\\naccountability and transparency . PMLR, 77‚Äì91.\\n[17] Arthur Capozzi, Gianmarco De Francisci Morales, Yelena Mejova, Corrado Monti,\\nAndr√© Panisson, and Daniela Paolotti. 2021. Clandestino or Rifugiato? Anti-\\nimmigration Facebook Ad Targeting in Italy. In CHI.\\n[18] Arthur Capozzi, Gianmarco De Francisci Morales, Yelena Mejova, Corrado Monti,\\nAndr√© Panisson, and Daniela Paolotti. 2020. Facebook Ads: Politics of Migration\\nin Italy. In ICSI.\\n[19] Cheng-Han Chiang and Hung-yi Lee. 2023. Can Large Language Models Be an\\nAlternative to Human Evaluations?. In The 61st Annual Meeting Of The Association\\nFor Computational Linguistics .\\n[20] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Se-\\nbastian Gehrmann, et al .2023. Palm: Scaling language modeling with pathways.\\nJournal of Machine Learning Research 24, 240 (2023), 1‚Äì113.[21] William G Cochran. 1952. The ùúí2 test of goodness of fit. The Annals of mathe-\\nmatical statistics (1952).\\n[22] Jamell Dacon and Haochen Liu. 2021. Does gender matter in the news? detecting\\nand examining gender bias in news articles. In Companion Proceedings of the Web\\nConference 2021 . 385‚Äì392.\\n[23] Sunhao Dai, Chen Xu, Shicheng Xu, Liang Pang, Zhenhua Dong, and Jun Xu.\\n2024. Bias and Unfairness in Information Retrieval Systems: New Challenges in\\nthe LLM Era. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge\\nDiscovery and Data Mining . 6437‚Äì6447.\\n[24] Shih-Chieh Dai, Aiping Xiong, and Lun-Wei Ku. 2023. LLM-in-the-loop: Leverag-\\ning Large Language Model for Thematic Analysis. arXiv preprint arXiv:2310.15100\\n(2023).\\n[25] Stefano De Paoli. 2023. Can Large Language Models emulate an inductive The-\\nmatic Analysis of semi-structured interviews? An exploration and provocation\\non the limits of the approach and the model. arXiv preprint arXiv:2305.13014\\n(2023).\\n[26] Andrew Dessler and Lowman Student Center Theater. 1995. The science of\\nclimate change. (1995).\\n[27] Mark D√≠az, Isaac Johnson, Amanda Lazar, Anne Marie Piper, and Darren Gergle.\\n2018. Addressing age-related bias in sentiment analysis. In Proceedings of the\\n2018 chi conference on human factors in computing systems . 1‚Äì14.\\n[28] David Esiobu, Xiaoqing Tan, Saghar Hosseini, Megan Ung, Yuchen Zhang, Jude\\nFernandes, Jane Dwivedi-Yu, Eleonora Presani, Adina Williams, and Eric Smith.\\n2023. ROBBIE: Robust bias evaluation of large generative language models. In\\nProceedings of the 2023 Conference on Empirical Methods in Natural Language\\nProcessing . 3764‚Äì3814.\\n[29] Jianqing Fan, Fang Han, and Han Liu. 2014. Challenges of big data analysis.\\nNational science review 1, 2 (2014), 293‚Äì314.\\n[30] Xiao Fang, Shangkun Che, Minjia Mao, Hongzhe Zhang, Ming Zhao, and Xiao-\\nhang Zhao. 2024. Bias of AI-generated content: an examination of news produced\\nby large language models. Scientific Reports 14, 1 (2024), 5224.\\n[31] Jie Gao, Yuchen Guo, Gionnieve Lim, Tianqin Zhan, Zheng Zhang, Toby Jia-Jun\\nLi, and Simon Tangi Perrault. 2023. CollabCoder: A GPT-Powered Workflow for\\nCollaborative Qualitative Analysis. arXiv preprint arXiv:2304.07366 (2023).\\n[32] Fabrizio Gilardi, Meysam Alizadeh, and Ma√´l Kubli. 2023. ChatGPT outperforms\\ncrowd workers for text-annotation tasks. Proceedings of the National Academy of\\nSciences 120, 30 (2023), e2305016120.\\n[33] Moritz Hardt, Eric Price, and Nati Srebro. 2016. Equality of opportunity in\\nsupervised learning. Advances in neural information processing systems 29 (2016).\\n[34] Eitan D Hersh. 2015. Hacking the electorate: How campaigns perceive voters .\\nCambridge University Press.\\n[35] Luis E Hestres and Jill E Hopke. 2017. Internet-enabled activism and climate\\nchange. In Oxford Research Encyclopedia of Climate Science .\\n[36] Yasmeen Hitti, Eunbee Jang, Ines Moreno, and Carolyne Pelletier. 2019. Proposed\\ntaxonomy for gender bias in text; a filtering methodology for the gender general-\\nization subtype. In Proceedings of the First Workshop on Gender Bias in Natural\\nLanguage Processing . 8‚Äì17.\\n[37] Faye Holder, Sanober Mirza, Jake Carbone, and Ruth E McKie. 2023. Climate\\nobstruction and Facebook advertising: how a sample of climate obstruction\\norganizations use social media to disseminate discourses of delay. Climatic\\nChange 176, 2 (2023), 16.\\n[38] Jisu Huh and Edward C Malthouse. 2020. Advancing computational advertising:\\nConceptualization of the field and future directions. Journal of Advertising 49, 4\\n(2020), 367‚Äì376.\\n[39] Tunazzina Islam and Dan Goldwasser. 2022. Understanding COVID-19 Vaccine\\nCampaign on Facebook using Minimal Supervision. In 2022 IEEE International\\nConference on Big Data (Big Data) . IEEE, 585‚Äì595.\\n[40] Tunazzina Islam and Dan Goldwasser. 2024. Discovering Latent Themes in Social\\nMedia Messaging: A Machine-in-the-Loop Approach Integrating LLMs. arXiv\\npreprint arXiv:2403.10707 (2024).\\n[41] Tunazzina Islam and Dan Goldwasser. 2024. Uncovering Latent Arguments in\\nSocial Media Messaging by Employing LLMs-in-the-Loop Strategy. arXiv preprint\\narXiv:2404.10259 (2024).\\n[42] Tunazzina Islam, Shamik Roy, and Dan Goldwasser. 2023. Weakly Supervised\\nLearning for Analyzing Political Campaigns on Facebook. In Proceedings of the\\nInternational AAAI Conference on Web and Social Media , Vol. 17. 411‚Äì422.\\n[43] Tunazzina Islam, Ruqi Zhang, and Dan Goldwasser. 2023. Analysis of Climate\\nCampaigns on Social Media Using Bayesian Model Averaging. In Proceedings of\\nthe 2023 AAAI/ACM Conference on AI, Ethics, and Society (Montr√©al, QC, Canada)\\n(AIES ‚Äô23) . Association for Computing Machinery, New York, NY, USA, 15‚Äì25.\\nhttps://doi.org/10.1145/3600211.3604665\\n[44] Hosagrahar V Jagadish, Johannes Gehrke, Alexandros Labrinidis, Yannis Pa-\\npakonstantinou, Jignesh M Patel, Raghu Ramakrishnan, and Cyrus Shahabi. 2014.\\nBig data and its technical challenges. Commun. ACM 57, 7 (2014), 86‚Äì94.\\n[45] Bernard J Jansen, Kathleen Moore, and Stephen Carman. 2013. Evaluating the\\nperformance of demographic targeting using gender in sponsored search. Infor-\\nmation Processing & Management 49, 1 (2013), 286‚Äì302.ACM, 2024, N, NN, NNN Islam and Goldwasser\\n[46] Bernard J Jansen and Lauren Solomon. 2010. Gender demographic targeting in\\nsponsored search. In Proceedings of the SIGCHI Conference on Human Factors in\\nComputing Systems . 831‚Äì840.\\n[47] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, De-\\nvendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, et al .2023. Mistral 7B. arXiv preprint\\narXiv:2310.06825 (2023).\\n[48] Kai Kaspar, Sarah Lucia Weber, and Anne-Kathrin Wilbers. 2019. Personally rele-\\nvant online advertisements: Effects of demographic targeting on visual attention\\nand brand evaluation. PloS one 14, 2 (2019), e0212419.\\n[49] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke\\nIwasawa. 2022. Large language models are zero-shot reasoners. Advances in\\nneural information processing systems 35 (2022), 22199‚Äì22213.\\n[50] Hadas Kotek, Rikker Dockum, and David Sun. 2023. Gender bias and stereo-\\ntypes in large language models. In Proceedings of the ACM collective intelligence\\nconference . 12‚Äì24.\\n[51] Sachin Kumar, Vidhisha Balachandran, Lucille Njoo, Antonios Anastasopoulos,\\nand Yulia Tsvetkov. 2023. Language Generation Models Can Cause Harm: So\\nWhat Can We Do About It? An Actionable Survey. In Proceedings of the 17th\\nConference of the European Chapter of the Association for Computational Linguistics .\\n3299‚Äì3321.\\n[52] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Iliƒá, Daniel\\nHesslow, Roman Castagn√©, Alexandra Sasha Luccioni, Fran√ßois Yvon, Matthias\\nGall√©, et al .2022. Bloom: A 176b-parameter open-access multilingual language\\nmodel. (2022).\\n[53] Yingji Li, Mengnan Du, Rui Song, Xin Wang, and Ying Wang. 2023. A survey on\\nfairness in large language models. arXiv preprint arXiv:2308.10149 (2023).\\n[54] Yunqi Li, Lanjing Zhang, and Yongfeng Zhang. 2023. Fairness of chatgpt. arXiv\\npreprint arXiv:2305.18569 (2023).\\n[55] Luyang Lin, Lingzhi Wang, Jinsong Guo, and Kam-Fai Wong. 2024. Investigat-\\ning Bias in LLM-Based Bias Detection: Disparities between LLMs and Human\\nPerception. arXiv preprint arXiv:2403.14896 (2024).\\n[56] Tania Lombrozo. 2006. The structure and function of explanations. Trends in\\ncognitive sciences 10, 10 (2006), 464‚Äì470.\\n[57] Tania Lombrozo and Susan Carey. 2006. Functional explanation and the function\\nof explanation. Cognition 99, 2 (2006), 167‚Äì204.\\n[58] Kaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Amancharla, and Anupam Datta.\\n2020. Gender bias in neural natural language processing. Logic, language, and\\nsecurity: essays dedicated to Andre Scedrov on the occasion of his 65th birthday\\n(2020), 189‚Äì202.\\n[59] Sean Matthews, John Hudzina, and Dawn Sepehr. 2022. Gender and racial\\nstereotype detection in legal opinion word embeddings. In Proceedings of the\\nAAAI Conference on Artificial Intelligence , Vol. 36. 12026‚Äì12033.\\n[60] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram\\nGalstyan. 2021. A survey on bias and fairness in machine learning. ACM com-\\nputing surveys (CSUR) 54, 6 (2021), 1‚Äì35.\\n[61] Yelena Mejova and Kyriaki Kalimeri. 2020. COVID-19 on Facebook ads: competing\\nagendas around a public health crisis. In Proceedings of the 3rd ACM SIGCAS\\nConference on Computing and Sustainable Societies . 22‚Äì31.\\n[62] Craig Moritz and Rosa Agudo. 2013. The future of species under climate change:\\nresilience or decline? Science 341, 6145 (2013), 504‚Äì508.\\n[63] Grace Nosek. 2020. The Fossil Fuel Industry‚Äôs Push to Target Climate Protesters\\nin the US. Pace Envtl. L. Rev. 38 (2020), 53.\\n[64] Anja Prummer. 2020. Micro-targeting and polarization. Journal of Public Eco-\\nnomics 188 (2020), 104210.\\n[65] Filipe Nunes Ribeiro et al .2019. Inference of demographic data from digital\\nadvertising platforms based on social media. (2019).\\n[66] Kate Roberts, Anthony Dowell, and Jing-Bao Nie. 2019. Attempting rigour and\\nreplicability in thematic analysis of qualitative research data; a case study of\\ncodebook development. BMC medical research methodology 19 (2019), 1‚Äì8.\\n[67] Pasquale E Rummo, Omni Cassidy, Ingrid Wells, Jaime A Coffino, and Marie A\\nBragg. 2020. Examining the relationship between youth-targeted food marketing\\nexpenditures and the demographics of social media followers. International\\njournal of environmental research and public health 17, 5 (2020), 1631.\\n[68] Matthew Sag. 2023. Fairness and fair use in generative AI. Fordham Law Review,\\nForthcoming (2023).\\n[69] Alberto Salguero and Macarena Espinilla. 2018. A flexible text analyzer based\\non ontologies: an application for detecting discriminatory language. Language\\nResources and Evaluation 52 (2018), 185‚Äì215.\\n[70] Juan Carlos Medina Serrano et al .2020. The Political Dashboard: A Tool for\\nOnline Political Transparency. In ICWSM .\\n[71] M√°rcio Silva et al .2020. Facebook Ads Monitor: An Independent Auditing System\\nfor Political Ads on Facebook. In WWW .\\n[72] M√°rcio Silva and Fabr√≠cio Benevenuto. 2021. COVID-19 ads as political weapon.\\nInProceedings of the 36th Annual ACM Symposium on Applied Computing . 1705‚Äì\\n1710.\\n[73] Uthayasankar Sivarajah, Muhammad Mustafa Kamal, Zahir Irani, and Vishanth\\nWeerakkody. 2017. Critical analysis of Big Data challenges and analytical methods.Journal of business research 70 (2017), 263‚Äì286.\\n[74] Mark CJ Stoddart, Randolph Haluza-DeLay, and David B Tindall. 2016. Canadian\\nnews media coverage of climate change: historical trajectories, dominant frames,\\nand international comparisons. Society & Natural Resources 29, 2 (2016), 218‚Äì232.\\n[75] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\\nLachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal\\nAzhar, et al .2023. Llama: Open and efficient foundation language models. arXiv\\npreprint arXiv:2302.13971 (2023).\\n[76] Anthony G Tuckett. 2005. Applying thematic analysis theory to practice: A\\nresearcher‚Äôs experience. Contemporary nurse 19, 1-2 (2005), 75‚Äì87.\\n[77] Aleksandra Urman and Mykola Makhortykh. 2023. The silence of the LLMs: Cross-\\nlingual analysis of political bias and false information prevalence in ChatGPT,\\nGoogle Bard, and Bing Chat. (2023).\\n[78] Mojtaba Vaismoradi, Jacqueline Jones, Hannele Turunen, and Sherrill Snelgrove.\\n2016. Theme development in qualitative content analysis and thematic analysis.\\n(2016).\\n[79] Mojtaba Vaismoradi, Hannele Turunen, and Terese Bondas. 2013. Content analy-\\nsis and thematic analysis: Implications for conducting a qualitative descriptive\\nstudy. Nursing & health sciences 15, 3 (2013), 398‚Äì405.\\n[80] Stefanie Walter, Michael Br√ºggemann, and Sven Engesser. 2018. Echo cham-\\nbers of denial: Explaining user comments on climate change. Environmental\\nCommunication 12, 2 (2018), 204‚Äì217.\\n[81] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,\\nQuoc V Le, Denny Zhou, et al .2022. Chain-of-thought prompting elicits reasoning\\nin large language models. Advances in neural information processing systems 35\\n(2022), 24824‚Äì24837.\\n[82] Tianyu Wu, Shizhu He, Jingping Liu, Siqi Sun, Kang Liu, Qing-Long Han, and\\nYang Tang. 2023. A brief overview of ChatGPT: The history, status quo and\\npotential future development. IEEE/CAA Journal of Automatica Sinica 10, 5\\n(2023), 1122‚Äì1136.\\n[83] Ziang Xiao, Xingdi Yuan, Q Vera Liao, Rania Abdelghani, and Pierre-Yves Oudeyer.\\n2023. Supporting Qualitative Analysis with Large Language Models: Combining\\nCodebook with GPT-3 for Deductive Coding. In Companion Proceedings of the\\n28th International Conference on Intelligent User Interfaces . 75‚Äì78.\\n[84] Yanwu Yang, Yinghui Catherine Yang, Bernard J Jansen, and Mounia Lalmas. 2017.\\nComputational advertising: A paradigm shift for advertising and marketing?\\nIEEE Intelligent Systems 32, 3 (2017), 3‚Äì6.\\n[85] Yong Zhang, Hongming Zhou, Nganmeng Tan, Saeed Bagheri, and Meng Joo\\nEr. 2017. Targeted advertising based on browsing history. arXiv preprint\\narXiv:1711.04498 (2017).\\n[86] Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and Diyi\\nYang. 2024. Can large language models transform computational social science?\\nComputational Linguistics (2024), 1‚Äì55.\\n[87] IndrÀôe ≈Ωliobait Àôe. 2017. Measuring discrimination in algorithmic decision making.\\nData Mining and Knowledge Discovery 31, 4 (2017), 1060‚Äì1089.\\nA Prompting\\nFigure 5 shows the example prompts for age group prediction from\\nthe climate campaign dataset.\\nB Experiment Details\\nIn this section, we provide experimental results.\\nB.1 Confusion Matrix\\nFigure 6 shows the confusion matrices for the targeted demographic\\ngroup. Figure 6a shows the confusion matrix for gender, and Figure\\n6b shows the confusion matrix for age group prediction.\\nB.2 Demographic Classification Report\\nTable 8 provides the classification report with detailed performance\\nmetrics for each gender class. Table 9 shows the classification report\\nwith detailed performance metrics for each age group.\\nB.3 Demographic Misclassifications\\nTable 10 presents an analysis of ad misclassifications based on\\ngender and age group predictions.Post-hoc Study of Climate Microtargeting with LLMs: Thematic Insights and Fairness Evaluation ACM, 2024, N, NN, NNN\\nFigure 5: Prompt examples for age group prediction (shown as zero-shot). (a) young adults (18-24), (b) early working age (25-44),\\n(c) late working age (45-64). Inputs are shown in blue, and outputs are shown in red.\\n(a) gender.\\n (b) age\\nFigure 6: Confusion matrices.ACM, 2024, N, NN, NNN Islam and Goldwasser\\nGender Precision Recall F1-\\nScoreSupport\\nFemale 0.89 0.95 0.92 59\\nMale 0.93 0.85 0.89 47\\nAccuracy 0.91 106\\nMacro Avg 0.91 0.90 0.90 106\\nWeighted Avg 0.91 0.91 0.91 106\\nTable 8: Classification Report for Gender.Age Group Precision Recall F1-\\nScoreSupport\\nEarly Working 0.95 0.90 0.92 82\\nLate Working 0.50 0.75 0.60 8\\nSenior 1.00 0.33 0.50 6\\nYoung 0.76 0.88 0.81 25\\nAccuracy 0.86 121\\nMacro Avg 0.80 0.72 0.71 121\\nWeighted Avg 0.88 0.86 0.86 121\\nTable 9: Classification Report for Age Group.\\nC Bigrams and Trigrams\\nTable 11 shows the top-5 most frequent bigrams (two-word pairs)\\nand trigrams (three-word pairs) of the ad content for male, female,\\nyoung adults, and early working age groups.\\nDThematic Analysis of Age Group Explanation\\nTable 12 details the theme of the age group explanation and aspects\\nof that explanation.Post-hoc Study of Climate Microtargeting with LLMs: Thematic Insights and Fairness Evaluation ACM, 2024, N, NN, NNN\\nAd Text Actual Prediction (Mis-\\nclassification)Explanations by LLMs\\nMeet the first carbon negative fra-\\ngrance. Air Eau de Parfum converts CO2\\ninto something beautiful: sustainable\\nethanol blended with notes of orange\\npeel, fig leaf, powdery musk and to-\\nbacco invigorating scent that redefines\\nwhat is possible. Available exclusively\\nat aircompany.comMale Female The fragrance notes like fig leaf, powdery musk, and\\nsustainable ethanol typically resonate more with female\\naudiences in fragrance marketing.\\nOver generations, Illinois farm families\\nhave learned how to work with our cli-\\nmate to produce healthy crops and live-\\nstock.Female Male The ad emphasizes farming and agriculture, tradition-\\nally male-dominated fields, making men the more likely\\ntarget audience for this message.\\nRenewable energy is projected to sur-\\npass fossil gas as the dominant source\\nof energy production by 2030. Texas, a\\nleader in wind output, is in a great posi-\\ntion as the nation transitions.Young Early working This group is likely investing in sustainable energy op-\\ntions, driven by long-term economic and environmental\\nconsiderations.\\nTake Action Now to Fight\\nClimate Change! Plant a tree\\nin California treestoglobe.org/\\nplantingchallengecaliforniaEarly working Young This age group is often more environmentally active\\nand responds well to social media campaigns promoting\\nclimate action initiatives.\\nThe worst impacts of climate change\\ncould be irreversible by 2030. The time\\nto switch to a renewable energy plan is\\nnow. Not tomorrow. Not next week.Late working Early working They can switch energy plans and are motivated to act\\nbefore irreversible impacts by 2030.\\nTell Rep. Schrader: Now is the time to go\\nbig on climate. VOTE YES on the Build\\nBack Better Act.Senior Young Targets young adults passionate about climate action\\nand eager to influence political decisions for their future.\\nTable 10: Error Analysis.ACM, 2024, N, NN, NNN Islam and Goldwasser\\nDemo.\\nIndica-\\ntorGroups Top 5 Bigrams: # Occurrences Top 5 Trigrams: # Occurrences\\nYoung adults\\n(18-24)‚Äúbold climate‚Äù - 10 occurrences\\n‚Äúclimate emergency‚Äù - 10 occurrences\\n‚Äúdeclare climate‚Äù - 10 occurrences\\n‚Äúclimate leaders‚Äù - 6 occurrences\\n‚Äúemergency need‚Äù - 6 occurrences‚Äúdeclare climate emergency‚Äù - 10 occurrences\\n‚Äúbold climate leaders‚Äù - 6 occurrences\\n‚Äúclimate emergency need‚Äù - 6 occurrences\\n‚Äúclimate leaders protect‚Äù - 6 occurrences\\n‚Äúemergency need bold‚Äù - 6 occurrencesAge group\\nEarly working\\n(25-44)‚Äúclimate change‚Äù - 10 occurrences\\n‚Äúfight climate‚Äù - 7 occurrences\\n‚Äúclean energy‚Äù - 6 occurrences\\n‚Äúamerica clean‚Äù - 3 occurrences\\n‚Äúenergy future‚Äù - 3 occurrences‚Äúfight climate change‚Äù - 7 occurrences\\n‚Äúamerica clean energy‚Äù - 3 occurrences\\n‚Äúclean energy future‚Äù - 3 occurrences\\n‚Äúcalifornia treestoglobe org‚Äù - 2 occurrences\\n‚Äúfueling america clean‚Äù - 2 occurrences\\nMale‚Äúclimate change‚Äù - 5 occurrences\\n‚Äúclean energy‚Äù - 4 occurrences\\n‚Äú10 million‚Äù - 3 occurrences\\n‚Äúmillion trees‚Äù - 3 occurrences\\n‚Äúcarbon emissions‚Äù - 2 occurrences‚Äú10 million trees‚Äù - 2 occurrences\\n‚Äúcarbon emissions 50‚Äù - 2 occurrences\\n‚Äúclean energy corridor‚Äù - 2 occurrences\\n‚Äúdon live way‚Äù - 2 occurrences\\n‚Äútorched earth ale‚Äù - 2 occurrencesGender\\nFemale‚Äúclimate change‚Äù - 7 occurrences\\n‚Äúbuild back‚Äù - 6 occurrences\\n‚Äúback better‚Äù - 6 occurrences\\n‚Äúclean energy‚Äù - 5 occurrences\\n‚Äúprotect your‚Äù - 4 occurrences‚Äúbuild back better‚Äù - 6 occurrences\\n‚Äúprotect your kids‚Äù - 4 occurrences\\n‚Äúaffordable child care‚Äù - 3 occurrences\\n‚Äúfighting climate change‚Äù - 3 occurrences\\n‚Äúclimate change and‚Äù - 3 occurrences\\nTable 11: Top-5 Most Frequent Bigrams and Trigrams Across Different Demographic Indicators and Groups.\\nAge group Theme of Explanation Aspects of Explanation\\nYoung adults\\n(18-24)Activism and Environmental\\nConsciousness‚Ä¢Passion for Climate Action\\n‚Ä¢Support for Bold Environmental Leadership\\n‚Ä¢Engagement with Activism\\n‚Ä¢Desire for Immediate Change\\n‚Ä¢Participation in Training and Advocacy\\nEarly working\\n(25-44)Proactive and Responsible Mindset\\n‚Ä¢Environmental Consciousness\\n‚Ä¢Financial Stability and Disposable Income\\n‚Ä¢Parental and Future Concerns\\n‚Ä¢Career Engagement and Professional Roles\\n‚Ä¢Interest in Innovation and Technology\\n‚Ä¢Social and Political Engagement\\nLate working\\n(45-64)Responsibilities and Concerns\\n‚Ä¢Economic and Environmental Responsibility\\n‚Ä¢Homeownership and Financial Stability\\n‚Ä¢Voter and Policy Engagement\\n‚Ä¢Economic Concerns\\nSenior (65+) Health and Safety Concerns\\n‚Ä¢Health and Wellness Programs\\n‚Ä¢Vulnerability and Safety\\nTable 12: Age group based Themes and Aspects of Explanations.',\n",
       " 'Categorizing Social Media Screenshots for Identifying\\nAuthor Misattribution\\nAshlyn M. Farris\\nMathematics Department\\nHarding University\\nafarris4@harding.edu\\nMichael L. Nelson\\nDepartment of Computer Science\\nVirginia Modeling, Analysis and Simulation Center\\nOld Dominion University\\nmln@cs.odu.edu\\nAbstract\\nMis/disinformation is a common and dangerous occurrence on social media. Misattribution is a\\nform of mis/disinformation that deals with a false claim of authorship, which means a user is claiming\\nsomeone said (posted) something they never did. We discuss the difference between misinformation and\\ndisinformation and how screenshots are used to spread author misattribution on social media platforms.\\nIt is important to be able to find the original post of a screenshot to determine if the screenshot is being\\ncorrectly attributed. To do this we have built several tools to aid in automating this search process. The\\nfirst is a Python script that aims to categorize Twitter posts based on their structure, extract the metadata\\nfrom a screenshot, and use this data to group all the posts within a screenshot together. We tested this\\nprocess on 75 Twitter posts containing screenshots collected by hand to determine how well the script\\nextracted metadata and grouped the individual posts, F1= 0.80. The second is a series of scrapers\\nbeing used to collect a dataset that can train and test a model to differentiate between various social\\nmedia platforms. We collected 16,620 screenshots have been collected from Facebook, Instagram, Truth\\nSocial, and Twitter. Screenshots were taken by the scrapers of the web version and mobile version of\\neach platform in both light and dark mode.\\n1 Introduction\\nWhen talking about false information we generally refer to two categories, misinformation and disin-\\nformation. The main difference between these two categories is the intent behind them. In this section,\\nwe will discuss misattribution, a form of mis/disinformation, and how screenshots are used both as tools to\\nextend the functionality of a social media platform and spread author misattribution. We will also discuss\\nthe common methods used to locate the original post of a screenshot.\\n1.1 Misinformation and Disinformation\\nThe two main terms we use when referring to false information are misinformation and disinformation.\\nThe defining difference between these two categories is the intent with which the information is shared.\\nMisinformation is false information that is shared without the intent to mislead its audience. In other words,\\na person shares information they believe without knowing it is untrue. Disinformation, however, is informa-\\ntion that is shared with the intent to mislead its audience [1]. The focus of this report is mis/disinformation\\nin the form of misattribution. When identifying author misattribution, we are not concerned with the con-\\ntent of information but if the information is attributed to the correct person. If we take the tweet posted\\nby @TrungTPhan (Figure 1) as an example,1we can see that a screenshot of a post supposedly made by\\n@WarrenBuffett is being shared. We are not concerned if the information in @WarrenBuffett‚Äôs post is true,\\nbut if that post was created by @WarrenBuffett. This screenshot, though intended to be humorous, is an\\nexample of a fabricated screenshot and is not correctly attributed.\\n1https://x.com/TrungTPhan/status/1797659696712826969arXiv:2410.06443v1  [cs.IR]  9 Oct 20242\\nFigure 1: Screenshot of fabricated post, attributed to @WarrenBuffett, shared on Twitter.\\n1.2 Screenshots\\nOne way that misattribution commonly occurs is through screenshots shared on social media (Figure 1).\\nIn this context, a screenshot is an image of the contents of a social media post and then shared as an image,\\ninstead of via the platform‚Äôs own sharing idioms (e.g., Quote Tweeting). If a screenshot is taken of a social\\nmedia post and then shared there is no direct link back to the original post to verify if the image is attributed\\ncorrectly. Though screenshots can be fabricated and used to spread disinformation, they are often used to\\nextend a platform‚Äôs functionality. These screenshots can be used to deter engagement the original post would\\nreceive. This helps users to share information without allowing a post they disagree with or is perceived as\\nharmful to gain traction and be spread to more people (Figure 2). For example, this post2authored by\\n@TheOfficerTatum is shared claiming the post authored by @RepSwalwell contains disinformation. By\\nsharing this screenshot @TheOfficerTatum can discuss this perceived disinformation without increasing the\\ncomments, likes, and reposts of the original tweet written by @RepSwalwell. Screenshots can also be edited\\nto add emphasis or highlight important parts of a post (Figure 3), such as the yellow highlights added to this\\nscreenshot,3or pasted together to show consensus or disagreement (Figure 4). This image4demonstrates\\ntwo posts pasted together to create a concatenated screenshot. Additionally, screenshots can be used to share\\nposts across different platforms. Many social media platforms do not have a way to share a post directly\\nacross several platforms, screenshots are used to do this (Figure 5). It is quite simple to create a fake social\\nmedia post using online tools such as TweetGen5(Figure 6), GenerateStatus,6or developer tools. These\\nimages can then be posted on a social media platform and are indistinguishable from a screenshot of a real\\npost.\\n1.3 Searching for the Alleged Original Post in the Screenshot\\nThere are several ways to search for the original post of a screenshot shared on social media. The first is\\nusing a query string in a search engine such as Google. Second, searching through fact-checking websites to\\nsee if the screenshot has been debunked. Lastly, searching web archives for the existence of a screenshot‚Äôs\\noriginal post (Figure 7).8This image depicts a screenshot of a web archive page containing a Twitter post\\nauthored by @unusual whales. The post by @unusual whales contains a screenshot of two Twitter posts,\\nthe first by @OfficialLoganK and the second by @elonmusk. If the original post is found the screenshot\\n2https://x.com/TheOfficerTatum/status/1812372624347222391\\n3https://x.com/ggreenwald/status/1540768229651410944\\n4https://x.com/DefiantLs/status/1794138809254449167\\n5https://www.tweetgen.com/create/tweet.html\\n6https://generatestatus.com/\\n8https://archive.ph/h96jD3\\nFigure 2: Screenshot of a Twitter post, attributed to @RepSwalwell, shared on Twitter by @TheOfficerTatum, shared\\nto demonstrate perceived disinformation.\\nFigure 3: Screenshot of a post with yellow highlights and a red arrow added by @ggreenwald.4\\nFigure 4: Two posts shared on Twitter concatenated together to show perceived polar opinions posted by the same\\nuser.\\nFigure 5: Screenshot of a Truth Social post shared on Twitter demonstrating cross-platform sharing.75\\nFigure 6: Example of a fake tweet generated using Tweetgen (whether or not pineapple belongs on pizza is an ongoing\\ncontroversy in our research group).\\nmay be real, if it is not found by any of these methods a probability of the screenshot being misattributed\\nmust be determined. The above methods are time-consuming and would benefit from the process being\\nautomated.\\n2 Related Work\\nParikh et al. [2] developed a framework, with 83.33% accuracy, to detect tampered and impersonated\\ntweets on digital platforms. The framework can be described in several steps. First, a user uploads a screen\\ncapture to a server. Then the Name, Username, Tweet Text, and timestamp are extracted. The extracted data\\nis verified and a validation model using Twitter API keys is utilized. The Twitter database is queried and\\nan outcome of success or failure is applied. A comparison analysis is performed and used to determine if a\\ntweet has been tampered with or not. A dataset was collected using Twitter‚Äôs public API (no longer public,\\nas of February 2023 [3]) consisting of 100 screenshots. This work utilizes Twitter‚Äôs API, which is currently\\nno longer public. One limitation of this work is the inability to determine if a screenshot of a tweet has been\\nedited if the tweet is no longer live on Twitter.\\nHodges, Chaiet, and Gupta [4] studied the identification of memes in social media, including cross-\\nplatform propagation. They developed tools to classify the source of social media platforms in the images,\\nbut did not focus on verifying attribution.\\nAbdali et al. [5] proposes the use of visual cues contained in a website to detect misinformation. Using a\\nsemi-surpervised classification technique, referred to as VizFake, an F1score of 85% on a dataset of 50,000\\nnews article screenshots is achieved using less than 5% of the labels. The authors argue that the method of\\nusing the article‚Äôs text to identify misinformation has several drawbacks and thus the process is benefited by\\nusing domain-level features and visual cues.\\nZaki et al. [6] is working to automate the process of screenshot validation. To do this Zaki is annotating\\na dataset containing screenshot images collected primarily from Twitter. Zaki manually searches for their\\noriginal post on the live web or in web archives. When attempting to find the original post of a screenshot,\\nthe Tweet text, Twitter handle, and timestamp is extracted so that an effective search in the web archives\\ncan be made. To extract these elements Optical Character Recognition (OCR) is performed using Python-\\ntesseract9and the date is then extracted using the datefinder10module. Zaki has also identified several labels\\nto describe Twitter posts that can be applied to other platforms. These labels (Table 1) describe the structure\\nof a post that we can see in either an original post or a screenshot [7]. The most simple label is called a\\n9https://pypi.org/project/pytesseract/\\n10https://pypi.org/project/datefinder/6\\nFigure 7: Twitter post of a screenshot saved on web archive Archive.Today.\\nStatus11and refers to a single post (Figure 8). A combination of multiple statuses connected is called a\\nthread (Figure 9).12These posts generally include phrases such as ‚Äùreplying to‚Äù or are linked together to\\nshow the order of conversation. Co-tweets (Figure 2) specifically refers to a Twitter post with another post\\nnested within. The final label is a cropped snapshot (Figure 4), also described as concatenated posts. These\\nposts are created when a user combines multiple posts, this is an example of extending the functionality of\\na platform.\\nPost Type Structural Features\\nStatus Single Tweet\\nReply Tweet responding to another Tweet\\nCo-Tweet Nested, Retweet\\nCropped Snapshot Multiple Tweets pasted together\\nTable 1: Labels describing the structural features of social media posts [6].\\nBradford and Nelson [8], are developing methods of searching the web for evidence of a tweet‚Äôs exis-\\ntence. To do this a modular system called SSAuth is being implemented. SSAuth , automatically searches not\\nonly the web, but Snopes.com13and Reuters.com,14fact-checking websites, for the text content of a tweet.\\nHe also is developing tools to search Politwoops,15a website that saves deleted political tweets. Brad-\\n11https://x.com/bennyjohnson/status/1797411023881654627\\n12https://x.com/elonmusk/status/1260852444818362369\\n13https://www.snopes.com/\\n14https://www.reuters.com/\\n15https://projects.propublica.org/politwoops7\\nFigure 8: Status: Posted on Twitter.\\nFigure 9: Thread: Screenshot of Twitter post containing a thread.8\\nford built SSAuth using a Python script that passes the content of a tweet using command line arguments.\\nBradford has discovered that the most efficient queries only use the first 50 characters of a tweet.\\nNwala et al. describe a vocabulary to label the internal structure of web resources [9]. This vocabulary is\\nbased on an acronym system describing the number of posts and authors in a web resource (Table 2). These\\ncategories are used to describe the ways a resource naturally occurs in a system, not necessarily what we see\\nin a screenshot. Because of this we can only apply these categories to some of the labels observed by Zaki,\\nfor example, a status would occur in the system as P1A1, a thread and co-tweet could occur either as PnA1\\norPnAn, but a cropped-snapshot would never occur naturally on the platform since it is an extension of its\\nfunctionality.\\nAcronymn Post Count Author Count\\nP1A1 Single (1) Single (1)\\nP1An Single (1) Multiple (n)\\nPnA1 Multiple (n) Single (1)\\nPnAn Multiple (n) Single (1)\\nTable 2: Acronymn classification proposed by Alexander Nwala to categorize internal structure of web resources.\\n3 Methodology\\nTo automatically search for original social media posts, it is important to be able to identify from which\\nplatform a screenshot originated. It is quite easy for humans to identify the platform due to the visual\\ncues and prior knowledge of what platforms look like, but this process is difficult to automate. Besides\\ndifferentiating between platforms, the number of posts within the screenshot needs to be identified and the\\nmetadata extracted from the image to make effective search queries for the original post to determine correct\\nattribution.\\n3.1 Extracting Metadata\\nBefore metadata could be extracted Optical Character Recognition (OCR) was performed on the screen-\\nshot using Python-tesseract (Figure 10). When an image was passed to the OCR module, a text string\\ncontaining all the characters contained within the image was returned. To extract the timestamp the Python\\ndatefinder module was used to identify and reformat dates. To differentiate between valid timestamps and\\ninessential dates (Figure 11)16contained in the text body, the Python RegEx17module was utilized to create\\na pattern that would identify the possible formats a date could occur in specifically for Twitter. The string\\nreturned by OCR was parsed into individual lines and any line containing a date that matched the predeter-\\nmined pattern was identified. Each line containing a date was then searched for any words that should not\\nbe present in the same line as the meaningful date and if found were eliminated. A text file forked from the\\nGitHub repository, google-10000-english [10], was used to identify words that should not be contained in\\na line with a meaningful date. This isolated the meaningful dates from the inessential ones present within\\nthe text body. A similar logic to the date module was applied to differentiate authors from users mentioned\\nin the body of a post. Authors were identified by an address sign (@) followed by 4‚Äì15 characters. Any\\nauthor that was embedded in a line containing a common word was eliminated as a potential author for the\\npost. All dates and authors were then returned in the order they occurred in the screenshot and used to create\\nself-contained units within a single screenshot.\\n16https://x.com/NASAClimate/status/1817939517094986097\\n17https://pypi.org/project/regex/9\\nFigure 10: Post compared to OCR: Side-by-side comparison of a Twitter post and the post after OCR is performed.\\nFigure 11: Screenshot of Twitter post with inessential dates (green boxes added for emphasis, not original to post).\\n3.2 Categorizing Screenshots\\nAfter all necessary metadata was extracted the posts were then categorized based on the internal struc-\\nture that the original post would have likely occurred in by first grouping posts contained in the screenshot\\ntogether. The number of posts was determined by the number of meaningful dates contained in the screen-\\nshot and the authors were determined by the number of meaningful usernames found in the screenshot. The\\nnumber of posts and authors were used to determine the internal structure as described by Nwala et al. [9].\\nPosts were separated by the location of authors, for example in the screenshot of the thread conversation\\nbetween @elonmusk and @ylecun (Figure 9), the first post was identified by all of the text above the post\\nauthored by @ylecun, the second identified by the text between the two post between @elonmusk, and the\\nfinal post consisting of all the text remaining (Figure 12).\\n3.3 Building a Dataset\\nTo determine which platform a screenshot originated from a collection of images from different plat-\\nforms needed to be collected so that we could later build an accurate model. We developed a modular scraper\\nsystem using the headless browser Puppeteer18to take screenshots of posts on four platforms: Twitter, In-\\nstagram, Truth Social, and Facebook. The scraper system was passed a list of URLs obtained from four\\nseparate publicly available datasets via command line arguments (Table 3). If a full URL was not available\\nin a dataset, the link could be built backward using the post ID. For example, when given an Instagram post\\nID,InstagramID , the complete URL was built using the format https://www.instagram.com/p/ InstagramID /.\\nThe URLs were extracted from files within each dataset and saved in a .txt file (Figure 13). Screenshots were\\ntaken of each URL in light mode, dark mode, web version, and mobile version. If needed, screenshots were\\ncropped using ImageMagick.19Pre-built datasets were also searched for to increase the size of the dataset.\\nWe found one dataset created by Bot Sentinel [11] that contains 1,363 screenshots of tweets that harassed\\nKamala Harris. These images were cropped using ImageMagick and added to the total of the Twitter mobile\\nlight category.\\n18https://pptr.dev/\\n19https://imagemagick.org/10\\nFigure 12: Twitter posts of conversation between @elonmusk and @ylecun (Figure 9) grouped together by metadata\\nafter OCR.\\nhttps://twitter.com/BMW LifeMorals/status/241301682477232128\\nhttps://twitter.com/BMW LifeMorals/status/253567721424445441\\nhttps://twitter.com/BMW LifeMorals/status/257181645101211649\\nhttps://twitter.com/BMW LifeMorals/status/261522710146977792\\nhttps://twitter.com/BMW LifeMorals/status/261995784894029824\\nhttps://twitter.com/BMW LifeMorals/status/269467393900830721\\nhttps://twitter.com/BMW LifeMorals/status/272380339362623488\\nhttps://twitter.com/BMW LifeMorals/status/272402989015244801\\nhttps://twitter.com/BMW LifeMorals/status/272873624023732224\\nFigure 13: .txt file containing Twitter URLs.11\\nPlatform URL Citation\\nFacebook https://www.kaggle.com/datasets/mchirico/cheltenham-s-facebook-group [12]\\nInstagram https://www.kaggle.com/datasets/shmalex/instagram-images [13]\\nTruth Social https://zenodo.org/records/7531625 [14]\\nTwitter http://nlp.uned.es/replab2013/ [15]\\nTable 3: Dataset links from which the URLs were obtained and used to take screenshots of each platform.\\n4 Results and Discussion\\nTo evaluate the categorization of Twitter posts by internal structure, 75 screenshots that were shared on\\nTwitter were collected and annotated by hand. Each screenshot was categorized by its internal structure.\\nThe categorization script was then tested on these 75 screenshots and a precision, recall, and F1score was\\ncalculated for each internal category as well as the mean scores (Table 4). Note the deficiency in the number\\nof screenshots in the internal category PnA1, this category had significantly fewer images than the other\\nthree and thus there an increase in the number of images would drastically change the evaluation metrics\\nfor this category. The internal category of P1A1performed the best, as would be expected since it is the\\nleast complicated form of Twitter Post. The most common reason a post was categorized incorrectly was an\\nincorrect extraction of text from the image when OCR was performed.\\nCategory Precision Recall F1\\nPnAn(k=18) 0.93 0.72 0.81\\nPnA1(k=4) 0.50 0.75 0.60\\nP1A1(k=53) 0.95 0.98 0.96\\nOverall (k=75) 0.79 0.82 0.80\\nTable 4: Precision, recall, and F1score for classification of images ( k= 75 ) by internal structure.\\nA confusion matrix (Figure 14) was then generated to demonstrate the number of images that were\\ncategorized correctly and incorrectly by class. The matrix shows the true labels, what the actual structure\\nof the post was, and how many times they were classified correctly. The predicted labels, the structure the\\nclassifier assigned to a post, show how many times the image were correctly and incorrectly classified by\\nour script.\\nA stacked bar chart (Figure 15) was generated to demonstrate the percentage of images grouped by their\\nmetadata correctly and incorrectly. This means that for every post within a screenshot, the author, timestamp,\\nand text body were grouped correctly. The bar chart displays the percentage of all images ( k= 75 ) grouped\\ncorrectly and incorrectly alongside the percentage of metadata grouped correctly for images belonging to\\neach structure. Again, due to the deficiency in images within the internal structure PnA1it is difficult to\\ndraw conclusions based on this category. However, we can see about 73% of the images were grouped by\\ntheir metadata correctly. A difficulty presented with the category PnAnwas concatenated images. Often a\\nconcatenated image would not contain the necessary metadata to group the images. A second complication\\nwas OCR not extracting text from an image accurately. This would distort the posts and make them difficult\\nto group.\\nTo eventually build a model to effectively differentiate between various platforms a large dataset first\\nneeded to be created for training and testing purposes. Images were collected from Facebook, Instagram,\\nTruth Social, and Twitter in light mode, dark mode, web version, and mobile version. Since four platforms\\nwere being focused on there were four categories for each platform, 16 social media scrapers were built to\\ncapture screenshots of the posts. Each scraper was passed the .txt file of URLs as an argument and took12\\nFigure 14: Confusion matrix for number of images ( k= 75 ) placed into the correct internal category.\\nFigure 15: Percentage of Images ( k= 75 ) grouped by metadata broken down by internal structure.13\\nscreenshots of all non-broken URLs. Overall we were able to collect about 16,000 images (Table 5).\\nSeveral barriers needed to be crossed when taking screenshots of social media posts. For example, when\\nattempting to only capture the contents of a post instead of the entire screen an HTML element needed to\\nbe identified to mark the boundaries of the post. However, the Facebook web version does not contain a\\nmeaningful element of the isolated post that can be used to do this. To overcome this a screenshot was taken\\nby the scraper of the full web page and ImageMagick was used to crop all the images collected. A second\\nissue that needed to be overcome was the potential of the platform to recognize the scraper as a bot. Several\\nmethods were used to overcome this with varying success. A sleep function was added to the scrapers to\\npause for a random amount of time between two and twelve seconds. A second delay was added to pause\\nthe scrapers for five minutes for every 100th URL. This allowed more images to be taken at a time without\\nsignificant delay, however, in the case of Instagram only several hundred images could be taken without\\nhaving to pause for an extended period due to the platform no longer loading posts.\\naaaaaaaaaaaaWeb /Mobile\\nLight /DarkPlatform\\nFB IG TS T\\nMD 788 454 1,777 1,027\\nML 797 524 1,781 2,563\\nWD 989 398 848 1,285\\nWL 987 823 846 734\\nPlatform Total 3,561 2,199 5,252 5,609\\nTable 5: Number of images collected using scrapers.\\n5 Conclusions\\nFalse information in the form of misattribution is often spread by sharing screenshots on social media.\\nThough these screenshots help extend the functionality of a platform they often are used to persuade others\\nto believe someone is saying something that they never actually said. Because a screenshot has no link back\\nto the original post, it is difficult to tell if the post is being correctly attributed without searching the web or\\ninternet archives. This calls for tools to be developed to automate the search process for an original tweet.\\nThis project aims to develop tools to aid in the process of automating queries for an original post. So far\\nthis research has focused on extracting metadata from screenshots of Twitter posts, assigning a category to\\nthese screenshots based on the internal structure they would likely occur in on the Twitter platform, and\\ngrouping all posts captured in a screenshot together by their metadata. We evaluated the categorizer module\\nof the script by performing a precision and recall test and calculating the F1score. We found that the overall\\nprecision was 0.79, the recall was 0.82, and the F1score was 0.80. To understand how well the grouping\\nmodule of the script was performing, we calculated the percentage of images correctly grouped by their\\nmetadata and found it to be 73.33%. We also collected a dataset that can aid in training and testing a model\\nto differentiate between various social media posts. The platforms focused on in this dataset were Facebook,\\nInstagram, Truth Social, and Twitter. So far over 16,000 images have been collected.14\\n6 Acknowledgements\\nThis work is supported in part by the National Science Foundation Research Experience for Undergrad-\\nuates Site Award #2149607. We would like to thank the graduate students in the Web Sciences and Digital\\nLibraries Research Group at Old Dominion University, especially Tarannum Zaki, for their assistance with\\nthis work, as well as the faculty who dedicated their time and experience to advising this project. We are also\\nvery appreciative of Prof. Jennifer Golbeck and her recommendation of the dataset created by Bot Sentinel.15\\nReferences Cited\\n[1] American Psychological Association, ‚ÄúMisinformation and disinformation.‚Äù\\nhttps://www.apa.org/topics/journalism-facts/misinformation-disinformation, 2022.\\n[2] S. B. Parikh, S. R. Khedia, and P. K. Atrey, ‚ÄúA framework to detect fake tweet images on social media,‚Äù in 2019\\nIEEE Fifth International Conference on Multimedia Big Data (BigMM) , pp. 104‚Äì110, 2019.\\n[3] A. Blok, ‚ÄúTwitter API is going behind the paywall.‚Äù https://www.cnet.com/news/social-media/twitter-api-is-\\ngoing-behind-the-paywall/, 2023.\\n[4] J. A. Hodges, M. Chaiet, and P. Gupta, ‚ÄúForensic analysis of memetic image propagation: introducing the\\nSMOC BRISQUEt method,‚Äù Proceedings of the Association for Information Science and Technology , vol. 58,\\nno. 1, pp. 196‚Äì205, 2021.\\n[5] S. Abdali, R. Gurav, S. Menon, D. Fonseca, N. Entezari, N. Shah, and E. E. Papalexakis, ‚ÄúIdentifying misin-\\nformation from website screenshots,‚Äù in Vol. 15 (2021): Fifteenth International AAAI Conference on Web and\\nSocial Media , pp. 2‚Äì13, 2021.\\n[6] T. Zaki, M. L. Nelson, and M. C. Weigle, ‚ÄúExtracting information from twitter screenshots,‚Äù Tech. Rep.\\narXiv:2306.08236, 2023.\\n[7] T. Zaki, ‚ÄúDisinformation spread on social media through screenshot sharing: Dataset description.‚Äù https://ws-\\ndl.blogspot.com/2022/12/2022-12-12-disinformation-spread-on.html, 2022.\\n[8] C. Bradford and M. L. Nelson, ‚ÄúDid they really tweet that? Querying fact-checking sites and Politwoops to\\ndetermine tweet misattribution,‚Äù Tech. Rep. arXiv:2211.09681, 2022.\\n[9] A. C. Nwala, M. C. Weigle, and M. L. Nelson, ‚ÄúUsing micro-collections in social media to generate seeds for\\nweb archive collections,‚Äù in Proceedings of the 2019 ACM/IEEE Joint Conference on Digital Libraries (JCDL) ,\\npp. 251‚Äì260, 2019.\\n[10] J. Kaufman, ‚Äúgoogle-10000-english.‚Äù https://github.com/first20hours/google-10000-english, 2021.\\n[11] Bot Sentinel Inc., ‚ÄúTwitter‚Äôs response to abuse and bigotry directed at vice president Kamala Harris.‚Äù\\nhttps://botsentinel.com/reports/documents/kamala-harris/report-05-26-2022.pdf, 2022.\\n[12] M. Chirico, ‚ÄúCheltenham‚Äôs facebook groups.‚Äù https://www.kaggle.com/datasets/mchirico/cheltenham-s-\\nfacebook-group/data, 2018.\\n[13] A. Matusevski, ‚ÄúInstagram images - 1,211,625 posts.‚Äù https://www.kaggle.com/datasets/shmalex/instagram-\\nimages, 2022.\\n[14] P. Gerard, N. Botzer, and T. Weninger, ‚ÄúTruth social dataset [data set].‚Äù https://doi.org/10.5281/zenodo.7531625,\\n2023.\\n[15] E. Amig ¬¥o, J. Carrillo de Albornoz, I. Chugur, A. Corujo, J. Gonzalo, T. Mart ¬¥ƒ±n, E. Meij, M. de Rijke, and\\nD. Spina, ‚ÄúOverview of RepLab 2013: Evaluating online reputation monitoring systems,‚Äù in Proceedings of the\\nFourth International Conference of the CLEF initiative , pp. 333‚Äì352, 2013.',\n",
       " \"On the Use of Proxies in Political Ad Targeting\\nPIOTR SAPIEZYNSKI‚àó‚Ä†, LEVI KAPLAN‚àó, and ALAN MISLOVE, Northeastern University, USA\\nALEKSANDRA KOROLOVA, Princeton University, USA\\nDetailed targeting of advertisements has long been one of the core offerings of online platforms. Unfortunately,\\nmalicious advertisers have frequently abused such targeting features, with results that range from violating\\ncivil rights laws to driving division, polarization, and even social unrest. Platforms have often attempted\\nto mitigate this behavior by removing targeting attributes deemed problematic, such as inferred political\\nleaning, religion, or ethnicity. In this work, we examine the effectiveness of these mitigations by collecting\\ndata from political ads placed on Facebook in the lead up to the 2022 U.S. midterm elections. We show that\\nmajor political advertisers circumvented these mitigations by targeting proxy attributes: seemingly innocuous\\ntargeting criteria that closely correspond to political and racial divides in American society. We introduce\\nnovel methods for directly measuring the skew of various targeting criteria to quantify their effectiveness as\\nproxies, and then examine the scale at which those attributes are used. Our findings have crucial implications\\nfor the ongoing discussion on the regulation of political advertising and emphasize the urgency for increased\\ntransparency.\\nCCS Concepts: ‚Ä¢Social and professional topics ‚ÜíComputing / technology policy ;‚Ä¢Information\\nsystems‚ÜíSocial advertising ;‚Ä¢Applied computing ‚ÜíDigital libraries and archives .\\nAdditional Key Words and Phrases: political advertising, ad targeting, social media, demographic proxies, filter\\nbubble, algorithm audits\\n1 Introduction\\nOnline social media platforms, including Facebook, Google and TikTok, have developed powerful\\nadvertising services based on targeted advertising , by which an advertiser can specify which segment\\nof the user base they want to reach [ 89]. These targeting criteria are significantly more fine-grained\\nthan what was possible through previous forms of advertising (e.g., newspapers or television).\\nThese targeting criteria often includes targeting options ranging from demographic characteristics,\\nto location, education, ethnic affinity, political affiliation, and many thousands of interests, both\\nself-declared by the users and inferred by the platform [ 114]. The resulting capability to select\\nnarrow groups of users is welcome by the advertisers but also raises societal concerns in domains\\nranging from life opportunity advertising (e.g., housing, credit, and jobs) [ 7,9,110] to health [ 70].\\nIn this paper, we focus on targeted political advertising on Facebook, which has raised similar\\nconcerns [ 3,13,61,98,105,118,129]. Fine-grained targeting has been shown to enable voter ma-\\nnipulation and misinformation, by allowing political actors to craft different and manipulative ad\\nmessages for specific audiences on Facebook, unbeknownst to the recipients and without over-\\nsight [ 19,98]. Despite these controversies, Facebook has remained steadfast in their commitment\\nto allowing political advertisements on the platform; arguing that they are essential to a free and\\nopen democracy [ 25]. Although Facebook blocked the creation of new political ads in the U.S. the\\nweek before the 2022 U.S. midterm elections, it allowed preexisting ads to continue running and\\nquickly rescinded the ban on new ads after the election. Facebook‚Äôs primary response to concerns\\nregarding the abuse of fine-grained targeting in political advertising took the form of creating\\nthe Facebook Ad Library [ 28] and removal of a number of targeting capabilities [ 82]. Targeting\\n‚àóThese authors contributed equally to this research.\\n‚Ä†Corresponding author.\\nAuthors‚Äô Contact Information: Piotr Sapiezynski, p.sapiezynski@northeastern.edu; Levi Kaplan, kaplan.l@northeastern.edu;\\nAlan Mislove, amislove@ccs.neu.edu, Northeastern University, Boston, MA, USA; Aleksandra Korolova, korolova@princeton.\\nedu, Princeton University, Princeton, NJ, USA.arXiv:2410.14617v1  [cs.CY]  18 Oct 20242 Piotr Sapiezynski, Levi Kaplan, Alan Mislove, and Aleksandra Korolova\\ncriteria removed to date include the ability to target by ethnicity1and political affiliation, as well\\nas a number of interests and topics that Facebook deems sensitive, such as health causes, sexual\\norientation, religious practices and groups, and social issues, causes, organizations, and public\\nfigures [ 82]. Importantly, Facebook did not provide a comprehensive list of what targeting options\\nthey removed, and left the possibility for Facebook to continue to review, update and remove\\ntargeting options.\\nIn this work, we investigate whether the approach taken by Facebook‚Äîthat of removal of\\nparticular targeting capabilities‚Äîachieves its intended public interest goals, or whether advertisers\\nare able to work around this restriction. Specifically, it has long been known‚Äîincluding by Facebook\\nitself [ 113]‚Äîthat certain facially-neutral interests can serve as proxies for political affiliation, race,\\nor gender. For example, most users with an interest in Country music are white, whereas most\\nusers with an interest in R‚Äôn‚ÄôB music are Black. Thus, even though musical preferences are facially\\nneutral, when used for targeting ads they can serve as efficient proxies for race, and thus, enable\\nexclusion of certain racial groups from particular kinds of advertising.\\nWe focus on the case of political advertising, and study whether, in response to removal of\\nexplicit political interest targeting categories by Facebook, the political advertisers have moved to\\ntargeting by proxies instead. We do so in three steps. First, we develop methodologies to measure\\nwhich interests serve as effective proxies for political affiliation and ethnic affinity without access\\nto any proprietary Facebook data, and without relying on the names of interests. Second, we crawl\\nthe Facebook Ad Library to collect targeting information from all advertising accounts which ran\\npolitical ads during the run-up to the U.S. 2022 midterm election. We focus on political advertising\\nbecause that is the only category of advertisers for whom Facebook makes targeting data available in\\nits Ad Library. Finally, we utilize the methodologies we develop and data we obtain to measure the\\nextent to which political advertisers rely on proxy attributes to circumvent the targeting limitations\\nintroduced by Facebook during the U.S. 2022 midterm elections. We hypothesize that the trends for\\nproxy use apply also to advertisers in other categories, such as employment, housing, and health.\\n1.1 Contributions\\nOur contributions can be summarized as follows:\\nNovel methodologies for measuring skews of targeting parameters to establish their\\nproxy power. In the five weeks leading up to the 2022 midterm elections, political advertisers\\nused over 19,000 unique interests, demographics, and behaviors to target ads. A core challenge in\\nidentifying which of these were effectively proxies for the targeting options removed by Facebook\\nis the measurement of the political and racial skew of these varied targeting criteria at scale, without\\nreliance on proprietary datasets, and without reliance on the names of these criteria. To address\\nthis challenge, we develop and implement two novel measurement approaches. They both rely on\\nthe use of publicly available data combined with non-traditional use of Facebook advertiser tools,\\nand they do not require any privileged access to internal Facebook data. Moreover, they do not\\nrequire running ads or paying Facebook, making the identification of highly skewed interests at\\nscale feasible to other public interest researchers.\\nThe first approach allows for a direct measurement of popularity of different targeting criteria\\namong politically and racially uniform Custom Audiences. The second approach combines data\\nobtained through the Facebook Audience Insights tool‚Äîwhich reports Facebook pages popular\\namong users associated with each interest‚Äîwith an external dataset containing measures of political\\nbiases of different Internet domains. Crucially, neither of these approaches relies on interpreting\\n1Facebook had previously allowed targeting by ‚Äúethnic affinity‚Äù rather than ethnicity explicitly. The categories included\\n‚ÄúAfrican-American Affinity‚Äù, ‚ÄúHispanic Affinity‚Äù, and ‚ÄúAsian Affinity‚Äù. ‚ÄúWhite‚Äù was not an available category.On the Use of Proxies in Political Ad Targeting 3\\nthe labels that the platform uses to describe the targeting interests. Instead, we report qualitative\\nmeasures of the efficacy of each interest as a proxy for a variable of interest. The skew measurement\\nresults of both approaches are highly correlated, demonstrating their viability and providing\\nconfirmation that they are indeed measuring the potential for these targeting features to serve as\\nproxies.\\nDocumenting the use of proxies in the wild. We then examine the extent to which real political\\nadvertisers target and exclude particular sub-populations via proxies. We find over 22,000 unique\\nadvertising accounts with active political ads in the five weeks leading up to the 2022 midterm\\nelections. We show advertisers spent millions of dollars targeting or excluding audiences using\\ninterests with strong partisan and/or race skews. Such behavior is problematic both because it\\nis accomplishing things that are, under the current ad targeting system design, intended to be\\nforbidden and because, by doing so via proxies, it makes the behavior harder to detect.\\nTaken together, our findings demonstrate that the current approach taken by platforms (and\\nFacebook in particular) to prevent targeting by race or political leaning through the removal of select\\ntargeting criteria is ineffective, and that advertises can‚Äîand do‚Äîcircumvent the protections put in\\nplace with some sophistication and negligible cost. Using political advertising as a case study, we\\ndemonstrate that proxy targeting is widely used by actors on both sides of the U.S. political spectrum.\\nOur findings, and the process of obtaining them, motivate the need for increased transparency\\nfrom the ad platforms as well as regulation that would require a radically different approach to\\npreventing abuse of fine-grained ad targeting capabilities in domains of societal importance.\\n2 Background and Related Work\\nThe fine-grained targeting options offered by Facebook enable both benign and malicious advertis-\\ners to reach individuals with unparalleled precision. In this section, we first provide the necessary\\nbackground on targeting capabilities offered by Facebook to advertisers. We detail notable examples\\nof use and abuse of the targeting features that led to spreading of hate speech, discrimination in\\naccess to opportunities, or political manipulation. We highlight the steps that legislators and Face-\\nbook have taken to address those harms, such as limiting the targeting capability and progressive\\nintroducing of transparency mechanisms.\\n2.1 Online targeting capabilities\\nTraditional print media and television advertising allows for rudimentary targeting of broad\\naudiences by placing ads strategically in outlets particularly popular among the target audience. For\\nexample, an advertiser seeking attention from men could place an ad in the Men‚Äôs Health magazine\\nand reach an audience of predominantly men. Online advertising, on the other hand, enables far\\nmore fine-grained targeting of individuals. Advertisers can narrow down the target audience by\\ndescribing their location (from countries down to particular neighborhoods), demographic attributes\\n(including age, gender, education, or marital and parental status), as well as both self-reported, and\\ninferred interests (more than 50,000 unique interests were available for targeting on Facebook at\\nthe time of this work) [ 89]. To further increase the breadth of targeting criteria, Facebook has also\\npurchased inferences about their users from third parties in the past [ 121]. Moreover, advertisers\\ncan use a tool called Custom Audiences to target particular individuals directly through personally\\nidentifiable information (PII), such as names and addresses, device/advertising IDs, emails, or\\nphone numbers [ 87]. When using Custom Audiences, the advertiser uploads a list containing PII of\\nindividuals; Facebook matches this information with actual Facebook users and delivers the ads to\\na subset of them. A Lookalike Audience feature allows the advertiser to direct Facebook to create an\\naudience of users similar to the users in the Custom Audience. The definition of that ‚Äúsimilarity‚Äù4 Piotr Sapiezynski, Levi Kaplan, Alan Mislove, and Aleksandra Korolova\\nis not public, but previous work showed that Lookalike Audiences carry over demographic and\\npolitical biases from the custom audiences they were built upon [ 110,114]. Combining various\\nforms of targeting through boolean logic further extends the advertisers‚Äô ability to precisely target\\nusers [ 93]. For example, advertisers can narrow down a Custom Audience to users who have a\\nspecific interest and fall into a particular age group. Furthermore, all of these tools can also be used\\nforexcluding , rather than targeting, i.e. specifying the individuals who should notbe shown the ad.\\nDespite the vast data collection and inference apparatus, multiple studies reported that users\\nfind a significant fraction of inferences made about them inaccurate [ 109,121]. Nevertheless, the\\naccuracy of these inferences is irrelevant for their effectiveness as tools for micro-targeting, as\\nlong as they enable advertisers to reach homogeneous groups. For example, if Facebook infers that\\nBlack users are much more likely than white users to be interested in ‚ÄúThe Breakfast Club (movie)‚Äù\\nthat interest can be used as an effective proxy for race, despite the fact that this inference is likely\\nwrong.2\\n2.2 Ad delivery optimization\\nThe process of audience selection does not end with the advertiser‚Äôs targeting decisions. Because\\nad budgets are generally not high enough for the ad to reach every user in the targeted audience,\\nFacebook strategically sub-selects who among the targeted audience will actually see the ad. This\\nchoice is not random; instead, it is made in a way that optimizes both the stated goal of the advertiser\\n(for example, maximizing the number of clicks, or product purchases, or impressions received), and\\nFacebook‚Äôs optimization objectives, such maximizing user time spent on the platform. This process,\\nreferred to as ‚Äúad delivery optimization‚Äù, has unfortunately been shown to produce discriminatory\\neffects through skewed distribution of opportunity ads [ 2,55,56,59]. In the case of political ads,\\nAli et al . [3] have shown that the ad delivery optimization can lead to price discrimination based\\non whether a politician‚Äôs political leaning is deemed aligned with that of the target audience by\\nFacebook‚Äôs machine learning algorithms. This means that the delivery algorithm may charge two\\npolitical advertisers vastly different amounts to show their ads to the same target audience at the\\nsame time, effectively limiting the diversity of political messages shown to the users.\\n2.3 Micro-targeting\\nThe practice of carefully targeting political ads predates social media. Even prior to the arrival\\nof Facebook and the proliferation of data brokers, political strategists would purchase customer\\ninformation from loyalty programs, subscriptions, online stores, or car dealers [ 33,49,99]. In\\ncombination with voter records, these datasets enabled them to identify proxy attributes, such\\nas which car brands, or alcohol types were preferred by voters in each party. For example, in\\n2004, the successful reelection of George W. Bush was at least partially attributed to the outreach\\ntargeting individuals identified using such methods [ 49]. The 2008 presidential campaign of Barrack\\nObama relied heavily on combining the above mentioned data sources with extensive surveys,\\nresulting in rich profiles of individual voters. These profiles were then used to predict susceptibility\\nof individuals to different conversation scripts and forms of outreach [ 57]. Despite the rapidly\\ngrowing emphasis on data-driven voter mobilization, online advertising remained a minuscule\\nfraction of political advertising budgets in 2012, below 1.7% [20].\\n2This example is not hypothetical; we found that the interest ‚ÄúThe Breakfast Club (movie)‚Äù is indeed predominantly\\nassociated with Black users on Facebook. We note that the The Breakfast Club is also a name of a radio show that is\\nunrelated to the 1985 movie The Breakfast Club (to which the name of the targeting interest refers). We hypothesize that\\nFacebook‚Äôs inference methodology is incorrectly associating interest in the radio show with the movie, but regardless, the\\nattribute serves as an effective proxy.On the Use of Proxies in Political Ad Targeting 5\\nHowever, advertising platforms began collecting significant amounts of data and introducing\\npowerful targeting features to advertisers. This had the effect of democratizing access to targeted\\nadvertising: rather than each political campaign having to obtain and develop its own data to identify\\nparticular kinds of constituents, their propensity to vote, susceptibility to particular messages,\\nFacebook performed all the data mining, inferences, and packaging, and offered it as a service.\\nMoreover, Facebook did so based on all profile and activity data available about each individual,\\non and off the platform, rather than being limited to the more coarse grained information in\\nvoter records and loyalty programs. Facebook even provided dedicated employees to help political\\ncampaigns take advantage of its tools [14].\\nThe use of micro-targeting for political advertising entered the public consciousness during the\\n2016 U.S. presidential election. First, the winning campaign of Donald J. Trump relied heavily on\\ntargeted advertising on Facebook. The campaign ran up to 175,000 variants of the same ad per\\nday [ 66], and did not deploy traditional TV advertising until late in the campaign [ 97]. Importantly,\\nthe campaign used the available tools to reach Democratic voters in order to dissuade them from\\nvoting altogether [ 46]. The digital director of Trump‚Äôs campaign, Brad Parscale, went as far as\\nto say that ‚ÄúFacebook was going to be how he [Trump] won‚Äù, and praised the ability to target\\nsmall, specific audiences, otherwise unreachable using traditional advertising [ 14]. Second, the\\nRussian company Internet Research Agency (IRA) ran thousands of targeted Facebook ads in an\\nattempt to sow division in the American public, lower African American turnout, and thus bolster\\nTrump‚Äôs campaign [ 98]. IRA relied on micro-targeting via proxy criteria whose names indicate\\nthe demographics they tried to reach: ‚ÄúAfrican-American history‚Äù, ‚ÄúAfrican-American Civil Rights\\nMovement‚Äù, ‚ÄúMalcolm X‚Äù, etc. to target Black Americans, and ‚ÄúChicano Movement‚Äù, ‚ÄúChicano\\nrap‚Äù, ‚ÄúHispanidad‚Äù, etc. to reach Hispanic/Latinx users [ 105]. Third, two years after the election,\\na whistleblower revealed that Trump‚Äôs campaign also retained services of Cambridge Analytica,\\nwhich collected large amounts of data on Facebook users and then constructed ‚Äúpsychological\\nprofiles‚Äù of these unsuspecting users and craft ads that would exploit their particular susceptibility\\nto persuasion [77].\\nAs a result, lawmakers have attempted to take steps to constrain such political micro-targeting.\\nFor example, the proposed Honest Ads Act in the U.S. would have forced platforms to publish\\ntargeting information for all ads in their libraries [ 101], but it was never passed. A recently-\\napproved E.U. regulation on political advertising will require platforms to seek explicit user opt-in\\nfor processing their data for targeting of political ads [ 73]. Data that can reveal a users revealing a\\nracial/ethnic origin, political opinion, religious beliefs, or sexual orientation, will not be permitted\\nfor targeting and delivery optimization (see Section 2.2). However, the law stops shy of completely\\nbanning microtargeting.\\n2.4 Platform policies\\nAfter the 2016 U.S. presidential election, much research explored risks that micro-targeted political\\nadvertising poses to democracy and human rights. These include the risks of sealing people into\\necho-chambers while limiting their access to pluralistic information; facilitating campaigns‚Äô abilities\\nto misrepresent their positions by sending tailored messages with different focus to narrow groups;\\nand enabling the spread of mis- and dis-information, the discouragement of civil participation, and\\nthe manipulation of voters [ 16,38,41,116]. As a result, in the years that followed, many social\\nmedia platforms changed their policies to attempt to address the risks of micro-targeted political\\nadvertising. For example, TikTok explicitly banned all political and issue ads [ 22]; Twitter banned6 Piotr Sapiezynski, Levi Kaplan, Alan Mislove, and Aleksandra Korolova\\nTime Action undertaken by Facebook\\nFeb 2017 Set to disapprove opportunity ads that use multicultural affinity segments [96].\\nNov 2017 Undertook review of exclusion, focusing on the use of exclusion for multicultural affinity groups\\nand other potentially sensitive segments (e.g., segments that relate to the LGBT community or\\nto religious groups) [44].\\nApr 2018 Removed thousands of categories from exclusion targeting; focused mainly on topics that\\nrelate to potentially sensitive personal attributes, such as race, ethnicity, sexual orientation and\\nreligion [80].\\nMay 2018 Introduced the Ad Library to archive all political ads [68].\\nAug 2018 Removed 5,000 targeting options to help prevent misuse. Includes limiting the ability for\\nadvertisers to exclude audiences that relate to attributes such as ethnicity or religion [95].\\nMar 2019 A settlement with civil rights organizations requires removing gender, age, and ethnic affinity\\nfrom opportunity ads targeting, along with features that describe or appear to be related to\\nrace, color, national origin, ethnicity, gender, age, religion, family status, disability, and sexual\\norientation [119].\\nAug 2020 Removed some targeting options such as multicultural affinity segments [81].\\nJan 2022 Removed detailed targeting options that relate to topics people may perceive as sensitive, such\\nas options referencing causes, organizations, or public figures that relate to health, race or\\nethnicity, political affiliation, religion, or sexual orientation [83, 85].\\nMay 2022 Introduced aggregated targeting information to the Ad Library [62].\\nMar 2024 Removed targeting options that relate to topics people perceive as sensitive (e.g., targeting\\noptions referencing causes related to health, race or ethnicity), or because of legal or regulatory\\nrequirements [94].\\nTable 1. Timeline of changes that Facebook introduced to limit micro-targeting.\\npolitical ads;3and Google only allowed political ad targeting by location, age, and gender, with\\ninterest or custom audience targeting prohibited [45].\\nFacebook, however, has not banned political advertising or selected only a few permissible\\ntargeting criteria. Instead, Facebook‚Äôs changes to its advertising system have been largely driven by\\nlegal challenges that allege violations of U.S. anti-discrimination law. We systematize the detailed\\ntimeline of these changes in Figure 1, and note a few of the key events here.\\nAfter 2016 reporting by ProPublica showed that Facebook‚Äôs targeting system enabled advertisers\\nto exclude people from housing ads based on their ethnicity [ 8], Facebook first limited the ability\\nof advertisers to use multicultural affinity targeting as an exclusion criteria [ 34,44,80,96]. After\\nfurther pressure from the press [ 63,79] and a lawsuit from civil rights organizations [ 119], Facebook\\nremoved more than 5,000 targeting options that ‚Äúdescribe or appear to be related to‚Äù a range of\\nprotected characteristics or classes such as ethnicity and religion [ 81,95]. At the same time, civil\\nrights organizations, regulators, and lawmakers pressured Facebook to address the micro-targeting\\nrisks of political advertising [ 5,6,29,35‚Äì37,42,100,123,124]. In 2022, Facebook made the largest\\nchange to its available targeting options, affecting all ads. Namely, it removed targeting options ‚Äúthat\\nrelate to topics people may perceive as sensitive, such as options referencing causes, organizations,\\nor public figures that relate to health, race or ethnicity, political affiliation, religion, or sexual\\norientation‚Äù with examples including options that ‚Äúrelate to political beliefs, social issues, causes,\\norganizations and figures‚Äù [53, 83, 85].\\n3Although, following change in leadership, it announced in January 2023 that it would reinstate ‚Äúcause-based ads‚Äù [27].On the Use of Proxies in Political Ad Targeting 7\\nAs Facebook continues to remove targeting options‚Äîincluding the most recent update in March\\n2024 [ 94]‚Äîanecdotal reports have pointed to advertisers circumventing these limitations by using\\nproxies [ 30,60,76,117,125]. Our study, therefore, aims to develop a scalable methodology to\\nbetter understand whether and how political advertisers use proxies, focusing on the U.S. 2022\\nmidterm elections. Our goal is to determine in an independent and systematic fashion whether\\nthe safeguards Facebook put in place are achieving the desirable societal and democratic goals,\\nand whether accompanying transparency mechanisms implemented by Facebook facilitate such\\nresearch.\\n2.5 Harms beyond political polarization\\nWe already described how malicious actors can use detailed targeting to sow political division.\\nUnfortunately, the range of harms that can occur as a result of targeted advertising does not end\\nthere. Here, we describe related documentation of other harms.\\nProliferation of hate speech. In 2017 ProPublica reported that, prior to their alerting Facebook,\\nthe platform enabled targeting several anti-Semitic interest topics such as ‚ÄòJew hater‚Äô, ‚ÄòHow to\\nburn jews‚Äô, and ‚ÄòHistory of ‚Äòwhy jews ruin the world‚Äô ‚Äô (all spelling original) [ 10]. Presumably,\\nthese targeting criteria allowed advertisers to speak directly to extreme anti-Semites. Importantly,\\nthese ad categories were algorithmically created, and made available to advertisers without manual\\ninspection.\\nExploiting vulnerability. Targeted advertising is perceived by many as manipulative [ 127], and\\nhas been described as a form of ‚Äúslow violence‚Äù, where the gradual accumulation of exposure\\nto harmful targeted ads can manifest itself as real emotional harm [ 40]. The slow violence of\\ntargeted advertising is expanded upon by Wu and colleagues, who describe it as consisting of four\\nharms: psychological distress, loss of autonomy, constriction of user behavior, and algorithmic\\nmarginalization and traumatization [127].\\nPlatforms enable this harm by exposing sensitive characteristics of users through targeting crite-\\nria. Advertisers can narrow down the users to those who suffer from body-image insecurities, as well\\nas health-related anxieties, and even specific diseases. Already in 2017, leaked documents obtained\\nbyThe Australian indicated that Facebook was identifying teens feeling ‚Äúinsecure‚Äù, ‚Äúworthless‚Äù,\\n‚Äústressed‚Äù, ‚Äúanxious‚Äù for targeting use [ 74]. More recently, The Markup found that pharmaceutical\\ncompanies would target users ‚Äúinterested‚Äù in awareness of diseases as a proxy for individuals who\\nhave that disease [ 70]. For example, cancer treatments were shown to be targeted to users interested\\nin ‚ÄòCancer Awareness‚Äô or ‚ÄòNational Breast Cancer Awareness Month‚Äô and ads for drugs treating\\nbipolar were targeted at those interested in the ‚ÄòDepression and Bipolar Support Alliance‚Äô and\\nthe ‚ÄòNational Alliance on Mental Illness.‚Äô Evidence suggests that over-exposure to such triggering\\ncontent can worsen anxiety symptoms in the vulnerable individuals [ 40,43], in addition to raising\\nprivacy fears from just the fact that Facebook infers such interests.\\n2.6 Efficacy of targeted advertising\\nIn the previous section we described that micro-targeting continues to be wide-spread and employed\\nby various parties, often with multi-million dollar budgets. However, systematically demonstrating\\nthat narrowly targeted ads actually do work for influencing recipients is difficult. Observational\\nstudies cannot typically be used to identify causality, so special care must be taken in the study\\ndesign to minimize the effect of confounding variables. Here, we mention a few studies that did\\nso and showed increased efficacy of targeted ads. Matz et al . [77] were the first to perform an\\nexperimental validation of targeting people with particular psychological profiles via proxies, i.e.,\\nthe approach the Cambridge Analytica was later found to be using. They created ads that would8 Piotr Sapiezynski, Levi Kaplan, Alan Mislove, and Aleksandra Korolova\\nappeal to those with certain psychological traits (for example low extraversion, or high openness,4\\netc.) and then used proxies to target these individuals. The results of the experiment indicated\\nusers were more likely to click through the ads whose content was congruent with their personally\\ntargeted via proxies. Tappin et al . [115] ran issue ads targeted using demographics and psychological\\ntraits, and tested to resonate with the target group. In their experiment, messages crafted this way\\nperformed up to 70% better than the best performing non-personalized messages. While the two\\nstudies described above were experimental, Ribeiro et al . [105] took a different approach. They\\ndocumented IRA‚Äôs actual use of micro-targeting using proxy attributes in real ads. Then, they\\nperformed a representative survey and showed that voters whose ethnicity matched that implied\\nby the proxy attributes used for targeting were less likely to identify any misinformation within,\\nand less likely to report these ads. Instead, they were more likely to approve of their content.\\n2.7 Ad Library\\nIn May 2018, Facebook introduced the Ad Library [ 90], an online searchable archive of all political\\nand issue ads that are currently running or have run on the platform since its launch [ 28]. The\\nlibrary shows the content of each political ad, the details of the account that ran the ad as well as\\ninformation on who paid for it, and a range of meta-information: the amount spent and the count\\nof unique impressions, as well as geographic and demographic breakdowns of the audience who\\nwere delivered the ad [ 68]. This content is available through the Library‚Äôs website and a subset\\nof it can also be retrieved through an API, subject to strict rate limits that prevent cataloging\\nof all ads by third parties [ 67]. As of 2022, the Library also provides aggregated information on\\ntargeting parameters used by every political advertiser, but does not make this information available\\nthrough the official API. The reported targeting criteria include: age and gender ( self-reported by the\\nuser), location, whether the advertiser used the Custom Audience [ 86] or Lookalike Audience [ 88]\\nfeature, and detailed targeting (which includes both self-reported andplatform-inferred interests,\\ndemographics, and behaviors). Additionally, each targeting criterion is accompanied with the count\\nof ads it was used in, as well as the fraction of the total budget that was spent on ads targeted\\nwith that criterion. Ads that are not classified as political or issue ads are only visible in the Ad\\nLibrary while they are running, and are not accompanied with the budget, targeting, and delivery\\ninformation, unlike political ads.\\n2.7.1 Limitations of the Ad Library. While Facebook did take into account some of the desiderata\\nexpressed by legislators in Honest Ads Act [ 101] and civil society advocates [ 75,106] when designing\\nthe Ad Library, its deployment has not been without criticism. For example, because of the faulty\\ndetection mechanism of which ads are political, the library fails to report significant portions of\\npolitical advertising on the platform [ 32,67]. Furthermore, at the time of writing this work, the\\ntargeting information is presented in a way that prevents researchers from asking a number of\\nfundamental questions. Importantly, the targeting parameters are not grouped per ad, but instead\\naggregated per week of an advertiser‚Äôs activity, regardless of how many ads they ran in this period.\\nFor example, the Ad Library might report that the advertiser ran two ads; and that among these\\ntwo, one was targeted to users interested in ‚ÄòNPR‚Äô, and one to users interested in ‚ÄòDuck Dynasty‚Äô.\\nFrom the presented information, the researcher does not know whether each of these interest was\\nused to target a different ad (indicating that the advertiser is trying to reach different audiences\\nwith different ads), or they were both used to target the same ad (targeting a politically-broad\\naudience with a uniform message). Finally, there is currently no official API access to the targeting\\n4The authors relied on a five trait personally model referred to as The Big Five or OCEAN for openness to experience,\\nconscientiousness, agreeableness, extraversion, neuroticism. They had previously shown that these traits can be reliably\\ninferred using Facebook likes [128]On the Use of Proxies in Political Ad Targeting 9\\ninformation and researchers have to instead rely on an undocumented API that powers the website,\\nwith stringent rate limits.\\n2.8 Prior work on measuring skews, alignments, biases and slants in social media\\nSo far we have referred to various targeting criteria as proxies for other characteristics of interest.\\nWe do not anticipate finding perfect proxies, i.e. interests targeting which would yield an audience\\nwhere all users share the desired characteristic. Therefore, we need a quality (purity) measure to\\nquantify how close to perfect the proxies are. We now provide an overview of related measures\\nintroduced in previous work.\\nBudak et al . [17] set out to quantify ideological slant of news stories. They used crowd-workers to\\njudge, on a five-point scale, the extent to which each story was favorable towards the Democratic\\nparty, and then, separately, to which extent it was favorable towards the Republican party. They\\ndefined ideological slant as the difference between the two, scaled from -1 (only favorable to\\nDemocrats) to 1 (only favorable to Republicans).\\nBakshy et al . [12] , as employees of Facebook and using Facebook‚Äôs internal data, set out to measure\\nthe diversity of view points that Facebook users were exposed to through their algorithmically\\ncurated News Feeds. To that end, they identified news stories which were shared by at least 20 users\\nwho self-reported their political alignment in their profiles. They then averaged these affiliations\\nto create a score ranging from -1 for ‚Äúonly shared by Democrats‚Äù through 0 for ‚Äúshared equally\\nby Democrats and Republicans‚Äù to 1 for ‚Äúshared only by Republicans‚Äù. Note that this score does\\nnot describe the slant of the news story‚Äìinstead, it quantifies the alignment of the audience that\\nshared it.\\nFollowing a similar methodology, Robertson et al . [107] created a panel of Twitter users whose\\npolitical alignment they learned by matching them to publicly available voter records with self-\\nreported party affiliation. They identified the domains that the panel of users mentioned in their\\ntweets, and used these users‚Äô known affiliation to compute the audience bias on a scale from -1\\nto 1. Notably, Robertson‚Äôs et al. metric allows for measuring biases in situations where the two\\ngroups are not of similar sizes. A zero in their metric indicates that a members of one group are as\\nlikely as those in the other group to share a piece of content, not that an even number of members\\nin those groups did so.\\nFinally, Ribeiro et al . [104] introduced a method for measuring news outlets‚Äô readership bias.\\nThey were able to use the Facebook API to directly query the number of users of different ethnicities\\nand political views ‚Äúinterested‚Äù in various media outlets and measure the bias directly. Since the\\nremoval of targeting by ethnicity and political views, that method can no longer be replicated\\ndirectly, but it does serve as a starting point to the methods we propose in this paper.\\nAll the methodologies rely on either extensive manual effort to rate partisanship of sources and\\ncraft audiences, or access to private datasets, thus making them difficult to maintain, scale, and\\nreproduce. In contrast, our first approach relies only on data that is publicly and freely available\\nthrough the Facebook advertising interface and official election websites.\\nThe works described in this section have used different names for related phenomena: slant,\\nalignment, and bias. Given the nature of our data, none of these metrics translates directly to the\\nproblem we set out to measure. Therefore, throughout this work we will use the term skew to\\nemphasize that the formulation is different than the previously described measures.\\n3 Data and Methods\\nAmong the core contributions of this work are two methods of measuring the skews of a targeting\\ninterest. In this section, we first provide a brief overview of both approaches. We then elaborate on\\ntechniques for obtaining the necessary data and performing the calculations. Finally, we reveal the10 Piotr Sapiezynski, Levi Kaplan, Alan Mislove, and Aleksandra Korolova\\ndistributions of the skews among interests that were actually used by advertisers during the 2022\\nU.S. Midterm Elections.\\nThefirst approach , summarized in Figure 1, exploits the Facebook advertising interface. We cre-\\nate politically or demographically homogeneous Custom Audiences and use the delivery_estimate\\nAPI to estimate the number of active users in each. We then use the same API to estimate the\\nfraction of users in each audience that share the particular interest. We compare these fractions\\nusing a skew measure we introduce in Section 3.2 to assign a numerical value to the interest‚Äôs\\nskew. The second approach , depicted in Figure 2, leverages the fact that the Facebook Audience\\nInsights tool reports Facebook Business pages popular among users who share each interest. Each\\nof these pages represents an entity (a public person, a business, a media outlet, etc.), and reveals\\ntheir non-Facebook website URL. These URLs, in turn, can be looked up in the audience skew\\ndatabase published by Robertson et al . [107] . Once we obtain the audience skews for each of the URLs\\nassociated with a particular interest, we average them to estimate the audience skew of that interest .\\nBoth approaches require an interest as input to calculate its skew, so before we describe these\\napproaches in detail, we explain how we obtained a list of interests used by political advertisers.\\n3.1 Obtaining targeting interests from the Facebook Ad Library\\nWe collected the targeting information from the Ad Library daily in two steps. First, we scraped\\nthe Meta Ad Library Report which lists all advertisers active during the previous day.5Second,\\nusing this list, we requested, one by one, the targeting report for each of these advertisers using an\\nundocumented API [ 111]. This work analyses the data obtained during a month leading up to the\\nU.S midterm elections of 2022, i.e., October 8, 2022 through November 8, 2022. The data contains\\ninformation about all 22,479 advertising accounts that ran any political ads to U.S. audiences in\\nthat period, even if the ads did not pertain to the midterm elections. These accounts targeted the\\naudiences using a total of 14,662 unique interests, 4,236 demographic criteria, and 269 behaviors.\\nFurther, the advertisers ran ads whose targeting excluded a total of 1,025 interests, 151 demographic\\ncriteria, and 40 behaviors. Through the remainder of this paper we will refer to this collection as\\nthePolitical Ad Targeting Dataset .\\nAlthough the targeting information was available when manually loading the advertiser page in\\nthe Ad Library, the official API for the same data did not report this info at the time of our data\\ncollection. We thus relied on an undocumented API, which we identified by analyzing the requests\\nour Internet browser made while browsing the Ad Library [ 111]. We note that Facebook‚Äôs underlying\\nsystem and, as a consequence, the undocumented API did not always work in a predicable fashion.\\nThe delay with which it reported the data varied from two to six days, with 95% of successful\\nrequests returning information dated three or four days before. Furthermore, the API did not\\nreport any targeting information at all in 4.9% of our requests, despite the fact that we knew that\\ncorresponding ad accounts were indeed running political ads. We found the vast majority of missing\\ndata comes from the same set of accounts, with less than 10% of successful requests for 3.9% of\\naccounts.\\n3.2 Measuring political and racial skews using voter records\\nOurfirst approach leverages our ability to create racially and politically homogeneous Custom\\nAudiences to then measure the popularity of different targeting criteria among these groups. In\\nparticular, we obtain voter records6from the states where voters self-report their race (see Panel 1\\n5See: https://www.facebook.com/ads/library/report/\\n6The data was purchased by our Institution from L2, see: https://l2-data.com/datamapping/. The purchase of The L2 List\\nwas made unrelated to this work, but the data is available upon request to all faculty, students, and staff at the institution.\\nThis research work is a derivative from the use of The L2 List and, per the terms of use, is a property of the authors. L2On the Use of Proxies in Political Ad Targeting 11\\nFig. 1. Estimating the skew of targeting interests using the overlap between custom audiences and inferred\\ninterests. Step 1: Consolidate voter records from all states where voters self-report race. Step 2: Create Custom\\nAudience out of party- or race-uniform samples of voters. Step 3: Use the delivery_estimate API to estimate\\nhow many users can actually be reached in each custom audience. Step 4: Use the delivery_estimate API\\nto estimate what fraction of users in each custom audience share a given interest (here: Ted Nugent). Step 5:\\nUse Eq. 1 to calculate skew.\\nof Figure 1): Alabama, Florida, Georgia, Louisiana, North Carolina, South Carolina, and Tennessee.\\nNote that all of these states are in the South, thus the ethnic skews we observe will be more\\nrepresentative of the residents of that region of the US. We split these voters into groups such\\nthat each one contains only Democratic voters, and another contains only Republican voters; we\\nadditionally create one for each ethnic group: white, Black, and Hispanic. These groups are of\\ndifferent sizes so to avoid biasing the results by discrepancies in coverage. We randomly sample\\ntwo million individuals from each and upload them as separate Custom Audiences to Facebook, as\\nshown in Panel 2 of Figure 1.\\nOnce we have crafted our Custom Audiences, we turn to another Facebook tool called the Ad\\nManager interface, and the delivery_estimate API that underlies it. The API provides advertisers\\nwith rough estimates of how many users they can expect to reach given their targeting criteria\\nand budget, without actually running the ad. As shown in Panel 3 of Figure 1, we first target each\\nCustom Audience separately without other criteria and use the API to measure the number of\\nactive users in each. Then, we narrow down our targeted audience to only these users in each\\naudience who share a given interest. We use the same API to measure the number of such users.\\nWe then divide this number by the total number of users in each audience, see Panel 4 of Figure 1.\\nFor example, we can create an ad with a daily budget of $1M and target the Custom Audience that\\ncontains only the Republican voters. The interface then uses the delivery_estimate endpoint\\nof the Graph API to obtain an estimate, in this case 903,884 unique accounts. We use this number\\nas the estimate of the total number of weekly active users among our Republican list, ùëÅùëÖ. Using\\nthe same API endpoint and the conjunctive combinations feature we can also obtain the estimated\\nnumber of Republicans in our custom audience interested in ‚ÄòTed Nugent‚Äô, ùëÅùëñ\\nùëÖ, in this case 111,374,\\nor 12.3% of the Republican audience. We use the delivery_estimate API to obtain these estimates\\nfor all 19K interests for all five custom audiences we created.\\nhas had no influence on the research direction, nor did they have access to the manuscript before its submission. Before\\ntransitioning to the L2 dataset, we had used the publicly available voter records for North Carolina and Florida. The results\\nwe present here can be closely replicated using that data.12 Piotr Sapiezynski, Levi Kaplan, Alan Mislove, and Aleksandra Korolova\\nFig. 2. Estimating the skew of targeting interests using the Audience Insights data.\\nStep 1: Obtain the list of interests used by advertisers from the Facebook Ad Library.\\nStep 2: Obtain Facebook Pages popular among the users associated with a given interest.\\nStep 3: Obtain the domain of the external URL self-reported by that Facebook Page.\\nStep 4: Look-up the audience skew of each domain.\\nStep 5: Average the audience skews of known domains and assign it to the interest.\\nAs a final step, we can compare the relative popularity of any interest ùëñbetween two non-\\noverlapping audiences ùê¥andùêµ(for example Republican vs Democratic, white vs Black, etc.) to\\nestimate that interests‚Äô skew (See Panel 5 in Fig 1). We do so using the following equation:\\nùëÜùëñ\\nùê¥ùêµ=ùëÅùëñ\\nùê¥\\nùëÅùê¥‚àíùëÅùëñ\\nùêµ\\nùëÅùêµ\\nùëÅùëñ\\nùê¥\\nùëÅùê¥+ùëÅùëñ\\nùêµ\\nùëÅùêµ, (1)\\nwhereùëÅùëñ\\nùê¥is the estimated number of weekly active users in group ùê¥for whom Facebook inferred\\nan interest in ùëñandùëÅùê¥is the total estimate of weekly active users in group ùê¥. Similarly to measures\\nwe described in Section 2.8, it is continuous from ‚àí1to1.\\nFor example, the API estimates that up to 111,374, or 12.3% of Republicans in our custom audience\\nare interested in a country musician Ted Nugent. At the same time 43,085 out of 822,108, or 5.2%\\nDemocrats in our custom audience are interested in ‚ÄòTed Nugent‚Äô. Therefore, the skew of ‚ÄòTed\\nNugent‚Äô on the Republican vs. Democrat spectrum is (12.3%‚àí5.2%)/(12.3%+5.2%)=0.40, i.e., it is\\nskewing Republican and we can say that interest in ‚ÄòTed Nugent‚Äô is a proxy for being Republican.\\nWe note that this approach allows for continuous monitoring of the skew of different interests ‚Äì\\nCustom Audiences can be continuously re-created with the newest releases of the voter records,\\ntargeting criteria that are actually used by advertisers can be obtained from the Ad Library, and the\\nAPI can be then queried again to measure the prevalence of each interest in each Custom Audience.\\n3.3 Estimating political skew through skew of external websites\\nOursecond approach is summarized in Figure 2. It involves estimating the audience skew of a\\nparticular interest by leveraging known audience skews of websites that are popular among users\\nwho share that interest. After obtaining the list of interests as described in the previous section, we\\nvisited the Facebook Audience Insights page for each interest. This tool allows any logged in user\\nto enter a targeting specification (in our case: one interest) and to learn the characteristics of usersOn the Use of Proxies in Political Ad Targeting 13\\nwho share that interest: age, gender, and geographical distributions, as well as their most popularly\\nliked Facebook Pages, as shown in Panel 2 of Figure 2. Note that these Facebook Pages are notof\\nindividual users; instead they correspond to businesses, prominent figures, and celebrities. Then,\\nwe visited each of these pages and collected the external URL that corresponds to that entity‚Äôs\\nwebsite outside of Facebook, see Panel 3 of Figure 2. We find that the Audience Insights for 99% of\\ninterests mention between seven and ten Facebook pages with associated domains. To minimize\\nthe strain on Facebook‚Äôs server and increase the speed of data collection, we collect the Pages data\\nthrough the Graph API, rather than by visiting the Insights Page for every interest. We repeat the\\nprocess for all unique interests in the Political Ad Targeting Dataset and thus obtain a mapping\\nbetween each interest and the external URLs popular among users who share that interest.\\nTo estimate the audience skew of an interest given the URLs associated with it, we need a\\nmeasure of each URL‚Äôs audience skew (see Panel 4 of Figure 2). We use the dataset published by\\nRobertson et al . [107] .7Each of the domains in the dataset has a score between -1 (only Democratic\\nvoters linked to that domain in their tweets) through 0 (Democratic and Republican voters linked\\nto that domain equally often) to 1 (only Republican voters linked to that domain). We note that the\\ndataset is far from complete: the scores have not been updated since February 2019 and the skew of\\nparticular domains might have changed over time, some domains may no longer exist, and new\\ndomains appeared. Regardless, we believe the available scores are a good first approximation of the\\ncurrent reality, and we do have the bias estimate for 68% of domain mentions.8Finally, as shown in\\nPanel 5 of Figure 2, we take the average audience skew of all URLs associated with an interest as\\nthe estimate of that interests‚Äô audience skew.\\n3.4 Establishing the partisanship on political advertisers.\\nTo understand the political alignment of the advertisers present in the Ad Library data, we turned\\nto an ad transparency non-profit called WhoTargetsMe.9This organization allowed us to use a\\ndataset which identifies the partisanship of 7,926 advertisers who ran political ads targeting users\\nin the U.S. Based on their identity and advertised content each advertiser is manually assigned\\none of the following nine labels: Non(for non-political advertisers), GOP,Dems (affiliated with\\nthe Republican Party or the Democratic Party, respectively), R-PACs ,D-PACs (Political Action\\nCommittees associated with each party), Conservative ,Progressive (advertising a right- or left-\\nleaning point of view without official party affiliation), Independent , orOther . For the purpose of\\nthe analysis we collapse the labels into ‚ÄúConservatives‚Äù, ‚ÄúProgressives‚Äù, and ‚ÄúOther‚Äù, as described\\nin Table 2.\\n4 Results\\nWe now present results of investigations into each of the research questions we set out to answer.\\nIn short, we find that numerous interests that still remain targetable serve as effective proxies for\\npolitical affiliation and race, and are actively exploited by advertisers on both sides of the political\\nspectrum.\\n4.1 The existing targeting options can be used as proxies for political leaning and race.\\nFigure 3 illustrates the prevalence of skews in the Political Targeting dataset. In the top panel figures\\neach dot represents an interest. The color of the dots in the top panel encodes the skew: the more\\npurple interests skew towards the population described in the ùë¶-axis and the interests in green\\n7The dataset can be downloaded from https://github.com/gitronald/domains/\\n8The Robertson dataset covers only 21% of unique domains from the Page Insights dataset, but missing domains are more\\nlikely to be less popular, which translates to higher empirical coverage.\\n9See: https://whotargets.me/en/projects/.14 Piotr Sapiezynski, Levi Kaplan, Alan Mislove, and Aleksandra Korolova\\nGrouping Label Count\\nConservativesGOP 1,289\\nR-PACs 372\\nConservative 221\\nTotal 1,882\\nProgressivesDems 1,235\\nD-PACs 572\\nProgressive 148\\nTotal 1,955\\nOtherNon 3,924\\nOther 137\\nIndependent 27\\nTotal 4,088\\nTotal 7,926\\nTable 2. Label breakdown in the advertiser affiliation dataset from WhoTargetsMe.\\n103\\n102\\n101\\n100\\nCoverage of Republican users103\\n102\\n101\\n100Coverage of Democratic users  = 0.98***\\n103\\n102\\n101\\n100\\nCoverage of white usersCoverage of Black users\\n = 0.91***\\n103\\n102\\n101\\n100\\nCoverage of white usersCoverage of Hispanic users = 0.96***\\n103\\n102\\n101\\n100\\nCoverage of Black usersCoverage of Hispanic users = 0.92***\\n1.0\\n 0.5\\n 0.0 0.5 1.0\\nSkew SiRD0.000.050.100.150.200.250.30Fraction of interests\\n1.0\\n 0.5\\n 0.0 0.5 1.0\\nSkew SiWB1.0\\n 0.5\\n 0.0 0.5 1.0\\nSkew SiWH1.0\\n 0.5\\n 0.0 0.5 1.0\\nSkew SiBH\\nFig. 3. The coverage of interests between populations are highly correlated. Still, a non-trivial fraction of\\ninterests reveal large skews (color-coded in the figures) and could be used to selectively target populations\\nthat are not explicitly targetable.\\nskew towards the population described on the ùë•-axis. The figures show that, in general, coverage\\nof interests is highly correlated between disjoint groups of users, i.e., if interest ùëñis popular among\\nusers in group ùê¥, it is likely also popular among users in group ùêµ. Among the comparisons, we see\\nthat Democrats and Republicans are the most similar to each other, with the coverage correlation of\\nùúå=0.98andùëùùë£ùëéùëô<0.001. White and Black users are the most dissimilar but still show a very high\\ncorrelation at ùúå=0.91,ùëùùë£ùëéùëô<0.001. The histograms in the bottom panel show the distribution\\nof skews and confirm the results measured using correlation: a small fraction of interests skewsOn the Use of Proxies in Political Ad Targeting 15\\n0.550.600.65Pearson's \\n0 5 10 15 20 25 30\\nNumber of Top pages ignored0.20.40.60.81.0Fraction\\nof interests covered\\nFig. 4. The skews calculated by audience and by page are correlated\\nstrongly towards Democrats or Republicans, but there exist many interests that skew strongly\\ntowards Black and Hispanic users.\\nImportantly, we do not observe interests that would constitute perfect proxies. There are no clear\\noutliers in the top left or bottom right in any of the plots‚Äîthese would represent interests that\\nfully cover one group without including any users from the other. Nevertheless, as we show in the\\nfollowing sections, it does not mean that the advertisers do not use the available imperfect proxies.\\n4.1.1 Comparison between the page-based and voter-based metrics. Our two metrics attempt to\\nquantify the same phenomenon using disjoint methods and information. We expect their scores to\\nbe different but still highly correlated, and they are indeed, at Pearson‚Äôs ùúå=0.51andùëùùë£ùëéùëô<0.001.\\nWe also recognize that the most common top domains may carry little information as they are\\nassociated with the majority of interests. For example, Walmart appears among the Top Pages for\\n90% of all interests. We find that as we remove top domains, the coverage of interest for which we\\ncan estimate political skew drops, but the correlation between the page-based, and voter-based skew\\nmetric grows, see Figure 4. When the top four domains are dropped, the correlation between the\\npage-based and the voter-based skew metrics achieves ùúå=0.66andùëùùë£ùëéùëô<0.001, while retaining\\nestimates for 97% of interests. Beyond this point the coverage drops significantly without much\\ngain in correlation.\\nWe introduced the page-based skew measure as a proof of concept, without fine-tuning, to\\ndemonstrate that even when voter records are not available (which is the case in many countries\\noutside of the U.S.), the skew of interests could be approximated using a third-party domain-based\\nskew lookup. Further fine-tuning of this page-based skew measure is, of course, possible, including\\nthrough machine-learning based analyses. We leave the exploration of these approaches to future\\nwork.\\nFor the remainder of the paper we focus on the voter-based skew measure.\\n4.2 Real political advertisers spend millions of dollars on highly skewed interests.\\nTable 3 shows the interests used for targeting of political ads sorted by total amount spent, while\\ndistinguishing between targeting and exclusion. The majority of these interests are related to the\\ncontent and stated purpose of advertising, with ‚ÄòPolitics‚Äô, ‚ÄòCommunity Issues‚Äô, ‚ÄòVoting‚Äô, ‚ÄòSocial\\nchange‚Äô, and ‚ÄòPolitics and social issues‚Äô, ‚ÄòActivism‚Äô, ‚ÄòCharity and causes‚Äô, and ‚ÄòSocial movement‚Äô16 Piotr Sapiezynski, Levi Kaplan, Alan Mislove, and Aleksandra Korolova\\nInterest Spend (exclusion) Spend (inclusion) Political ùëÜùëÖùê∑ùëÜùëäùêµùëÜùëäùêªùëÜùêµùêª\\nPolitics 0.0 M 6.6 M ‚úì 0.06 0.13 0.12 -0.01\\nCommunity issues 0.0 M 6.2 M ‚úì 0.04 -0.03 0.08 0.12\\nVoting 0.0 M 4.8 M ‚úì -0.00 0.12 0.24 0.12\\nSocial change 0.2 M 4.1 M ‚úì -0.07 -0.17 0.04 0.21\\nPolitics and social issues 0.0 M 4.0 M ‚úì 0.03 0.04 0.06 0.03\\nJoe Rogan 3.7 M 0.3 M 0.10 0.03 0.04 0.01\\nActivism 0.0 M 3.7 M ‚úì -0.02 -0.10 0.07 0.17\\nKid Rock 3.7 M 0.0 M 0.24 0.43 0.47 0.05\\nCharity and causes 0.0 M 3.7 M ‚úì 0.02 0.01 0.08 0.07\\nSocial movement 0.1 M 3.1 M ‚úì -0.07 -0.12 0.06 0.18\\nTed Nugent 3.0 M 0.1 M 0.40 0.53 0.48 -0.07\\nCurrent events 0.0 M 3.0 M 0.03 0.01 0.08 0.07\\nNPR 0.7 M 2.3 M -0.13 0.18 0.19 0.01\\nVolunteering 0.0 M 2.8 M 0.09 0.12 0.17 0.05\\nHunting 1.3 M 1.5 M 0.18 0.15 0.22 0.07\\nLocal news 0.0 M 2.7 M 0.08 0.05 0.19 0.14\\nElection 0.0 M 2.5 M ‚úì 0.12 0.31 0.26 -0.05\\nLocal government 0.0 M 2.3 M ‚úì 0.11 0.24 0.08 -0.16\\nDuck Dynasty 1.6 M 0.7 M 0.44 0.46 0.42 -0.05\\nNASCAR 1.7 M 0.6 M 0.26 0.24 0.26 0.02\\nParents 0.0 M 2.2 M 0.13 0.03 0.18 0.15\\nFox News 1.6 M 0.4 M - - - -\\nNewspapers 0.0 M 2.0 M 0.06 0.14 0.03 -0.10\\nNonprofit organization 0.0 M 1.9 M 0.08 0.11 0.17 0.06\\nBreaking news 0.1 M 1.8 M 0.05 0.05 0.11 0.06\\nFishing 0.3 M 1.5 M 0.17 0.16 0.14 -0.02\\nGovernment 0.0 M 1.7 M ‚úì 0.15 0.15 0.16 0.01\\nSustainable energy 0.1 M 1.6 M 0.04 0.15 0.02 -0.13\\nElection day 0.0 M 1.6 M ‚úì -0.04 -0.07 -0.11 -0.04\\nRoseanne Barr 1.5 M 0.0 M -0.03 0.22 0.20 -0.03\\nCommunity and Social Services 0.0 M 1.5 M ‚úì -0.16 0.18 0.16 -0.02\\nLove & Hip Hop 0.0 M 1.5 M -0.31 -0.62 -0.07 0.58\\nParents with early school-age children 0.0 M 1.5 M 0.05 0.20 0.13 -0.07\\nAfrican-American literature 0.0 M 1.5 M -0.34 -0.63 -0.00 0.63\\nThe Real Housewives of Atlanta 0.0 M 1.5 M -0.22 -0.51 0.03 0.53\\nParents with preteens 0.0 M 1.5 M 0.10 0.17 0.16 -0.01\\nClint Eastwood 1.4 M 0.0 M 0.33 0.56 0.41 -0.20\\nElectoral reform 0.0 M 1.4 M ‚úì 0.14 0.13 0.33 0.21\\nParents with teenagers 0.0 M 1.4 M 0.12 0.00 0.18 0.17\\nCollege grad 0.3 M 1.1 M 0.02 0.01 0.01 0.00\\nCharitable organization 0.0 M 1.4 M 0.03 0.06 0.15 0.09\\nRenewable energy 0.1 M 1.2 M 0.04 0.14 -0.02 -0.15\\nBlack-ish 0.0 M 1.3 M -0.41 -0.69 -0.10 0.63\\nNews media 0.1 M 1.2 M 0.06 -0.06 0.18 0.23\\nPhilanthropy 0.0 M 1.3 M -0.01 -0.23 0.05 0.27\\nElon Musk 1.2 M 0.0 M 0.14 0.09 0.09 -0.00\\nSmall business 0.0 M 1.2 M 0.05 -0.06 0.04 0.10\\nBasketball Wives 0.0 M 1.2 M -0.28 -0.58 0.02 0.60\\nHip hop music 0.0 M 1.2 M -0.09 -0.30 -0.05 0.25\\nSaturday Night Live 0.5 M 0.7 M -0.06 0.03 0.07 0.04\\nBarstool Sports 0.6 M 0.6 M 0.22 0.11 0.23 0.12\\nBig-game hunting 0.7 M 0.5 M 0.22 0.25 0.11 -0.14\\nThe Shade Room 0.0 M 1.2 M -0.74 -0.88 -0.32 0.78\\nSoul music 0.0 M 1.2 M -0.20 -0.45 0.00 0.45\\nDeer hunting 0.5 M 0.7 M 0.15 0.21 0.10 -0.11\\nSmall business owners 0.0 M 1.2 M 0.08 -0.03 0.01 0.04\\nGolf 0.2 M 1.0 M 0.12 0.15 0.11 -0.04\\nMaster‚Äôs degree 0.3 M 0.9 M -0.09 -0.20 -0.12 0.08\\nAyn Rand 1.1 M 0.0 M 0.27 0.35 0.29 -0.07\\nHealth care 0.0 M 1.1 M ‚úì 0.02 -0.02 0.02 0.04\\nTable 3. Top 60 Interest ordered by spend in USD. Political advertisers in the U.S. spent millions of dollars\\ntargeting highly skewed interest circumventing the platform regulation.On the Use of Proxies in Political Ad Targeting 17\\nFig. 5. Examples of ads whose advertisers appear to use proxies. Virginia House Republicans (on the left)\\ntarget users interested in (among others) NASCAR, Big-game hunting, and Deer hunting. On the right, CA\\nBlack Power Network targets users interested in The Shade Room, Basketball Wives, Love & Hip Hop.\\nin the top ten spots by spend. We also observe that these interests are not skewed towards either\\npolitical option or any ethnic group.\\nHowever, we also find that advertisers spent millions targeting and excluding interests that are\\nnot explicitly related to politics. Users interested in ‚ÄòJoe Rogan‚Äô, ‚ÄòKid Rock‚Äô, ‚ÄòTed Nugent‚Äô, ‚ÄòDuck\\nDynasty‚Äô, ‚ÄòNASCAR‚Äô, ‚ÄòRoseanne Barr‚Äô, ‚ÄòClint Eastwood‚Äô, ‚ÄòElon Musk‚Äô, or ‚ÄòAyn Rand‚Äô are often\\nexcluded from seeing political messaging. The skew measures presented in the table show why\\nadvertisers would make such choices: all but two of these interests skew strongly Republican and\\nwhite. Notably, there is no skew information for the often excluded ‚ÄúFox News‚Äù. At the time of\\nwriting this paper, this targeting criteria referred to the people who were employed there, not\\nmerely interested in the topic. Thus, that population was too small to obtain skew measures.\\nOn the other hand, many interests that lean strongly towards Black voters‚Äîand at times refer to\\nthem directly‚Äîare used for including audiences for political ads, for example ‚ÄòLove & Hip-Hop‚Äô,\\n‚ÄòAfrican-American literature‚Äô, ‚ÄòBlack-ish‚Äô, ‚ÄòBasketball Wives‚Äô, or ‚ÄòThe Shade Room‚Äô. Note that, with\\nthe exception of ‚ÄòNPR‚Äô (National Public Radio), the interests with stronger Democratic skews are\\npredominantly of interest to Black voters, who are largely Democratic, rather than Democrats at\\nlarge. Finally, some interests ware used both for inclusion and exclusion: ‚ÄòSaturday Night Live‚Äô,\\n‚ÄòBarstool Sports‚Äô, ‚ÄòBig-game hunting‚Äô, or ‚ÄòDeer hunting‚Äô.\\nThe prevalence of use of interests with no apparent connection to politics, but often with high\\nskews towards political or ethnic groups, indicates that advertisers are skillfully circumventing the\\nplatform‚Äôs limitations on targeting.\\nA reader familiar with the U.S. media landscape likely understands the cultural background\\nfor most of the skewed interests in Table 3, especially that the majority of the interests skew\\nin the direction they would anticipate. Still, for the benefit of other readers we provide a short\\ndescription. Musicians Kid Rock and Ted Nugent, actors Clint Eastwood and Roseanne Barr, and\\nbillionaire Elon Musk have all been outspoken supporters of the Republican Party [ 47,52]. Joe18 Piotr Sapiezynski, Levi Kaplan, Alan Mislove, and Aleksandra Korolova\\nDemocratic Neutral Republican\\nInterest skew0.00.10.20.30.40.5Fraction of all interests usedTargeting by interests\\nDemocrats\\nRepublicans\\nDemocratic Neutral Republican\\nInterest skew0.000.010.020.030.040.050.060.07Fraction of all interests usedExclusion by interests\\nDemocrats\\nRepublicans\\nFig. 6. Ad accounts associated with both sides of the spectrum use more congruent than incongruent interests.\\nRepublican-leaning advertisers used 4.5x more interests that skew Republican than interests with a Democratic\\nskews. Democrats used 50% more Democratic skewing interests than Republican skewing. Both parties use\\nexclusion more sparingly.\\nRogan is a podcast host, who‚Äîwhile declining to align himself with either side of the political\\nspectrum‚Äîtends to center right-wing voices [ 58]. Duck Dynasty is a reality show that features\\na family patriarch whose right to hold and promote anti-gay views was defended by prominent\\nRepublican politicians [ 72]; NASCAR is a motor sport that, while it has no political presence,\\nboasts a fan base that is predominantly older, white, male, and Republican [ 31]. Ayn Rand is an\\nauthor whose writing continues to inspire conservatives [ 39]. Love & Hip-Hop and Basketball\\nWives are reality TV series, showcasing hiphop and R‚Äôn‚ÄôB musicians (the former), and women\\nwho are romantically involved with NBA players (the latter). Black-ish is a sitcom with a largely\\nAfrican-American cast, and The Shade Room is a media company focused on celebrity and trending\\nnews catering to the Black population. Figure 5 shows two examples of ads and the targeting\\nparameters used by their advertisers in the period when they ran.\\nSome of the results in Table 3 might appear counter intuitive, for example relatively low skew of\\nJoe Rogan or Roseanne Bar. They could be driven by, for example, outdated interest associations,\\nornegative interest , i.e., one group of users engaging with cross-cutting content in a negative way.\\nHowever, regardless of how correct these associations are, our metric describes the actual skew of\\nthe targeted audience should such targeting criterion be used.\\n4.3 Both Democrats and Republicans use proxies\\nWe now investigate the prevalence of use of proxies among actors on each side of the political\\nspectrum. To establish a mapping between an advertising account and its political alignment we\\nrely on data for a subset of advertising accounts reported by WhoTargetsMe (see Section 3.4).\\nFigure 6 shows an overview of the results, with interests divided into three equally sized groups\\n(tertiles) along the Republican-Democratic skew: interests with ùëÜùëÖùê∑<‚àí0.073are designated to\\nhave a Democratic skew, ‚àí0.73‚â§ùëÜùëÖùê∑<0.063are considered Neutral, and those with ùëÖùëÖùê∑‚â•0.063\\n‚Äì Republican.\\nWe observe that accounts associated with both sides of the political spectrum tend to target\\ninterests that skew in their direction more than those who skew opposite. This effect is particularly\\npronounced among Republican-leaning accounts: 46.7% of interests they use include Facebook users\\nassociated with Republican-skewing interests, compared to 9.6% of interests with a Democratic-skew,\\nnearly a five-fold difference. Conversely, Democratic-leaning accounts used more Democratic-\\nskewing than Republican skewing interests to target audiences. We also note that accounts on both\\nsides use fewer interests for exclusion than for inclusion. Only 9.0% of interests used by DemocratsOn the Use of Proxies in Political Ad Targeting 19\\n0.000.250.500.751.00Interests CDF (Targeting)Democratic accounts\\nDemocratic skew\\nNeutral\\nRepublican skewRepublican accounts\\n101102103104105106\\nSpend per interest [USD]0.000.250.500.751.00Interests CDF (Excusion)\\n101102103104105106\\nSpend per interest [USD]\\nFig. 7. Ad accounts associated with both sides of the spectrum spend on average more targeting interests\\nwith aligned skews.\\nand 5.4% used by Republicans were exclusions. Nevertheless, Democrats were far more likely to\\nexclude users associated with Republican-skewing interests than those with Democratic skewing\\ninterests (5.8% and 1.1%, respectively). That difference for Republican accounts is not statistically\\nsignificant.\\nSo far, we have focused on just the count of interest while disregarding how much money the\\nadvertisers actually spend on targeting and exclusion using these interests. We now take a closer\\nlook at how advertisers allocated their budgets with respect to different interests. Figure 7 shows\\nthe cumulative distribution function (CDF) of spend per interests by Democratic (left panels) and\\nRepublican accounts (right panels), both for targeting (top panels) and exclusion (bottom panels).\\nThe dotted lines indicate the mean values.10\\nBeginning with the distribution on targeting spend by Democrats we see that both the median\\nand mean spend per targeted interest is higher for interests with a Democratic skew and the lowest\\nfor those with Republican skew. In fact, Democrat-leaning accounts‚Äô median spend on aligned\\ninterests is an order of magnitude higher than their spend on the incongruent interests at $14,696\\nvs $1,387, respectively. In the bottom left panel, we see that the opposite trends are apparent for\\nexclusion. Interestingly, both median and mean spend per excluded Republican-leaning interest are\\nhigher than the respective values for targeted Democrat-leaning interests.\\nSurprisingly, not all of these characteristics hold for the spend distributions among Republican-\\nleaning accounts. The median spend per targeted interests regardless of skew is around $2,300,\\nwhile the average is still higher for Republican skewing interests, at $51,524 vs $17,812 for interests\\nwith a Democratic skew. Perhaps due to limited prevalence of exclusion among Republican-aligned\\naccounts, the distributions in the lower right panel do not paint a clear picture.\\nStill, we note that there is a non-negligible fraction of targeted interests with incongruent skew\\nand excluded aligned interests. This ‚Äúreaching across the aisle‚Äù could be driven by a number of\\n10A convenient property of a CDF plot allows us to read the median by looking at the value of the x-axis when the plot\\ncrosses 0.5 (the mid-point) on the y-axis.20 Piotr Sapiezynski, Levi Kaplan, Alan Mislove, and Aleksandra Korolova\\n1.0\\n 0.5\\n 0.0 0.5 1.0\\nAudience skew SRD\\n1.0\\n0.5\\n0.00.51.0Spend skew (targeting)\\n1.0\\n 0.5\\n 0.0 0.5 1.0\\nAudience skew SRD\\n1.0\\n0.5\\n0.00.51.0Spend skew (exclusion)Interests\\nLogistic fit\\n95% CIAudience skew of interests predicts spend skew\\nFig. 8. Logistic regression fits show the strong positive relationship between audience skews of targeted\\ninterests, and the spend skew between advertisers associated with either party, as well as a negative relation-\\nship for excluded interests. This indicates that advertisers on both sides do leverage proxies to reach their\\nsupporters and exclude their opponents.\\nmotivations: it could indicate a genuine effort at bridging the divide, discouraging the other side\\nfrom voting, or simply relying on interests that skew in a different direction than anticipated\\nby the advertiser. Unfortunately, because the Ad Library only reports targeting parameters in\\nweekly aggregate, rather than per-ad, it is difficult to match particular ads to their targeting criteria.\\nWithout that matching it is not always obvious which of these three motivations were the most\\nlikely. We leave the further exploration of reaching across the aisle to future work.\\nSpending and interest use analysis is finally summarized in Figure 8. We introduce a spend skew\\nmeasure with a formula that resembles the audience skew, i.e., we divide the difference between\\nRepublican and Democratic spend on a particular interest by the sum of their spend on that interest.\\nNote that this measure is not normalized and that we do not know the affiliation of all advertising\\naccounts. Therefore the value of 0 does not necessarily mean even spend among both parties (but\\nonly among the accounts for which we have the affiliation labels). We perform a logistic regression\\nfit with audience skew as the independent variable and the spend skew as the dependent variable,\\nto show that the former is predictive of the latter. The ùëÖ2values for the targeting and exclusion fits\\nare 0.19 and 0.14 and the coefficient associated with the audience skew is significant (see Table 4).\\nThis shows that the audience skew is predictive of the spend skew, and confirms that advertisers on\\nboth sides do leverage proxies to reach their supporters and exclude their opponents. Nevertheless,\\nthe audience skew of interests does not fully explain the targeting strategies of the advertisers.\\nVariable Targeting Exclusion\\nIntercept ‚àí0.73‚àó‚àó‚àó‚àí0.87‚àó‚àó‚àó\\nAudience skew ùëÜùëÖ‚àíùê∑6.25‚àó‚àó‚àó‚àí4.92‚àó‚àó‚àó\\nùëÖ20.19 0 .14\\nTable 4. Logistic regression table for predicting spend skew from audience skew. It shows that both parties\\nspend more targeting interests that are congruent in skew (i.e., Democrats spent more targeting interests\\nthat reach predominantly Democratic audiences), and excluding interests that are incongruent. ùëù<0.001‚àó‚àó‚àó;\\nùëù<0.01‚àó‚àó,ùëù<0.05‚àó.On the Use of Proxies in Political Ad Targeting 21\\nTaken together, the findings in this section indicate that advertisers use interests-based targeting\\npredominantly‚Äîbut not exclusively‚Äîto reach users who are more likely to be aligned with the\\nadvertised messages and, to a lesser extent, exclude those who are not.\\n5 Discussion\\nWe now discuss the ethical considerations taken into account throughout our study, the study‚Äôs\\nlimitations, and conclude with implications of our findings for further regulatory and product\\nchanges needed and for future research.\\n5.1 Ethics\\nThroughout this work, we took careful steps to minimize the impact of our research on all stake-\\nholders. We limited the strain on Facebook‚Äôs server infrastructure by enforcing intervals between\\nissuing queries. We used real Facebook accounts, one at a time, and we did not attempt to cir-\\ncumvent the rate limits by parallelizing our scraping. For the collection of targeting information\\nas well as audience insights we relied on public-facing services that are part of the transparency\\nefforts undertaken by Facebook. They only present identifiable information about advertisers, not\\nindividual users.\\nPrevious work found that Facebook did not extract any information from the custom audience\\nfiles it did not already possess [ 120]. Thus, we do not create additional privacy exposure for\\nindividuals whose data is contained within the voter records that we upload to create custom\\naudiences. Further, we do not actually run any ads, and thus we do not interact with or alter the\\nonline experience of the individuals in our custom audiences. Our analysis did not require collecting\\nany additional non-aggregated user information beyond projected counts.\\nWe acknowledge that the methods presented in our work could be used by malicious advertisers\\nto find proxy interests according to political skew, race, or ethnicity. However, our results show\\nthat trying to maintain obscurity is futile: mainstream advertisers are already relying on proxies to\\ncircumvent Facebook‚Äôs limits on targeting. To limit the risk of promoting this practice further, we\\ndo not publish the full list of interest skews. We believe that by publishing this work we can help\\nencourage Facebook and other ad targeting platforms to take the steps necessary to prevent this\\nkind of abuse.\\n5.2 Limitations\\nOur work suffers from limitations that stem both from possible data quality issues, as well as\\nsimplifying choices we have made during the analysis. First of all, the targeting information API\\ndid not always work reliably, which leads to uneven data quality among advertisers. Second, the\\ndata is returned in weekly aggregates but collected daily. We normalized spends to account for this\\ndiscrepancy, but, given the variable delays and selectively missing data, our estimates of spend\\nmight not be exact. While these two shortcomings might somewhat affect the accuracy of reported\\nspends, we expect that the sorting and direction of skews closely reflects reality.\\nFurther, we relied on manually labeled political affiliations on a selected subset of advertising\\naccounts. We recognize that the manual labeling process is prone to error and the coverage is far\\nfrom perfect. Nevertheless, obtaining higher coverage would only strengthen the results as it would\\ncover more instances of proxy use by partisan actors. We note that our results do not point to either\\nof the sides of the U.S. political spectrum as a main perpetrator in proxy use.\\nWe note that our racial (Black, white, and Hispanic) and political labels (Republican, Democrat)\\nare necessarily reductive and do not cover the diverse pool of ethnicities and political stances of\\nU.S. voters. We consider this work as the first step towards understanding the use of proxies among22 Piotr Sapiezynski, Levi Kaplan, Alan Mislove, and Aleksandra Korolova\\nthe political advertisers and the methods we proposed can be extended to measure biases using\\nmore labels, and even dimensions beyond race and political views.\\nFinally, the methods we presented cannot be readily replicated on other platforms. In our\\nresearch we rely on using information we obtain through voter records to craft politically or\\nracially homogeneous Custom Audiences and then asking the platform to estimate the prevalence\\nof each interest in a given Custom Audience. While all major online social media platforms allow\\nadvertisers to use tools equivalent to Facebook‚Äôs Custom Audiences, they do so with caveats. TikTok\\nand Twitter do not support matching by mailing address, only by phone number, email address,\\nusernames, and advertising IDs. Only some states publish phone numbers and/or email addresses of\\ntheir voters, and even then, only a few voters actually provide them. Google, on the other hand, does\\nsupport both Custom Audiences based on physical addresses, and intersecting Custom Audiences\\nwith inferred interests. We therefore expect that our method could be applied with minor changes\\nto investigate the skews of targeting interests on Google and we leave the implementation to future\\nwork.\\n5.3 Implications\\nTaken together, our findings demonstrate that the Facebook advertising system continues to allow\\nadvertisers to selectively reach or exclude users with certain political views and ethnicities, and\\nthat the removal of select interests from the targeting options did not address this problem. Our\\nfindings show that the problem is not just a hypothetical. We documented that real political\\nadvertisers spent millions of dollars in the 2022 midterm elections exploiting proxy targeting to\\nreach‚Äîor exclude‚Äîselect supporters of specific political parties or members of racial and ethnic\\ngroups. The methodology we proposed allows us to directly measure the potential of various\\ntargeting criteria to serve as proxies so, as opposed to press reporting on this problem so far [ 76],\\nwe are not limited to criteria with obvious cultural interpretation. Although we are not able to\\nmeasure proxy use in opportunity advertising given the limitations of the Ad Library, our findings\\nof their extensive use in political advertising likely extend to the feasibility of their use in the\\nopportunity advertising context. Taken together, our work has implications for policy in political\\nand opportunity advertising and platform design, further platform transparency desiderata, and\\nraises a number of questions for future work, including for the platforms themselves.\\n5.3.1 Targeted removal of select interests is counter-productive to societal goals. As discussed in\\nbackground, public interest entities view the ability to target political messages by racial or ethnic\\norigin, sexual orientation, political opinion, and other inferred personal data as a threat to democratic\\nprocess with a potential for creating echo chambers, sowing division and in contradiction with the\\nright of individuals to be informed in an objective, transparent and pluralistic way [ 73,102,103,123].\\nFacebook‚Äôs removal of ability to target political messages by race, ethnicity and political affiliation,\\nas well as other select topics, and the letter from its employees [ 116] is evidence that Facebook\\nalso views such dangers as significant. Furthermore, Facebook‚Äôs internal documents‚Äîas reported\\non by the Wall Street Journal in 2021 [ 53]‚Äîexplicitly speak of the power of targeting to suppress\\nturnout among certain demographics, and to ‚Äúlead to polarization, political disenfranchisement, and\\ninformation siloing depending on the content, targeting and delivery of the ad‚Äù. Thus, by limiting\\nadvertisers‚Äô ability to reach politically charged interest-based audiences, Facebook‚Äôs goal is to reduce\\nthe ease with which advertisers could seek to disenfranchise a subset of voters [ 53]. However, as\\nour work demonstrates, the current approach to such removal may not be sufficiently effective, as\\nadvertisers may (and do) circumvent such restrictions through proxy targeting. Moreover, one may\\nargue that such removal, in fact, has the opposite effect of the one publicly stated in Facebook‚Äôs\\ngoals of increased transparency and user control [4, 26, 50, 69].On the Use of Proxies in Political Ad Targeting 23\\nFirst, it concentrates power in political incumbents. A key argument in 2018 in Facebook‚Äôs choice\\nnot to remove political advertising from its platform was that ‚Äúbanning political ads on Facebook\\nwould tilt the scales in favor of incumbent politicians and candidates with deep pockets. [... ] it\\nwould make it harder for people running for local office‚Äîwho can‚Äôt afford larger media buys‚Äîto\\nget their message out‚Äù [ 50]. Facebook reiterated this argument also in 2022 [ 83], stating that the\\ntargeting is particularly valuable to small advertisers who may not be able to afford broadcast\\ntelevision. The sentiment for the need of political parties to have equality of opportunity to compete\\nfor voters‚Äô support is echoed by public interest researchers [ 123], and even by the Equal Time Rule.11\\nHowever, what the select removal of targeting criteria, including the easily explainable high-level\\ntargeting criteria such as political affiliation, while allowing proxy targeting leads to the opposite\\ndynamic. Entities that have larger budgets to acquire data on individuals from data brokers for use\\nin Custom Audiences, those that have more extensive data teams to connect interest data with their\\nproxy power, or those that can otherwise leverage their incumbent positions for proxy targeting\\nefforts, may gain significant political advantage. Campaign political strategists from both sides of\\nthe aisle in the U.S. were openly discussing their intent to do so [ 30,117] in 2021 ‚Äì 2022 in response\\nto the removals announced by Facebook coming to force in January 2022. Furthermore, several\\nrecent studies of incumbent data advantage for use in ad targeting in Hungarian election [ 123] and\\nUK election [ 6] demonstrate that this concentration of power is not a merely theoretical possibility.\\nSecond, it makes divisive and discriminatory advertising harder to study. Although the practice\\nof select targeting criteria removal by Facebook in response to concerns began in 2017, our study is\\nthe first of its kind to systematically study proxy use without reliance on anecdotal correlations\\nbetween interests and ethnic or political affinities. Thus the removal practice, combined with\\nFacebook‚Äôs current limited Ad Library transparency choices [ 15] increases the barrier for effective\\nstudy of whether racial, ethnic, or other kinds of political manipulation are continuing to take place\\nfor public interest researchers independent of Facebook. Furthermore, the same increase in the\\ndifficulty of study applies to the studies of potential discrimination using proxies in the domains of\\nopportunity advertising.\\nFinally, it makes it more difficult for Facebook users to exercise meaningful control over the\\npersonalization of advertising they receive, a desire held by users [ 11,48,51,78] and legislators [ 73,\\n102,103], and a stated goal by Facebook [ 69]. Specifically, the removal of certain interests from\\navailable targeting criteria also removes them from ad preference control choices and the user-\\nfacing explanations of why they have been shown a particular ad, the two key entry points where\\nusers can express their preferences. Thus, rather than being able to explicitly specify that the\\nuser does not want to receive advertising tailored to their ethnic origin, political affiliation or\\nnewspaper readership, whose consequences one may be able to reason about, the user is faced with\\nan insurmountable task of guessing which interests may be proxies.\\nOur work thus suggests that the most immediate and feasible solution (albeit with a significant\\ncaveat discussed in Section 5.3.3) is the one that European policy makers, civil rights advocacy\\ngroups in the U.S. and the E.U., and prominent figures such as Ellen Weintraub and Bill Gates have\\nbeen advocating for: limiting the targeting capabilities for high-stakes contexts such as political\\nadvertising to a very small list of ‚Äúsafe‚Äù targeting parameters such as broad location and requiring\\nexplicit user consent for use of any data beyond that [ 42]. Another approach would be to limit the\\navailable targeting criteria to a very short list of those directly relevant to political engagement\\nand give users robust controls associated with those.\\n11Section 315 of the Communications Act of 1934 requires that U.S. radio and broadcast TV to provide competing political\\ncandidates with equivalent access to present their messages.24 Piotr Sapiezynski, Levi Kaplan, Alan Mislove, and Aleksandra Korolova\\n5.3.2 Balancing fine-grained targeting and societal desiderata. As a middle ground to significantly\\nlimiting the targeted advertising in political and opportunity domains, platforms can aim to develop\\nnew approaches that balance their desire to provide political advertisers, including small ones,\\nwith enhanced abilities to engage audiences in the political process, with effective transparency\\nefforts enabling independent oversight of the political advertising for public interest entities, and\\nmeaningful controls for individual users.\\nPlatform work on proxy identification and removal. A path for platforms to keep the fine-\\ngrained targeting capabilities at the variety level of today while satisfying the societal desiderata is\\nfor the platforms to take it upon themselves to continuously identify and remove proxy attributes\\nand attribute combinations among their offered targeting capabilities. These decisions should\\nnot be limited to proxy attributes whose names clearly indicate skew, and should be based on\\nmore systematic methods, such as those we presented in this work. However, for this approach to\\nconvincingly meet the societal and regulatory goals, and to gain public trust, platforms‚Äô decisions\\nand data on which they are based should be available for independent third party and public interest\\nscrutiny. This is not the case today: despite thousands of attribute removals over the course of\\nseven years, Facebook did not make the targeting attributes removed, or the attributes left, nor the\\nspecific reasoning for their choices, available [122].\\nTransparency of targeting. Making the targeting choices of political and opportunity advertisers\\navailable to public interest scrutiny has long been a key request of civil society organizations\\nfrom the platform [ 35,41]. Facebook itself makes the claim that ‚Äúinformation about advertisers‚Äô\\ntargeting choices is critical to understanding the impact of digital advertising on elections and\\nsocial discourse‚Äù [62].\\nHowever, currently, Facebook‚Äôs Ad Library presents the targeting criteria used by political adver-\\ntisers as a weekly aggregate across all of that advertiser‚Äôs campaigns, rather than per advertisement\\nor ad campaign.12For opportunity advertisers, the targeting criteria are not part of the Ad Library\\nat all. Facebook cites user privacy concerns as the key reason behind withholding that data [ 24,62].\\nFor example, if targeting criteria for each ad are publicly available, then if an attacker knows\\nthat an ad was targeted using criterion ùëã, and can also observe that user ùë¢has interacted with\\nthis ad, then the attacker can infer that user ùë¢matches criteria ùëã[64]. Such an attack becomes\\nparticularly worrisome given the availability and use of proxy interests that could thus imply\\nsensitive information about the user.\\nOne path to approaching the transparency-privacy trade-off is through deploying legal protec-\\ntions to minimize risk of abuse from those granted access. This is an approach that Facebook began\\nto pursue in May 2022 through its Facebook Open Research and Transparency (FORT) platform [ 92].\\nSpecifically, ‚Äúeligible university-affiliated researchers‚Äù can apply to FORT to access targeting data\\nat the ad level, if Facebook approves their study‚Äôs research scope and objectives [ 91], and if the\\nresearchers accept the terms in its Research Platform Addendum [ 84],13which include provisions\\nprohibiting privacy violations. Although a viable path, it has the shortcoming of leaving the control\\nof what research can be done, and by whom, in the platform‚Äôs hands, which may stifle the ability\\nto perform and publish findings with unfavorable outcomes to the platform.\\nAnother potential path to mitigate the privacy issues is by changing the visibility of user\\ninteractions with paid content, particularly paid political content. For example, rather than making\\nthe user interactions with political ads visible to the entire world (as is done currently), the default\\nprivacy setting for such interactions could be set to private or friends-only. The restriction would\\n12Furthermore, there is no official maintained or documented API to access the targeting information, an omission that\\nlimits inquiry and should be remedied.\\n13We did not seek access to this data as we find that the terms of the Addendum are incompatible with academic freedom.On the Use of Proxies in Political Ad Targeting 25\\nmake the interactions much harder to access at scale for potential attackers; and thus make it\\ndifficult and not cost effective to violate privacy using the transparency tools. A complementary\\napproach, would be to make clear to the users affirmatively opting-in to public interactions what\\ninformation about them may be revealed. Developing ways to make the targeting explanations\\naccurate, comprehensive and interpretable by the users, and ways to provide them in context [ 54],\\nis an important area for future work.\\nBeyond targeting transparency. We take a further step back to re-imagine how to expand the\\nAd Library to make it an effective tool for research on the questions that inspired its creation:\\n(1) do advertisers exploit the targeting options to reach relatively narrow segments of users that\\ndisproportionately share a characteristic that can be negatively exploited in the political process or\\nin opportunity advertising, and (2) do advertisers send conflicting messages to separate groups\\nof users? We suggest that rather than approaching it as a question of merely making some data\\navailable and placing the burden of its analysis on the (usually) less technically sophisticated and\\nresourced public interest researchers, platforms can play an active and constructive role in enabling\\nsuch research. For example, platforms could develop and deploy privacy-preserving ways to examine\\nskew in the composition of an advertiser‚Äôs targeted audience along a variety of characteristics (and\\ntheir intersections), and highlight such advertisers along with the characteristics for public interest\\nscrutiny. Methods for doing so already exist in other domains [ 1,108]. Furthermore, platforms could\\ndevelop techniques for flagging when ad message tailoring techniques cross from the valuable to\\nthe potentially harmful and manipulative, and bring those messages to public interest scrutiny. The\\nholistic view that the platform has of the messages used as well as their evolution over time can\\nmake this task feasible. Finally, the platforms, while leveraging user reports [ 65], could develop\\nnew techniques for identifying when a combination of an ad‚Äôs message with the targeting chosen\\nis manipulative or pernicious, even when each is seemingly benign on its own. Platforms have\\nhinted at efforts to do so, but have not made the techniques nor the findings public [ 112]. These\\napproaches can be complementary to the others, can rely on privacy and verification techniques\\nthat offer better trade-offs between privacy and utility for data analyses rather than data releases,\\nand can lead to a constructive dialogue between platforms, users and civil society that expands the\\nset of possible solutions to the benefit of all stakeholders.\\nCenter the needs of end users. Finally, platforms should continue improving transparency in\\nways that center the needs of their end-users. In particular, individuals should have meaningful\\noverview and control of the information that is used to personalize delivery of political (and\\nopportunity) advertising to them and be protected from manipulation. According to PEW research\\nin 2020, as many as 77% of Americans said that the practice of using their online activities to show\\nthem ads from political campaigns is not very or not at all acceptable [ 11]. The statistics are echoed\\nby a Gallup study that indicates the majority of Americans do not want to be micro-targeted for\\npolitical ads [ 78]. In the EU, this desiderata in the political ads context has recently been mandated\\nby European regulation [ 103] but the pathway for practically achieving it remains an open question.\\nInitial survey based methods have begun to explore user needs related to advertising controls\\nbroadly [ 48], and platforms should expand and tailor such studies to political and opportunity\\nadvertising. In particular, in addition to the reasons previously discussed related to discrimination\\nin opportunity advertising, Facebook frames their choices for targeting attribute removal as those\\nthat relate to ‚Äútopics people perceive as sensitive‚Äù [ 83]. However, what is, in fact, sensitive for\\neach individual, may be individual- and context-dependent [ 71]. Furthermore, for individuals,\\nreasoning about sensitivity may be easier at the high-level category levels, rather than at the\\nlevel of categories whose proxy power, individually and in combination, the user does not know.\\nThird-party organizations, such as WhoTargetsMe, have begun to re-package the information\\navailable through the Ad Library and combine it with ad targeting explanations collected through26 Piotr Sapiezynski, Levi Kaplan, Alan Mislove, and Aleksandra Korolova\\nbrowser plugins to make such understanding possible [ 126]. However, the onus of these kinds of\\ntransparency and controls should be on the much better-resourced platform, and innovation in this\\nspace remains an urgent direction for future work.\\n5.3.3 Role of algorithmic ad delivery when targeting is limited. A consequence of removing fine-\\ngrained and other targeting abilities of advertisers is that it shifts the power over how political\\nspeech and opportunity advertising is distributed from the advertiser to the platform. Facebook\\nin particular encourages greater reliance on its algorithmic tools, such as Lookalike Audiences,\\nMetaAdvantage+ audience when removing the targeting abilities [ 23,81,85,94]. As we discussed in\\nSection 2.2, algorithmic identification of the right audience and machine learning driven ad delivery\\noptimization, are processes that can (and do) lead to echo chambers in political ad delivery [ 3,18]\\nand discrimination in opportunity advertising [ 2,55,56,59,110], even when the advertiser targets a\\npolitically or racially balanced audience. Furthermore, recent work shows that for ads with greater\\nreliance on algorithmic targeting tools, the user-facing explanations and controls are inaccurate\\nand ineffective [ 21]. Thus, the question of reconciling the tension between granularity of targeting\\navailable to advertisers and ceding the control of targeting and delivery to the platform, and, hence,\\nthe transparency desiderata for the implementations of the ad delivery algorithms in political and\\nopportunity advertising domains is an important question for future work.\\n6 Acknowledgements\\nThis work work was funded in part by National Science Foundation grants CNS-1956435, CNS-\\n2344925, CNS-1955227, and CNS-2318290. We thank the CSCW reviewers for their thoughtful and\\ndetailed comments that have helped improve this work.\\nReferences\\n[1]Rachad Alao, Miranda Bogen, Jingang Miao, Ilya Mironov, and Jonathan Tannen. 2021. How Meta is working to\\nassess fairness in relation to race in the US across its products and systems. Meta Technical Report (2021).\\n[2]Muhammad Ali, Piotr Sapiezynski, Miranda Bogen, Aleksandra Korolova, Alan Mislove, and Aaron Rieke. 2019.\\nDiscrimination through optimization: How Facebook‚Äôs Ad delivery can lead to biased outcomes. Proceedings of the\\nACM on human-computer interaction 3, CSCW (2019), 1‚Äì30.\\n[3]Muhammad Ali, Piotr Sapiezynski, Aleksandra Korolova, Alan Mislove, and Aaron Rieke. 2021. Ad delivery algorithms:\\nThe hidden arbiters of political messaging. In Proceedings of the 14th ACM International Conference on Web Search and\\nData Mining . 13‚Äì21.\\n[4]Richard Allan. Mar 2019. Protecting Elections in the EU. https://about.fb.com/news/2019/03/ads-transparency-in-\\nthe-eu/.\\n[5]Asha Allen. 2024. New EU Rules on Political Advertising Set to Have Limited Impact on Advertising Ecosystem. https:\\n//cdt.org/insights/new-eu-rules-on-political-advertising-set-to-have-limited-impact-on-advertising-ecosystem/.\\n[6]Gary Wright Amber Macintyre and Stephanie Hankey. 2018. United Kingdom: Data and Democracy in the UK.\\nhttps://ourdataourselves.tacticaltech.org/posts/overview-uk/\\n[7]Julia Angwin and Terry Parris Jr. 2016. Facebook lets advertisers exclude users by race. https://www.propublica.org/\\narticle/facebook-lets-advertisers-exclude-users-by-race\\n[8]Julia Angwin and Terry Parris Jr. 2016. Facebook lets advertisers exclude users by race. https://www.propublica.org/\\narticle/facebook-lets-advertisers-exclude-users-by-race\\n[9]Julia Angwin, Noam Scheiber, and Ariana Tobin. 2017. Dozens of companies are using facebook to exclude older\\nworkers from job ads. https://www.propublica.org/article/facebook-ads-age-discrimination-targeting\\n[10] Julia Angwin, Madeleine Varner, and Ariana Tobin. 2017. Facebook enabled advertisers to reach ‚Äújew haters‚Äù.\\nhttps://www.propublica.org/article/facebook-enabled-advertisers-to-reach-jew-haters\\n[11] Brooke Auxier. 2020. 54% of Americans say social media companies shouldn‚Äôt allow any political\\nads. https://www.pewresearch.org/short-reads/2020/09/24/54-of-americans-say-social-media-companies-shouldnt-\\nallow-any-political-ads/\\n[12] Eytan Bakshy, Solomon Messing, and Lada A Adamic. 2015. Exposure to ideologically diverse news and opinion on\\nFacebook. Science 348, 6239 (2015), 1130‚Äì1132.On the Use of Proxies in Political Ad Targeting 27\\n[13] Solon Barocas. 2012. The price of precision: Voter microtargeting and its potential harms to the democratic process.\\nInProceedings of the first edition workshop on Politics, elections and data . 31‚Äì36.\\n[14] Lois Beckett. 2017. Trump digital director says Facebook helped win the White House. https://www.theguardian.\\ncom/technology/2017/oct/08/trump-digital-director-brad-parscale-facebook-advertising.\\n[15] Davide Beraldo, Stefania Milan, Jeroen de Vos, Claudio Agosti, Bruno Nadalic Sotic, Rens Vliegenthart, Sanne\\nKruikemeier, Lukas P Otto, Susan A. M. Vermeer, Xiaotong Chu, and Fabio Votta. 2021. Political advertising exposed:\\ntracking Facebook ads in the 2021 Dutch elections. https://policyreview.info/articles/news/political-advertising-\\nexposed-tracking-facebook-ads-2021-dutch-elections/1543\\n[16] Frederik J Zuiderveen Borgesius, Judith M√∂ller, Sanne Kruikemeier, Ronan √ì Fathaigh, Kristina Irion, Tom Dobber,\\nBalazs Bodo, and Claes De Vreese. 2018. Online political microtargeting: Promises and threats for democracy. Utrecht\\nLaw Review 14, 1 (2018), 82‚Äì96.\\n[17] Ceren Budak, Sharad Goel, and Justin M Rao. 2016. Fair and balanced? Quantifying media bias through crowdsourced\\ncontent analysis. Public Opinion Quarterly 80, S1 (2016), 250‚Äì271.\\n[18] Dominik B√§r, Francesco Pierri, Gianmarco De Francisci Morales, and Stefan Feuerriegel. 2024. Systematic discrepancies\\nin the delivery of political ads on Facebook and Instagram. PNAS Nexus 3, 7 (June 2024).\\n[19] Carole Cadwalladr and Emma Graham-Harrison. 2018. Revealed: 50 million Facebook profiles harvested for Cambridge\\nAnalytica in major data breach. https://www.theguardian.com/news/2018/mar/17/cambridge-analytica-facebook-\\ninfluence-us-election\\n[20] Kip Cassino. 2017. The Final Analysis: what happened to political advertising in 2016 (and forever). https:\\n//borrellassociates.com/ip_releases/the-final-analysis-political-advertising-in-2016/\\n[21] Jane Castleman and Aleksandra Korolova. 2024. Why am I Still Seeing This: Measuring the Effectiveness Of Ad\\nControls and Explanations in AI-Mediated Ad Targeting Systems. In Seventh AAAI/ACM Conference on Artifical\\nIntelligence, Ethics and Society (AIES 2024) .\\n[22] Blake Chandlee. 2019. Understanding our policies around paid ads. https://newsroom.tiktok.com/en-us/\\nunderstanding-our-policies-around-paid-ads\\n[23] Salim Chouaki, Islem Bouzenia, Oana Goga, and Beatrice Roussillon. 2022. Exploring the Online Micro-targeting\\nPractices of Small, Medium, and Large Businesses. Proceedings of the ACM on Human-Computer Interaction 6, CSCW2\\n(Nov. 2022), 1‚Äì23. https://doi.org/10.1145/3555103\\n[24] Mike Clark. 2023. Research Cannot Be the Justification for Compromising People‚Äôs Privacy. https://techcrunch.com/\\n2023/07/17/norway-meta-ads-ban/.\\n[25] Nick Clegg. 2019. Facebook, elections and political speech. https://about.fb.com/news/2019/09/elections-and-\\npolitical-speech/\\n[26] Nick Clegg. 2023. New Features and Additional Transparency Measures as the Digital Services Act Comes Into\\nEffect. https://about.fb.com/news/2023/08/new-features-and-additional-transparency-measures-as-the-digital-\\nservices-act-comes-into-effect/.\\n[27] Kate Conger. 2023. Twitter to relax ban on political ads. https://www.nytimes.com/2023/01/03/technology/twitter-\\npolitical-ads.html\\n[28] Josh Constine. 2019. Facebook launches searchable Transparency Library of all active ads. https://techcrunch.com/\\n2019/03/28/facebook-ads-library/\\n[29] Nick Corasaniti. 2021. Political Campaigns Can Still Target You on Facebook. https://www.nytimes.com/2021/11/11/\\nus/politics/facebook-political-ads.html\\n[30] Nick Corasaniti. 2021. Political Campaigns Can Still Target You on Facebook. https://www.nytimes.com/2021/11/11/\\nus/politics/facebook-political-ads.html\\n[31] John Dick. 2013. NASCAR Has a Republican Kind of Problem. https://www.huffpost.com/entry/nascar-has-a-\\nrepublican-k_b_3992677.\\n[32] Laura Edelson, Tobias Lauinger, and Damon McCoy. 2020. A security analysis of the facebook ad library. In 2020\\nIEEE Symposium on Security and Privacy (SP) . IEEE, 661‚Äì678.\\n[33] Thomas B Edsall. 2006. Democrats‚Äô data mining stirs an intraparty battle. The Washington Post (2006).\\n[34] Erin Egan. 2016. Improving enforcement and promoting diversity: Updates to ethnic affinity marketing. https:\\n//about.fb.com/news/2016/11/updates-to-ethnic-affinity-marketing/\\n[35] Yael Eisenstat. 2019. I worked on political ads at Facebook. They profit by manipulating us. https://www.\\nwashingtonpost.com/outlook/2019/11/04/i-worked-political-ads-facebook-they-profit-by-manipulating-us/\\n[36] Anna Eshoo. 2020. Rep. Eshoo Introduces bill to Ban Microtargeted Political Ads. https://eshoo.house.gov/media/press-\\nreleases/rep-eshoo-introduces-bill-ban-microtargeted-political-ads\\n[37] Anna Eshoo. 2021. Rep. Eshoo Reintroduces Legislation to Ban Microtargeted Political Ads. https://eshoo.house.\\ngov/media/press-releases/rep-eshoo-reintroduces-legislation-ban-microtargeted-political-ads28 Piotr Sapiezynski, Levi Kaplan, Alan Mislove, and Aleksandra Korolova\\n[38] European Parliament. 2023. Why new EU rules for political advertising are important. https://www.europarl.europa.\\neu/topics/en/article/20230202STO71504/why-new-eu-rules-for-political-advertising-are-important\\n[39] Johnathan Freedland. 2017. The new age of Ayn Rand: how she won over Trump and Silicon Valley. https:\\n//www.theguardian.com/books/2017/apr/10/new-age-ayn-rand-conquered-trump-white-house-silicon-valley.\\n[40] Liza Gak, Seyi Olojo, and Niloufar Salehi. 2022. The distressing ads that persist: Uncovering the harms of targeted\\nweight-loss ads among users with histories of disordered eating. Proceedings of the ACM on Human-Computer\\nInteraction 6, CSCW2 (2022), 1‚Äì23.\\n[41] Jascha Galaski and Eva Simon. 2021. Solutions for Regulating Targeted Political Advertising on Online Platforms.\\nhttps://www.liberties.eu/f/mm-oxv\\n[42] Ahiza Garcia. 2019. Bill Gates: The problem with political ads is targeting, not fact-checking. https://www.cnn.com/\\n2019/11/06/tech/bill-gates-political-ad-targeting-fact-checking/index.html\\n[43] Dorota G≈Çowacka and Karolina Iwa≈Ñska. 2021. Algorithms of trauma: new case study shows that Facebook doesn‚Äôt\\ngive users real control over disturbing surveillance ads. https://en.panoptykon.org/algorithms-of-trauma.\\n[44] Rob Goldman. Nov 2017. Statement from Rob Goldman, Vice President, Ad Products. https://s3.documentcloud.org/\\ndocuments/4312937/Facebook-Statement-11-29-17.pdf.\\n[45] Google. 2023. Political advertising on Google. https://adstransparency.google.com/political?region=US&amp;topic=\\npolitical.\\n[46] Joshua Green and Sasha Issenberg. 2016. Inside the Trump bunker, with days to go. https://www.bloomberg.com/\\nnews/articles/2016-10-27/inside-the-trump-bunker-with-12-days-to-go. Bloomberg Businessweek 27 (2016).\\n[47] Alessia Grunberger. 2018. Ted Nugent likens Democrats, media, academics to ‚Äòrabid coyotes‚Äô. https://www.cnn.com/\\n2018/04/07/politics/nugent-democrats-media-advocates-rabid-coyotes/index.html.\\n[48] Hana Habib, Sarah Pearman, Ellie Young, Ishika Saxena, Robert Zhang, and Lorrie FaIth Cranor. 2022. Identifying\\nuser needs for advertising controls on Facebook. Proceedings of the ACM on Human-Computer Interaction 6, CSCW1\\n(2022), 1‚Äì42.\\n[49] Tom Hamburger and Peter Wallsten. 2005. Parties are tracking your habits. Though both democrats and republicans\\ncollect personal information, the GOP‚Äôs Mastery of Data is changing the very nature of campaigning. Los Angeles\\nTimes, July 25 (2005), 1.\\n[50] Katie Harbath and Steve Satterfield. 2018. Hard questions: Why doesn‚Äôt Facebook just ban political ads? https:\\n//about.fb.com/news/2018/05/hard-questions-political-ads/\\n[51] Eitan D Hersh and Brian F Schaffner. 2013. Targeted campaign appeals and the value of ambiguity. The Journal of\\nPolitics 75, 2 (2013), 520‚Äì534.\\n[52] Brian Hiatt. 2016. Kid Rock: ‚ÄòI‚Äôm Digging Donald Trump‚Äô. https://www.rollingstone.com/politics/politics-news/kid-\\nrock-im-digging-donald-trump-241271/.\\n[53] Jeff Horwitz. 2021. Facebook Parent Meta Limits Ad Targeting for Politics and Other Sensitive Issues. https:\\n//www.wsj.com/articles/facebook-parent-meta-bans-targeting-for-political-ads-11636488053\\n[54] Jane Im, Ruiyi Wang, Weikun Lyu, Nick Cook, Hana Habib, Lorrie Faith Cranor, Nikola Banovic, and Florian Schaub.\\n2023. Less is not more: Improving findability and actionability of privacy controls for online behavioral advertising.\\nInProceedings of the 2023 CHI Conference on Human Factors in Computing Systems . 1‚Äì33.\\n[55] Basileal Imana, Aleksandra Korolova, and John Heidemann. 2021. Auditing for discrimination in algorithms delivering\\njob ads. In Proceedings of the Web Conference 2021 . 3767‚Äì3778.\\n[56] Basileal Imana, Aleksandra Korolova, and John Heidemann. 2024. Auditing for Racial Discrimination in the Delivery\\nof Education Ads. In ACM Conference on Fairness, Accountability, and Transparency (FAccT 2024) . 2348‚Äì2361.\\n[57] Sasha Issenberg. 2012. How Obama‚Äôs Team Used Big Data to Rally Voters. https://www.technologyreview.com/\\n2012/12/19/114510/how-obamas-team-used-big-data-to-rally-voters/\\n[58] Peniel E. Joseph. 2022. Don‚Äôt pretend you don‚Äôt know what Joe Rogan is all about. https://www.cnn.com/2022/02/10/\\nopinions/joe-rogan-myth-spotify-joseph/index.html.\\n[59] Levi Kaplan, Nicole Gerzon, Alan Mislove, and Piotr Sapiezynski. 2022. Measurement and analysis of implied identity\\nin ad delivery optimization. In Proceedings of the 22nd ACM Internet Measurement Conference . 195‚Äì209.\\n[60] Jon Keegan. 2021. Facebook got rid of racial ad categories. or did it? ‚Äì the markup. https://themarkup.org/citizen-\\nbrowser/2021/07/09/facebook-got-rid-of-racial-ad-categories-or-did-it\\n[61] Young Mie Kim, Jordan Hsu, David Neiman, Colin Kou, Levi Bankston, Soo Yun Kim, Richard Heinrich, Robyn\\nBaragwanath, and Garvesh Raskutti. 2018. The stealth media? Groups and targets behind divisive issue campaigns\\non Facebook. Political Communication 35, 4 (2018), 515‚Äì541.\\n[62] Jeff King. 2022. Bringing More Transparency to Social Issue, Electoral and Political Ads. https://www.facebook.com/\\nbusiness/news/transparency-social-issue-electoral-political-ads\\n[63] Ava Kofman and Ariana Tobin. 2019. Facebook ads can still discriminate against women and older workers, despite a\\ncivil rights settlement. https://www.propublica.org/article/facebook-ads-can-still-discriminate-against-women-On the Use of Proxies in Political Ad Targeting 29\\nand-older-workers-despite-a-civil-rights-settlement\\n[64] Aleksandra Korolova. 2011. Privacy Violations Using Microtargeted Ads: A Case Study. Journal of Privacy and\\nConfidentiality 3, 1 (2011), 27‚Äì49.\\n[65] Michelle S Lam, Ayush Pandit, Colin H Kalicki, Rachit Gupta, Poonam Sahoo, and Dana√´ Metaxa. 2023. Sociotechnical\\nAudits: Broadening the Algorithm Auditing Lens to Investigate Targeted Advertising. Proceedings of the ACM on\\nHuman-Computer Interaction 7, CSCW2 (2023), 1‚Äì37.\\n[66] Issie Lapowsky. 2018. What Facebook Isn‚Äôt Saying About Trump and Clinton‚Äôs Campaign Ads. Wired, Conde Nast 28\\n(2018).\\n[67] Victor Le Pochat, Laura Edelson, Tom Van Goethem, Wouter Joosen, Damon McCoy, and Tobias Lauinger. 2022.\\nAn Audit of Facebook‚Äôs Political Ad Policy Enforcement. In 31st USENIX Security Symposium (USENIX Security 22) .\\n607‚Äì624.\\n[68] Rob Leathern. 2018. Shining a light on ads with political content. https://about.fb.com/news/2018/05/ads-with-\\npolitical-content/\\n[69] Rob Leathern. 2020. Expanded transparency and more controls for political ads. https://about.fb.com/news/2020/01/\\npolitical-ads/\\n[70] Colin Lecher. 2021. How big pharma finds sick users on Facebook ‚Äì the markup. https://themarkup.org/citizen-\\nbrowser/2021/05/06/how-big-pharma-finds-sick-users-on-facebook\\n[71] Hao-Ping Hank Lee, Jacob Logas, Stephanie Yang, Zhouyu Li, Nata Barbosa, Yang Wang, and Sauvik Das. 2023. When\\nand Why Do People Want Ad Targeting Explanations? Evidence from a Four-Week, Mixed-Methods Field Study. In\\n2023 IEEE Symposium on Security and Privacy (SP) . IEEE, 2903‚Äì2920.\\n[72] Gary Levin. 2014. Sarah Palin defends ‚ÄòDuck Dynasty‚Äô star Phil Robertson. https://www.usatoday.com/story/life/tv/\\n2014/01/10/sarah-palin-phil-robertson-duck-dynasty/4410665/.\\n[73] Natasha Lomas. 2023. Political ads face tougher targeting restrictions in EU if MEPs get their way. TechCrunch (2023).\\nhttps://techcrunch.com/2023/02/02/eu-political-ads-transparency-rules-parliament-mandate/.\\n[74] Sam Machkovech. 2017. Report: Facebook helped advertisers target teens who feel ‚Äúworthless‚Äù [updated]. https://\\narstechnica.com/information-technology/2017/05/facebook-helped-advertisers-target-teens-who-feel-worthless/\\n[75] Alexis C. Madrigal. 2018. Will facebook‚Äôs new ad-transparency tools protect democracy? https://www.theatlantic.\\ncom/technology/archive/2018/05/facebook-ad-transparency-democracy/559853/\\n[76] Lachlan Markay. 2022. Targeting two Americas. https://www.axios.com/2022/10/12/digital-ad-targeting-facebook-\\nmeta-campaigns\\n[77] Sandra C Matz, Michal Kosinski, Gideon Nave, and David J Stillwell. 2017. Psychological targeting as an effective\\napproach to digital mass persuasion. Proceedings of the National Academy of Sciences 114, 48 (2017), 12714‚Äì12719.\\n[78] Justin McCarthy. 2020. In U.S., Most Oppose Micro-Targeting in Online Political Ads. https://news.gallup.com/\\nopinion/gallup/286490/oppose-micro-targeting-online-political-ads.aspx.\\n[79] Jeremy B. Merrill. 2020. Does facebook still sell discriminatory ads? https://themarkup.org/the-breakdown/2020/08/\\n25/does-facebook-still-sell-discriminatory-ads\\n[80] Meta. 2018. Reviewing Targeting to Ensure Advertising is Safe and Civil. https://www.facebook.com/business/news/\\nreviewing-targeting-to-ensure-advertising-is-safe-and-civil.\\n[81] Meta. 2020. Simplifying Targeting Categories. https://www.facebook.com/business/news/update-to-facebook-ads-\\ntargeting-categories.\\n[82] Meta. 2021. Removing Certain Ad Targeting Options and Expanding Our Ad Controls. https://www.facebook.com/\\nbusiness/news/removing-certain-ad-targeting-options-and-expanding-our-ad-controls.\\n[83] Meta. 2021. Removing Certain Ad Targeting Options and Expanding Our Ad Controls. https://www.facebook.com/\\nbusiness/news/removing-certain-ad-targeting-options-and-expanding-our-ad-controls.\\n[84] Meta. 2021. Research Platform Addendum. https://developers.facebook.com/terms/facebook_research_platform_\\nterms_addendum/.\\n[85] Meta. 2022. Preparing for Upcoming Removal of Certain Ad Targeting Options. https://www.facebook.com/\\ngovernment-nonprofits/blog/preparing-for-upcoming-removal-of-certain-ad-targeting-options.\\n[86] Meta. 2023. About custom audiences. https://www.facebook.com/business/help/744354708981227?id=\\n2469097953376494&content_id=mQ2eMjCsjU2sFOO&ref=sem_smb&utm_term=dsa-1731453239943.\\n[87] Meta. 2023. About customer list Custom Audiences. https://www.facebook.com/business/help/341425252616329?id=\\n2469097953376494.\\n[88] Meta. 2023. About lookalike audiences. https://www.facebook.com/business/help/164749007013531?id=\\n401668390442328&content_id=mQ2eMjCsjU2sFOO&ref=sem_smb&utm_term=dsa-1731453265583.\\n[89] Meta. 2023. Audience ad targeting. https://www.facebook.com/business/ads/ad-targeting.\\n[90] Meta. 2023. Facebook Ad Library. https://www.facebook.com/ads/library/.\\n[91] Meta. 2023. Meta for Developers Onboarding. https://developers.facebook.com/docs/fort/get-access.30 Piotr Sapiezynski, Levi Kaplan, Alan Mislove, and Aleksandra Korolova\\n[92] Meta. 2023. Targeting Transparency Information for Ads About Social Issues, Elections or Politics. https://www.\\nfacebook.com/business/help/736091520909332.\\n[93] Meta. 2023. Use detailed targeting. https://www.facebook.com/business/help/440167386536513?id=176276233019487.\\n[94] Meta. 2024. Updates to detailed targeting. https://www.facebook.com/business/help/458835214668072.\\n[95] Meta. Aug 2018. Keeping Advertising Safe and Civil. https://www.facebook.com/business/news/keeping-advertising-\\nsafe-and-civil.\\n[96] Meta. Feb 2017. Improving Enforcement and Promoting Diversity: Updates to Ads Policies and Tools. https://about.\\nfb.com/news/2017/02/improving-enforcement-and-promoting-diversity-updates-to-ads-policies-and-tools/.\\n[97] Jake Miller. 2016. 2016 by the numbers: Hillary Clinton dwarfs Donald Trump in TV ad spending. https://www.\\ncbsnews.com/news/2016-election-numbers-hillary-clinton-donald-trump-tv-advertisement-spending/.\\n[98] Robert Mueller. 2019. Report on the investigation into Russian interference in the 2016 presidential election . Vol. 1. US\\nDepartment of Justice Washington, DC.\\n[99] Gregg R Murray and Anthony Scime. 2010. Microtargeting and electorate segmentation: data mining the American\\nNational Election Studies. Journal of Political Marketing 9, 3 (2010), 143‚Äì166.\\n[100] Access Now et al .2023. Civil Society Open Letter on the ongoing negotiations regarding the Regulation of Political\\nAdvertising: EU Lawmakers must uphold human rights to privacy and free expression. https://cdt.org/wp-content/\\nuploads/2023/06/2023-12-06-Joint-Civil-Society-Letter-on-draft-EU-Political-Advertising-Regulation.pdf.\\n[101] Library of Congress. 2018. S.1989 - 115th Congress (2017-2018): Honest Ads Act. https://www.congress.gov/bill/115th-\\ncongress/senate-bill/1989.\\n[102] Council of the European Union. 2022. https://www.consilium.europa.eu/en/press/press-releases/2022/12/13/\\ntransparency-and-targeting-of-political-advertising-council-agrees-its-negotiating-mandate/.\\n[103] Council of the European Union. 2024. REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL\\non the transparency and targeting of political advertising. https://data.consilium.europa.eu/doc/document/PE-90-\\n2023-INIT/en/pdf.\\n[104] Filipe Ribeiro, Lucas Henrique, Fabricio Benevenuto, Abhijnan Chakraborty, Juhi Kulshrestha, Mahmoudreza Babaei,\\nand Krishna Gummadi. 2018. Media bias monitor: Quantifying biases of social media news outlets at large-scale. In\\nProceedings of the International AAAI Conference on Web and Social Media , Vol. 12.\\n[105] Filipe N Ribeiro, Koustuv Saha, Mahmoudreza Babaei, Lucas Henrique, Johnnatan Messias, Fabricio Benevenuto,\\nOana Goga, Krishna P Gummadi, and Elissa M Redmiles. 2019. On microtargeting socially divisive ads: A case\\nstudy of russia-linked ad campaigns on facebook. In Proceedings of the conference on fairness, accountability, and\\ntransparency . 140‚Äì149.\\n[106] Aaron Rieke and Miranda Bogen. 2018. Leveling the platform. https://www.upturn.org/work/leveling-the-platform/\\n[107] Ronald E. Robertson, Shan Jiang, Kenneth Joseph, Lisa Friedland, David Lazer, and Christo Wilson. 2018. Auditing\\nPartisan Audience Bias within Google Search. 2, CSCW, Article 148 (nov 2018), 22 pages. https://doi.org/10.1145/\\n3274417\\n[108] Ryan Rogers, Subbu Subramaniam, Sean Peng, David Durfee, Seunghyun Lee, Santosh Kumar Kancha, Shraddha\\nSahay, and Parvez Ahammad. 2020. LinkedIn‚Äôs Audience Engagements API: A privacy preserving data analytics\\nsystem at scale. arXiv preprint arXiv:2002.05839 (2020).\\n[109] Aafaq Sabir, Evan Lafontaine, and Anupam Das. 2022. Analyzing the impact and accuracy of Facebook activity on\\nfacebook‚Äôs ad-interest inference process. Proceedings of the ACM on Human-Computer Interaction 6, CSCW1 (2022),\\n1‚Äì34.\\n[110] Piotr Sapiezynski, Avijit Ghosh, Levi Kaplan, Aaron Rieke, and Alan Mislove. 2022. Algorithms that ‚ÄúDon‚Äôt See Color‚Äù\\nMeasuring Biases in Lookalike and Special Ad Audiences. In Proceedings of the 2022 AAAI/ACM Conference on AI,\\nEthics, and Society . 609‚Äì616.\\n[111] Piotr Sapiezynski and Leon Yin. 2023. Browser Automation. In Inspect Element: A practitioner‚Äôs guide to auditing\\nalgorithms and hypothesis-driven investigations , Leon Yin, Piotr Sapiezynski, and Inioluwa Deborah Raji (Eds.).\\nhttps://inspectelement.org.\\n[112] Elliot Schrage. 2017. Hard Questions: Russian Ads Delivered to Congress. hhttps://about.fb.com/news/2017/10/hard-\\nquestions-russian-ads-delivered-to-congress/\\n[113] Facebook Data Science. 2014. Politics and Culture on Facebook in the 2014 Midterm Elections. https://www.facebook.\\ncom/notes/10158928045073415/.\\n[114] Till Speicher, Muhammad Ali, Giridhari Venkatadri, Filipe Nunes Ribeiro, George Arvanitakis, Fabr√≠cio Benevenuto,\\nKrishna P Gummadi, Patrick Loiseau, and Alan Mislove. 2018. Potential for discrimination in online targeted\\nadvertising. In Conference on fairness, accountability and transparency . PMLR, 5‚Äì19.\\n[115] Ben M Tappin, Chloe Wittenberg, Luke B Hewitt, Adam J Berinsky, and David G Rand. 2023. Quantifying the\\npotential persuasive returns to political microtargeting. Proceedings of the National Academy of Sciences 120, 25 (2023),\\ne2216261120.On the Use of Proxies in Political Ad Targeting 31\\n[116] The New York Times. 2019. Read the Letter Facebook Employees Sent to Mark Zuckerberg About Political Ads.\\nhttps://www.nytimes.com/2019/10/28/technology/facebook-mark-zuckerberg-letter.html\\n[117] Casey Tolan. 2022. How political candidates are targeting you on social media based on your music tastes, shopping\\nhabits and favorite TV shows.\\n[118] Zeynep Tufekci. 2014. Engineering the public: Big data, surveillance and computational politics. First Monday (2014).\\n[119] American Civil Liberties Union. 2019. Summary of Settlements Between Civil Rights Advocates and Facebook.\\nhttps://www.aclu.org/other/summary-settlements-between-civil-rights-advocates-and-facebook.\\n[120] Giridhari Venkatadri, Elena Lucherini, Piotr Sapiezynski, and Alan Mislove. 2019. Investigating sources of PII used in\\nFacebook‚Äôs targeted advertising. Proc. Priv. Enhancing Technol. 2019, 1 (2019), 227‚Äì244.\\n[121] Giridhari Venkatadri, Piotr Sapiezynski, Elissa M Redmiles, Alan Mislove, Oana Goga, Michelle Mazurek, and\\nKrishna P Gummadi. 2019. Auditing offline data brokers via facebook‚Äôs advertising platform. In The World Wide Web\\nConference . 1920‚Äì1930.\\n[122] Angie Waller and Colin Lecher. 2022. Facebook promised to remove ‚Äúsensitive‚Äù ads. Here‚Äôs what it left behind.\\nhttps://www.niemanlab.org/2022/05/facebook-promised-to-remove-sensitive-ads-heres-what-it-left-behind/\\n[123] Human Rights Watch. 2022. Trapped in a Web The Exploitation of Personal Data in Hungary‚Äôs 2022 Elections.\\nhttps://www.hrw.org/sites/default/files/media_2022/12/hungary1122web.pdf\\n[124] Ellen L. Weintraub. 2019. Don‚Äôt abolish political ads on social media. Stop microtargeting. https://www.\\nwashingtonpost.com/opinions/2019/11/01/dont-abolish-political-ads-social-media-stop-microtargeting/.\\n[125] WhoTargetsMe. 2022. Swedish election - snap analysis. https://whotargets.me/en/swedish-election-snap-analysis/.\\n[126] WhoTargetsMe. 2023. How to download and install the Who Targets Me browser extension. https://whotargets.me/\\nen/install/installing-the-browser-extension/.\\n[127] Yuxi Wu, Sydney Bice, W Keith Edwards, and Sauvik Das. 2023. The Slow Violence of Surveillance Capitalism: How\\nOnline Behavioral Advertising Harms People. In Proceedings of the 2023 ACM Conference on Fairness, Accountability,\\nand Transparency . 1826‚Äì1837.\\n[128] Wu Youyou, Michal Kosinski, and David Stillwell. 2015. Computer-based personality judgments are more accurate\\nthan those made by humans. Proceedings of the National Academy of Sciences 112, 4 (2015), 1036‚Äì1040.\\n[129] Frederik Zuiderveen Borgesius, Judith M√∂ller, Sanne Kruikemeier, Ronan √ì Fathaigh, Kristina Irion, Tom Dobber,\\nBalazs Bodo, and Claes H de Vreese. 2018. Online political microtargeting: Promises and threats for democracy.\\nUtrecht Law Review 14, 1 (2018), 82‚Äì96.\\nReceived July 2023; revised April 2024; accepted July 2024\",\n",
       " 'IPL: Leveraging Multimodal Large Language Models for Intelligent\\nProduct Listing\\nKang Chen*,1,2,‚Ä†Qingheng Zhang*,1Chengbao Lian*,1Yixin Ji1Xuwei Liu1\\nShuguang Han1,‚Ä°Guoqiang Wu1Fei Huang1Jufeng Chen1\\n1Alibaba Group2Fudan University\\nkchen24@m.fudan.edu.cn, {qingheng.zqh, lianchengbao.lcb, jiyixin.jyx,\\nxuweiliu.lxw, shuguang.sh, kingwu.wgq, jufeng.cjf}@alibaba-inc.com\\nAbstract\\nUnlike professional Business-to-Consumer\\n(B2C) e-commerce platforms (e.g., Amazon),\\nConsumer-to-Consumer (C2C) platforms (e.g.,\\nFacebook marketplace) are mainly targeting\\nindividual sellers who usually lack sufficient\\nexperience in e-commerce. Individual sellers\\noften struggle to compose proper descriptions\\nfor selling products. With the recent advance-\\nment of Multimodal Large Language Models\\n(MLLMs), we attempt to integrate such state-\\nof-the-art generative AI technologies into the\\nproduct listing process. To this end, we de-\\nvelop IPL, an Intelligent Product Listing tool\\ntailored to generate descriptions using vari-\\nous product attributes such as category, brand,\\ncolor, condition, etc. IPL enables users to com-\\npose product descriptions by merely upload-\\ning photos of the selling product. More im-\\nportantly, it can imitate the content style of\\nour C2C platform Xianyu1. This is achieved\\nby employing domain-specific instruction tun-\\ning on MLLMs and adopting the multi-modal\\nRetrieval-Augmented Generation (RAG) pro-\\ncess. A comprehensive empirical evaluation\\ndemonstrates that the underlying model of IPL\\nsignificantly outperforms the base model in\\ndomain-specific tasks while producing less hal-\\nlucination. IPL has been successfully deployed\\nin our production system, where 72% of users\\nhave their published product listings based on\\nthe generated content, and those product list-\\nings are shown to have a quality score 5.6%\\nhigher than those without AI assistance.\\n1 Introduction\\nWith the rise of the circular economy, second-\\nhand e-commerce has played a vital role in our\\ndaily lives. Unlike Business-to-Consumer (B2C)\\n*These authors contributed equally to this work.\\n‚Ä†Work done during an internship at Alibaba Group.\\n‚Ä°Corresponding author: Shuguang Han (email:\\nshuguang.sh@alibaba-inc.com)\\n1Xianyu is the largest C2C e-commerce platform in\\nChina.\\nFigure 1: Intelligent Product Listing on C2C Platforms\\ne-commerce (e.g., Amazon, Walmart), second-\\nhand e-commerce is often operating in the form of\\nConsumer-to-Consumer (C2C) transactions. Dif-\\nferent from professional sellers on B2C platforms,\\nindividual sellers in second-hand marketplaces are\\nusually inexperienced. They face unique chal-\\nlenges when listing their products ‚Äî navigating\\nthrough the complicated listing procedure, and cre-\\nating high-quality product descriptions. These is-\\nsues not only affect the success rate of product\\nlistings but also impact the overall quality and dis-\\ncoverability of the listed products.\\nTo address the above issues, it is imperative to\\nsimplify the listing process for individual users\\nby leveraging automation to generate high-quality\\nproduct descriptions. A typical product listing pro-\\ncess involves users manually filling in basic prod-\\nuct attributes, uploading product photos, and com-\\nposing content descriptions. Among these steps,\\npreparing product photos is relatively straightfor-\\nward. If we can automatically generate product de-\\nscriptions based on the uploaded photos, it would\\nsignificantly reduce the listing effort and enhancearXiv:2410.16977v1  [cs.CL]  22 Oct 2024user experience, as illustrated by Figure 1.\\nFortunately, product photos contain a wealth of\\ninformation, enabling us to infer basic attribute in-\\nformation such as category, brand, and model from\\nthe imagery in most cases. Moreover, recent ad-\\nvancements in Multimodal Large Language Models\\n(MLLMs) (Bai et al., 2023b; Achiam et al., 2023)\\nhave significantly improved both visual understand-\\ning and natural language generation capabilities,\\nmaking it feasible to generate product descriptions\\nbased on product photos in an automatic manner.\\nSeveral large e-commerce platforms, including\\neBay (Herold et al., 2024) and Amazon (Jiang et al.,\\n2024), have begun to explore this direction by intro-\\nducing product listing assistants. However, these\\ntools are still in their infant stages. They still re-\\nquire substantial user input, and the generated con-\\ntent is commonly in the professional marketing\\nstyles which lowers the information authenticity\\nfor a C2C platform. In the context of second-hand\\ne-commerce, we encounter more challenges.\\nLack of Domain Knowledge. To generate high-\\nquality product descriptions, models must possess\\nstrong capabilities for domain understanding (Es-\\ncursell et al., 2021; Poerner et al., 2019). C2C e-\\ncommerce differs from traditional B2C platforms,\\nits product listings often exhibit more unique and\\nvaried characteristics. Unlike professional mar-\\nketing descriptions that emphasize persuasive lan-\\nguage, product descriptions in C2C platforms typi-\\ncally exhibit a more colloquial style, focusing on\\ninformation authenticity. This helps foster trust be-\\ntween buyers and sellers and potentially facilitates\\ntransactions. However, existing MLLMs often fall\\nshort in these areas.\\nHallucination Problem. Ideally, users only\\nneed to upload a photo, and the corresponding con-\\ntent description including core product attributes is\\nautomatically generated. However, achieving this\\ngoal imposes a significant challenge on the current\\nMLLMs (Liang et al., 2022; Ji et al., 2023). In prac-\\ntice, MLLMs sometimes produce product attributes\\ngoing beyond the image itself. This is known as\\nthe hallucination problem in Large Language Mod-\\nels(LLMs). As the core part of the product listing\\nexperience, we need to find a proper solution.\\nChallenges for Production Deployment. De-\\nploying generative LLMs on production systems,\\nparticularly for applications with a large-scale user\\nbase, imposes high requirements on system latency,\\ncost consumption (Kwon et al., 2023), and content\\nsafety (Perez and Ribeiro, 2022). Meeting thesedemands necessitates a comprehensive system en-\\ngineering effort.\\nTo address the above issues, we develop an\\nIntelligent Product Listing ( IPL) system, aiming to\\nimprove the efficiency and effectiveness for prod-\\nuct listings on our production system.\\nFirstly, we present a notable case study of in-\\njecting domain knowledge into an MLLM through\\nfurther instruction tuning of an open-source model.\\nOur domain-specific model significantly enhances\\nthe base model‚Äôs understanding of domain knowl-\\nedge and enables it to generate product descriptions\\nin the unique style characteristic of C2C platforms.\\nSecondly, we introduce an innovative multi-\\nmodal Retrieval-Augmented Generation (RAG) ap-\\nproach for visual-based content generation, lever-\\naging identical product retrieval, to enhance de-\\nscription quality and mitigate hallucination risks in\\npractical applications.\\nFinally, we have successfully deployed the sys-\\ntem in our production system, delivering intelligent\\ncomposition service to real-world individual users.\\nThis system demonstrates high user acceptance and\\neffectively enhances the efficiency and quality of\\nproduct listings.\\nOur extensive empirical studies demonstrate that\\nIPL has the potential to transform the landscape\\nof product listings, offering a robust, scalable solu-\\ntion to challenges faced by individual sellers and\\nplatforms alike.\\n2 Approach\\nThe overall architecture of our intelligent prod-\\nuct listing system can be illustrated in Figure 2,\\nwhich comprises an online multi-modal Retrieval-\\nAugmented Generation (RAG) process for identify-\\ning similar products, and an offline-trained domain-\\nspecific MLLM for product description generation.\\nIn our product listing system, user-uploaded pho-\\ntos will go through several sub-modules: category\\nprediction, retrieval of similar products, and extrac-\\ntion of key attributes (e.g., brand, model, etc.) from\\nthe descriptions of these similar products. Subse-\\nquently, the product photo, category, and extracted\\nattributes are fed into the domain-specific MLLM\\nas contextual information to generate the product\\ndescription. With this automatically generated de-\\nscription, users only need to make minimal adjust-\\nments to complete the product listing.Figure 2: Overview of the Intelligent Product Listing (IPL) system architecture.\\n2.1 Domain-Specific Model Training\\nThe crucial stages in training domain-specific mod-\\nels include the construction of training data and the\\nprocess of model instruction tuning.\\n2.1.1 Domain Instruction Tuning Data\\nThe training data for the model encompasses prod-\\nuct description generation, domain content under-\\nstanding, and general instruction tasks. The general\\ninstruction data are derived from both automati-\\ncally generated and open-source data. An overview\\nof the training data is provided in Table 1.\\nData Type Size Source Modality\\nProduct Description Generation 267k In-house Visual-Language\\nDomain Content Understanding 200k In-house Visual-Language, Text Only\\nAuto Generated Datasets 378k In-house Visual-Language\\nGeneral QA Datasets 424k Open source Visual-Language, Text Only\\nALL 1.27M Mixture Visual-Language, Text Only\\nTable 1: Instruction tuning training data\\nThe description generation dataset, which consti-\\ntutes the primary focus of this work, involves gener-\\nating descriptions based on user-provided product\\nphotos. By cleaning data from actual user-posted\\nproduct listings, we obtained pairs of product pho-\\ntos and descriptions. Subsequently, we converted\\nthe data into various types of instruction formats,\\nincluding generating product descriptions directly\\nfrom photos and generating descriptions based on\\na combination of product photos, key attribute tem-\\nplates, and reference information, as illustrated in\\nTable 2. Detailed data construction procedures are\\nprovided in Appendix A.1.\\nThe content understanding tasks primarily in-\\nclude fundamental tasks in e-commerce scenarios,\\nespecially those on C2C platforms, aimed at en-\\nhancing the model‚Äôs domain knowledge. These\\ntasks include product image category prediction,\\nproduct attribute extraction, and text similarity\\nmatching, among others. This data is derived frommanually annotated data accumulated over time in\\nbusiness scenarios. Further details on the data can\\nbe found in Appendix A.2.\\nFinally, the general instruction dataset are used\\nto enable the model to retain general capabilities\\nand enhance its generalization ability. We employ\\nlarge language models to generate general instruc-\\ntions and answers in the native language based\\non product photos, while also incorporating high-\\nquality open-source academic datasets as supple-\\nmentary resources. For further details, please refer\\nto Appendix A.3.\\n2.1.2 Model Training\\nWe chose Alibaba‚Äôs Qwen-VL(Bai et al., 2023b)\\nmodel as the base model, primarily due to its\\nstrong performance in the native language and its\\nrobust open-source ecosystem. We employed full-\\nparameter fine-tuning for model training, freezing\\nthe visual encoder module while updating the VL-\\nAdapter and LLM components only (7B parame-\\nters).\\nThe training objectives focused on classic next\\ntoken generation for language model optimization,\\nspecifically excluding loss calculation for prompt\\nprefixes and focusing on the special markers and\\nthe model output tokens. The objective can be\\nformally defined as:\\nL=‚àíTX\\nt=1logP(yt|y<t, X) (1)\\nwhere Xdenotes the model input instructions,\\nyrepresents the generated tokens, trefers to the\\nposition within the generated sequence, and Tis\\nthe length of the final generated sequence. Further\\ntraining details can be found in Appendix B.\\n2.2 Online Retriever-Augmented Generation\\nIn the online phase, the fine-tuned domain model is\\ncapable of generating descriptions for product pho-tos. To further mitigate hallucinations, our instruc-\\ntions are not to directly generate descriptions from\\nproduct photos but to refer to product categories,\\ncore attribute templates, and retrieved information,\\nas detailed in Table 2.\\nGeneration with Reference Information\\nPrompt: You are an experienced seller on a\\nsecond-hand trading platform and need to post\\nacell phone category with the product image\\nas shown in the picture, and the copy template\\nisBrand + Model + Storage Capacity + Color\\n+ Version + Screen Condition . In which, the\\nbrand is Huawei, the model is Mate10pro, the\\nstorage capacity is 6+64GB , please write a\\nparagraph description for this product.\\nResponse:\\nPersonal used Huawei Mate10pro 6+64GB, Blue,\\ncondition as shown in the pictures, Mainland\\nChina version, screen in perfect condition\\nwithout aging or scratches, all original,\\nfor those interested, please contact me\\nprivately.\\nTable 2: Instruction for product description generation\\nwith Retriever-Augmented Generation.\\nTherefore, in online scenario, product descrip-\\ntion generation is a Retrieval-Augmented Gener-\\nation (RAG) process. We conduct category pre-\\ndiction on the input product photos and simultane-\\nously retrieve identical products through vector re-\\ntrieval. From the retrieved products, we extract key\\nattribute values to serve as reference information\\nfor generating descriptions. The extraction of key\\nattribute values is accomplished using a domain-\\nspecific large model we trained, with the prompt\\nshown in Table 3. Key attribute sets for each cate-\\ngory are derived from offline mining and manual\\nsummarization, and can be retrieved through prod-\\nuct category queries. By incorporating attributes\\ntemplate into the instructions, we can further con-\\ntrol the attributes and their sequence that the model\\nmust mention in the generated product descriptions,\\nensuring the richness of the information in the out-\\nput descriptions.\\nThe category prediction model utilizes the AL-\\nBEF network architecture(Li et al., 2021; Zhang\\net al., 2018), a classic vision-language multimodal\\nmodel. The model has been pre-trained on domain-\\nspecific data and fine-tuned with millions of man-\\nually annotated datasets, achieving an accuracy of\\nover 80% across tens of thousands of categories.\\nThe implementation of the visual search draws\\nupon the work conducted by (Zhang et al., 2018).\\nWe select the most similar result from the retrieval\\noutcomes as the identical product and impose a\\nsimilarity score threshold to further enhance theAttribute Extraction Example\\nPrompt: Extract the Brand, Model, Storage\\nCapacity, Color, Version, Screen Condition\\nfor the following smartphone product.\\nOutput the result in JSON format. Product\\ndescription: Huawei mate10Pro 6+64G\\ncompletely original unrefurbished smartphone\\nMainland China version light scratches.\\nResponse:\\n{\\n\"Brand\": \"Huawei\",\\n\"Model\": \"mate10Pro\",\\n\"Storage Capacity\": \"6+64G\",\\n\"Version\": \"Mainland China\"\\n}\\nTable 3: Attribute extraction instruction examples.\\naccuracy. In offline evaluations, the accuracy of\\nimage retrieval for identical products is over 60%,\\nand for similar products, it is over 90%. For more\\ndetails on the evaluation of visual retrieval, please\\nrefer to Appendix C.1.\\n3 Deployment\\nKey considerations for LLM deployment included\\nminimizing online latency, ensuring user experi-\\nence, and addressing safety risks associated with\\ncontent generation. We deployed the system online,\\nwith the LLM model hosted on NVIDIA ¬ÆTesla ¬Æ\\nV100 machines. Through various acceleration tech-\\nniques, such as model quantization, ViT operation\\noptimization, key-value caching, kernel operation\\nfusion, and parallel computation(Aminabadi et al.,\\n2022; Dao et al., 2022; Dao, 2023), the overall\\npipeline‚Äôs average response time (RT) was reduced\\nfrom 5 seconds to below 3 seconds. The adoption\\nof streaming output ensured user experience by\\nreducing wait times.We perform preemptive risk\\nassessment on user-uploaded product photos and se-\\ncurity checks on generated descriptions to prevent\\nnon-compliant content, thereby effectively avoid-\\ning public opinion risks. For more detailed error\\ndetection and exception handling, please refer to\\nAppendix D.\\n4 Experiment\\n4.1 Data\\nOur experimental data comprises both domain-\\nspecific and general datasets. All data were sourced\\nfrom real e-commerce scenarios and the target\\nlabels were either manually annotated or con-\\nfirmed by actual platform users, then converted\\ninto instruction format. We constructed validationdatasets encompassing tasks such as sentiment anal-\\nysis, information extraction, content topic selection,\\ntagging/classification, and attribute-based visual\\nquestion answering within the e-commerce domain\\n(For more details, refer to Appendix E). Addition-\\nally, we included datasets specifically designed to\\nevaluate generative style and hallucination.\\n4.2 Model\\nDomain-Specific Models: To assess the effective-\\nness of domain knowledge injection, we trained\\nseveral models with varying amounts of training\\ndata. The datasets were randomly shuffled and\\ntruncated. The comparison models include: Qwen-\\nVL (baseline, without domain training), 10% Data\\n(trained with 10% of the data), 20% Data, 50%\\nData, and 100% Data.\\nOnline RAG System: In addressing hallucination\\nalleviation, we conducted experiments on various\\ncomponents of our online RAG system. This in-\\ncluded evaluating the use of product category in-\\nformation, reference information from identical or\\nsimilar products.\\n4.3 Metrics\\nOur evaluation encompasses comprehensive met-\\nrics to assess different aspects of model perfor-\\nmance:\\nN-gram-Based Metrics : We employed BLEU (Pa-\\npineni et al., 2002), ROUGE and ROUGE-L (Lin,\\n2004) to evaluate the alignment of generated text\\nwith ground truth product descriptions.\\nSemantic Similarity Metrics : BERT embed-\\ndings measured semantic similarity (SIM) between\\nmodel outputs and ground truth using BERT-Score.\\nTask-Specific Accuracy Metrics : These metrics\\nwere used for domain-specific knowledge ques-\\ntions, assessing model accuracy in understanding\\nand responding to task-specific prompts.\\nHuman Assessment: Evaluation was conducted\\nby experts in the C2C domain, assessing whether\\nthe generated descriptions adhere to domain-\\nspecific style and identify key attributes(Chen et al.,\\n2024) accurately. We perform a quantitative analy-\\nsis of the results.\\n5 Results\\nIn the following subsections, we discuss the five\\nkey research questions regarding our domain model\\nand the online RAG system:\\n‚Ä¢Q1: Does the domain-specific model, afterinstruction tuning, exhibit a stronger under-\\nstanding of domain knowledge?\\n‚Ä¢Q2: Does the domain-specific model generate\\nproduct descriptions with a more distinct C2C\\ndomain style?\\n‚Ä¢Q3: Can the model maintain its general capa-\\nbilities after being trained on domain-specific\\ndata?\\n‚Ä¢Q4: Does the online RAG mitigate hallucina-\\ntions in product description generation?\\n‚Ä¢Q5: How does the IPL system perform in\\nreal-world online scenarios?\\nAmong them, Q1-Q3 investigate the effects of do-\\nmain knowledge injection, Q4 explores the role of\\nonline RAG, and Q5 addresses online performance.\\n5.1 RQ1: Enhanced Domain-Specific\\nKnowledge\\nTo evaluate the model‚Äôs understanding of domain-\\nspecific knowledge, we compared its performance\\non C2C e-commerce tasks involving both language-\\nonly and visual-language hybrid modalities. As\\nshown in Table 4, the domain-specific model sig-\\nnificantly outperforms baseline across various met-\\nrics. Notably, the model shows substantial improve-\\nments in tasks such as e-commerce topic selection\\nand category recognition, while the gains in senti-\\nment analysis are relatively smaller. This can be\\nattributed to the close alignment of sentiment clas-\\nsification with general tasks, as well as its superior\\nbaseline performance.\\nBy truncating the training data to 10%, 20%,\\n50%, and 100% of the original dataset, we obtained\\ndifferent models. The model trained with the full\\ndataset achieved the highest average accuracy, fol-\\nlowed by the model trained with 50% of the data.\\nIn the Topic Selection and Category Recognition\\ntasks, The accuracy increased significantly with\\nthe amount of training data. For the Content Tag-\\nging and Vision-Based Product Attribute Extrac-\\ntion tasks, accuracy improved significantly after\\nadding 20% of the data, but showed minor fluctua-\\ntions with further increases in training data beyond\\n20%.\\n5.2 RQ2: Enhanced Domain-Specific Style\\nGeneration Ability\\nWe also evaluated whether the model‚Äôs generated\\nlistings exhibit domain-specific stylistic elements.\\nGiven that style preferences are subjective, human\\nevaluation is the most reliable method. An experi-\\nenced e-commerce annotator was tasked with com-Model Domain Task (Visual-Language) Language Only Overall\\nTS CT CR V AE PDG SA TAE Average\\nQwen-VL 0.442 0.758 0.791 0.720 0.610 0.895 0.416 0.662\\n+10% Data 0.532 0.769 0.768 0.781 0.629 0.871 0.313 0.666\\n+20% Data 0.596 0.826 0.733 0.811 0.628 0.885 0.670 0.735\\n+50% Data 0.610 0.824 0.799 0.809 0.635 0.868 0.649 0.742\\n+100% Data 0.718 0.822 0.847 0.790 0.631 0.878 0.715 0.771\\nTable 4: We compare the performance of domain-specific models trained with different proportions of the dataset\\n(10%, 20%, 50%, and 100%) on various domain-specific tasks. These tasks include Topic Selection (TS), Content\\nTagging (CT), Category Recognition (CR), Vision-Based Product Attribute Extraction (V AE), Product Description\\nGeneration (PDG), Sentiment Analysis (SA) and Text-Based Product Attribute Extraction (TAE).\\nparing the linguistic style of listings generated by\\ndifferent models for the same product and casting\\nvotes. The results, presented in Table 5, indicate\\na significant preference for our model‚Äôs outputs.\\nIn contrast, Qwen-VL‚Äôs listings were often per-\\nceived as unnatural, verbose, and overly marketing-\\noriented, which is undesirable in C2C personal\\nseller scenarios. We also experimented with var-\\nious prompts for Qwen-VL to mitigate prompt-\\ninduced biases.\\n5.3 RQ3: Retains General Capabilities\\nWe assessed the model‚Äôs retention of general ca-\\npabilities using well-established benchmarks such\\nas MMBench (Liu et al., 2023) , MME(Fu et al.,\\n2023), and SeedBench(Li et al., 2024), drawing ref-\\nerence from the work of LLaV A 1.5 and Qwen-VL.\\nOur model outperforms LLaV A 1.5 and Qwen-VL\\non the MMBench task, and achieves performance\\nclosely comparable to LLaV A 1.5 on the MME\\ntask. However, it demonstrates relatively weaker\\nperformance on the SeedBench task.\\nOn one hand, SeedBench focuses on detailed im-\\nage analysis tasks, including scene understanding,\\ninstance identity, instance location, instance count-\\ning. In contrast, MMBench emphasizes overall\\nimage analysis, encompassing tasks such as image\\ntopic and attribute recognition. Our training sam-\\nples are based on commonly used general-domain\\ndata and additionally incorporate e-commerce prod-\\nuct understanding, encompassing tasks such as\\ncategory recognition and product attribute extrac-\\ntion. From the perspective of the high-quality train-\\ning samples, this demonstrates a greater improve-\\nment for MMBench compared to the SeedBench\\ntasks.On the other hand, the difficulty of the tasks\\nreveals that SeedBench is indeed more challenging,\\nas detailed image analysis requires the model to\\npossess strong pixel resolution, multi-object recog-\\nnition capabilities, and spatial recognition skills.Our model still has space for improvement on\\nthese tasks. The generalization obtained from ex-\\nisting universal samples aids in enhancing both\\ninstruction-following abilities and image recogni-\\ntion capabilities. Therefore, we will continue to\\nrefine these abilities in our future work.\\nModel Win:Loss Win Rate\\nOurs VS Qwen-VL 948:101 90.3%\\nTable 5: Model performance in description generation\\nstyle on C2C domain based on human evaluation.\\nModel MMBench(en/cn) MME SeedBench\\nLLaV A 1.5 65.2/57.3 1808.4 65.8\\nQwen-VL 61.8/56.3 1860.0 64.8\\nOurs 71.5/65.5 1813.0 49.0\\nTable 6: Performance of different models on open-\\nsource benchmarks to evaluate their general capabilities.\\n5.4 RQ4: RAG Can Alleviate Hallucinations\\nWe employed a combination of human and ma-\\nchine evaluations for this assessment.\\nKey Attribute Evaluation: Based on product\\nphotos, user-generated descriptions, and model-\\ngenerated descriptions, evaluators are required to\\nassess the accuracy of the attributes (e.g., brand,\\nmodel) in the model outputs. Subsequently, we can\\ncompute the accuracy rate.\\nMachine Automatic Evaluation: The content gen-\\nerated by the model was compared to the user-\\nwritten descriptions using metrics such as SIM,\\nBLEU and ROUGE.\\nThe specific results are shown in Table 7.\\nAs opposed to only giving the image to the\\nMLLMs, our model significantly improved all met-\\nrics.Especially in the human manual evaluation of\\nattribute accuracy, there was a 105% improvement.\\nThese enhancements can be attributed to RAG‚Äôs\\nability to provide richer and more accurate refer-Unit Human Machine Auto Evaluation\\nImage Category Reference ACC SIM BLEU1 BLEU2 BLEU3 BLEU4 ROUGE1 ROUGE2 ROUGEL\\n‚úì 0.36 0.633 0.132 0.027 0.009 0.003 0.155 0.034 0.153\\n‚úì ‚úì 0.35 0.639 0.134 0.027 0.009 0.004 0.157 0.036 0.156\\n‚úì ‚úì 0.74 0.720 0.173 0.057 0.029 0.018 0.216 0.080 0.191\\n‚úì ‚úì ‚úì 0.75 0.718 0.174 0.056 0.028 0.016 0.216 0.078 0.193\\nTable 7: Evaluation of component ablation effects in Retrieval-Augmented Generation Models\\nence information, which effectively mitigates hal-\\nlucination. This indicates that the information ob-\\ntained solely from product images is limited and ne-\\ncessitates supplementary references. On the other\\nhand, the direct contribution of product categories\\nis relatively minor. The primary function of cate-\\ngory prediction is to obtain the relevant attributes\\ntemplate, thereby enhancing the controllability of\\nthe generation process in RAG.\\n5.5 RQ5: Online A/B Test Results\\nTo evaluate the performance of the IPL system, we\\nconducted online A/B testing. The objective was to\\nmeasure the adoption rate of product descriptions\\ngenerated by IPL and to compare the advantages\\nover not using IPL. Our experiments demonstrate\\na high user acceptance rate for our system: up to\\n72% of users are willing to continue modifying the\\nautomatically generated descriptions to complete\\nproduct listings, and over 32% of users adopt more\\nthan 50% of the generated content. Furthermore,\\nproducts utilizing the auto-description generation\\nfeature exhibit a 5.6% improvement in overall qual-\\nity scores compared to similar products that do not\\nuse this feature. The product quality score, an inter-\\nnal metric used by the platform to assess product\\nquality, is primarily calculated based on the rich-\\nness of descriptions and the aesthetic authenticity\\nof photos. The details of the quality score definition\\ncan be found in Appendix C.2.\\n6 Related Work\\nMultimodal LLM: Recent advances in large lan-\\nguage models such as GPT-4, LLaMA(Touvron\\net al., 2023), and Qwen(Bai et al., 2023a), have\\ndemonstrated impressive capabilities in understand-\\ning world knowledge and generating diverse text.\\nThese models have shown significant potential in\\nzero-shot or few-shot(Wang et al., 2020) learning\\nscenarios, exhibiting strong instruction-following\\nabilities(Ouyang et al., 2022). Recent works, in-\\ncluding BLIP-2(Li et al., 2023), MiniGPT-4(Zhu\\net al., 2023), and Qwen-VL(Bai et al., 2023b), haveexplored integrating visual and textual modalities\\nfrom various perspectives. However, these models\\nlack training on domain-specific (C2C) private data,\\nresulting in insufficient domain understanding and\\ninconsistent domain-specific style outputs, which\\nlimits their effectiveness in related tasks.\\nRetrieval-Augmented Generation: Hallucina-\\ntion remains a major challenge in the develop-\\nment of LLMs(Guerreiro et al., 2023)(Ji et al.,\\n2023). Approaches such as VisualGPT(Wu et al.,\\n2023), HuggingGPT(Shen et al., 2024), and Tool-\\nFormer(Schick et al., 2024) leverage existing ma-\\nture modules to perform complex operations. An-\\nother method, involves text retrieval-based aug-\\nmentation(Guu et al., 2020; Izacard et al., 2023;\\nRobertson et al., 2009; Karpukhin et al., 2020),\\nwhere external resources(Guu et al., 2020) or web-\\nretrieved(Nakano et al., 2021) texts are fed into\\nthe prompts to provide LLMs with more accu-\\nrate (Mallen et al., 2022; Kandpal et al., 2023)ref-\\nerence information to mitigate hallucinations(Li\\net al., 2022; Kang and Choi, 2023). Unlike these\\nmethods, our research uniquely integrates visual-\\nbased retrieval augmentation with MLLMs and suc-\\ncessfully applies it in the e-commerce domain, ad-\\ndressing the hallucination problem while enhancing\\ntask-specific performance.\\n7 Conclusion\\nWe presented IPL system, a novel framework that\\ngenerates high-quality, accurate product descrip-\\ntions based on images, enhancing item listing effi-\\nciency in the C2C market. By leveraging MLLMs\\ntrained via Domain Injection, our model gains\\ndeeper domain-specific knowledge and style com-\\npared to the original model (Qwen-VL). The im-\\nplementation of Online RAG, which uses similar\\nproduct images as reference, reduces hallucination\\nin MLLMs, resulting in more precise descriptions.\\nThe effectiveness of our framework is demonstrated\\nthrough human evaluations, machine assessments,\\nand Online A/B testing.8 Limitations\\nOur IPL system generates precise descriptions tai-\\nlored to individual seller styles, streamlining the\\nposting process and enhancing the quality of list-\\nings. Our system exhibits notable potential for\\nfurther optimization. Firstly, the core attributes\\ntemplate is predominantly based on extensive de-\\nscriptive statistics and do not yet account for per-\\nsonalized user posting styles. Secondly, the accu-\\nracy of generated descriptions for certain long-tail\\ncategories requires improvement. To advance our\\nsystem, we intend to incorporate additional train-\\ning samples from long-tail categories and integrate\\nuser personalization data. This approach aims to en-\\nhance the accuracy and personalization of product\\ndescriptions, thereby increasing adoption rates and\\naiding users in efficiently producing high-quality\\ndescriptions.\\nReferences\\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\\narXiv preprint arXiv:2303.08774 .\\nReza Yazdani Aminabadi, Samyam Rajbhandari, Am-\\nmar Ahmad Awan, Cheng Li, Du Li, Elton Zheng,\\nOlatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff\\nRasley, et al. 2022. Deepspeed-inference: enabling\\nefficient inference of transformer models at unprece-\\ndented scale. In SC22: International Conference for\\nHigh Performance Computing, Networking, Storage\\nand Analysis , pages 1‚Äì15. IEEE.\\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\\nXiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\\nHuang, et al. 2023a. Qwen technical report. arXiv\\npreprint arXiv:2309.16609 .\\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,\\nSinan Tan, Peng Wang, Junyang Lin, Chang Zhou,\\nand Jingren Zhou. 2023b. Qwen-vl: A frontier large\\nvision-language model with versatile abilities. arXiv\\npreprint arXiv:2308.12966 .\\nGuiming Hardy Chen, Shunian Chen, Ziche Liu, Feng\\nJiang, and Benyou Wang. 2024. Humans or llms\\nas the judge? a study on judgement biases. arXiv\\npreprint arXiv:2402.10669 .\\nTri Dao. 2023. Flashattention-2: Faster attention with\\nbetter parallelism and work partitioning. arXiv\\npreprint arXiv:2307.08691 .\\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and\\nChristopher R√©. 2022. Flashattention: Fast and\\nmemory-efficient exact attention with io-awareness.Advances in Neural Information Processing Systems ,\\n35:16344‚Äì16359.\\nS√≠lvia Escursell, Pere Llorach-Massana, and M Blanca\\nRoncero. 2021. Sustainability in e-commerce pack-\\naging: A review. Journal of cleaner production ,\\n280:124314.\\nChaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,\\nMengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng,\\nKe Li, Xing Sun, Yunsheng Wu, and Rongrong Ji.\\n2023. Mme: A comprehensive evaluation bench-\\nmark for multimodal large language models. arXiv\\npreprint arXiv:2306.13394 .\\nNuno M Guerreiro, Duarte M Alves, Jonas Waldendorf,\\nBarry Haddow, Alexandra Birch, Pierre Colombo,\\nand Andr√© FT Martins. 2023. Hallucinations in large\\nmultilingual translation models. Transactions of the\\nAssociation for Computational Linguistics , 11:1500‚Äì\\n1517.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Mingwei Chang. 2020. Retrieval augmented\\nlanguage model pre-training. In International confer-\\nence on machine learning , pages 3929‚Äì3938. PMLR.\\nChristian Herold, Michael Kozielski, Leonid Ekimov,\\nPavel Petrushkov, Pierre-Yves Vandenbussche, and\\nShahram Khadivi. 2024. Lilium: ebay‚Äôs large\\nlanguage models for e-commerce. arXiv preprint\\narXiv:2406.12023 .\\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\\nHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-\\nYu, Armand Joulin, Sebastian Riedel, and Edouard\\nGrave. 2023. Atlas: Few-shot learning with retrieval\\naugmented language models. Journal of Machine\\nLearning Research , 24(251):1‚Äì43.\\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\\nMadotto, and Pascale Fung. 2023. Survey of halluci-\\nnation in natural language generation. ACM Comput-\\ning Surveys , 55(12):1‚Äì38.\\nLing Jiang, Keer Jiang, Xiaoyu Chu, Saaransh Gulati,\\nand Pulkit Garg. 2024. Hallucination detection in\\nllm-enriched product listings. In Proceedings of the\\nSeventh Workshop on e-Commerce and NLP LREC-\\nCOLING 2024 , pages 29‚Äì39.\\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric\\nWallace, and Colin Raffel. 2023. Large language\\nmodels struggle to learn long-tail knowledge. In In-\\nternational Conference on Machine Learning , pages\\n15696‚Äì15707. PMLR.\\nCheongwoong Kang and Jaesik Choi. 2023. Impact\\nof co-occurrence on factual knowledge of large lan-\\nguage models. arXiv preprint arXiv:2310.08256 .\\nVladimir Karpukhin, Barlas O Àòguz, Sewon Min, Patrick\\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\\nWen-tau Yih. 2020. Dense passage retrieval for\\nopen-domain question answering. arXiv preprint\\narXiv:2004.04906 .Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\\nGonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-\\ncient memory management for large language model\\nserving with pagedattention. In Proceedings of the\\nACM SIGOPS 29th Symposium on Operating Systems\\nPrinciples .\\nBohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui\\nWang, Ruimao Zhang, and Ying Shan. 2024. Seed-\\nbench: Benchmarking multimodal large language\\nmodels. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition , pages\\n13299‚Äì13308.\\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\\n2023. Blip-2: Bootstrapping language-image pre-\\ntraining with frozen image encoders and large lan-\\nguage models. In International conference on ma-\\nchine learning , pages 19730‚Äì19742. PMLR.\\nJunnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,\\nShafiq Joty, Caiming Xiong, and Steven Chu Hong\\nHoi. 2021. Align before fuse: Vision and language\\nrepresentation learning with momentum distillation.\\nAdvances in neural information processing systems ,\\n34:9694‚Äì9705.\\nShaobo Li, Xiaoguang Li, Lifeng Shang, Zhenhua Dong,\\nChengjie Sun, Bingquan Liu, Zhenzhou Ji, Xin Jiang,\\nand Qun Liu. 2022. How pre-trained language mod-\\nels capture factual knowledge? a causal-inspired anal-\\nysis. arXiv preprint arXiv:2203.16747 .\\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\\nmar, et al. 2022. Holistic evaluation of language\\nmodels. arXiv preprint arXiv:2211.09110 .\\nChin-Yew Lin. 2004. Rouge: A package for automatic\\nevaluation of summaries. In Text summarization\\nbranches out , pages 74‚Äì81.\\nYuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,\\nSongyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi\\nWang, Conghui He, Ziwei Liu, et al. 2023. Mm-\\nbench: Is your multi-modal model an all-around\\nplayer? arXiv preprint arXiv:2307.06281 .\\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\\nDaniel Khashabi, and Hannaneh Hajishirzi. 2022.\\nWhen not to trust language models: Investigating\\neffectiveness of parametric and non-parametric mem-\\nories. arXiv preprint arXiv:2212.10511 .\\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\\nLong Ouyang, Christina Kim, Christopher Hesse,\\nShantanu Jain, Vineet Kosaraju, William Saunders,\\net al. 2021. Webgpt: Browser-assisted question-\\nanswering with human feedback, 2021. arXiv\\npreprint arXiv:2112.09332 .\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.2022. Training language models to follow instruc-\\ntions with human feedback. Advances in neural in-\\nformation processing systems , 35:27730‚Äì27744.\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\\nJing Zhu. 2002. Bleu: a method for automatic evalu-\\nation of machine translation. In Proceedings of the\\n40th annual meeting of the Association for Computa-\\ntional Linguistics , pages 311‚Äì318.\\nF√°bio Perez and Ian Ribeiro. 2022. Ignore previous\\nprompt: Attack techniques for language models.\\narXiv preprint arXiv:2211.09527 .\\nNina Poerner, Ulli Waltinger, and Hinrich Sch√ºtze. 2019.\\nE-bert: Efficient-yet-effective entity embeddings for\\nbert. arXiv preprint arXiv:1911.03681 .\\nStephen Robertson, Hugo Zaragoza, et al. 2009. The\\nprobabilistic relevance framework: Bm25 and be-\\nyond. Foundations and Trends ¬Æin Information Re-\\ntrieval , 3(4):333‚Äì389.\\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess√¨, Roberta\\nRaileanu, Maria Lomeli, Eric Hambro, Luke Zettle-\\nmoyer, Nicola Cancedda, and Thomas Scialom. 2024.\\nToolformer: Language models can teach themselves\\nto use tools. Advances in Neural Information Pro-\\ncessing Systems , 36.\\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\\nWeiming Lu, and Yueting Zhuang. 2024. Hugging-\\ngpt: Solving ai tasks with chatgpt and its friends\\nin hugging face. Advances in Neural Information\\nProcessing Systems , 36.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timoth√©e Lacroix,\\nBaptiste Rozi√®re, Naman Goyal, Eric Hambro,\\nFaisal Azhar, et al. 2023. Llama: Open and effi-\\ncient foundation language models. arXiv preprint\\narXiv:2302.13971 .\\nYaqing Wang, Quanming Yao, James T Kwok, and Li-\\nonel M Ni. 2020. Generalizing from a few examples:\\nA survey on few-shot learning. ACM computing sur-\\nveys (csur) , 53(3):1‚Äì34.\\nChenfei Wu, Shengming Yin, Weizhen Qi, Xi-\\naodong Wang, Zecheng Tang, and Nan Duan.\\n2023. Visual chatgpt: Talking, drawing and edit-\\ning with visual foundation models. arXiv preprint\\narXiv:2303.04671 .\\nYanhao Zhang, Pan Pan, Yun Zheng, Kang Zhao,\\nYingya Zhang, Xiaofeng Ren, and Rong Jin. 2018.\\nVisual search at alibaba. In Proceedings of the 24th\\nACM SIGKDD international conference on knowl-\\nedge discovery & data mining , pages 993‚Äì1001.\\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\\nMohamed Elhoseiny. 2023. Minigpt-4: Enhancing\\nvision-language understanding with advanced large\\nlanguage models. arXiv preprint arXiv:2304.10592 .A Data Processing\\nOur model‚Äôs training data comprises tasks related\\nto product description generation, e-commerce do-\\nmain understanding, and general capability tasks.\\nThe methods for collecting and constructing data\\nfor each type of task vary accordingly.\\nA.1 Description Generation Data\\nProduct description generation is the core task of\\nour model, with the goal of generating product de-\\nscriptions in the style of C2C platforms based on\\nuser-uploaded images. To achieve this goal, the\\nbest data source can be considered as the products\\nposted by actual users on the platform. Given the\\nvarying quality of user-posted products, data selec-\\ntion and cleaning are also crucial. Additionally, it\\nis necessary to construct various description gen-\\neration instructions to increase the richness and\\ncontrollability of product description generation.\\nData cleaning and selection include the following\\nkey steps:\\n‚Ä¢First, filter out low-quality products based on\\nproduct quality scores, which mainly consider\\nthe completeness of basic descriptions and the\\naesthetic quality of product photos;\\n‚Ä¢Filter out products with negative risks present\\non the platform, such as low-priced traffic at-\\ntraction, traffic attraction to other platforms,\\nand potential fraudulent products;\\n‚Ä¢Use a self-developed image-text matching\\nmodel similar to CLIP to filter out products\\nwith low similarity between photos and de-\\nscriptions;\\n‚Ä¢Apply heuristic rules to exclude products that\\ndo not meet generation standards, such as ex-\\ncessively long or short descriptions, inclusion\\nof user privacy information, or special charac-\\nters;\\n‚Ä¢Finally, perform stratified sampling based on\\ncategories to obtain training sample candi-\\ndates with balanced categories.\\nFor the diversity of instructions, we mainly pro-\\nvide three types of instructions: generating product\\ndescriptions directly from images, generating de-\\nscriptions based on images + core attributes tem-\\nplate, and generating product descriptions based\\non product images + core attributes template + ref-\\nerence information. Examples of the three types\\nof instructions and model responses are shown in\\nTable 8.\\nFor generating product descriptions directlyfrom images, we can directly format the cleaned\\nproduct image and description pairs as instructions.\\nFor the second type of task, we need to first per-\\nform core attribute extraction on the target product\\ndescriptions and then concatenate the extracted at-\\ntribute names as part of the description generation\\ninstructions to obtain the corresponding format of\\ntraining data. Similarly, based on the second type\\nof instructions, we include the extracted attribute\\nvalues as part of the reference information within\\nthe instruction prompt, thus obtaining the third type\\nof instruction tuning data.\\nA.2 E-commerce Understanding Data\\nIntroducing e-commerce domain task data aims to\\nenhance the model‚Äôs understanding of e-commerce\\nknowledge, particularly the unique data distribu-\\ntion of C2C e-commerce platforms. To ensure the\\ndiversity of this data, we collect metadata based on\\ntwo dimensions: technical direction and specific\\ntask type. The technical directions include classic\\nproduct understanding on e-commerce platforms,\\nsearch query understanding, relevance matching,\\ndata mining, and e-commerce QA, etc., while the\\ntask types include classification tasks, matching\\ntasks, ranking tasks, and sequence labeling tasks.\\nAdditionally, our domain task data are all de-\\nrived from the platform‚Äôs historically accumulated\\ndata, all of which have been manually annotated\\nor ensured by other accuracy assurance methods to\\nguarantee data quality. Finally, all the metadata are\\nconverted into instruction format for model train-\\ning.\\nA.3 General Instruction Data\\nTraining a model solely on domain-specific tasks\\ninduces overfitting to the instructions within the\\ntraining data, thereby diminishing the model‚Äôs gen-\\neralization capability and its ability to follow gen-\\neral instructions. To mitigate this issue, we incor-\\nporated general task data into the training dataset,\\nprimarily sampling from the open-source data pro-\\nvided by the LLaV A1.5 project.\\nSince high-quality open-source data are typically\\nin English, to enhance the model‚Äôs performance in\\nthe native language and adapt to the platform‚Äôs\\nown data distribution, we automatically generate\\ngeneral instruction QA data using large language\\nmodels for product photos. Specifically, for each\\nproduct photo, we utilize a large language model\\nto generate multiple potential instruction questions\\nand their corresponding answers. Table 9 providesInstruction Design for Product Description Generation:\\nUser Description\\n(Generation Target) Apple iPhone 11, China version, 256GB, Silver, 90% new, purchased from the official\\nwebsite. If interested, please contact me privately.\\nAttribute Extraction\\nResults { \"Brand\": \"Apple\", \"Model\": \"iPhone 11\", \"Version Type\": \"China Version\", \"Memory\\nCapacity\": \"256GB\", \"Color\": \"Silver\", \"Condition\": \"90% New\", \"Purchase Channel\":\\n\"Official Website\" }\\nInstruction Instance\\n(generating product de-\\nscriptions directly from\\nimages)You are an experienced seller on a second-hand trading platform and need to post a\\nlisting for a mobile phone product. The product images are as shown. Please write a\\nproduct description for this item, enhancing and expanding it reasonably.\\nInstruction Instance\\n(generating descriptions\\nbased on images + core\\nattribute templates)You are an experienced seller on a second-hand trading platform and need to post a\\nlisting for a mobile phone product. The product images are as shown.The copywriting\\ntemplate is Brand+Model+Version Type+Memory Capacity+Color+Condition+Purchase\\nChannel. Please write a product description for this item, enhancing and expanding it\\nreasonably according to the template.\\nInstruction Instance\\n(generating product de-\\nscriptions based on prod-\\nuct images + core at-\\ntribute templates + ref-\\nerence information.)You are an experienced seller on a second-hand trading platform and need to post a\\nlisting for a mobile phone product. The product images are as shown.The copywriting\\ntemplate is Brand+Model+Version Type+Memory Capacity+Color+Condition+Purchase\\nChannel, where Brand is Apple, Model is iPhone 11, and Memory Capacity is 256GB.\\nPlease write a product description for this item, enhancing and expanding it reasonably\\naccording to the template.\\nTable 8: Examples of different instruction designs for product description generation.\\nan example of the prompt engineering process uti-\\nlized in this step. To further improve the accu-\\nracy of the generated answers, for each instruction\\nquestion, we use a robust large language model to\\ngenerate answers based on the given picture and\\ninstruction, thereby producing the final training\\ndata.\\nB Details of Training\\nB.1 Data Format of Supervised Fine-tuning\\nRegarding the format of the training data, we fol-\\nlow the approach of Qwen-VL, converting the pre-\\npared instruction tuning data into ChatML (Ope-\\nnAI) format, marking each interaction statement\\nwith special tokens (<im_start> and <im_end>) to\\ndenote dialogue termination. Training objectives\\nfocused on classic next token generation for lan-\\nguage model optimization, excluding prompt pre-fixes and emphasizing special markers and model\\noutputs (depicted in Table 10).\\nThe Dataset Format Example of ChatML\\n<im_start>user\\nPicture 1: <img>vg/VG_100K_2/649.jpg</img>What\\nis the sign in the picture?<im_end>\\n<im_start>assistant\\nThe sign is a road closure with an orange\\nrhombus.<im_end>\\n<im_start>user\\nHow is the weather in the picture?<im_end>\\n<im_start>assistant\\nThe shape of the road closure sign is an orange\\nrhombus.<im_end>\\nTable 10: Instruction Fine-Tuning data format.\\nB.2 Training Hyperparameters\\nTable 11 presents some of the parameter settings\\nused in the training process of our domain-specific\\nmodel.Parameters Value\\nViT init Qwen-VL-Chat\\nLLM init Qwen-VL-Chat\\nVL Adapter init Qwen-VL-Chat\\nImage resolution 448x448\\nViT sequence length 1024\\nLLM sequence length 1024\\nLearnable query number 256\\nLearning rate 1e-5\\nEpoch 3\\nTraining steps 4788\\nLearning rate schedule Cosine decay\\nGlobal batch size 768\\nGradient accumulation 16\\nNumerical precision BF16\\nDeepSpeed Stage1\\nTable 11: Parameter settings used in the training pro-\\ncess.\\nWe employ the DeepSpeed ZeRO stage 1 ap-\\nproach for parallel training, utilizing 24 A800\\nGPUs to train on 1.27M data for 3 epochs, taking\\n16 hours, with an average throughput of 2.5 sam-\\nples per second per GPU. We use the AdamW opti-\\nmizer with Œ≤1= 0.9,Œ≤2= 0.98, andœµ= 1√ó10‚àí6.\\nWe also apply a cosine learning rate schedule with\\na warmup ratio of 0.01.\\nC Details of Internal Evaluation Method\\nC.1 Evaluation Metrics for Visual Retrieval\\nWe evaluate the effectiveness of visual retrieval\\nby assessing whether the query image and the re-\\ntrieved images are identical or similar. Specifically,\\nidentical products refers to that the two items share\\nthe same SKU (Stock Keeping Unit), where both\\nthe key attributes (such as product name, brand,\\nmode, etc.) and non-key attributes (such as color,\\nsize, etc.) must be exactly the same. Similar prod-\\nucts stands for that the two items with the same\\nSPU (Standard Product Unit), where only the key\\nattributes is asked to be matched, leading to a signif-\\nicantly high accuracy compared to the same level.\\nIn our experiments, we found that similarity at\\nthe SPU level can provide accurate essential at-\\ntributes, which significantly aids in the generation\\nof final description.\\nC.2 Calculation of product quality score\\nProduct quality score is computed using an\\nexplainable-and-linearly weighted formula basedon the content description. Key features include cat-\\negories, attributes, descriptions, images, videos and\\nprice. The weight for each feature is determined\\nby professional operators based on the importance\\nof each of the above-mentioned dimension. The\\nformula is listed in the below.\\nquality _score =NX\\ni=1wi‚àófeature i (2)\\nwhere feature denotes the characteristics con-\\nsidered for quality score, such as the accuracy of\\nthe category, the attribute filling rate and the flu-\\nency of the description. wrepresents the weight\\nassigned to each corresponding feature, and Nis\\nthe total number of features, which in this case is\\n11.\\nD Error Detection and Exception\\nHandling in Online Services\\nWe designed a set of exception-handling mecha-\\nnisms over multiple stages for better accommodat-\\ning the production system.\\nDuring the input stage, the uploaded images may\\ncontain non-compliant content, such as prohibited\\nproducts, pornography, and etc. To avoid such\\ncases, we applied several machine learning models\\nfor security check, which can provide proper guide-\\nline when such harmful content has been identified.\\nIn the pipeline stage, exceptions may also occur\\nfrom different sub-modules, such as empty cate-\\ngory prediction, empty visual search results, and\\netc. All of them would change the reference in-\\nformation of the MLLM input. To address such\\nissue, we designed instructions that cover all of\\nthose cases during model training (more details in\\nTable 8). In the worst case, the model is allowed to\\ngenerate product descriptions solely based on the\\nuploaded image. For instance, if the image search\\nyields no results, the MLLM will utilize the image\\ninformation, along with domain knowledge, to gen-\\nerate product description. It is worth noting that,\\nthe chance of hallucination increases in this case\\n(refer to Table 7).\\nDuring the output stage, in the process of stream-\\ning output, we keep monitoring content safety.\\nOnce the harmful content is detected, the content\\ngeneration process will be halted, with an subse-\\nquent notification to the user for modification. Ad-\\nditionally, if the output exceeds the pre-definedcontent length limit, we will automatically truncate\\nit to avoid system failure.\\nLastly, in the case of request timeout, we keep\\nthe existing product listing function intact, allowing\\nusers to manually edit the content description.\\nE Summary of the In-house Evaluation\\nBenchmarks\\nIn the experimental section, we designed multiple\\nin-house validation datasets to evaluate the domain\\nadaptation capabilities of our model. All data were\\nsourced from real e-commerce scenarios and the\\ntarget labels were either manually annotated or con-\\nfirmed by actual platform users, then converted into\\ninstruction format. Table 12 presents the various\\nevaluation datasets along with their evaluation de-\\ntails.\\nA unified test instruction is used for the eval-\\nuation tasks without special optimizations for the\\nmodel. Additionally, some tasks will provide a few-\\nshot examples to ensure the model outputs answers\\nin the expected format. For the calculation of evalu-\\nation metrics, we use string matching to determine\\nwhether the generated results are consistent with\\nthe target answers. Manual verification has shown\\nthat this method has extremely high accuracy in\\nour evaluation task.Prompt Engineering for Automatic Generation of General QA Data:\\nProduct Photo\\nPrompt Based on the given image, design multiple types of task questions and answers. The task\\ntypes include: image information description, image emotion analysis, image action\\nrecognition, existence check of elements in the image, image text extraction, analysis\\nof object interactions in the image, object attribute recognition in the image, image\\nmultiple-choice question answering, visual reasoning, visual common sense reasoning,\\nimage style appreciation, content creation based on the image, and writing product\\ndescriptions based on the image. Below, provide up to 20 diverse instructions for all the\\nabove tasks, including different language styles and precise answers. The instructions\\nshould include both questions and statements. Answers should be less than 100 words.\\nEach task should have fewer than 3 instructions. Output format:\\nInstruction1: Example Instruction1\\nAnswer1: Example Answer1\\nTask1: Example Task1\\nInstruction2: Example Instruction2\\nAnswer2: Example Answer2\\nTask2: Example Task2\\n...\\nResponse Instruction1: Please describe the objects in the image.\\nAnswer1: The image features a pair of purple sneakers placed in a cardboard box, with\\nwhite shoelaces.\\nTask1: Image Information Description\\nInstruction2: How does the color of these shoes make you feel?\\nAnswer2: The purple color of the shoes gives a gentle and refreshing feeling.\\nTask2: Image Emotion Analysis\\nInstruction3: What is the person in the image doing?\\nAnswer3: The person in the image is holding the shoes with their hands, as if they are\\ndisplaying or inspecting the shoes.\\nTask3: Image Action Recognition\\nInstruction4: Is there a label shown in the image?\\nAnswer4: No, there is no label visible in the image.\\nTask4: Existence Check of Elements in the Image\\n...\\nInstruction9: Based on the image, what activities are these shoes suitable for?\\nAnswer9: Since they are sneakers, they are suitable for sports or casual activities.\\nTask9: Visual Reasoning\\nInstruction10: What are such sneakers usually used for?\\nAnswer10: Such sneakers are typically used for running, workouts, or casual daily wear.\\nTask10: Visual Common Sense Reasoning\\nInstruction11: What do you think of the design style of these shoes?\\nAnswer11: The design style of these shoes is modern and minimalist, with gentle colors\\nthat make them look comfortable and pleasant.\\nTask11: Image Style Appreciation\\nInstruction12: Based on the image, write a short story about these shoes.\\nAnswer12: One day, Xiaoming found his dream purple sneakers. He wore them and\\nwon the school marathon championship.\\nTask12: Content Creation Based on the Image\\nTable 9: Sample of automatically generated general instructional QA data based on product photos using Large\\nLanguage Model prompt engineering.Task Dataset Description Size Metric\\nMulti-choice\\nQuestionTopic Selec-\\ntion (TS)Given the content of a post published\\nby a user in e-commerce scenario, along\\nwith a set of candidate topics, the objec-\\ntive is to select the topic from the candi-\\ndate set that matches the post content.5k Accuracy( ‚Üë)\\nMulti-choice\\nQuestionContent Tag-\\nging (CT)Given the content of a user‚Äôs post in an\\ne-commerce scenario and a set of candi-\\ndate categorys, the objective is to select\\nthe category from the candidate set that\\nmatch the post content.5k Accuracy( ‚Üë)\\nMulti-choice\\nQuestionCategory\\nRecognition\\n(CR)Given the product image and text in-\\nformation posted by users in the e-\\ncommerce scenario, as well as the candi-\\ndate category set, the goal is to select the\\ncategory to which the product belongs.5k Accuracy( ‚Üë)\\nMulti-choice\\nQuestionVision-\\nBased\\nProduct\\nAttribute\\nExtraction\\n(V AE)Given a product photo, the desired at-\\ntribute, and a list of candidate attribute\\nvalues, the goal is to select the correct\\nattribute value from the candidate list.5k Accuracy( ‚Üë)\\nImage Caption Product\\nDescription\\nGeneration\\n(PDG)Given user-uploaded product photos, the\\ngoal is to generate corresponding prod-\\nuct descriptions that closely match the\\ncontent written by the users themselves.2k SIM( ‚Üë)\\nMulti-choice\\nQuestionSentiment\\nAnalysis\\n(SA)After purchasing products, users pro-\\nvide feedback on their buying experi-\\nence. The objective is to distinguish\\nwhether the user‚Äôs review is positive or\\nnegative.5k Accuracy( ‚Üë)\\nInformation\\nExtractionText-Based\\nProduct\\nAttribute\\nExtraction\\n(TAE)Given the textual description of a prod-\\nuct and the list of desired attributes to be\\nextracted, the objective is to extract the\\ncorresponding attribute values from the\\ndescription text.5k Accuracy( ‚Üë)\\nTable 12: Summary of the domain evaluation benchmarks.',\n",
       " 'Measuring Network Dynamics of Opioid Overdose Deaths in the\\nUnited States\\nKushagra Tiwari1‚àó, M. Amin Rahimian1‚àó, Mark S. Roberts2,\\nPraveen Kumar2and Jeannine M. Buchanich3\\n1Department of Industrial Engineering,2Department of Health Policy and Management,\\nand3Department of Biostatistics\\nUniversity of Pittsburgh\\n‚àóTo whom correspondence should be addressed; emails: kut20@pitt.edu, rahimian@pitt.edu.\\nAbstract\\nThe US opioid overdose epidemic has been a major public health concern in recent decades.\\nThere has been increasing recognition that its etiology is rooted in part in the social contexts that\\nmediate substance use and access; however, reliable statistical measures of social influence are lacking\\nin the literature. We use Facebook‚Äôs social connectedness index (SCI) as a proxy for real-life social\\nnetworks across diverse spatial regions that help quantify social connectivity across different spatial\\nunits. This is a measure of the relative probability of connections between localities that offers\\na unique lens to understand the effects of social networks on health outcomes. We use SCI to\\ndevelop a variable, called ‚Äúdeaths in social proximity‚Äù, to measure the influence of social networks\\non opioid overdose deaths (OODs) in US counties. Our results show a statistically significant effect\\nsize for deaths in social proximity on OODs in counties in the United States, controlling for spatial\\nproximity, as well as demographic and clinical covariates. The effect size of standardized deaths in\\nsocial proximity in our cluster-robust linear regression model indicates that a one-standard-deviation\\nincrease, equal to 11 .70 more deaths per 100 ,000 population in the social proximity of ego counties\\nin the contiguous United States, is associated with thirteen more deaths per 100 ,000 population in\\nego counties. To further validate our findings, we performed a series of robustness checks using a\\nnetwork autocorrelation model to account for social network effects, a spatial autocorrelation model\\nto capture spatial dependencies, and a two-way fixed-effect model to control for unobserved spatial\\nand time-invariant characteristics. These checks consistently provide statistically robust evidence\\nof positive social influence on OODs in US counties. Our analysis provides a pathway for public\\nhealth interventions informed by social network structures. The statistical robustness of our primary\\nvariable of interest, deaths in social proximity, supports the hypothesis of a social network effect on\\nOODs. Using agent-based modeling (ABM) to simulate social networks can offer an effective method\\nto design interventions that incorporate the dynamics of social networks for maximum impact.\\nIntroduction\\nThe opioid overdose epidemic is a major public health crisis in the US, with an exponentially increasing\\nnumber of drug overdose deaths in the last four decades [1, 2]. Alpert et al. (2022) report that opioid\\noverdose deaths (OODs) account for 75% of the increase in drug overdose deaths [3]. Addressing this\\ncrisis requires planned interventions that focus on supply-side regulations and the dynamics of social\\nconnections. The rate of initiation of opioid misuse is known to increase due to social influence [4].\\nCostello et al. (2021) report that of the 370 participants who entered an opioid withdrawal program,\\n97% identified knowing the individual with whom they initiated opioid use, with friendship being the\\nmost reported relationship between participants and their initiation partners [5]. Similarly, Rigg et al.\\n(2018) note that two-thirds of misused drugs are obtained from friends and family [6]. Guarino et al.\\n1arXiv:2410.17496v1  [cs.SI]  23 Oct 2024(2018) study of 539 young adults who misuse opioids and heroin indicates that their first experiences\\nwith the misuse of prescription opioids typically occur in a social setting with peers [7]. The misuse of\\nprescription opioids has been growing among young people [8]. Syvertsen et al. (2017) make similar\\nobservations about young people who experiment with drugs and the initiation of drug use [9]. Social\\nnetworks can have positive and negative impacts on substance use. Empirical results have indicated\\nthat peer networks with subjects who do not use substances have a positive influence on curbing drug\\nabuse; however, networks consisting of more substance users are likely to increase substance use [10]. In\\nintervention designs, recovery support strategies, including peer recovery, have shown encouraging results.\\nPeer workers who have completed their recovery help others in recovery from substance addiction. This\\nform of peer-supported recovery is found to be more effective in reducing the prevalence of opioid use\\ndisorder (OUD) [11].\\nProviding reliable measures of social influence on the opioid epidemic is complicated by the confound-\\ning factors that influence opioid misuse and social interactions, on the one hand, and the ethical barriers\\nto randomized experiments, on the other. Studies measuring social influence use complex systems-based\\ngenerative models to understand these phenomena in areas such as voting contagion [12]. Specifically,\\nBraha and Aguiar (2017) [12] use a generalized social voter model combined with spatial-statistical anal-\\nysis to examine how social influence has shaped voting behavior in the US presidential election over\\nthe past century. They distinguish between social contagion and external influences (e.g., media and\\nopinion leaders) to assess their impact on county-level vote share distributions over time and geogra-\\nphy. By analyzing spatial patterns, they demonstrate that social influence is geographically clustered\\nand spreads like a contagion across county borders. In contrast, applying similar modeling techniques\\nto the opioid epidemic to design targeted interventions has encountered certain limitations. Homer et\\nal. (2021) [13] discuss the complexity of modeling OUD and highlight the limitations of their models in\\naccurately capturing real-life scenarios due to their simplicity. Recognizing the complexities of modeling\\nthe opioid epidemic through generative processes, our research aims to address these gaps using controlled\\nregressions.\\nFrom the traditional setting to the new digital era of social networks, we have witnessed a significant\\nshift in informal social interactions. Facebook is the largest informal online social network globally, with\\n2.1 billion active users and 239 million active users in the US and Canada. Given its broad reach and\\nprevalence, Facebook connections can provide insight into real-life social networks in many geograph-\\nical regions. Facebook has released a social connectedness index dataset that measures how intensely\\ngeographical locations are connected according to the relative probability of connections [14]. In Sup-\\nplementary Information section S1 we provide a review of SCI use cases as a proxy for real-life social\\nconnections in public health, epidemiology, economics and development applications. In the following\\nparagraph, we justify our use of SCI for measuring the network dynamics of OODs in the US.\\nWe use SCI to construct a measure of OODs in the social proximity of counties in the United States\\nand investigate the statistical significance of its effect on county-level OODs after controlling for clinical,\\nspatiotemporal, and socioeconomic confounders. Our analysis seeks to provide information on the role\\nof social influence on OODs. Using SCI to represent the intensity of interpersonal networks between\\ncounties reflects the possibility of physical interactions between county residents. Bailey et al. (2020)\\n[15, 16] show that SCI is predictive of travel patterns within urban areas and throughout Europe. Coven et\\nal. (2023) [17] show that counties with higher social connections to New York City are the most preferred\\ndestinations for those moving away from the city during the pandemic, highlighting the association of\\nphysical interaction with SCI. Kuchler et al. (2022) [18] use SCI to show that COVID-19 is more likely to\\nspread between regions with stronger social networks and highlighted the potential of SCI to improve the\\nprediction of epidemics. Although SCI provides a robust measure of social connectedness, some studies\\nhave explored alternative data sources, such as social media platforms, to examine trends in the context\\nof the opioid epidemic. However, these approaches have faced significant challenges due to demographics\\nand other data limitations [19, 20, 21]. For example, Pandrekar et al. (2018) [20] use Reddit data to\\n2analyze psychological categories and patterns of opioid abuse on a national scale. A major limitation\\nof their study is that the data collected through the Reddit API do not provide access to user location\\ninformation. In addition, the Reddit data do not indicate whether users are friends, which restricts the\\nability to analyze the structure of social networks and their association with OODs.\\nIn contrast, SCI offers distinct advantages. SCI is location-based and provides detailed information\\nabout the structure of social networks in US counties. SCI measures friendship networks, serving as\\na proxy for real-world social connections. Unlike Reddit data, SCI allows for the analysis of location-\\nspecific friendship networks, which makes it particularly useful for studying how social networks influence\\nthe opioid epidemic on a population scale. Our choice of SCI as the network measure is informed by\\nprevious use cases that reflect the real-life dynamics of social connections in different domains such as\\neducation [22] and public health [18]. Our objective is to measure how these social connections contribute\\nto heterogeneous patterns of opioid overdose deaths in US counties.\\nIn our study, we present a novel perspective on analyzing the association between friendship networks\\nand opioid overdose deaths at the population level in counties within three distinct geographical regions:\\nthe eastern United States, the central and western United States, and the entire contiguous United\\nStates. We provide statistical evidence linking the geographical spread of OODs with the structure of\\nsocial connections.\\nTo achieve this, we use a range of statistical methodologies. We use cluster-robust linear regression to\\naccount for intra-cluster correlation between counties within the same states, network and spatial auto-\\ncorrelation methods to address the autocorrelation in error terms arising from unobserved factors shared\\namong spatially or socially connected units, and two-way fixed effects models to control for unobserved\\nspatial and temporal heterogeneities. Additionally, we perform robustness checks to account for the dis-\\ntance decay of proximity weights and apply a two-stage least squares method to jointly address spatial\\nand network effects, discussed in the Supplementary Information sections S5 and S4.5. Our multifaceted\\nstatistical analysis demonstrates that our variable of interest, deaths in social proximity, is statistically\\nsignificant across the three distinct geographical regions.\\nUnderstanding the role of social networks is important in designing interventions to combat opioid mis-\\nuse behavior. Research measuring the network dynamics of opioid overdose death on the US population\\nscale and at the resolution of counties remains limited, with the exception of [23, 24] that measure social\\nnetwork drivers of the opioid epidemic and use natural experiments to support their claims. M¬® ackle and\\nReunzi (2023) examine changes in county-level overdose deaths due to the reformulation of OxyContin\\nand the must-access Prescription Drug Monitoring Program (PDMP) [23]. They analyze policy-induced\\nshocks to estimate the indirect effect of friendship networks (measured by SCI) on OODs. Their anal-\\nysis includes the correlation of ‚Äúsocial proximity‚Äù with OODs by using a two-way fixed-effects model.\\nCompared to M¬® ackle and Reunzi (2023) [23], our study differs in the methodological approach. Along\\nwith cluster-robust linear regression and two-way fixed effects, our analysis uses a network and spatial\\nautocorrelation model to account for autocorrelation in error terms, which M¬® ackle and Reunzi (2023) [23]\\ndo not consider when investigating the association of ‚Äúsocial proximity‚Äù with OODs. We also include a\\nrobustness check to account for spatial and network autocorrelation together and provide evidence that\\nthe statistical significance of the ‚Äúdeaths in social proximity‚Äù variable is not sensitive to the choice of\\ndistance decay function. Furthermore, our regression includes domain-specific covariates such as avail-\\nability of naloxone and buprenorphine, opioid dispensing rate, mental health distress rate and state-level\\nfentanyl and analog seizure data to account for illicit opioids which M¬® ackle and Reunzi (2023) [23] do not\\nconsider when investigating the association between ‚Äúsocial proximity‚Äù and OOD. Furthermore, M¬® ackle\\nand Reunzi (2023) use the National Vital Statistics System (NVSS) public database and employ a backout\\nprocedure to recover mortality data points that the NVSS otherwise suppresses if the county has fewer\\nthan ten deaths. In contrast, our study uses mortality data from the National Center for Health Statistics\\n(NCHS) which provides access to these suppressed data points. In this regard, our results complement\\ntheir findings, as their coefficient for ‚Äúsocial proximity‚Äù is also statistically significant. Concurrently with\\n3our work, Cuttler and Donahoe (2024) [24] have explored the dynamics of opioid death rates, focusing\\non SCI and the distance between counties to analyze spillovers. They posit that the increase in opioid\\ndemand is endogenous, resulting from spillovers between affected populations. Their study underscores\\nthe importance of social and spatial spillovers in estimating opioid demand, which is correlated with\\nincreased mortality rates. However, our work differs in its methodological approach, concentrating on\\nconstructing a socially lagged variable to assess the impact of social influence on overdose death rates.\\nIn the following section, we formally introduce our variable of interest, ‚Äúdeaths in social proximity‚Äù.\\nThe descriptive statistics serve as a starting point for later estimating the effect size of deaths in social\\nproximity in the Results section.\\nOOD rates in social and spatial proximity\\nThe root of the opioid epidemic is partially associated with social contexts that mediate substance use\\nand accessibility. Existing studies integrating social network analysis with the geographical spread of\\noverdose deaths have demonstrated how social characteristics influence the trajectory of substance use,\\nfor example, geographic discordance, which means that the community in which the overdose death\\noccurs is different from the community of residence [25]. Using data on overdose deaths from 2017 to\\n2020 in Milwaukee County, Wisconsin, Forati et al. (2023) build a social-spatial network framework to\\ndetect network interaction hotspots for overdose deaths and analyze their geographical discordance [25].\\nHowever, their study is limited to Milwaukee County and does not extend to US population scales.\\nAs discussed in the Introduction, the significant contribution of social influence in initiating substance\\nuse is well documented in the literature. Researchers have highlighted the impact of social networks on\\nthe regulation of substance use patterns based on the ego network [26]. However, applying these findings\\nto a geographical context presents challenges, as geographical proximity substantially influences social\\nconnections and communication patterns across varying distances [27]. In our model, we control for the\\ninherent spatial patterns of geographical proximity to refine our estimate of the effect of social networks\\non OODs.\\nAssessing the strength of social ties within every individual‚Äôs network in a wide geographical area is\\nvery resource-intensive. To address this challenge, one potential approach is to aggregate and estimate\\nthe social networks of metapopulations residing in different localities, such as ZIP codes or counties in\\nthe United States. This offers a broader perspective and alleviates some of the constraints associated\\nwith individual-level analysis. In 2018, Meta Platforms, which operates the Facebook social network,\\nreleased a data set that measures the distribution of social ties of location-specific networks globally. SCI\\nis a surrogate for real-life friendship networks between registered Facebook users at each location [14]. It\\nquantifies the strength of friendship ties in various locations using relative probability and is available at\\nthe ZIP code and county level for the United States. Formally, the SCI between two locations iandjis\\ndefined as follows [14]:\\nSCIij=Facebook Connections ij\\nFacebook Users i√óFacebook Users j.\\nHere, Facebook Users irepresents the number of Facebook users in the county i. Facebook Connections ij\\nis the total number of Facebook friendship connections between individuals in counties iandj.\\nBuilding on the established link between SCI and real-world social connections, we introduce two\\nvariables, ‚Äúdeaths in social proximity‚Äù and ‚Äúdeaths in spatial proximity‚Äù, to capture the influence of\\nsocial and spatial factors on the distribution of OOD rates in counties in the United States. The term\\n‚Äúdeaths in social proximity‚Äù indicates the average number of death rates in alter locations weighted by\\ntheir SCI to the focal node, also known as ‚Äúego‚Äù. This variable operates as a socially lagged variable,\\naccounting for the influence of death rates in ‚Äúalter‚Äù locations, referring to the direct connections of the\\nego. On the other hand, ‚Äúdeaths in spatial proximity‚Äù measures the average number of death rates in the\\n4alters weighted by their inverse geographical distance to the ego. Unlike the socially lagged variable, this\\nis a spatially lagged variable to account for the effects of deaths in the spatial vicinity of the ego. These\\nfactors work together to provide a comprehensive picture of how deaths are distributed and influenced by\\nsocial and geographical factors in a given location. Quantitatively, deaths in social and spatial proximity,\\ndenoted by s‚àíiandd‚àíifor an ego location i, are defined as follows:\\ns‚àíi=X\\njÃ∏=iwijyj,and d‚àíi=X\\njÃ∏=iaijyj,\\nwhere yiis OOD rate in US county i. The social and spatial proximity weights ( wijandaij) are defined\\nas follows:\\nwij=njSCIijP\\nkÃ∏=inkSCIik,and aij=1 +1\\ndijP\\nkÃ∏=i(1 +1\\ndik),\\nwhere njis the population of county janddijis the distance between county locations iandj. Using\\n1 + 1 /dijinstead of 1 /dijin the definition of aijimproves numerical stability when dealing with long\\ndistances (large dijvalues) but does not change the decreasing nature of the spatial proximity weights\\nwith the increasing distances. To capture the effect of far away counties that would be otherwise down-\\nweighted heavily by 1 /dij, in Supplementary Information section S5 we repeat our analysis with a slowly\\ndecaying distance function (1 /d1/10\\nij) and observe that our main conclusions remain unchanged and are\\nnot sensitive to the choice of the distance decay function. To visualize the social and spatial adjacency\\nweights across the United States, we aggregate the county-level data to the state level. This aggregation\\nallows us to effectively depict the dense networks. We give the details of the state level aggregation in\\nSupplementary Information section S2. It is important to note that our analysis of estimating the effect\\nof ‚Äúdeaths in social proximity‚Äù on OODs is still conducted at the county level. The visualization of\\nthe proximity weights and proximity values, however, is presented at the state level to help understand\\nthe geographical dispersion of the social and spatial proximity weights in relation to OOD rates. For\\ncomparison, in Supplementary Information section S2 we give the county-level visualization of social\\nand spatial adjacency weights in Pennsylvania (PA). Figure 1A illustrates the state-level social network,\\nmeasured by the social proximity weights ( wxz) in the contiguous United States. Figures 1B and 1C show\\nthe spatial dispersion of the proximity weights for the socially and spatially lagged variables for two ego\\nstates, California (1B) and Pennsylvania (1C), in the contiguous US.\\nHaving formally introduced the socially and spatially lagged variables, we use mortality data from\\nthe National Center for Health Statistics (NCHS) for 2018-2019 to measure the state-level OOD rates.\\nFigure 2 shows the state-level spatial distributions of our main variables of interest in the contiguous\\nUS: OOD rates (Figure 2A), deaths in social proximity (Figure 2B), deaths in spatial proximity (Figure\\n2C), and their differences (Figure 2D). Figure 3 shows the scatter plots of death rates ( yi), deaths\\nin social proximity ( s‚àíi), and deaths in spatial proximity ( d‚àíi) for all counties in the contiguous US.\\nThe scatter plot matrix reveals a moderate linear dependence between death rates and spatially and\\nsocially lagged variables. In addition, there is a strong correlation between spatial and social proximity.\\nHowever, the histograms that represent the distribution of these two variables exhibit differences. This\\ncontrast helps us identify the spatial effects of social influence on OOD and estimate an effect size for\\nsocial proximity using controlled regressions described in the Results. Our choice of controls comes from\\ndomain knowledge consisting of clinical covariates and factors of social determinants of health (SDOH).\\nThe SDOH covariates are selected using the least absolute shrinkage and selection operator (LASSO)\\nfrom an array of 17 variables. The details of SDOH and clinical covariates are in Methods.\\n5Figure 1 : A) The spatial distribution of overdose death rates per 100,000 population in contiguous US\\nfrom 2018 to 2019. Superimposed on this map is a social network diagram with edge widths representing\\nthe state-level social proximity weights . B) The two middle maps show the social proximity weights of\\nalter states to California (on the left) and Pennsylvania (on the right). C) The bottom two maps show\\nthe spatial proximity weights of alter states to California (on the left) and Pennsylvania (on the right).\\nResults\\nEstimating the effect size of ‚ÄúDeaths in Social Proximity‚Äù variable\\nOur outcome of interest is the county-level OOD rates that we measure using NCHS data from 2018 to\\n2019. We use cluster-robust linear regression to estimate the coefficient of the socially lagged variable.\\nRobust estimators and clustering by state help us correct for correlation among counties, which might\\nbe higher for counties in the same state than between different states. Figure 4 shows the significant\\n6Figure 2 : A) The top left map shows the spatial spread of state-level opioid overdose death rates in\\nthe contiguous US. B) The top right map shows the spatial dispersion of ‚Äúdeaths in social proximity‚Äù\\nfor states in the contiguous US. C) The bottom left map shows the geographical spread of ‚Äúdeaths in\\nspatial proximity‚Äù. D) The bottom right map shows the difference between deaths in social and spatial\\nproximity from top right and bottom left maps.\\ncoefficient of the socially lagged variable and provides statistical evidence for the effect of social networks\\non the spatial spread of OOD rates. The estimate of the effect size of the socially lagged variables is\\nstatistically significant across the eastern, western-central, and the entire contiguous US. The positive\\ncoefficient aligns with the theoretical proposition of the literature on the importance of social influence\\nin the opioid epidemic [5]. The positive and significant magnitude of this effect size can originate from\\nthe dissemination of information through social networks about the initiation of use and availability of\\nsubstances, leading to more OODs. The effect sizes for s‚àíiandd‚àíi, derived from the cluster-robust\\nstandard error model, along with other covariates, are in Supplementary Tables S2, S3, and S4 for the\\neastern, western-central, and contiguous US.\\nWe standardize s‚àíiandd‚àíiprior to regression analysis. Consequently, the effect size for s‚àíiindicates\\nthat an increase of one standard deviation, equal to 11 .69523 and 12 .2417 more ‚Äúdeaths in social prox-\\nimity‚Äù per 100 ,000 population in the contiguous and eastern United States, respectively, is associated\\nwith an increase of nine deaths per 100 ,000 population in ego counties within the eastern United States\\nand thirteen deaths per 100 ,000 population in the contiguous United States. For counties in the western\\nand central United States, a similar increase of one standard deviation, equal to 5 .7145 more deaths per\\n100,000 population in the social proximity of ego counties, corresponds to six more deaths per 100 ,000\\npopulation in ego counties. Despite observing a significant effect for the socially lagged variable on OOD\\nrates, it is crucial to address potential issues arising from the inherent nature of the primary variable of\\ninterest. We recognize the methodological challenges due to correlated residuals when using statistical\\nmodels to analyze social and spatial effects and perform robustness checks using network and spatial\\nautocorrelation, as well as fixed-effects models to substantiate our findings.\\nGiven that lag variables have a social and spatial component, we expect the error terms in our\\nregression model to be correlated. To address this, we implemented a spatial error model (SEM) to test\\n7Figure 3 : The figure shows the distribution and relationships between the primary variables of interest\\n(death rates yi, deaths in social proximity s‚àíi, and deaths in spatial proximity d‚àíi). The histograms on\\nthe main diagonal depict the distributions of yi,s‚àíi, and d‚àíi. Moving to the upper triangle, we observe\\nthe degree of linear dependence between these variables, while the lower triangle displays scatter plots.\\nand correct for network and spatial autocorrelations in error terms. The autocorrelation in error terms\\narises from unobserved factors shared among spatially or socially connected units. Cluster-robust linear\\nregression may fail to capture these autocorrelation stemming from unobserved factors, which may lead\\nto bias and inefficient estimates. Hence, to add robustness to our analysis, we utilize spatial and network\\naurocorrelation models. The methodological frameworks for the network and spatial autocorrelation\\nmodels are explained in Supplementary Information sections S4.2 and S4.3. Supplementary Tables S5,\\nS6 and S7 provide the SEM regression results for the network and spatial autocorrelation models for the\\neastern, western-central and contiguous United States, in which we find statistical evidence for correlated\\nerror. It is important to note that we performed two distinct models to test and correct for correlated\\nerror terms originating from both spatial and network dependence separately. Network autocorrelation\\nmight come from the structure of the socially lagged variable s‚àíi, while spatially correlated error terms\\ncould be attributed to the spatially lagged variable d‚àíi. Our autocorrelation models provide statistical\\nevidence for the significance of social proximity in OODs.\\nTo address spatial and temporal heterogeneity and enhance robustness, we employed a two-way fixed-\\neffects model. Specifically, we included state-fixed effects to control for unobserved state-specific char-\\nacteristics, such as regulations and policy environments, that are constant over time, but vary between\\nstates. We also incorporate year-fixed effects to absorb nationwide shocks or trends that could influence\\nour outcome of interest. This modeling strategy enables us to robustly assess the statistical significance\\nof ‚Äúdeaths in social proximity‚Äù while accounting for unobserved spatial and temporal confounders. The\\nresults of this model are in Supplementary Tables S8, S9 and S10 for the eastern, western-central and con-\\ntiguous United States. Our findings consistently demonstrate a significant positive effect size for ‚Äúdeaths\\n8deaths social proximitydeaths spatial proximity\\n0 5 0 5 10 15\\ndeaths per 100,000 peoplecluster robust linear regression\\nnetw ork autocorrelation\\nspatial autocorrelation\\ntwo-way fixed eff ect\\n0 5 10 15A B CFigure 4 : A) The plot shows the coefficient confidence interval plots for western and central US counties.\\nThe coefficient for s‚àíifor cluster-robust linear regression (Supplementary Table S3), network and spatial\\nautocorrelation (Supplementary Table S6), and two-way fixed effects model (Supplementary Table S9),\\nall indicate a positive, significant (p <0.001) coefficient for s‚àíi. B). Shows the coefficient plot for social\\nand spatial proximity for counties in the contiguous US. The coefficient for s‚àíifor cluster-robust linear\\nregression (Supplementary Table S4), network and spatial autocorrelation (Supplementary Table S7),\\nand two-way fixed effects models (Supplementary Table S10) are all positive and significant (p <0.001).\\nC) Shows the coefficient plot for s‚àíiandd‚àíifor counties in the eastern US. The coefficient for s‚àíifor\\ncluster-robust linear regression (Supplementary Table S2), network and spatial autocorrelation models\\n(Supplementary Table S5), and two-way fixed effects models (Supplementary Table S8) are all positive and\\nsignificant (p <0.001). The effect sizes for standardized s‚àíiin the cluster-robust linear regression models\\nindicate that a one-standard-deviation increase, equal to 11 .69523, 12 .2417, and 5 .7145 more deaths per\\n100,000 population in the social proximity of the ego counties in contiguous, eastern and western-central\\nUnited States, respectively, is associated with thirteen more deaths per 100 ,000 population in contiguous\\nand nine more deaths eastern and six more deaths in western-central US counties.\\nin social proximity‚Äù. For a detailed explanation of the two-way fixed effect model, refer to Supplementary\\nInformation section S4.4.\\nFigure 4 shows the confidence intervals (CI) for the cluster-robust standard error linear model, network\\nautocorrelation, spatial autocorrelation, and the two-way, fixed-effects models. We consistently observe\\na positive and significant coefficient for s‚àíi, indicating the effect of social influence on the spread of\\nOOD, while the effect size for d‚àíihas a varying CI, changes sign, and is not always significant. We\\nalso observe cluster-robust linear regression has a broader confidence interval compared to the other\\nmodels, it is primarily because cluster-robust standard errors adjust for the intra-cluster correlation by\\naccounting for the fact that there is less independent information than the total number of observations\\nsuggests. This adjustment often results in larger standard errors compared to conventional ones, reflecting\\nthe reduced amount of independent information. There is also a loss of statistical power when doing\\nthis analysis, as the effective sample size becomes closer to the number of clusters rather than the\\ntotal number of observations. However, in spite of accounting for intra-cluster correlation, we observe\\n9a statistically significant coefficient for ‚Äúdeaths in social proximity‚Äù. A key takeaway from this result is\\nthat social connections are predominantly more significant than the effect of spatial proximity on OOD\\nrates. This robustness check adds to the consistency of our statistical evidence for the size of the effect\\nof ‚Äúdeaths in social proximity‚Äù. In our analysis, given the network and spatial configurations inherent in\\ns‚àíiandd‚àíi, we suspected and addressed the correlated errors that stem from the endogeneity of both\\nvariables simultaneously, using a two-stage least squares approach. The results of our implementation are\\ndiscussed in the Supplementary Information section S4.5 and confirm the positive and significant effect\\nsize of our social proximity variable in the eastern, western-central and entire contiguous United States\\n(Supplementary Figure S3 and Supplementary Table S11). To end our analysis, we show the robustness\\nof the effect of ‚Äúdeaths in social proximity‚Äù by accounting for the rate at which spatial adjacency weights\\ndecay with increasing distance. By accounting for the ‚Äúdeaths in spatial proximity‚Äù decay rate, we allow\\nthe effect of distant counties far from the focal county to be more pronounced. We use cluster-robust\\nlinear regression to test the significance of the coefficient s‚àíi. We observe a statistically significant\\neffect for the coefficient of ‚Äúdeaths in social proximity‚Äù between counties in the eastern, western-central,\\nand contiguous United States. The results of this implementation are discussed in the Supplementary\\nInformation Section S5. The confidence interval plot for the model is illustrated in Supplementary Figure\\nS4 and the effect sizes are shown in Supplementary Tables S12, S13, and S14.\\nDiscussion\\nOur research underscores that social influence, measured by the coefficient of s‚àíi, exhibits a statistically\\nsignificant impact on the spatial spread of OOD rates in US counties. This finding paves the way for a\\nmore in-depth exploration of the mechanisms driving the opioid epidemic, specifically within the social\\ncontagion framework. Harmon et al. (2015) [28] and Braha and Aguiar (2017) [12] have utilized complex\\ngenerative modeling to understand social contagion in the context of economic crises and voting behavior.\\nBuilding on their work in modeling social contagion, future studies can adopt a similar framework to model\\npeer influence in the opioid epidemic using SCI data.\\nOur analysis demonstrates that deaths in social proximity, as measured by the SCI, are associated\\nwith opioid overdose deaths in counties in the United States. Although previous studies recognize the\\ntendency for OODs to occur in isolated spaces [29, 30], data from the CDC‚Äôs State Unintentional Drug\\nOverdose Reporting System (SUDORS)1indicate that almost half of overdose deaths had a potential\\nbystander present, suggesting that the occurrence of OODs in isolation may be less pronounced than\\npreviously thought.\\nFurthermore, our measure of social influence is based on the social connections derived from the\\nsocial network structure of Facebook‚Äôs SCI across US counties and serves as a proxy for aggregated\\nfriendship network structures and their association with OODs; however, it does not capture the strength\\nof individual friendship ties among Facebook users in the counties. Focusing on these social connections\\nand their relation to the spatial patterns of OODs, such as geographic discordance (the community in\\nwhich overdose death occurs being different from the community of residence), can provide novel insights\\ninto the complex interplay of social and spatial factors in perpetuating this public health crisis.\\nIn addressing the opioid epidemic, accounting for social networks is crucial in understanding and\\npredicting OOD patterns at the community level to aid policy intervention. Studies using complex gen-\\nerative models with spatial clustering have yielded valuable insight into the dynamics of social influence\\nand can be crucial to designing targeted interventions. For example, Braha and Aguiar (2017) [12] use\\nsuch models to identify spatial clusters of voting contagion. In the context of the opioid epidemic, Liao\\net al. (2024) [31] introduce the Spatio-TEMporal Mutually Exciting Point Process (STEMMED) model\\nto quantify the interconnections between historical and future events, reflecting space-time clustering in\\n1https://www.cdc.gov/overdose-prevention/data-research/facts-stats/sudors-dashboard-fatal-overdose-data.\\nhtml\\n10OODs across various communities. This methodological approach can improve the prediction of OOD\\ntrends within localized settings by modeling unique community-specific OOD event streams, consider-\\ning spatial and social influences. Our research complements this perspective by bridging the gap in\\nunderstanding the role of social influence within these dynamics. Although STEMMED captures the\\nspatiotemporal clustering of OOD events, our approach highlights the importance of social structures in\\nshaping these patterns. By estimating the coefficient of the socially lagged variable, we measure the effect\\nof social interactions in different counties in the US, which in conjunction with the spatial focus of Liao et\\nal. [31] can offer a more comprehensive view of the factors driving OOD clusters. For policymakers, this\\ncomprehensive approach can provide a solid foundation for designing targeted interventions that address\\nboth the spatial and social dimensions of the opioid epidemic.\\nOne of our motivations for this study was to facilitate the creation of proxy networks for social\\ninteractions in agent-based models of the US population based on SCI data. Agent-based models with\\nSCI-calibrated social networks can provide valuable information about peer effects in the initiation of\\nopioid use and the development of use disorder. When used in conjunction with powerful agent-based\\nmodeling tools such as FRED (Framework for Reconstructing Epidemic Dynamics) [32], such models can\\nenable epidemiologists and policymakers to simulate the spread of the opioid epidemic in great mechanistic\\ndetail and evaluate interventions. Chu et al. (2020) use FRED to study social contagion in the use of\\ne-cigarettes [33], showing the potential of FRED to simulate social contagion of addictive behaviors. We\\nexpect that SCI-calibrated social networks, combined with agent-based models of OUD progression in\\nFRED (cf., e.g., [34]), can reveal important mechanistic details about the opioid epidemic in the US. Our\\nresults may be particularly useful for calibrating social contagion parameters. The contagion parameters\\ncan be tuned so that they reflect the same association between ‚Äúdeaths in social proximity‚Äù and OODs\\nas we measure in the mortality data.\\nSCI can also help design and evaluate intervention strategies based on social networks. Macmadu\\net al. (2022) highlight the need for interventions tailored to social networks [35]. Their research shows\\nthat after an overdose, network members exhibit observable behavioral changes, including increased risk\\ntaking to manage feelings of bereavement and trauma, protective actions, and some cases showing no\\nchange in drug use behavior. Based on these findings, opioid reduction efforts can be optimized in areas\\nwith greater social connectivity. Measurement of the effects of social networks across locations can also\\nhelp optimize the allocation of limited medications, such as naloxone, buprenorphine, and methadone. In\\nall these cases, social network-based strategies offer important opportunities for policy makers to mitigate\\nthe spread of the opioid epidemic.\\nWe acknowledge the limitations of using mortality data to measure the spread of this critical health\\ncrisis and propose to strengthen these findings with other OUD-related outcomes. Details on the limi-\\ntations of the mortality data are discussed in the Methods. Future work can use SCI to investigate the\\nsocial contagion of the opioid epidemic in mechanistic agent-based models and structural causal models\\nfor causal discovery and inference. We also plan to use SCI to improve our predictive accuracy in detect-\\ning OOD hotspots in the US. We have used SCI to measure the dynamics of social networks and their\\neffect on the opioid epidemic. Our choice of SCI is guided by existing literature and our objective to study\\nnetwork effects at the population scale. We recognize the limitations of using SCI because Facebook users\\nare not the same as the general population. SCI only acts as a proxy for real-life friendship networks, and\\nit may not be a true representative of real-life friendship connections and their association with opioid\\nuse behavior. Other datasets that capture real-world interactions within a social network framework can\\nalso be valuable to understand the effects of social networks in the opioid epidemic. For example, the\\nAdd Health dataset2, which includes information on best friends during high school and opioid misuse in\\nadulthood (14 years later), offers an alternative means of measuring these effects. Although this dataset\\nallows analysis within the age-specific bracket of 25 to 35 years, potential problems of population selection\\nbias may limit the generalizability of the findings to the broader population.\\n2https://data.cpc.unc.edu/projects/2/view#public_li\\n11Methods\\nData pipeline and preprocessing\\nOur data sources are shown in Figure 5 and explained in detail in the following paragraphs.\\nNCHS\\nMortality \\nData\\nIQVIA \\nXponent\\nDatabase\\nAHRQ\\nNFLIS\\nMeta\\nNaloxone, \\nODR, \\nand \\nBuprenorphine\\nFentanyl \\nand \\nanalogs \\nseizure \\ndata\\n \\nSocial \\nConnectedness \\nIndex\\nopioid \\noverdose \\ndeaths\\nclinical \\nand \\nfentanyl \\ncovariates\\nSDOH \\n+ \\nLASSO \\nSelection\\nSimple \\nFeatures\\nfor \\nR\\nShape \\nFile\\nsocio-economic\\ncovariates\\nsocial\\nproximity\\nspatial \\nproximity \\nand \\npopulation \\ndensity\\nModel \\nData \\nFrame\\nCluster-Robust \\nStandard \\nError \\nLinear \\nModel\\nNetwork \\nAutocorrelation\\nSpatial \\nAutocorrelation\\nTwo-way \\nfixed \\neffect \\nModel\\nNCHS: \\nNational \\nCenter \\nfor \\nHealth \\nStatistics\\nNFLIS: \\nNatioanl \\nForensic \\nLaboratory \\nInformation \\nSystem\\nAHRQ: \\nAgency \\nfor \\nHealth \\nResearch \\nand \\nQuality\\nODR: \\nOpioid \\nDispensing \\nRate\\nSDOH: \\nSocial \\nDeterminant \\nof \\nHealth\\n2016 \\nGeneral \\nElection\\n \\nResults \\nby \\nCounty \\npolitical \\naffiliation\\nCounty \\nHealth \\nRankings \\nand \\nRoadmaps\\nCounty \\nHealth \\nRanking\\nfrequent \\nmental \\nhealth \\ndistress\\nFigure 5 : The diagram depicts the data pipeline for our analysis, including the data streams for the\\nprimary variable of interests, s‚àíiandd‚àíi, as well as the relevant socioeconomic and clinical covariates.\\nIt also outlines our regression models for estimating the effect of peer influence as measured by SCI on\\ncounty-level OOD outcomes.\\nMortality data and census demographics. We measure the OOD rate from mortality data ob-\\ntained from the National Center for Health Statistics (NCHS) for the years 2018-2019. This data set\\nincludes demographic details of individuals who have lost their lives to opioid-related overdoses. To first\\nidentify overdose related deaths we utilise the following International Classification of Disease (ICD)\\ncodes ‚ÄúX40‚Äù,‚ÄúX41‚Äù, ‚ÄúX42‚Äù, ‚ÄúX43‚Äù, ‚ÄúX44‚Äù,‚ÄúX60‚Äù, ‚ÄúX61‚Äù, ‚ÄúX62‚Äù,‚ÄúX63‚Äù, ‚ÄúX64‚Äù, ‚ÄúX85‚Äù, ‚ÄúY10‚Äù, ‚ÄúY11‚Äù,\\n‚ÄúY12‚Äù, ‚ÄúY13‚Äù, ‚ÄúY14‚Äù. The X and Y codes provide information about deaths that have occurred due to\\nsubstance overdose. Furthermore, to specifically target opioid overdose deaths we use the ICD T codes\\n‚ÄúT400‚Äù, ‚ÄúT401‚Äù, ‚ÄúT402‚Äù, ‚ÄúT403‚Äù, ‚ÄúT404‚Äù, and ‚ÄúT406‚Äù. The T codes determine the cause of death in the\\nspecification of opioids from other substances. Despite the comprehensive nature of the NCHS mortality\\ndata and the use of specific ICD codes to identify opioid overdose deaths, there are important limitations\\nassociated with this dataset. Death certificates may not always specify the drugs involved in an over-\\ndose, and some overdose deaths involve multiple drugs, making it difficult to determine which substance\\nwas primarily responsible. In addition, the analysis also incorporates demographic data on the broader\\npopulation of the United States of America. The data is stratified at the county levels. To extract this\\ninformation, we utilize the R package ‚Äútidycensus‚Äù to systematically retrieve data from data.census.gov.\\n12Clinical and mental health covariates. Clinical factors such as the opioid dispensing rate (ODR),\\navailability of naloxone, and access to buprenorphine for the treatment of opioid use disorder are used as\\ncontrols in our regression. Morgan et al. (2018) [36] underscore the role of naloxone in reducing opioid-\\nrelated harm, while Pendergrass et al. (2019) [37] emphasize the importance of buprenorphine availability\\nin mitigating overdose fatalities. The data source for clinical factors such as naloxone, buprenorphine,\\nand ODR is the IQVIA Xponent database, which provides the number of prescriptions for naloxone and\\nbuprenorphine distributed throughout counties in the US through retail pharmacy channels. The ODR,\\nwhich captures the total number of opioid doses dispensed, is measured as morphine milligram equivalents\\n(MME). The current wave of the opioid epidemic from 2013 to the present has witnessed an increase in\\nthe illicit use of synthetic opioids such as fentanyl. Furthermore, Kuehn (2023) [38] discuses the impact\\nof fentanyl on overdose deaths, particularly among adolescents. Pergolizzi et al. (2018) [39] also discuss\\nthe role of fentanyl in exacerbating the opioid epidemic. To control for the supply of illegal substances\\nthat contribute to OODs, we incorporate state-level fentanyl and analog seizure data from the National\\nForensic Laboratory Information System as a control variable. Our selection of clinical and illicit-supply\\ncovariates is comprehensive based on the data sources available to us. In addition, several studies have\\nshown associations between mental health and opioid overdose deaths [40, 41, 42]. Thus, we used data on\\nfrequent mental health distress from County Health Rankings and Roadmaps (CHRR). Frequent mental\\ndistress is the percentage of adults who reported poor mental health for more than 14 days in response\\nto the question ‚ÄúHow many days during the past 30 days was your mental health not good?‚Äù [43]. We\\nuse this measure to control for the effect of mental health-related issues in counties.\\nPopulation density and political affiliation. We also control for population density and political\\naffiliation that are identified in the literature on the opioid epidemic and the structure of social networks.\\nTo account for the heterogeneity associated with SCI and opioid use among populations residing in urban\\nand rural counties, we include population density in our regression model [15, 44]. The risk status of\\nopioid misuse is also associated with the political affiliations and liberal status of states [45]. Therefore, we\\ncontrol for the effect of political affiliation in our regression by accounting for counties‚Äô political leanings\\nusing 2016 general election data at the county level.\\nSCI data. SCI is available at the ZIP code and county levels in the United States. We chose counties\\nas our analysis unit instead of ZIP codes because the latter had significant limitations. A considerable\\nnumber of ZIP codes have missing SCI data. Typically, ZIP codes without SCI data are those with low\\npopulations or those designated exclusively for institutions. Institutional ZIP codes are assigned to areas\\npredominantly occupied by specific institutions, such as hospitals, universities, or military bases. These\\ninstitutional ZIP codes can also introduce spatial bias; for example, ZIP codes with hospitals are more\\nlikely to report higher overdose death rates. To ensure a more continuous and representative spatial\\nframework, we use counties as our unit of analysis. This approach mitigates the issues of missing data\\nand spatial bias, providing a more robust basis for analysis.\\nSocial determinants of health. Socioeconomic factors shape social structures and ties; therefore,\\nincorporating these factors is essential to interpret the influence of social networks on OOD. Shared\\nsocioeconomic conditions can also influence behaviors that mirror peer influence, and therefore, we include\\nsocioeconomic covariates as controls in our analysis to mitigate the risk of bias due to missing confounders\\nwhen estimating the effect size of our deaths in the social proximity variable ( s‚àíi). Social determinants of\\nhealth (SDOH) encompass aspects of physical infrastructure, economic context, healthcare context, and\\nsocial environment of counties. Our selection of SDOH covariates is based on socioeconomic predictors\\nof the opioid epidemic. Liu et al. (2023) [46] demonstrate the effects of SDOH measures on drug\\noverdose death locations. Therefore, we use SDOH variables to control for socioeconomic factors that\\nmay confound the impact of s‚àíion county-level OOD rates. These covariates are selected from a pool\\n13of SDOH variables using the Least Absolute Shrinkage and Selection Operator (LASSO) technique to\\navoid multicollinearity issues. We select a subset of covariates from an array of 17 SDOH variables\\nthat are listed in Supplementary Table S1. We provide details of the LASSO selection process in the\\nSupplementary Information section S3.\\nStatistical models. The model coefficients are evaluated for statistical significance at p <0.05 level.\\nUsing county-level opioid overdose death rates as the outcome variable, we set up regression models to\\ntest the significance of deaths in social proximity to explain the county-level death rate after controlling\\nfor deaths in spatial proximity, clinical (mental health, availability of naloxone and buprenorphine and\\nopioid dispensing rates in pharmacies) and fentanyl covariates, and socioeconomic covariates selected\\nusing LASSO from SDOH variables, as well as population density and political affiliation covariates.\\nLinear regression and cluster-robust linear regression are analyzed for statistical significance (details in\\nSupplementary Information section S4.1). The residuals in the regression are weighted by population\\nsize of the counties, so that the inference is reflecting the population residing in the counties across\\nthe US. To account for correlated error terms from the spatial and network structure of the variables\\nd‚àíiands‚àíi, respectively, we use spatial and network autocorrelation models (details in Supplementary\\nInformation sections S4.3 and S4.2). To account for unobserved space- and time-invariant characteristics\\nthat might be associated with the covariates in our regression, we use a two-way fixed-effects model.\\nThis model includes fixed temporal effects for 2018 and 2019 and fixed spatial effects for states in the\\neastern United States, the western and central United States, and the contiguous United States (details\\nin Supplementary Information section S4.4). As a robustness check, we also use a two-stage least squares\\nregression to account for correlated error terms that arise from the simultaneous estimation of effects\\nfrom s‚àíiandd‚àíi(details in the Supplementary Information section S4.5). As a final robustness check,\\nwe use distance decay weight to account for the effect of faraway counties from the focal county (details\\nin the Supplementary Information section S5).\\nAcknowledgements\\nThe project is supported by the contract 75D30121C12574 from the Centers for Disease Control and Pre-\\nvention (CDC). The findings and conclusions in this work are those of the authors and do not represent\\nthe official position of the CDC. Along with this, the University of Pittsburgh Center for Research Com-\\nputing, RRID: SCR 022735, has supported our work. This work utilizes the H2P cluster, supported by\\nNSF award number OAC-2117681. The funders had no role in study design, data collection and analysis,\\ndecision to publish or preparation of the manuscript. The authors also acknowledge the contributions of\\nAlexander Hayes, Nisha Natraj, William Wei Wang, and Kun Zhang for their valuable discussions and\\nconstructive feedback.\\nData Availability\\nMortality data was obtained from the National Center for Health Statistics (NCHS). Due to confiden-\\ntiality concerns, this data set is not publicly accessible, but can be requested from NCHS at https:\\n//www.cdc.gov/nchs/nvss/nvss-restricted-data.htm . The clinical covariates were sourced from the\\nIQVIA Xponent database, which is also not publicly available. Access can be requested through IQVIA\\nathttps://www.iqvia.com/insights/the-iqvia-institute/available-iqvia-data . Data on illicit\\nfentanyl-related drugs were obtained from the National Forensic Laboratory Information System (NFLIS)\\nand can be accessed at https://www.nflis.deadiversion.usdoj.gov/ . Data on frequent mental\\nhealth distress are obtained from the County Health Rankings and Roadmaps (CHRR) at https://www.\\ncountyhealthrankings.org/sites/default/files/media/document/analytic_data2019 . The 2016\\n14General Election data can be accessed at https://raw.githubusercontent.com/tonmcg/US_County_\\nLevel_Election_Results_08-20/master/2016_US_County_Level_Presidential_Results . The social\\ndeterminants of health covariates are available from the Agency for Healthcare Research and Quality\\n(AHRQ) at https://www.ahrq.gov/sdoh/data-analytics/sdoh-data.html . The SCI data can be ac-\\ncessed using the Facebook Data for Good tools at https://dataforgood.facebook.com/dfg/tools/\\nsocial-connectedness-index . Data on drug overdose deaths used in our LASSO covariate selection\\nare available from the Bureau of Health Statistics and Registries of the Department of Health of the\\nCommonwealth of Pennsylvania and can be requested as follows: https://www.health.pa.gov/topics/\\nDocuments/Reporting-Registries/VR-Govt-Researchers/Application.%20for%20Access%20to%20Protected%\\n20Data%20for%20Public%20Health%20Researchers%20-%20User%27s%20Guide.pdf .\\nCode Availability\\nAnalysis code to reproduce figures and tables in the paper is available at https://github.com/kut97/\\nood-sci\\nReferences\\n[1] H. Jalal, J. M. Buchanich, M. S. Roberts, L. C. Balmert, K. Zhang, and D. S. Burke, ‚ÄúChanging\\ndynamics of the drug overdose epidemic in the United States from 1979 through 2016,‚Äù Science ,\\nvol. 361, no. 6408, p. eaau1184, 2018.\\n[2] H. Jalal and D. S. Burke, ‚ÄúExponential growth of drug overdose poisoning and opportunities for\\nintervention,‚Äù Addiction , vol. 117, no. 5, pp. 1200‚Äì1202, 2022.\\n[3] A. Alpert, W. N. Evans, E. M. Lieber, and D. Powell, ‚ÄúOrigins of the opioid crisis and its enduring\\nimpacts,‚Äù The Quarterly Journal of Economics , vol. 137, no. 2, pp. 1139‚Äì1179, 2022.\\n[4] T. Y. Lim, E. J. Stringfellow, C. A. Stafford, C. DiGennaro, J. B. Homer, W. Wakeland, S. L.\\nEggers, R. Kazemi, L. Glos, E. G. Ewing, et al. , ‚ÄúModeling the evolution of the US opioid crisis\\nfor national policy development,‚Äù Proceedings of the National Academy of Sciences , vol. 119, no. 23,\\np. e2115714119, 2022.\\n[5] B. J. Costello, B. J. Anderson, and M. Stein, ‚ÄúPeer influence in initiation to heroin use,‚Äù Journal of\\nDrug Issues , vol. 51, no. 2, pp. 323‚Äì339, 2021.\\n[6] K. K. Rigg, K. McLean, S. M. Monnat, G. E. Sterner III, and A. M. Verdery, ‚ÄúOpioid misuse\\ninitiation: implications for intervention,‚Äù Journal of addictive diseases , vol. 37, no. 3-4, pp. 111‚Äì122,\\n2018.\\n[7] H. Guarino, P. Mateu-Gelabert, J. Teubl, and E. Goodbody, ‚ÄúYoung adults‚Äô opioid use trajectories:\\nFrom nonmedical prescription opioid use to heroin, drug injection, drug treatment and overdose,‚Äù\\nAddictive behaviors , vol. 86, pp. 118‚Äì123, 2018.\\n[8] H. Jalal, J. M. Buchanich, D. R. Sinclair, M. S. Roberts, and D. S. Burke, ‚ÄúAge and generational\\npatterns of overdose death risk from opioids and other drugs,‚Äù Nature medicine , vol. 26, no. 5,\\npp. 699‚Äì704, 2020.\\n[9] J. L. Syvertsen, C. E. Paquette, and R. A. Pollini, ‚ÄúDown in the valley: Trajectories of injection\\ninitiation among young injectors in california‚Äôs central valley,‚Äù International Journal of Drug Policy ,\\nvol. 44, pp. 41‚Äì49, 2017.\\n15[10] T. W. Valente, A. Ritt-Olson, A. Stacy, J. B. Unger, J. Okamoto, and S. Sussman, ‚ÄúPeer acceleration:\\neffects of a social network tailored substance abuse prevention program among high-risk adolescents,‚Äù\\nAddiction , vol. 102, no. 11, pp. 1804‚Äì1815, 2007.\\n[11] E. J. Stringfellow, T. Y. Lim, K. Humphreys, C. DiGennaro, C. Stafford, E. Beaulieu, J. Homer,\\nW. Wakeland, B. Bearnot, R. K. McHugh, et al. , ‚ÄúReducing opioid use disorder and overdose deaths\\nin the United States: A dynamic modeling analysis,‚Äù Science Advances , vol. 8, no. 25, p. eabm8147,\\n2022.\\n[12] D. Braha and M. A. De Aguiar, ‚ÄúVoting contagion: Modeling and analysis of a century of US\\npresidential elections,‚Äù PLOS ONE , vol. 12, no. 5, p. e0177970, 2017.\\n[13] J. Homer and W. Wakeland, ‚ÄúA dynamic model of the opioid drug epidemic with implications for\\npolicy,‚Äù The American Journal of Drug and Alcohol Abuse , vol. 47, no. 1, pp. 5‚Äì15, 2021.\\n[14] M. Bailey, R. Cao, T. Kuchler, J. Stroebel, and A. Wong, ‚ÄúSocial connectedness: Measurement,\\ndeterminants, and effects,‚Äù Journal of Economic Perspectives , vol. 32, no. 3, pp. 259‚Äì80, 2018.\\n[15] M. Bailey, P. Farrell, T. Kuchler, and J. Stroebel, ‚ÄúSocial connectedness in urban areas,‚Äù Journal of\\nUrban Economics , vol. 118, p. 103264, 2020.\\n[16] M. Bailey, D. Johnston, T. Kuchler, D. Russel, B. State, and J. Stroebel, ‚ÄúThe determinants of social\\nconnectedness in europe,‚Äù in Social Informatics: 12th International Conference, SocInfo 2020, Pisa,\\nItaly, October 6‚Äì9, 2020, Proceedings 12 , pp. 1‚Äì14, Springer, 2020.\\n[17] J. Coven, A. Gupta, and I. Yao, ‚ÄúJUE Insight: Urban flight seeded the COVID-19 pandemic across\\nthe United States,‚Äù Journal of Urban Economics , vol. 133, p. 103489, 2023.\\n[18] T. Kuchler, D. Russel, and J. Stroebel, ‚ÄúJUE insight: The geographic spread of COVID-19 corre-\\nlates with the structure of social networks as measured by facebook,‚Äù Journal of Urban Economics ,\\nvol. 127, p. 103314, 2022.\\n[19] Y. Ge, S. Das, K. O‚ÄôConnor, M. A. Al-Garadi, G. Gonzalez-Hernandez, and A. Sarker, ‚ÄúReddit-\\nimpacts: A named entity recognition dataset for analyzing clinical and social effects of substance\\nuse derived from social media,‚Äù arXiv preprint arXiv:2405.06145 , 2024.\\n[20] S. Pandrekar, X. Chen, G. Gopalkrishna, A. Srivastava, M. Saltz, J. Saltz, and F. Wang, ‚ÄúSocial\\nmedia based analysis of opioid epidemic using reddit,‚Äù in AMIA Annual Symposium Proceedings ,\\nvol. 2018, p. 867, American Medical Informatics Association, 2018.\\n[21] D. Balsamo, P. Bajardi, A. Salomone, and R. Schifanella, ‚ÄúPatterns of routes of administration\\nand drug tampering for nonmedical opioid consumption: data mining and content analysis of reddit\\ndiscussions,‚Äù Journal of Medical Internet Research , vol. 23, no. 1, p. e21212, 2021.\\n[22] A. Diemer and T. Regan, ‚ÄúNo inventor is an island: social connectedness and the geography of\\nknowledge flows in the us,‚Äù Research Policy , vol. 51, no. 2, p. 104416, 2022.\\n[23] K. M¬® ackle and S. Ruenzi, ‚ÄúFriends with drugs: The role of social networks in the opioid epidemic,‚Äù\\nAvailable at SSRN , 2022.\\n[24] D. M. Cutler and J. T. Donahoe, ‚ÄúThick market externalities and the persistence of the opioid\\nepidemic,‚Äù tech. rep., National Bureau of Economic Research, 2024.\\n[25] A. Forati, R. Ghose, F. Mohebbi, and J. R. Mantsch, ‚ÄúThe journey to overdose: Using spatial social\\nnetwork analysis as a novel framework to study geographic discordance in overdose deaths,‚Äù Drug\\nand alcohol dependence , vol. 245, p. 109827, 2023.\\n16[26] N. Koram, H. Liu, J. Li, J. Li, J. Luo, and J. Nield, ‚ÄúRole of social network dimensions in the\\ntransition to injection drug use: actions speak louder than words,‚Äù AIDS and Behavior , vol. 15,\\npp. 1579‚Äì1588, 2011.\\n[27] Y. Xu, P. Santi, and C. Ratti, ‚ÄúBeyond distance decay: Discover homophily in spatially embedded\\nsocial networks,‚Äù Annals of the American Association of Geographers , vol. 112, no. 2, pp. 505‚Äì521,\\n2022.\\n[28] D. Harmon, M. Lagi, M. A. De Aguiar, D. D. Chinellato, D. Braha, I. R. Epstein, and Y. Bar-Yam,\\n‚ÄúAnticipating economic market crises using measures of collective panic,‚Äù PloS one , vol. 10, no. 7,\\np. e0131871, 2015.\\n[29] R. E. Gicquelais, B. L. Genberg, J. L. Maksut, A. S. Bohnert, and A. C. Fernandez, ‚ÄúPrevalence and\\ncorrelates of using opioids alone among individuals in a residential treatment program in michigan:\\nimplications for overdose mortality prevention,‚Äù Harm reduction journal , vol. 19, no. 1, p. 135, 2022.\\n[30] Y. Li, H. J. Miller, E. D. Root, A. Hyder, and D. Liu, ‚ÄúUnderstanding the role of urban social and\\nphysical environment in opioid overdose events using found geospatial data,‚Äù Health & Place , vol. 75,\\np. 102792, 2022.\\n[31] C.-Y. Liao, G.-G. Garcia, K. Paynabar, Z. Dong, Y. Xie, and M. S. Jalali, ‚ÄúTides need stemmed:\\nA locally operating spatio-temporal mutually exciting point process with dynamic network for im-\\nproving opioid overdose death prediction,‚Äù arXiv preprint arXiv:2211.07570 , 2022.\\n[32] J. J. Grefenstette, S. T. Brown, R. Rosenfeld, J. DePasse, N. T. Stone, P. C. Cooley, W. D. Wheaton,\\nA. Fyshe, D. D. Galloway, A. Sriram, et al. , ‚ÄúFred (a framework for reconstructing epidemic dynam-\\nics): an open-source software system for modeling infectious diseases and control strategies using\\ncensus-based populations,‚Äù BMC Public Health , vol. 13, pp. 1‚Äì14, 2013.\\n[33] K.-H. Chu, A. Shensa, J. B. Colditz, J. E. Sidani, B. L. Hoffman, D. Sinclair, M. G. Krauland, and\\nB. A. Primack, ‚ÄúIntegrating social dynamics into modeling cigarette and e-cigarette use,‚Äù Health\\nEducation & Behavior , vol. 47, no. 2, pp. 191‚Äì201, 2020.\\n[34] A. A. Ahmed, M. A. Rahimian, and M. S. Roberts, ‚ÄúEstimating treatment effects using costly\\nsimulation samples from a population-scale model of opioid use disorder,‚Äù in 2023 IEEE EMBS\\nInternational Conference on Biomedical and Health Informatics (BHI) , pp. 1‚Äì4, IEEE, 2023.\\n[35] A. Macmadu, L. Frueh, A. B. Collins, R. Newman, N. P. Barnett, J. D. Rich, M. A. Clark, and\\nB. D. Marshall, ‚ÄúDrug use behaviors, trauma, and emotional affect following the overdose of a\\nsocial network member: a qualitative investigation,‚Äù International Journal of Drug Policy , vol. 107,\\np. 103792, 2022.\\n[36] J. Morgan and A. L. Jones, ‚ÄúThe role of naloxone in the opioid crisis,‚Äù Toxicology Communications ,\\nvol. 2, no. 1, pp. 15‚Äì18, 2018.\\n[37] S. A. Pendergrass, R. C. Crist, L. K. Jones, J. R. Hoch, and W. H. Berrettini, ‚ÄúThe importance of\\nbuprenorphine research in the opioid crisis,‚Äù Molecular Psychiatry , vol. 24, no. 5, pp. 626‚Äì632, 2019.\\n[38] B. M. Kuehn, ‚ÄúFentanyl drives startling increases in adolescent overdose deaths,‚Äù JAMA , vol. 329,\\nno. 4, pp. 280‚Äì281, 2023.\\n[39] J. V. Pergolizzi Jr, J. A. LeQuang, R. Taylor Jr, R. B. Raffa, and N. R. Group, ‚ÄúGoing beyond\\nprescription pain relievers to understand the opioid epidemic: the role of illicit fentanyl, new psy-\\nchoactive substances, and street heroin,‚Äù Postgraduate Medicine , vol. 130, no. 1, pp. 1‚Äì8, 2018.\\n17[40] A. T. Dinwiddie, ‚ÄúReported non-substance-related mental health disorders among persons who died\\nof drug overdose‚ÄîUnited States, 2022,‚Äù MMWR. Morbidity and Mortality Weekly Report , vol. 73,\\n2024.\\n[41] T. Santo Jr, G. Campbell, N. Gisev, D. Martino-Burke, J. Wilson, S. Colledge-Frisby, B. Clark, L. T.\\nTran, and L. Degenhardt, ‚ÄúPrevalence of mental disorders among people with opioid use disorder:\\nA systematic review and meta-analysis,‚Äù Drug and Alcohol Dependence , vol. 238, p. 109551, 2022.\\n[42] J. van Draanen, C. Tsang, S. Mitra, V. Phuong, A. Murakami, M. Karamouzian, and L. Richardson,\\n‚ÄúMental disorder and opioid overdose: a systematic review,‚Äù Social Psychiatry and Psychiatric\\nEpidemiology , pp. 1‚Äì25, 2022.\\n[43] T.-C. Yang, S. A. Matthews, F. Sun, and M. Armendariz, ‚ÄúModeling the importance of within- and\\nbetween-county effects in an ecological study of the association between social capital and mental\\ndistress,‚Äù Preventing Chronic Disease , vol. 16, p. E75, June 2019.\\n[44] D. J. Peters, S. M. Monnat, A. L. Hochstetler, and M. T. Berg, ‚ÄúThe opioid hydra: Understanding\\noverdose mortality epidemics and syndemics across the rural-urban continuum,‚Äù Rural Sociology ,\\nvol. 85, no. 3, pp. 589‚Äì622, 2020.\\n[45] R. L. Haffajee, L. A. Lin, A. S. Bohnert, and J. E. Goldstick, ‚ÄúCharacteristics of US counties with\\nhigh opioid overdose mortality and low capacity to deliver medications for opioid use disorder,‚Äù\\nJAMA network open , vol. 2, no. 6, pp. e196373‚Äìe196373, 2019.\\n[46] M. Liu, J. M. Caplan, L. W. Kennedy, I. K. Moise, D. J. Feaster, V. E. Horigian, J. M. Roll, S. M.\\nMcPherson, and J. S. Rao, ‚ÄúGeo-spatial risk factor analysis for drug overdose death in south florida\\nfrom 2014 to 2019, and the independent contribution of social determinants of health,‚Äù Drug and\\nalcohol dependence , vol. 248, p. 109931, 2023.\\n[47] B. Khanal, ‚ÄúRole of social connectedness in response to a public health crisis: The case study of the\\nflint water crisis,‚Äù Available at SSRN 3969415 , 2021.\\n[48] B. Charoenwong, A. Kwan, and V. Pursiainen, ‚ÄúSocial connections with COVID-19‚Äìaffected areas\\nincrease compliance with mobility restrictions,‚Äù Science Advances , vol. 6, no. 47, p. eabc3054, 2020.\\n[49] D. Holtz, M. Zhao, S. G. Benzell, C. Y. Cao, M. A. Rahimian, J. Yang, J. Allen, A. Collis,\\nA. Moehring, T. Sowrirajan, et al. , ‚ÄúInterdependence and the cost of uncoordinated responses to\\nCOVID-19,‚Äù Proceedings of the National Academy of Sciences , vol. 117, no. 33, pp. 19837‚Äì19843,\\n2020.\\n[50] M. Bailey, D. Johnston, M. Koenen, T. Kuchler, D. Russel, and J. Stroebel, ‚ÄúSocial networks shape\\nbeliefs and behavior: Evidence from social distancing during the COVID-19 pandemic,‚Äù Journal of\\nPolitical Economy Microeconomics , vol. 2, no. 3, pp. 000‚Äì000, 2024.\\n[51] A. K. Basu, N. H. Chau, and O. Firsin, ‚ÄúSocial connections and COVID-19 vaccination,‚Äù in Discus-\\nsion Paper Series , Institute of Labor Economics (IZA), 2023.\\n[52] B. Vahedi, M. Karimzadeh, and H. Zoraghein, ‚ÄúSpatiotemporal prediction of COVID-19 cases us-\\ning inter-and intra-county proxies of human interactions,‚Äù Nature Communications , vol. 12, no. 1,\\np. 6440, 2021.\\n[53] R. Chetty, M. O. Jackson, T. Kuchler, J. Stroebel, N. Hendren, R. B. Fluegge, S. Gong, F. Gonzalez,\\nA. Grondin, M. Jacob, et al. , ‚ÄúSocial capital ii: determinants of economic connectedness,‚Äù Nature ,\\npp. 1‚Äì13, 2022.\\n18[54] R. Chetty, M. O. Jackson, T. Kuchler, J. Stroebel, N. Hendren, R. B. Fluegge, S. Gong, F. Gon-\\nzalez, A. Grondin, M. Jacob, et al. , ‚ÄúSocial capital I: measurement and associations with economic\\nmobility,‚Äù Nature , vol. 608, no. 7921, pp. 108‚Äì121, 2022.\\n[55] C. M. Cutter, R. C. Larson, and M. Abir, ‚ÄúSocial network theory‚Äîan underutilized opportunity to\\nalign innovative methods with the demands of the opioid epidemic,‚Äù The American Journal of Drug\\nand Alcohol Abuse , vol. 47, no. 3, pp. 305‚Äì310, 2021.\\n[56] N. Dasgupta, L. Beletsky, and D. Ciccarone, ‚ÄúOpioid crisis: no easy fix to its social and economic\\ndeterminants,‚Äù American Journal of Public Health , vol. 108, no. 2, pp. 182‚Äì186, 2018.\\n[57] J. Friedman, T. Hastie, and R. Tibshirani, ‚ÄúRegularization paths for generalized linear models via\\ncoordinate descent,‚Äù Journal of Statistical Software , vol. 33, no. 1, p. 1, 2010.\\n[58] L.-f. Lee and X. Liu, ‚ÄúEfficient GMM estimation of high order spatial autoregressive models with\\nautoregressive disturbances,‚Äù Econometric Theory , vol. 26, no. 1, pp. 187‚Äì230, 2010.\\n19Supplementary Information\\nContents\\nS1 Additional related work S1\\nS2 Social and spatial proximity at the state level and across counties in PA S2\\nS3 Covariate selection S2\\nS4 Robustness checks S5\\nS4.1 Linear regression with cluster-robust standard error correction . . . . . . . . . . . . . . . S5\\nS4.2 Network autocorrelation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S9\\nS4.3 Spatial autocorrelation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S9\\nS4.4 Two-way fixed-effect model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S13\\nS4.5 Two-stage least squares estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S16\\nS5 Robustness check on the spatial weight matrix S19\\nS1 Additional related work\\nSCI data has been used in different social settings. For example, in counties where people have more\\nconcentrated social networks within the counties than outside, they also tend to have worse socioeconomic\\noutcomes. Specifically, these counties exhibit lower average income, lower levels of education, higher rates\\nof teenage births, lower life expectancy, less social capital, and reduced social mobility [14]. Diemer and\\nRegan (2021) use SCI data to establish informal social interaction and highlight where social learning and\\nknowledge flow might occur independently of geographical constraints [22]. In public health applications,\\nBinod Khanal (2021) uses SCI to explain the variability of bottled water as avoidance behavior in the\\nUnited States during the Flint water crisis [47]. Charonweng et al. (2020) use SCI to show how social\\nnetworks are conduits of information about the pandemic and an economically important factor that\\naffects compliance with and impact of mobility restrictions [48]. The findings of their work also suggest\\nthat SCI is likely to be a good proxy for real-life social connections. SCI data has also been used\\nto understand better public health policies, for example, by studying the social network spillovers of\\nregional lockdown policies during the pandemic [49]. Others have used SCI data to show the effect of\\nsocial connections on beliefs and behaviors toward social distancing [50] and vaccination [51]. The results\\nof Basu et al. [51] highlight that increased vaccination rates among Facebook friends correlate with higher\\nvaccination rates in the individual‚Äôs county. Vahedi et al. (2021) show the utility of SCI in spatio-temporal\\nmodels to predict hot spots and incidence rates of COVID-19 at the county level in the United States [52].\\nIn all these recent works, SCI is a primary tool for understanding real-life social connections in various\\ndomains and acts as an essential tool to represent both economic and social interactions. As indicated\\nin the analysis by Chetty et al.(2022) [53, 54], SCI serves as a proxy for friendship links that mediate\\neconomic mobility [53] and affect other outcomes ranging from education to health [54]. Following this\\ngrowing literature, we use SCI as a proxy for real-life social connections and not as merely a metric of\\nonline connections. Through our analysis, we argue that the usage of SCI is not limited to economic\\noutcomes and can be used to model intricate human networks that underlie public health crises such as\\nthe opioid epidemic.\\nS1S2 Social and spatial proximity at the state level and across\\ncounties in PA\\nFigures 1 and 2 display the state-level dispersion of the proximity weights and variables of interest. We\\nformally define the aggregated state-level social and spatial proximity weights as follows:\\nwxz=P\\ni‚ààxP\\nj‚ààzwijP\\nz‚Ä≤Ã∏=xP\\ni‚ààxP\\nj‚ààz‚Ä≤wij,and axz=1 +1\\ndxzP\\nmÃ∏=x(1 +1\\ndxm),\\nwhere wijis the social proximity weights (defined in the main text) between county iin state xand county\\njin state z,z‚Ä≤represents the set of states excluding state xanddxzrepresent the distance between states\\nxandz. Similarly, we calculate ‚Äúdeaths in social proximity‚Äù and ‚Äúdeaths in spatial proximity‚Äù at the\\nstate level as follows:\\ns‚àíx=X\\nzÃ∏=xwxzyz,and d‚àíi=X\\nzÃ∏=xaxzyz,\\nwhere yzis the death rate per 100 ,000 in state z. It is important to note that our analysis of the effect of\\n‚Äúdeaths in social proximity‚Äù on overdose deaths is conducted at the county level. Due to the unavailability\\nof state-level Social Connectedness Index (SCI) measures, we aggregated the social proximity weights for\\nvisualization of state-level variables in Figures 1 and 2. Using state-level outcomes in these figures allows\\nus to observe how the aggregated county units contribute to differences across states. This approach also\\naids in visualizing the network layout, which is otherwise infeasible given the density of the connectivity\\nbetween counties.\\nFor comparison, we also visualize the layout of social networks and the dispersion of proximity weights\\nbetween counties in Pennsylvania (PA). Figure S1A illustrates the social network at the county level\\nmeasured by their social proximity weights ( wij) in PA counties. Figures S1B and S1C show the spatial\\ndispersion of the proximity weights for the socially and spatially lagged variables for two ego counties,\\nAllegheny (S1B) and Philadelphia (S1C). In Figure S2, we show the spatial distributions of our primary\\nvariables of interest: the OOD rates, s‚àíi,d‚àíi, and their differences ( s‚àíi‚àíd‚àíi), in PA counties.\\nS3 Covariate selection\\nThe opioid epidemic is a complex interplay between social networks and socioeconomic status. Studies\\nhave shown that prescription opioids became a means of establishing and maintaining social capital\\nwithin social networks in low-socioeconomic-status communities [55]. Similarly, Gupta et al. (2018) [56]\\nunderscore the relationship between the opioid epidemic and its association with socioeconomic factors.\\nTo control the influence of socioeconomic factors on ‚Äúdeaths in social proximity‚Äù, we used data on social\\ndeterminants of health (SDOH) available from the Agency for Healthcare Research and Quality (AHRQ).\\nThis data set includes information on the physical and health infrastructure and socioeconomic indicators\\nfor each county in the United States. We further select the most relevant SDOH covariates using the\\nLeast Absolute Shrinkage and Selection Operator (LASSO) to avoid overfitting and multicollinearity in\\nour model. To incorporate domain knowledge variables informed by literature, we specifically use the\\npartially penalized LASSO regression [57].\\nThe LASSO objective function for the model we utilize for the analysis:\\nminimize\\nŒ≤\\uf8ee\\n\\uf8f01\\n2nnX\\ni=1(yi‚àípX\\nj=1xijŒ≤j)2+ŒªpX\\nj=1œÄj|Œ≤j|\\uf8f9\\n\\uf8fb\\nS2Supplementary Figure S1 : A) The spatial distribution of overdose death rates in 67 counties in\\nPennsylvania (PA) in 2018 and 2019. The map highlights the higher proportion of opioid overdose death\\nrate in Philadelphia County. Superimposed on this map is a social network diagram with edge widths\\nrepresenting the social proximity weights ( wij). B) The two middle maps show the social proximity\\nweights of alter counties to Allegheny County (on the left) and Philadelphia County (on the right). C)\\nThe bottom two maps show the spatial proximity weights of alter counties to Allegheny County (on the\\nleft) and Philadelphia County (on the right).\\nHere Œ≤is the vector of regression coefficients, xis the design matrix containing SDOH variables, œÄ\\nis an indicator variable for the penalized factor, if œÄj= 0 then the corresponding covariates are not\\nreduced to zero ensuring that they stay in the model and if œÄj= 1 their corresponding covariates are\\nsubject to shrinkage, enabling variable selection. We use this LASSO framework to ensure that covariates\\ninformed by domain knowledge remain in the regression model, while the LASSO selection operates on\\nother SDOH covariates. Specifically, variables such as the Opioid Dispensing Rate (ODR), naloxone\\navailability, buprenorphine availability, population density, mental health distress rate, political affiliation,\\nand median household income are considered crucial based on domain expertise and are therefore not\\npenalized. The remaining covariates listed in Supplementary Table S1 are subject to selection through\\nS3Supplementary Figure S2 : A) The top left map shows the spatial spread of opiod overdose death rates\\nacross PA counties. B) The top right map shows the spatial dispersion of death in social proximity ( s‚àíi)\\nfor PA counties. C) The bottom left map shows the geographical spread of ‚Äúdeaths in spatial proximity‚Äù\\n(d‚àíi) for PA counties. D) The bottom right map shows the difference between deaths in social and spatial\\nproximity ( s‚àíi‚àíd‚àíi) in PA counties.\\nthe LASSO method. In the model yis the vector of the response variable ‚Äúdeath rates‚Äù aggregated\\nover four years (2013-2017) for counties in Pennsylvania. The data source is from the Commonwealth\\nof Pennsylvania Department of Health, Bureau of Health Statistics and Registries. The data source\\nlacks information on ‚ÄòT codes‚Äô; consequently, the LASSO selection for SDOH covariates utilizes overall\\noverdose death rates instead of specifically targeting opioid overdose death rates. Here, nis the number\\nof observations and pis the number of predictors. Performing selection using a different dataset and\\noutcome variable helps avoid post-selection inference issues, ensuring the confidence interval derived\\nfrom our models has a valid interpretation. The bias introduced by the L1penalty term forces some\\nregression coefficients to shrink to zero. The zero coefficients are removed from the model, which serves\\nas a way to perform variable selection. To estimate the optimal value of the tuning parameter Œª, we\\nperformed a k-fold cross-validation ( k= 6), using the mean squared error (MSE) performance metric.\\nSupplementary Table S1 shows the set of coefficient estimates of SDOH variables after selection of\\nLASSO. The selected variables indicate their significance for the death rates. Thus, this Supplementary\\nTable collectively provides a comprehensive understanding of the influential variables that play a sig-\\nnificant role in OODs. To isolate the effect of ‚Äúdeaths in social proximity‚Äù on opioid overdose deaths,\\nwe used the variables selected by LASSO regression (see Table S1) as controls in our model. We ex-\\ncluded state-level data on the count of illicit opioids reported from the variable selection pool because\\nthis measure lacks variation within Pennsylvania counties; consequently, LASSO would automatically\\ndrop this variable even with penalized regression. In Supplementary Table S1, a ‚Äú‚Äì‚Äù indicates variables\\nthat were not selected by the LASSO regression. When analyzing the National Center for Health Statis-\\ntics (NCHS) data to estimate the effect of ‚Äùdeaths in social proximity‚Äù on OODs, we included only the\\ncovariates selected by the LASSO, along with state-level control. It is important to note that in our anal-\\nysis, counties lacking reported values for clinical covariates, specifically the availability of naloxone and\\nbuprenorphine, are assumed to have zero values. This assumption is based on the observation that these\\nS4counties recorded negligible overdose deaths during the years 2018-2019 and have very small populations.\\nIn the next section, we discuss the specifics of the robustness checks conducted in our study.\\nSupplementary Table S1 : Columns one to four list SDOH variable names, their descriptions, LASSO\\nmodel coefficients (- values are dropped post-LASSO), and the corresponding covariates in our models.\\nVariable Description Coefficient Covariates\\n1 ACS TOT POP POV Total population for whom\\npoverty status is determined- poverty status\\n2 AMFAR MHFAC RATE Total number of facilities that\\nprovide mental health services\\nper 1,000 population- mental health service rate\\n3 ACS PCT HUNOVEH Percentage of housing units\\nwith no vehicle available- percentage no vehicle\\n4 ACS PCT LTHS Percentage of population with\\nless than high school education\\n(ages 25 and over)- education level (below high school)\\n5 POS MEAN DIST ALC Mean distance to nearest hos-\\npital with alcohol/drug abuse\\ncare, using pop-weighted cen-\\ntroids in county‚àí7.518093 √ó10‚àí5mean distance to hospital\\n6 ACS PCT OTHER INS Percentage with other health\\ninsurance coverage- health insurance coverage\\n7 CCBP TOT BWLSTORES Rate Number of beer, wine, liquor\\nstores per 1,000 population- alcohol stores per 1,000 Pop\\n8 AHRF TOT COM HEALTH GRANT Total community health cen-\\nters, grantees only- total community health centers\\n9 ACS PCT UNEMPLOY percentage unemployed (civil-\\nian labor force, ages 16+)6.249815 √ó10‚àí5percentage unemployed\\n10 ACS MEDIAN HHINC median household income 8 .190151 √ó10‚àí5median household income\\n11 ACS MEDIAN AGE Median age of population - median age\\n12 ACS PCT WHITE Percentage of population\\nwhite- percentage White\\n13 ACS PCT BLACK Percentage of population black - percentage Black\\n14 ACS PCT AIAN Percentage American Indian\\nand Alaska Native‚àí2.546380 √ó10‚àí5percentage AIAN\\n15 ACS PCT NHPI Percentage Native Hawaiian\\nand Other Pacific Islander1.307321 √ó10‚àí5percentage NHPI\\n16 ACS PCT MULT Race Percentage two or more races - percentage multi-race\\n17 ACS PCT ASIAN Percentage of population\\nAsian- percentage Asian\\n18 ODR Opioid Dispensing Rate 1 .993769 √ó10‚àí4ODR\\n19 Naloxone Available Availability of naloxone 7 .221718 √ó10‚àí4naloxone available\\n20 buprenorphine Available Availability of buprenorphine 6 .462634 √ó10‚àí3buprenorphine available\\n21 population density Population per square kilome-\\nters‚àí1.385327 √ó10‚àí8population density\\n22 frequent mental health distress Rate of frequent mental health\\ndistress3.000371 √ó10‚àí3mental health distress rate\\n23 political affiliation political affiliation measure 2 .921279 √ó10‚àí4political affiliation\\nS4 Robustness checks\\nAs robustness checks, we set up linear regression, spatial and network autocorrelation, two-way fixed-\\neffect models, two-stage least squares estimates. These steps help us ensure consistent reporting and\\nfurther bolster our claims regarding the significance of our socially lagged variable.\\nS4.1 Linear regression with cluster-robust standard error correction\\nAs a first robustness check model, we utilize a simple linear regression as shown in the equation:\\nyi=Œ≤0+Œ≤1s‚àíi+Œ≤2d‚àíi+Œ≤3TCi+Œ≤4TXi+œµi\\nHere, Œ≤1is the coefficient for ‚Äúdeaths in social proximity‚Äù, Œ≤2is the coefficient for ‚Äúdeaths in spatial\\nproximity‚Äù, Œ≤3Tdenotes the vector of the coefficients associated with clinical covariates, and Œ≤4Tis the\\nvector of the coefficient associated with SDOH variables. The model allows us to estimate the effect of\\nour socially lagged variable on death rates.\\nTo ensure that the inference is reflecting the population residing in the counties across the US, we\\nweight the residuals by population size. Weighing the residuals also enhances the ability of the model to\\nS5capture the heterogeneity between different spatial units. In our simple linear model, we ensure that we\\naccount for the clustering of standard errors using cluster-robust standard error correction.\\nSupplementary Tables S2, S3, and S4 illustrate the results of the linear regression model for the eastern,\\nwestern-central, and contiguous United States. The socially lagged variable remains significant. However,\\nwe lose the statistical significance for ‚Äúdeaths in spatial proximity‚Äù when the geographical focus shifts\\nfrom counties in the eastern US to counties in the contiguous US. Our previous discussion has highlighted\\nthe underlying spatial and network structure of our variable of interest. Spatial and social variables are\\noften susceptible to correlated error terms because of their autocorrelated construction. Hence, as a\\nmitigating measure, we use the spatial error model to account for network and spatial autocorrelation in\\nthe model. In the next section, we discuss the network autocorrelation model.\\nSupplementary Table S2 :Linear regression for measuring the significance of the effect size of ‚Äúdeaths in social\\nproximity‚Äù, for counties in the eastern United States\\nDependent variable: death rate per 100,000 people\\nOLS Cluster-Robust OLS\\n(1) (2)\\ndeaths social proximity 9.187‚àó‚àó‚àó9.187‚àó‚àó‚àó\\n(0.978) (2.414)\\ndeaths spatial proximity 5.462‚àó‚àó‚àó5.462‚àó‚àó‚àó\\n(0.921) (2.021)\\nODR 54.106‚àó‚àó‚àó54.106‚àó‚àó\\n(12.571) (23.513)\\nnaloxone available 162.003‚àó‚àó‚àó162.003‚àó\\n(17.037) (85.602)\\nbuprenorphine available 42.685‚àó‚àó‚àó42.685\\n(13.212) (40.431)\\nstate count illicit opioid reported 6,303.967‚àó‚àó6,303.967\\n(3,193.753) (8,982.550)\\npopulation density ‚àí0.001‚àó‚àó‚àó‚àí0.001‚àó\\n(0.0002) (0.001)\\nmental health distress rate 182.710‚àó‚àó182.710\\n(77.001) (185.534)\\npolitical affiliation ‚àí3.649‚àó‚àó‚àó‚àí3.649\\n(1.340) (2.706)\\npercentage unemployed 68.805‚àó‚àó‚àó68.805‚àó‚àó\\n(9.677) (29.286)\\nmean distance to hospital ‚àí72.700‚àó‚àó‚àó‚àí72.700‚àó‚àó‚àó\\n(7.093) (11.279)\\nmedian household income ‚àí24.389‚àó‚àó‚àó‚àí24.389\\n(6.898) (15.699)\\npercentage AIAN ‚àí41.111 ‚àí41.111‚àó\\n(35.741) (23.164)\\npercentage NHPI ‚àí15.308 ‚àí15.308\\n(12.245) (13.041)\\nconstant 2.770 2.770\\n(10.378) (26.515)\\nobservations 1,606\\nR20.464\\nadjusted R20.460\\nresidual std. error 3.179 (df = 1590)\\nF statistic 94.409‚àó‚àó‚àó(df = 14; 1590)\\nNote:‚àóp<0.1;‚àó‚àóp<0.05;‚àó‚àó‚àóp<0.01\\nS6Supplementary Table S3 :Linear regression for measuring the significance of the effect size of ‚Äúdeaths in social\\nproximity‚Äù, for counties in the western and central United States\\nDependent variable: death rate per 100,000 people\\nOLS Cluster-Robust OLS\\n(1) (2)\\ndeaths social proximity 5.768‚àó‚àó‚àó5.768‚àó‚àó‚àó\\n(0.340) (0.637)\\ndeaths spatial proximity 0.128 0.128\\n(0.528) (1.873)\\nODR 17.101‚àó‚àó‚àó17.101‚àó‚àó\\n(3.847) (8.698)\\nnaloxone available 69.584‚àó‚àó‚àó69.584‚àó‚àó\\n(10.675) (30.149)\\nbuprenorphine available 43.633‚àó‚àó‚àó43.633‚àó‚àó\\n(8.586) (19.757)\\nstate count illicit opioid reported 6,468.861 6,468.861\\n(4,288.239) (9,546.581)\\npopulation density 0.006‚àó‚àó‚àó0.006‚àó‚àó‚àó\\n(0.0005) (0.001)\\nmental health distress rate 58.461 58.461\\n(38.647) (147.098)\\npolitical affiliation 0.578 0.578\\n(0.723) (2.070)\\npercentage unemployed ‚àí6.495 ‚àí6.495\\n(6.179) (9.489)\\nmean distance to hospital ‚àí15.183‚àó‚àó‚àó‚àí15.183\\n(3.604) (10.792)\\nmedian household income ‚àí4.777 ‚àí4.777\\n(3.467) (10.669)\\npercentage AIAN ‚àí7.516 ‚àí7.516\\n(6.795) (12.271)\\npercentage NHPI ‚àí2.114 ‚àí2.114\\n(3.862) (6.128)\\nconstant 0.926 0.926\\n(5.349) (22.011)\\nobservations 1,502\\nR20.459\\nadjusted R20.454\\nresidual std. error 1.306 (df = 1486)\\nF statistic 90.049‚àó‚àó‚àó(df = 14; 1486)\\nNote:‚àóp<0.1;‚àó‚àóp<0.05;‚àó‚àó‚àóp<0.01\\nS7Supplementary Table S4 :Linear regression for measuring the significance of the effect size of ‚Äúdeaths in social\\nproximity‚Äù, for counties in the contiguous United States.\\nDependent variable: death rate per 100,000 people\\nOLS Cluster-Robust OLS\\n(1) (2)\\ndeaths social proximity 12.503‚àó‚àó‚àó12.503‚àó‚àó‚àó\\n(0.706) (2.163)\\ndeaths spatial proximity 1.132‚àó1.132\\n(0.648) (1.526)\\nODR 49.014‚àó‚àó‚àó49.014‚àó‚àó‚àó\\n(8.187) (18.479)\\nnaloxone available 153.300‚àó‚àó‚àó153.300‚àó‚àó‚àó\\n(11.252) (57.614)\\nbuprenorphine available 56.048‚àó‚àó‚àó56.048\\n(10.038) (41.411)\\nstate count illicit opioid reported 11,396.620‚àó‚àó‚àó11,396.620\\n(2,391.373) (8,899.337)\\npopulation density 0.0003‚àó0.0003\\n(0.0002) (0.001)\\nmental health distress rate 21.961 21.961\\n(45.829) (119.935)\\npolitical affiliation ‚àí3.589‚àó‚àó‚àó‚àí3.589‚àó\\n(0.829) (1.893)\\npercentage unemployed 40.118‚àó‚àó‚àó40.118‚àó\\n(6.755) (23.199)\\nmean distance to hospital ‚àí50.508‚àó‚àó‚àó‚àí50.508‚àó‚àó‚àó\\n(5.560) (12.930)\\nmedian household income ‚àí17.588‚àó‚àó‚àó‚àí17.588\\n(4.330) (10.696)\\npercentage AIAN ‚àí17.954 ‚àí17.954\\n(10.987) (13.417)\\npercentage NHPI ‚àí9.727‚àó‚àí9.727\\n(5.689) (10.393)\\nconstant 9.849 9.849\\n(6.194) (16.294)\\nobservations 3,108\\nR20.502\\nadjusted R20.500\\nresidual std. error 1.899 (df = 3092)\\nF Statistic 222.608‚àó‚àó‚àó(df = 14; 3092)\\nNote:‚àóp<0.1;‚àó‚àóp<0.05;‚àó‚àó‚àóp<0.01\\nS8S4.2 Network autocorrelation\\nAutocorrelation, in essence, refers to the tendency of a variable to be correlated with itself at different\\npoints in space or time. In the context of our study, we anticipate encountering correlated error terms\\nwhen analyzing death rates related to social proximity. This likelihood comes from the inherent structural\\ncharacteristics and the defining attributes of our variable of interest. Thus, we utilize the Spatial Error\\nModel (SEM) to account for spatial and network autocorrelation. SEM is a specific linear regression\\nmodel that accounts for spatial dependence in error terms. The so-called network autocorrelation model\\nfor our study is defined by, yi=Œ≥0+Œ≥1s‚àíi+Œ≥2d‚àíi+Œ≥3TCi+Œ≥4TXi+ui, where Œ≥1represents the\\ncoefficient associated with ‚Äúdeaths in social proximity‚Äù, Œ≥2illustrates the coefficient size related to ‚Äúdeaths\\nin spatial proximity‚Äù, Œ≥3Tdenotes the vector of the coefficient associated with clinical covariates, and\\nŒ≥4Tbe the vector of the coefficient associated with SDOH variables. The error term uis assumed to\\nbe spatially autocorrelated, which means that geographically close observations are more likely to have\\nsimilar error terms than distant observations .\\nTo account for autocorrelation in error terms, the SEM assumes that error terms can be decomposed\\ninto two components: an autocorrelated error term uand a noise error term œµ, where œµ‚àºN(0, œÉ2I).\\nThe autocorrelated error term is modeled as a linear combination of the error terms of neighboring\\nobservations , weighted by a spatial weight matrix W. The error model is given by:\\nui=ŒªX\\njwijuj+œµi.\\nIn the weight matrix W, each element\\nwij=njSCIijP\\nkÃ∏=inkSCIik,\\nwhere Œªis a parameter that captures the strength of network autocorrelation, and Wcalculates the\\nweight for the connection between two areas i and j in a network. The noise in error term œµis assumed\\nto be independently and identically distributed (IID) with mean zero and variance œÉ2. To estimate\\nthe model, we utilize the spatialreg package in R, specifically employing the errorsarlm function.\\nThe function implements maximum-likelihood estimation for spatial simultaneous autoregressive error\\nmodels. The parameter Œªis initially estimated using the optimize function. Subsequently, the Œ≤and\\nother parameters are estimated using generalized least squares. Same estimation method is used when\\naccounting for spatial autocorrelation model.\\nThe findings of the network autocorrelation model are shown in the Supplementary Tables S5, S6 and\\nS7 for the eastern, western-central and contiguous United States. To further solidify our analysis, we\\naccount for spatial dependence employing spatial autocorrelation.\\nS4.3 Spatial autocorrelation\\nFrom a methodological perspective, the network and spatial autocorrelation share many similarities.\\nHowever, a key difference is induced through the definition of weights. The weights are designed to\\nencapsulate the underlying network structure in a network autocorrelation model. In contrast, in the\\ncase of spatial autocorrelation, the weights are formulated to capture the geographical dependencies.\\nThe error model is defined as follows, where aijis the spatial weights,\\nui=ŒªX\\njaijuj+œµi,\\naij=1 +1\\ndijP\\nkÃ∏=i(1 +1\\ndik).\\nS9It is important to note that the equations are used similarly for both the network and the spatial\\nautocorrelation models. However, within the scope of this study, the only distinction between the two\\nmodels is the definition of these weights. Supplementary Tables S5, S6 and S7 show the results of\\nthe autocorrelation model for the eastern, western-central, and contiguous United States. This result\\nreinforces the validity of our findings derived from simple linear regression and supports the significance\\nobtained for s‚àíi. The models we have considered thus far account for heterogeneity across spatial\\nboundaries but do not fully capture variations across space and time. Therefore, as a final robustness\\ncheck, we will incorporate a two-way fixed effect model into our analysis to better capture these nuances.\\nSupplementary Table S5 :Autocorrelation Models for the counties in the eastern United States\\nDependent variable: death rate per 100,000 people\\nNetwork Spatial\\n(1) (2)\\ndeaths social proximity 18.791‚àó‚àó‚àó13.314‚àó‚àó‚àó\\n(0.644) (0.894)\\ndeaths spatial proximity ‚àí2.335‚àó‚àó‚àó1.296\\n(0.624) (1.035)\\nODR 47.971‚àó‚àó‚àó53.261‚àó‚àó‚àó\\n(6.504) (7.209)\\nnaloxone available 31.139‚àó‚àó‚àó31.797‚àó‚àó‚àó\\n(10.486) (11.219)\\nbuprenorphine available 7.028 17.375‚àó‚àó‚àó\\n(5.426) (6.631)\\nstate count illicit opioid reported ‚àí1,945.999 ‚àí130.128\\n(1,756.263) (2,477.706)\\npopulation density 0.001‚àó‚àó0.002‚àó‚àó\\n(0.001) (0.001)\\nmental health distress rate ‚àí160.796‚àó‚àó‚àó‚àí89.597\\n(42.584) (56.496)\\npolitical affiliation ‚àí6.124‚àó‚àó‚àó‚àí4.957‚àó‚àó‚àó\\n(1.136) (1.330)\\npercentage unemployed 7.073 6.073\\n(4.791) (5.503)\\nmean distance to hospital ‚àí19.737‚àó‚àó‚àó‚àí26.527‚àó‚àó‚àó\\n(3.362) (3.983)\\nmedian household income ‚àí22.393‚àó‚àó‚àó‚àí27.317‚àó‚àó‚àó\\n(5.596) (7.087)\\npercentage AIAN 47.620‚àó‚àó‚àó45.101‚àó‚àó‚àó\\n(13.005) (14.542)\\npercentage NHPI 1.963 4.928\\n(6.104) (6.411)\\nconstant 53.460‚àó‚àó‚àó43.923‚àó‚àó‚àó\\n(6.393) (8.606)\\nobservations 1,606 1,606\\nlog likelihood ‚àí6,931.226 ‚àí6,944.193\\nœÉ2322.701 333.249\\nŒª ‚àí0.81195 0.68115\\nAkaike Inf. Crit. 13,896.450 13,922.390\\nWald test (df = 1) 65.324‚àó‚àó‚àó11.167‚àó‚àó‚àó\\nLR Test (df = 1) 28.489‚àó‚àó‚àó2.554\\nNote:‚àóp<0.1;‚àó‚àóp<0.05;‚àó‚àó‚àóp<0.01\\nS10Supplementary Table S6 :Autocorrelation models for the counties in the western and central United States\\nDependent variable:\\ndeath rates\\nNetwork Spatial\\n(1) (2)\\ndeaths social proximity 7.284‚àó‚àó‚àó6.454‚àó‚àó‚àó\\n(0.405) (0.566)\\ndeaths spatial proximity 0.058 0.400\\n(0.473) (0.662)\\nODR 6.515‚àó‚àó‚àó6.862‚àó‚àó‚àó\\n(2.436) (2.559)\\nnaloxone available ‚àí1.754 ‚àí3.460\\n(5.690) (5.905)\\nbuprenorphine available 24.510‚àó‚àó‚àó30.127‚àó‚àó‚àó\\n(5.681) (6.021)\\nstate count illicit opioid reported 568.756 712.539\\n(2,520.982) (3,707.215)\\npopulation density 0.009‚àó‚àó‚àó0.010‚àó‚àó‚àó\\n(0.001) (0.002)\\nmental health distress rate ‚àí81.327‚àó‚àó‚àó‚àí54.738‚àó\\n(23.426) (30.530)\\npolitical affiliation ‚àí1.077 ‚àí0.716\\n(0.980) (1.093)\\npercentage unemployed 6.172‚àó5.312\\n(3.741) (4.075)\\nmean distance to hospital ‚àí1.382 ‚àí1.576\\n(2.355) (2.587)\\nmedian household income ‚àí7.234‚àó‚àó‚àí5.256\\n(3.648) (4.007)\\npercentage AIAN 7.932‚àó‚àó6.295\\n(3.299) (4.160)\\npercentage NHPI ‚àí0.985 ‚àí0.638\\n(4.439) (4.760)\\nconstant 18.843‚àó‚àó‚àó14.717‚àó‚àó‚àó\\n(3.716) (4.621)\\nobservations 1,502 1,502\\nlog likelihood ‚àí5,860.207 ‚àí5,868.434\\nœÉ2142.595 144.851\\nŒª ‚àí0.54591 0.52449\\nAkaike Inf. Crit. 11,754.410 11,770.870\\nWald test (df = 1) 22.991‚àó‚àó‚àó3.504‚àó\\nLR test (df = 1) 18.083‚àó‚àó‚àó1.629\\nNote:‚àóp<0.1;‚àó‚àóp<0.05;‚àó‚àó‚àóp<0.01\\nS11Supplementary Table S7 :Autocorrelation models for counties in the contiguous United States\\nDependent variable: death rate per 100,000 people\\nNetwork Spatial\\n(1) (2)\\ndeaths social proximity 17.894‚àó‚àó‚àó14.517‚àó‚àó‚àó\\n(0.509) (0.671)\\ndeaths spatial proximity ‚àí3.386‚àó‚àó‚àó‚àí1.516‚àó\\n(0.481) (0.795)\\nODR 36.510‚àó‚àó‚àó39.822‚àó‚àó‚àó\\n(4.320) (4.596)\\nnaloxone available 21.914‚àó‚àó‚àó19.275‚àó‚àó‚àó\\n(6.056) (6.314)\\nbuprenorphine available 13.723‚àó‚àó‚àó26.807‚àó‚àó‚àó\\n(4.535) (5.338)\\nstate count illicit opioid reported 363.577 1,052.167\\n(1,446.788) (1,916.814)\\npopulation density 0.003‚àó‚àó‚àó0.003‚àó‚àó‚àó\\n(0.0005) (0.001)\\nmental health distress rate ‚àí124.751‚àó‚àó‚àó‚àí92.180‚àó‚àó‚àó\\n(23.424) (30.657)\\npolitical affiliation ‚àí4.311‚àó‚àó‚àó‚àí4.086‚àó‚àó‚àó\\n(0.790) (0.892)\\npercentage unemployed 1.457 1.790\\n(3.439) (3.787)\\nmean distance to hospital ‚àí6.814‚àó‚àó‚àó‚àí9.530‚àó‚àó‚àó\\n(2.585) (2.948)\\nmedian household income ‚àí12.265‚àó‚àó‚àó‚àí14.344‚àó‚àó‚àó\\n(3.653) (4.286)\\npercentage AIAN 20.064‚àó‚àó‚àó17.316‚àó‚àó‚àó\\n(3.815) (4.736)\\npercentage NHPI ‚àí0.048 1.835\\n(5.213) (5.787)\\nconstant 35.723‚àó‚àó‚àó32.721‚àó‚àó‚àó\\n(3.625) (5.584)\\nobservations 3,108 3,108\\nlog likelihood ‚àí13,021.780 ‚àí13,025.660\\nœÉ2253.641 255.359\\nŒª ‚àí0.58906 0.90999\\nAkaike Inf. Crit. 26,077.560 26,085.310\\nWald test (df = 1) 47.679‚àó‚àó‚àó214.894‚àó‚àó‚àó\\nLR test (df = 1) 24.566‚àó‚àó‚àó16.817‚àó‚àó‚àó\\nNote:‚àóp<0.1;‚àó‚àóp<0.05;‚àó‚àó‚àóp<0.01\\nS12S4.4 Two-way fixed-effect model\\nThe two-way fixed-effects model accounts for cross-sectional heterogeneity between counties in the US\\nand periods (year). It controls for any unobserved space- and time-invariant characteristics that may be\\ncorrelated with the covariates, furthering more accurate and unbiased estimates. Mathematically, we can\\nexpress the two-way fixed effect model by\\nyit=Œ∂0+Œ∂1s‚àíit+Œ∂2d‚àíit+Œ∂3TCit+Œ∂4TXit+¬µi+œït+œµit.\\nHere, ¬µiandœïtaccount for spatial and temporal heterogeneity. Supplementary Tables S8, S9 and\\nS10 show the result obtained from the two-way fixed effect model in the eastern, western-central and\\nthe entire contiguous US. After controlling for covariates from SDOH, ‚Äúdeaths in spatial proximity‚Äù, and\\nclinical covariates, we still witness statistically significant effect sizes for ‚Äúdeaths in social proximity.‚Äù\\nSupplementary Table S8 :Two-way fixed-effect model for counties in the eastern United States\\nDependent variable: death rate per 100,000 people\\ndeaths social proximity 3.366‚àó‚àó‚àó\\n(0.428)\\ndeaths spatial proximity 3.078‚àó‚àó‚àó\\n(0.784)\\nODR 16.025\\n(32.745)\\nnaloxone available 7,593.117‚àó‚àó‚àó\\n(748.879)\\nbuprenorphine available 0.002‚àó‚àó‚àó\\n(0.0001)\\nstate count illicit opioid reported ‚àí1,220.069\\n(3,935.194)\\npopulation density ‚àí0.001‚àó‚àó‚àó\\n(0.0001)\\nmental health distress rate 351.109‚àó‚àó‚àó\\n(35.178)\\npolitical affiliation 1.988‚àó‚àó‚àó\\n(0.475)\\npercentage unemployed 16.336‚àó‚àó‚àó\\n(3.852)\\nmean distance to hospital ‚àí23.773‚àó‚àó‚àó\\n(2.462)\\nmedian household income ‚àí7.408‚àó‚àó‚àó\\n(2.748)\\npercentage AIAN ‚àí33.687‚àó‚àó‚àó\\n(12.145)\\npercentage NHPI 0.112\\n(5.062)\\nobservations 3,212\\nR20.552\\nadjusted R20.546\\nresidual std. error 3,405.648 (df = 3170)\\nNote:‚àóp<0.1;‚àó‚àóp<0.05;‚àó‚àó‚àóp<0.01\\nS13Supplementary Table S9 :Two-way fixed-effect model for counties in the western and central United States\\nDependent variable: death rate per 100,000 people\\ndeaths social proximity 2.335‚àó‚àó‚àó\\n(0.193)\\ndeaths spatial proximity 0.011\\n(0.355)\\nODR 471.881‚àó‚àó‚àó\\n(54.213)\\nnaloxone available 3,058.806‚àó‚àó‚àó\\n(610.337)\\nbuprenorphine available 0.0002‚àó‚àó‚àó\\n(0.0001)\\nstate count illicit opioid reported ‚àí365.014\\n(11,282.540)\\npopulation density 0.003‚àó‚àó‚àó\\n(0.0002)\\nmental health distress rate 174.873‚àó‚àó‚àó\\n(25.425)\\npolitical affiliation ‚àí0.162\\n(0.275)\\npercentage unemployed 3.459\\n(2.526)\\nmean distance to hospital ‚àí6.863‚àó‚àó‚àó\\n(1.357)\\nmedian household income 8.489‚àó‚àó‚àó\\n(1.786)\\npercentage AIAN ‚àí23.412‚àó‚àó‚àó\\n(3.021)\\npercentage NHPI ‚àí1.874\\n(1.157)\\nobservations 3,004\\nR20.510\\nadjusted R20.504\\nresidual std. error 1625.446 (df = 2967)\\nNote:‚àóp<0.1;‚àó‚àóp<0.05;‚àó‚àó‚àóp<0.01\\nS14Supplementary Table S10 :Two-way fixed-effect model for counties in the contiguous United States\\nDependent variable: death rate per 100,000 people\\ndeaths social proximity 4.065‚àó‚àó‚àó\\n(0.317)\\ndeaths spatial proximity 1.741‚àó‚àó\\n(0.720)\\nODR 141.232‚àó‚àó‚àó\\n(26.094)\\nnaloxone available 8,212.523‚àó‚àó‚àó\\n(538.807)\\nbuprenorphine available 0.001‚àó‚àó‚àó\\n(0.0001)\\nstate count illicit opioid reported ‚àí101.839\\n(3,171.807)\\npopulation density ‚àí0.00004\\n(0.0001)\\nmental health distress rate 301.861‚àó‚àó‚àó\\n(24.263)\\npolitical affiliation 0.258\\n(0.304)\\npercentage unemployed 16.511‚àó‚àó‚àó\\n(2.750)\\nmean distance to hospital ‚àí16.874‚àó‚àó‚àó\\n(2.026)\\nmedian household income 6.457‚àó‚àó‚àó\\n(2.019)\\npercentage AIAN ‚àí30.523‚àó‚àó‚àó\\n(4.315)\\npercentage NHPI 0.343\\n(1.918)\\nobservations 6,216\\nR20.565\\nadjusted R20.560\\nresidual std. error 2,889.289 (df = 6152)\\nNote:‚àóp<0.1;‚àó‚àóp<0.05;‚àó‚àó‚àóp<0.01\\nS15S4.5 Two-stage least squares estimation\\nOur regression analysis identified two endogenous variables, s‚àíiandd‚àíi. Initially, we attempted to cor-\\nrect the correlation in error terms separately, employing network and spatial autocorrelation methods.\\nHowever, these approaches do not account for correlated error terms that emerge from the simultaneous es-\\ntimation of effects from socially and spatially lagged variables. To enhance the robustness of our findings,\\nwe adopt a two-stage least squares (2SLS) methodology. This approach effectively handles the correlated\\nerror terms associated with including endogenous variables in the regression model. The implementation\\nof this model proceeds in two stages. In the first stage, we regress s‚àíiandd‚àíion a matrix of instrumental\\nvariables, denoted by Q. This matrix is defined as Q= (Zn, WZ n, AZ n, W2Zn, A2Zn, WAZ n, AWZ n),\\nwhere Znis an n√ókmatrix. Here, nrepresents the total number of spatial units, and ksignifies the\\nnumber of covariates. Furthermore, WandAare the social and spatial weight matrices, each with di-\\nmensions n√ón. Our choice of instruments is based on the results of Lee et al. (2010) [58] for higher-order\\nlags.\\nThe equations for the first stage are given by:\\ns‚àíi=œâ0+œâ1TQi+us‚àíi,\\nd‚àíi=Œ¥0+Œ¥1TQi+ud‚àíi.\\nwere us‚àíiandud‚àíirepresent the error term for endogenous variable s‚àíiandd‚àíiandQirepresents the\\nset of instrumental variables for i-th county observation.\\nIn the second stage, the predicted values of s‚àíiandd‚àíiderived from the first stage are used to fit\\nthe final model for yi. The model is defined by:\\nyi=Œ±0+Œ±1ÀÜs‚àíi+Œ±2ÀÜd‚àíi+Œ±3TCi+Œ±4TXi+œµi.\\nSupplementary Figure S3 shows the CIs for our 2SLS estimates of s‚àíiandd‚àíieffect sizes in the\\neastern, western-central and the entire contiguous US. The estimated effect sizes for s‚àíiin all three\\nregions are significant and positive. Supplementary Table S11 provides the 2SLS estimation results and\\neffect sizes.\\nS16deaths social proximitydeaths spatial proximity\\n0 5 10 15\\ndeaths per 100,000 population2SLS Easter n United States\\n2SLS W estern‚àíCentr al United States\\n2SLS United StatesSupplementary Figure S3 : This figure shows the confidence intervals for the two-stage least squares\\nestimates, comparing the counties in the eastern, western, and entire contiguous United States. The\\neffect sizes for s‚àíiare statistically significant (p <0.001) for the three regions. The estimation results are\\nin Supplementary Table S11.\\nS17Supplementary Table S11 :This table shows the results of the two-stage least squares estimation, comparing the\\nimpact of social and spatial variables on overdose death rates in distinct geographical regions. Column (1) presents the\\nestimates for the counties in the eastern United States , column (2) for the counties in the western and central United\\nStates, and column (3) encompasses counties in the entire contiguous United States.\\nDependent variable: death rate per 100,000 people\\n(1) (2) (3)\\ndeaths social proximity 13.20017‚àó‚àó‚àó6.58148‚àó‚àó‚àó14.25920‚àó‚àó‚àó\\n(1.19966) (0.62021) (0.88471)\\ndeaths spatial proximity 1.45265 0.89995 ‚àí1.38995‚àó\\n(1.06666) (0.63798) (0.78416)\\nODR 52.22437‚àó‚àó‚àó6.17012‚àó‚àó38.43051‚àó‚àó‚àó\\n(7.53112) (2.57692) (4.80499)\\nnaloxone available 32.04615‚àó‚àó‚àó‚àí3.78452 21.51028‚àó‚àó‚àó\\n(11.78907) (5.97941) (6.64167)\\nbuprenorphine available 15.08753‚àó‚àó27.46304‚àó‚àó‚àó23.85361‚àó‚àó‚àó\\n(6.84219) (6.06131) (5.48752)\\nstate count illicit opioid reported ‚àí483.76320 860.05570 1,156.91900\\n(2,496.26400) (3,439.58200) (1,898.00800)\\npopulation density 0.00161‚àó‚àó0.00955‚àó‚àó‚àó0.00294‚àó‚àó‚àó\\n(0.00070) (0.00158) (0.00058)\\nmental health distress rate ‚àí104.88350‚àó‚àí75.99799‚àó‚àó‚àó‚àí111.36910‚àó‚àó‚àó\\n(57.89841) (28.63051) (29.82314)\\npolitical affiliation ‚àí5.34279‚àó‚àó‚àó‚àí0.47798 ‚àí4.48322‚àó‚àó‚àó\\n(1.38088) (1.09760) (0.92130)\\npercentage unemployed 7.35985 4.65552 2.53671\\n(5.72459) (4.06029) (3.92320)\\nmean distance to hospital ‚àí26.03972‚àó‚àó‚àó‚àí2.17998 ‚àí7.79723‚àó‚àó‚àó\\n(4.18689) (2.58041) (2.99407)\\nmedian household income ‚àí25.27024‚àó‚àó‚àó‚àí7.75322‚àó‚àí12.02835‚àó‚àó‚àó\\n(7.36773) (4.00888) (4.44148)\\npercentage AIAN 45.87866‚àó‚àó‚àó8.02707‚àó‚àó19.49589‚àó‚àó‚àó\\n(15.17279) (3.97502) (4.72510)\\npercentage NHPI 5.78337 ‚àí2.46150 1.34235\\n(6.75925) (4.80422) (6.01157)\\nconstant 45.35172‚àó‚àó‚àó18.23200‚àó‚àó‚àó33.05425‚àó‚àó‚àó\\n(8.79892) (4.39869) (4.59876)\\nobservations 1,606 1,502 3,108\\nR20.40527 0.30141 0.42091\\nadjusted R20.40003 0.29483 0.41829\\nresidual std. error 19.28330 (df = 1591) 12.20299 (df = 1487) 16.92010 (df = 3093)\\nNote:‚àóp<0.1;‚àó‚àóp<0.05;‚àó‚àó‚àóp<0.01\\nS18S5 Robustness check on the spatial weight matrix\\nTo enhance the robustness of our findings, we analyze the spatial adjacency matrix with the distance\\ndecay function. The major reason to do so is to ensure that the modified distance decay function reduces\\nthe rate at which adjacency weights decrease with distance. This allows us to capture the effect of farther\\naway counties that would otherwise have been under-weighted. We define the the updated weight matrix\\nas follows:\\na‚Ä≤\\nij=1 +1\\nd1/10\\nij\\nP\\nkÃ∏=i\\x12\\n1 +1\\nd1/10\\nik\\x13,\\nwhere dijis the distance matrix between county iandj. We run the cluster robust linear regression\\nfor counties in eastern US, western and central US pooled together, and counties in the contiguous US.\\nTable S12, S13 and S14 consistently show the coefficient for ‚Äúdeaths in social proximity‚Äù is statistically\\nsignificant and adds additional robustness to our analysis. Supplementary figure S4 shows the confidence\\ninterval plot highlighting the statistically significant effect of coefficient for ‚Äúdeaths in social proximity‚Äù.\\ndeaths social proximitydeaths spatial proximity\\n0 5 10 15\\ndeath‚Äôs per 100,000 thousand peoplecluster robust linear regression entire US\\ncluster robust linear regression western-central US\\ncluster robust linear regression eastern US\\nSupplementary Figure S4 : This figure shows the confidence intervals for the cluster-robust linear\\nregression with spatial proximity based on the slow-decaying distance weights, comparing the counties\\nin the eastern, western, and entire contiguous United States. The effect sizes for s‚àíiare statistically\\nsignificant (p <0.001) for the three regions. The estimation results are in Supplementary Table S12, S13\\nand S14.\\nS19Supplementary Table S12 : Linear regression for measuring the significance of the effect size of ‚Äúdeaths\\nin social proximity‚Äù, for counties in the eastern United States with slowly decaying distance weights\\nDependent variable: death rate per 100,000 people\\nOLS Cluster-Robust OLS\\n(1) (2)\\ndeaths social proximity 13.643‚àó‚àó‚àó13.643‚àó‚àó‚àó\\n(0.863) (2.243)\\ndeaths spatial proximity ‚àí1.604‚àó‚àó‚àí1.604\\n(0.780) (1.546)\\nODR 39.663‚àó‚àó‚àó39.663\\n(12.750) (25.848)\\nnaloxone available 172.358‚àó‚àó‚àó172.358‚àó‚àó\\n(17.162) (83.825)\\nbuprenorphine available 57.944‚àó‚àó‚àó57.944\\n(13.297) (44.091)\\nstate count illicit opioid reported 10,648.380‚àó‚àó‚àó10,648.380\\n(3,208.401) (10,006.390)\\npopulation density ‚àí0.0001 ‚àí0.0001\\n(0.0002) (0.001)\\nmental health distress rate 154.389‚àó‚àó154.389\\n(77.943) (152.686)\\npolitical affiliation ‚àí3.258‚àó‚àó‚àí3.258\\n(1.354) (2.850)\\npercentage unemployed 77.127‚àó‚àó‚àó77.127‚àó‚àó\\n(9.729) (30.586)\\nmean distance to hospital ‚àí69.209‚àó‚àó‚àó‚àí69.209‚àó‚àó‚àó\\n(7.184) (11.786)\\nmedian household income ‚àí12.072‚àó‚àí12.072\\n(6.896) (14.549)\\npercentage AIAN ‚àí28.784 ‚àí28.784\\n(36.110) (21.705)\\npercentage NHPI ‚àí19.562 ‚àí19.562\\n(12.359) (14.146)\\nConstant ‚àí0.219 ‚àí0.219\\n(10.465) (22.483)\\nobservations 1,606\\nR20.454\\nadjusted R20.449\\nresidual std. error 3.210 (df = 1590)\\nF statistic 94.369‚àó‚àó‚àó(df = 14; 1590)\\nNote:‚àóp<0.1;‚àó‚àóp<0.05;‚àó‚àó‚àóp<0.01\\nS20Supplementary Table S13 :Linear regression for measuring the significance of the effect size of ‚Äúdeaths in social\\nproximity‚Äù, for counties in the western and central United States with slowly decaying distance weights\\nDependent variable: death rate per 100,000 people\\nOLS Cluster-Robust OLS\\n(1) (2)\\ndeaths social proximity 6.013‚àó‚àó‚àó6.013‚àó‚àó‚àó\\n(0.234) (0.781)\\ndeaths spatial proximity ‚àí3.694‚àó‚àó‚àó‚àí3.694‚àó\\n(0.375) (2.108)\\nODR 14.954‚àó‚àó‚àó14.954\\n(3.734) (9.451)\\nnaloxone available 93.746‚àó‚àó‚àó93.746‚àó‚àó\\n(10.214) (39.755)\\nbuprenorphine available 46.277‚àó‚àó‚àó46.277‚àó‚àó\\n(8.315) (19.070)\\nstate count illicit opioid reported ‚àí8,620.923‚àó‚àó‚àí8,620.923\\n(4,324.188) (12,994.870)\\npopulation density 0.005‚àó‚àó‚àó0.005‚àó‚àó‚àó\\n(0.0005) (0.001)\\nmental health distress rate 92.493‚àó‚àó92.493\\n(37.035) (126.872)\\npolitical affiliation ‚àí0.472 ‚àí0.472\\n(0.704) (2.299)\\npercentage unemployed 15.921‚àó‚àó‚àó15.921\\n(5.979) (17.045)\\nmean distance to hospital ‚àí9.119‚àó‚àó‚àó‚àí9.119\\n(3.513) (10.910)\\nmedian household income 6.268‚àó6.268\\n(3.249) (10.881)\\npercentage AIAN ‚àí12.689‚àó‚àí12.689\\n(6.560) (11.559)\\npercentage NHPI 12.089‚àó‚àó‚àó12.089\\n(3.693) (10.008)\\nConstant ‚àí10.105‚àó‚àó‚àí10.105\\n(5.011) (20.221)\\nobservations 1,502\\nR20.492\\nadjusted R20.487\\nresidual std. error 1.004 (df = 1486)\\nF statistic 102.823‚àó‚àó‚àó(df = 14; 1486)\\nNote:‚àóp<0.1;‚àó‚àóp<0.05;‚àó‚àó‚àóp<0.01\\nS21Supplementary Table S14 :Linear regression for measuring the significance of the effect size of ‚Äúdeaths in social\\nproximity‚Äù, for counties in the contiguous United States with slowly decaying distance weights\\nDependent variable: death rate per 100,000 people\\ndeath rates\\nOLS Cluster-Robust OLS\\n(1) (2)\\ndeaths social proximity 15.013‚àó‚àó‚àó15.013‚àó‚àó‚àó\\n(0.622) (1.787)\\ndeaths spatial proximity ‚àí2.357‚àó‚àó‚àó‚àí2.357‚àó‚àó\\n(0.582) (1.035)\\nODR 48.719‚àó‚àó‚àó48.719‚àó‚àó\\n(8.169) (19.212)\\nnaloxone available 150.050‚àó‚àó‚àó150.050‚àó‚àó‚àó\\n(11.250) (56.853)\\nbuprenorphine available 60.425‚àó‚àó‚àó60.425\\n(10.014) (42.605)\\nstate count illicit opioid reported 13,248.850‚àó‚àó‚àó13,248.850\\n(2,365.555) (9,362.013)\\npopulation density 0.001‚àó‚àó‚àó0.001\\n(0.0002) (0.001)\\nmental health distress rate 69.993 69.993\\n(45.879) (112.128)\\npolitical affiliation ‚àí3.194‚àó‚àó‚àó‚àí3.194‚àó\\n(0.828) (1.884)\\npercentage unemployed 42.330‚àó‚àó‚àó42.330‚àó\\n(6.718) (23.953)\\nmean distance to hospital ‚àí49.510‚àó‚àó‚àó‚àí49.510‚àó‚àó‚àó\\n(5.545) (13.203)\\nmedian household income ‚àí12.136‚àó‚àó‚àó‚àí12.136\\n(4.271) (9.640)\\npercentage AIAN ‚àí26.900‚àó‚àó‚àí26.900‚àó\\n(11.020) (14.641)\\npercentage NHPI ‚àí18.952‚àó‚àó‚àó‚àí18.952‚àó\\n(5.748) (10.007)\\nconstant 1.248 1.248\\n(6.158) (15.344)\\nobservations 3,108\\nR20.504\\nadjusted R20.502\\nresidual std. error 1.895 (df = 3090)\\nF statistic 224.521‚àó‚àó‚àó(df = 14; 3090)\\nNote:‚àóp<0.1;‚àó‚àóp<0.05;‚àó‚àó‚àóp<0.01\\nS22',\n",
       " ' 1 \\n \\n \\n \\nA Systematic Review of Machine Learning Approaches for Detecting Deceptive \\nActivities on Social Media: Methods, Challenges, and Biases  \\nYunchong Liu1, Xiaorui Shen2, Yeyubei Zhang1, Zhongyan Wang3, Yexin Tian4, Jianglai Dai5, \\nand Yuchen Cao2 \\n1School of Engineering and Applied Science, University of Pennsylvania  \\n2Khoury college of computer science, Northeastern University  \\n3Center for Data Science, New York University  \\n4Georgia Institute of Technology, College of Computing \\n5Department of EECS, University of California, Berkeley  \\n \\n \\n \\n \\n \\n \\n \\n \\n  2 \\nAbstract  \\nSocial media platforms like Twitter, Facebook, and Instagram have facilitated the spread of \\nmisinformation, necessitating automated detection systems. This systematic review evaluates 36 \\nstudies that apply machine learning (ML) and deep learning (DL) model s to detect fake news, \\nspam, and fake accounts on social media. Using the Prediction model Risk Of Bias ASsessment \\nTool (PROBAST), the review identified key biases across the ML lifecycle: selection bias due to \\nnon-representative sampling, inadequate handl ing of class imbalance, insufficient linguistic \\npreprocessing (e.g., negations), and inconsistent hyperparameter tuning. Although models such as \\nSupport Vector Machines (SVM), Random Forests, and Long Short -Term Memory (LSTM) \\nnetworks showed strong potenti al, over -reliance on accuracy as an evaluation metric in imbalanced \\ndata settings was a common flaw. The review highlights the need for improved data preprocessing \\n(e.g., resampling techniques), consistent hyperparameter tuning, and the use of appropriate metrics \\nlike precision, recall, F1 score, and AUROC. Addressing these limitations can lead to more reliable \\nand generalizable ML/DL models for detecting deceptive content, ultimately contributing to the \\nreduction of misinformation on social media.  \\n \\nKeywords:  fake news , deceptive activities, social media, bias evaluation with PROBAST, \\nsystematic review, machine learning, deep learning  \\n   3 \\nIntroduction  \\nOver the past two decades, social media platforms such as Twitter, Facebook, Instagram, \\nand Reddit have dramatically reshaped how people communicate, share information, and engage \\nwith global events (Smith & Anderson, 2018). While these platforms provide u nparalleled \\nopportunities for connection and information dissemination, they have also become fertile ground \\nfor the spread of misinformation, fake news, spam, and fraudulent activities (Vosoughi, Roy, & \\nAral, 2018). The rise in deceptive content has far -reaching societal consequences For example, \\nduring the 2016 U.S. presidential election, widespread dissemination of false political content is \\nbelieved to have influenced voter behavior, with research indicating that this type of content played \\na significan t role in shaping public opinion (Allcott & Gentzkow, 2017). Similarly, the COVID -\\n19 pandemic saw social media become a hotbed for health -related misinformation. False \\ninformation about vaccines and the virus spread rapidly, contributing to public confusio n, \\nincreased vaccine hesitancy, and the undermining of global public health efforts.  (Zarocostas, \\n2020) . In addition to these, misinformation has also played a role in inciting social unrest. For \\ninstance, during the Russia ‚ÄìUkraine war, false narratives spread through Turkish social media \\nplatforms contributed to heightened tensions and public panic. These fa ke posts fueled divisive \\nnarratives, exacerbating the conflict‚Äôs impact on local and international communities (Ulu≈üan & \\n√ñzejder, 2024 ). \\nIn addition to fake news, fake accounts, including bots, spam accounts, and identity theft \\nprofiles, have proliferated across social media platforms. These accounts are frequently used to \\nspread misinformation or carry out fraudulent activities, exacerbati ng the already significant \\nproblem of deceptive content online. Bots, in particular, can artificially amplify the reach of fake \\nnews by inflating engagement metrics such as likes, shares, and retweets, creating the illusion of  4 \\nwidespread belief in false information. This issue became especially pronounced during the \\nCOVID -19 pandemic, where bots played a crucial role in spreading health misinformation, further \\ncomplicating public understanding and responses to the crisis (Himele in-Wachowiak et al., 2021 ). \\nOne of the key challenges in combating misinformation is the sheer volume of data \\ngenerated on social media. Every day, millions of posts, tweets, images, and videos are shared, \\nmaking manual monitoring and verification impossible (Shu et al., 2017). An ev en bigger \\nchallenge lies in the speed and scale at which misinformation spreads. Social media algorithms, \\ndesigned to prioritize user engagement, frequently promote sensational or controversial content, \\nregardless of its accuracy (Vosoughi et al., 2018). T hese algorithms are structured to maximize \\ntime spent on platforms, making the spread of false information an unintended but inherent \\nconsequence of their design. As a result, false information tends to spread faster than the truth, \\nespecially in contexts like political elections or public health emergencies. A study by Vosoughi, \\nRoy, and Aral (2018) found that false news stories on Twitter were 70% more likely to be \\nretweeted than true stories. This tendency is not accidental but is embedded in the busines s model \\nof platforms like Facebook, which profit from increased engagement with sensational content \\n(Lauer, 2021 ).  \\nTraditional moderation methods, which often rely on human intervention, are  insufficient \\nto address the scale and speed at which deceptive content spreads. As such, automated, efficient, \\nand accurate methods to detect and mitigate the impact of deceptive content are essential to \\nsafeguard information integrity online  (Lazer et al., 2018).  \\nIn response to these challenges, machine learning (ML) and deep learning (DL) techniques \\nhave emerged as powerful tools for detecting deceptive content on social media platforms (Zhou \\n& Zafarani, 2020). ML algorithms, such as Support Vector Machines (SVM),  Random Forests,  5 \\nand Logistic Regression, have been employed to classify content based on features extracted from \\ntextual, visual, and network data (Conroy, Rubin, & Chen, 2015). These models are highly \\neffective at identifying patterns in the data that distinguish between  legitimate and deceptive \\ncontent, but they often rely on handcrafted features, which can limit their adaptability to new types \\nof misinformation.  \\nTo address this limitation, deep learning (DL) models have significantly advanced \\ndetection capabilities, including Convolutional Neural Networks (CNNs) and Recurrent Neural \\nNetworks (RNNs), can automatically learn complex patterns and representations from large \\namounts of unstructured data, such as text and images (Ruchansky, Seo, & Liu, 2017). These \\nmodels eliminate the need for manual feature extraction, making them more versatile in adapting \\nto evolving forms of misinformation.  \\nIn particular, natural Language Processing (NLP) models play a crucial role in analyzing \\ntextual content. By detecting subtle linguistic cues and anomalies that indicate fake news or spam, \\nNLP models are particularly useful for processing large volumes of data in real time (Zhang & \\nGhorbani, 2020). These models can dynamically adapt to shifting patterns of deceptive behavior, \\nwhich is essential for keeping pace with the rapid spread of misinformation.  \\nThe integration of ML and DL techniques provides a scalable solution to identify and filter \\nout malicious content, thereby safeguarding the integrity of information on social media platforms. \\nThese automated approaches provide the efficiency and accuracy n eeded to combat \\nmisinformation on a large scale, where manual methods fall short (Shu et al., 2017).   6 \\nChallenges of Using Machine Learning/Deep Learning Models in Detecting Deceptive Content  \\nDespite the significant advancements in the application of ML and DL models for detecting \\nmisinformation and deceptive activities on social media, several challenges hinder the full \\neffectiveness and scalability of these approaches. These challenges arise from the complex nature \\nof social media data, technical limitations of the models, and broader concerns about bias and \\nethical use.  \\nOne of the fundamental challenges arises from the complexity of social media data itself. \\nSocial media content is highly unstructured and multimodal, encompassing text, images, videos, \\nand metadata that vary across users and platforms. Posts are not unifor m; they often include slang, \\nabbreviations, emojis, and varying linguistic styles , making traditional text -processing techniques \\nless effective (Kaplan & Haenlein, 2010; Li & Li, 2014 ; Dan  et al., 20 24). Additionally, the context \\nof posts plays a critical  role in determining whether it is deceptive or legitimate, but algorithms \\ntypically struggle to interpret such nuances. For instance, a post that is sarcastic, humorously \\nexaggerated, or part of an ongoing conversation can be mistakenly flagged as misinfo rmation if \\nthe model does not accurately capture the context (Gupta & Kumaraguru, 2012). Moreover, the \\ndynamic and rapidly evolving nature of social media further adds to the complexity, as models \\nmust adapt to new forms of deception and manipulation techn iques (Ferrara et al., 2016). One such \\nexample is the proliferation of social bots, a type of fake account designed to simulate human \\nbehavior. Social bots are often used to distort online discussions and amplify misinformation. \\nDuring the 2016 U.S. Presid ential Election, social bots were deployed to artificially inflate political \\ndiscourse, giving the illusion of widespread support or opposition and ultimately influencing \\npublic perception (Bessi & Ferrara, 2016) . The increasing sophistication of such fake accounts  7 \\npresents a significant challenge for detection models, which must continuously evolve to keep pace \\nwith new forms of automated deception.  \\nAnother major issue is the class imbalance inherent in the detection of misinformation and \\nfake content. One of the most significant technical challenges in the application of ML and DL \\nmodels to misinformation detection is the class imbalance problem.  Le gitimate content vastly \\noutnumbers fake or deceptive content, leading to datasets where the minority class (e.g., fake news, \\nspam, or bot accounts) is severely underrepresented (Chawla, Japkowicz, & Kotcz, 2004). This \\nimbalance skews the model‚Äôs learning p rocess, causing it to prioritize accuracy on the majority \\nclass ‚Äîlegitimate content ‚Äîwhile failing to correctly identify the minority class of interest, such \\nas fake accounts or misinformation (He & Garcia, 2009). As a result, the models may exhibit high \\noverall accuracy but poor sensitivity to the very content they are designed to detect. Addressing \\nclass imbalance is essential to improve the sensitivity of models to deceptive activities without \\ncompromising their specificity (Sun, Wong, & Kamel, 2009).  \\nFurthermore, generalization is a persistent problem. ML and DL models often struggle with \\ngeneralization ‚Äîthe ability to perform well on data that differs from the training dataset. Many \\nmodels are trained on specific datasets from certain social media plat forms or under controlled \\nconditions, which limits their ability to generalize across platforms or adapt to new contexts. This \\nissue is particularly evident in deep learning models, which, despite their capacity to learn complex \\npatterns, can be  overfit to  the training data and perform poorly when applied to new or unseen data \\n(Bay & Yearick, 2024 ). For instance, a model trained on Twitter data may not perform as \\neffectively on Reddit or Facebook, where user behavior and content structures differ significantly.   8 \\nObjective of the Review  \\nThis systematic review aims to critically evaluate the application of machine learning and \\ndeep learning techniques in detecting misinformation, spam, fake profiles, and other deceptive \\nactivities on social media platforms. By analyzing studies published b etween January 2010 and \\nJuly 2024, the review seeks to:  \\n‚óè Assess the effectiveness of various ML and DL models employed in this domain.  \\n‚óè Identify common challenges and limitations faced by these models, including issues related \\nto data complexity, class imbalance, generalization, and other methodological biases.  \\n‚óè Propose future directions for research to enhance model performance, generalizability, and \\nethical considerations.  \\nThrough this comprehensive evaluation, the review intends to contribute to the \\ndevelopment of more robust, accurate, and ethically responsible detection methods. By addressing \\nexisting gaps and highlighting best practices, it aims to support efforts in mit igating the spread of \\nmisinformation and enhancing the reliability of information shared on social media platforms.  \\nThe subsequent sections will outline the methodologies employed in the reviewed studies, \\npresent the results of this analysis, and discuss the implications of these findings for future research \\nand practice. We will also explore the potential for improving  model performance and reliability \\nin real -world applications of ML and DL to combat misinformation on social media.  \\nMethod  \\nSearch Strategy  \\nThis review focused on identifying studies that utilized machine learning (ML) and deep \\nlearning (DL) techniques to detect misinformation, spam, fake profiles, and other deceptive \\nactivities on social media platforms such as Twitter, Facebook, Instagram , and Reddit. These  9 \\nplatforms were chosen due to their prevalence in social discourse and their known issues with the \\nspread of deceptive content, making them key areas for research on misinformation detection. The \\nsearch terms included combinations of \"machine learning\", \"de ep learning\", \"Twitter\", \\n\"Facebook\", \"Instagram\", \"Reddit\",  \"fraud\", \"fake account\", and \"fake content.\" The focus on \\nthese terms aimed to capture a broad range of deceptive activities and the ML/DL techniques used \\nfor their detection.  \\nThe comprehensive search covered multiple academic databases, including five databases, \\nPubMed, Google Scholar, IEEE Xplore, ResearchGate, and ScienceDirect, chosen for their \\nextensive coverage of peer -reviewed literature relevant to both technical and med ical fields. These \\ndatabases were selected to ensure that studies from a wide array of disciplines, including computer \\nscience, data science, and social media analysis, were included. This extensive search was \\ndesigned to capture a wide range of studies re levant to research objectives. The search process was \\ncarried out from June to July 2024.  \\nThe search terms were carefully selected to cover the key aspects of the studies under \\nreview, including the social media platforms under investigation, the types of deceptive activities \\nbeing detected, and the machine learning techniques used. The followi ng terms were applied:  \\n1. Social Media Platforms: \"Twitter\", \"Facebook\", \"Instagram\", and \"Reddit\".  \\n2. Deceptive Activities: \"fake news\", \"spam\", \"fraud\", \"fake accounts\", \"faked content\", and \\n\"risk\".  \\n3. Machine Learning Techniques: \"machine learning\", \"deep learning\", \"neural networks\".  \\nThe search query used for this review was: ((\"Twitter\" OR \"Facebook\" OR \"Instagram\" \\nOR \"Reddit\") AND (\"fake news\" OR \"spam\" OR \"fraud\" OR \"fake accounts\" OR \"fake content\"  \\nOR \"risk\") AND (\"machine learning\" OR \"deep learning\" OR \"neural networks\")).   10 \\nInclusion/Exclusion Criteria and Study Selection Process  \\nStudies were included in this review if they met the following criteria:  \\n‚óè Publication Date: Studies published from January 2010 to July 2024 were selected to \\ncapture the most recent  developments in ML and DL applications for detecting deceptive \\nactivities.  \\n‚óè Language: Only studies published in English were considered, ensuring accessibility and \\nconsistent analysis across the literature.  \\n‚óè Research Focus: Studies that applied ML and DL techniques specifically to detect various \\nforms of deceptive behavior (e.g., misinformation, spam, fake accounts) on social media \\nplatforms were included.  \\n‚óè Social Media Data: Studies that analyzed posts from popular social media platforms like \\nTwitter, Reddit, Instagram, and Facebook, were eligible, ensuring that the focus remained \\non widely used platforms with prevalent deceptive activities.  \\n‚óè Event -Related Data: Studies analyzing data related to specific events (e.g., crises, crimes, \\nor topics to understand the context of deceptive activities) were included.  \\n‚óè Relevant Datasets: The inclusion criteria required the use of datasets relevant to the \\ndetection of deceptive content, such as those focusing on spam detection, social media \\nmisbehavior, or fake account detection.  \\n‚óè Study Design: Primary research articles employing observational or experimental \\nmethodologies were included, as well as studies utilizing ML/DL algorithms for detection \\npurposes.  \\nStudies were excluded if they fell into the following categories:  \\n‚óè Publication Type: Non -peer-reviewed articles, review articles, and conference abstracts \\nwere excluded to ensure the focus remained on original, peer-reviewed research. \\nIrrelevant/Duplicates: Studies with non -relevant focus or those identified as duplicates \\nwere excluded. Studies based on private social media accounts were also excluded due to \\ndata privacy concerns.   11 \\n‚óè Insufficient Information: Studies lacking sufficient information for classification or those \\nwithout detailed methodology (e.g., missing data sources, algorithms) were excluded.  \\n‚óè Language Restrictions: Studies Published in languages other than English were excluded.  \\n‚óè Incomplete Data: Studies with incomplete or missing data that prevents classification or \\nanalyses, were excluded.  \\n‚óè Scope: Studies that did not focus on social media or failed to use ML/DL techniques for \\ndetection of deceptive activities were excluded.  \\nThe study selection process involved several structured steps to ensure the relevance and \\nquality of included studies:  \\n1. Initial Identification: Duplicates were removed, and preliminary screening was conducted \\nbased on titles and abstracts to determine relevance.  \\n2. Title and Abstract Screening: Two reviewers independently assessed the studies to ensure \\nthey met the inclusion and exclusion criteria.  Any differences were addressed and resolved \\nthrough discussion to maintain consistency in the selection process.  \\n3. Full-Text Screening: A thorough  review of the full texts was performed, with any \\ndiscrepancies addressed and resolved through further discussion.  \\n4. Data Extraction and Final Inclusion: Relevant information was extracted from the selected \\nstudies. The final list of included studies was established, ensuring they meet all criteria \\nand are aligned with the research objectives.  \\n5. Documentation: The entire selection process was documented, including reasons for \\nexclusion at each stage and the final list of studies, ensuring transparency and \\nreproducibility of the research.  \\nData Extraction and Analysis  \\nThe data extraction process was carried out by systematically gathering comprehensive \\ninformation from each included study through a standardized template. This template captured \\ncrucial details such as the authors, study titles, journals of publication, a nd publication dates, as \\nwell as information on study design, settings, sample sizes, and inclusion/exclusion criteria. In  12 \\naddition to this, the template tracked the ML and DL models utilized, the social media platforms \\nexamined (e.g., Twitter, Facebook, Instagram), and the primary and secondary outcomes observed \\nin each study. Performance metrics, including accuracy, precisio n, recall, F1 score, and Area \\nUnder the Receiver Operating Characteristic Curve (AUROC), were extracted when reported. \\nMoreover, potential biases, study limitations, and funding information were documented to ensure \\na thorough understanding of the study‚Äôs reliability and context.  \\n  \\nInsert Table 1 about here  \\n  \\nThe review examined the types of data and content analyzed in the studies, such as \\ndistinctions between public and private posts, language -specific data, and the relevance of content \\nto specific topics. This approach provided a nuanced understanding of how dif ferent data sources \\nand contexts impact the performance and applicability of ML/DL models used for detecting \\nmisinformation and spam across various social media platforms.  \\nTo ensure a thorough evaluation of bias, an established tool, the Prediction model Risk Of \\nBias ASsessment Tool (PROBAST) (Wolff et al., 2019), was employed. This tool helped assess \\nthe credibility and reliability of findings by identifying where models mi ght be overfitting or \\nunderperforming due to dataset limitations.  \\nA critical part of the evaluation was assessing key performance metrics ‚Äîsuch as accuracy, \\nprecision, recall, F1 scores, and AUROC ‚Äîparticularly in relation to challenges like imbalanced \\ndata. These metrics were essential for understanding how well the model s performed in detecting \\ndeceptive content, given the disproportionate presence of legitimate versus fake content on social \\nmedia. This analysis highlighted how biases in model training, such as favoring majority classes,  13 \\ncould undermine model effectiveness in identifying minority -class events like misinformation or \\nspam.  \\nThe review also identified other research gaps, such as how well these models generalize \\nto diverse real -world social media environments. By integrating performance metrics and bias \\nassessments, this study aimed to provide a comprehensive overview of curre nt research, offering \\ninsights into areas for improving model accuracy, handling imbalanced data, and ensuring wider \\napplicability in detecting misinformation and other deceptive activities on social media platforms.  \\nResults  \\nStudy Selection  \\nA comprehensive literature search was conducted across multiple databases to identify \\nrelevant studies on machine learning models used for fake news detection on social media \\nplatforms. The databases searched included PubMed, IEEE Xplore, ScienceDirect, and Google \\nScholar. The search terms used were combinations of keywords such as \"machine learning,\" \\n\"Twitter,\" \"fake,\" \"fraud,\" \"spam,\" \"Reddit,\" \"Facebook,\" \"Instagram,\" and \"deep learning.\" The \\ndiagram in Figu re 1 illustrates the study selection process based on the search results.  \\n  \\nInsert Figure 1 about here  \\n  \\nThe comprehensive literature search resulted in 903 studies across four major databases: \\nIEEE Xplore contributed the largest portion with 361 studies, followed by Google Scholar with \\n271 studies, PubMed with 181 studies, and ScienceDirect with 90 studies. After removing 98 \\nduplicate studies, a total of 805 unique titles and abstracts were subjected to initial screening.  14 \\nDuring this phase, 690 studies were excluded based on several exclusion criteria: 241 studies were \\ndeemed irrelevant to the research topic, 172 used inappropriate data sources, 138 applied \\nunsuitable techniques, 104 focused on unrelated disorders, and 34 w ere excluded due to \\npublication type.  \\nThe remaining 115 studies underwent full -text screening, which resulted in the exclusion \\nof 79 additional publications. Reasons for exclusion at this stage included irrelevance (39 studies), \\nissues with data sources (3 studies), inappropriate techniques (3  studies), unrelated disorders (20 \\nstudies), unsuitable publication type (8 studies), language limitations (1 study), and the \\nunavailability of data (5 studies). Ultimately, 36 studies met the inclusion criteria and were \\nincluded in the narrative synthesis  for the systematic review, as shown  in the diagram in Figure 1.  \\nCharacteristics of Included Studies  \\nThe studies reviewed in this systematic analysis focused on detecting fake news and \\ndeceptive activities across various social media platforms. Among the 36 studies included, Twitter \\nwas the most frequently analyzed platform, appearing in 19 studies. Insta gram was the subject of \\n9 studies, and both Facebook and Reddit were analyzed in 3 studies each. Additionally, 1 study \\nfocused on Weibo, a Chinese platform like Twitter, while 2 studies explored data from multiple \\nplatforms, including Twitter, Facebook, Yo uTube, and email. This reflects the diverse \\nenvironments where misinformation and fake profiles are commonly found.  \\nThe types of deceptive content analyzed in these studies were equally varied. The most \\nprevalent focus was on fake profiles and fake accounts detection, which was explored in 15 studies. \\nAnother significant area of interest was the general detection of fak e news, addressed in 12 studies. \\nSpam and scam detection was examined in 5 studies, while phishing detection was the focus of 2 \\nstudies. There were also 2 studies that addressed health -related misinformation, such as detecting  15 \\nsigns of suicidal ideation, and 1 study that focused specifically on misinformation spread during \\ncrisis events like natural disasters or terrorist attacks.  \\nA variety of machine learning and deep learning models were employed across the studies. \\nSupervised machine learning models were particularly popular, with Random Forest being the \\nmost widely used, appearing in 17 studies. Support Vector Machines (SVM) wer e applied in 16 \\nstudies, while Naive Bayes and Logistic Regression were used in 12 and 14 studies, respectively. \\nOther models, such as Decision Trees, K -Nearest Neighbors (KNN), Gradient Boosting, and \\nStochastic Gradient Descent (SGD), were also featured, though less frequently.  \\nDeep learning models played a prominent role in several studies, with Artificial Neural \\nNetworks (ANN) and Multi -layer Perceptron (MLP) employed in 8 studies. Convolutional Neural \\nNetworks (CNN) and Recurrent Neural Networks (RNN) were each used in 2 studi es, and Long \\nShort -Term Memory (LSTM) models were also applied in 2 studies. Additionally, autoencoders \\nand hybrid models, which combine techniques such as CNN with RNN or other machine learning \\napproaches, were explored in 4 studies. Transformer -based mod els and deep stacked autoencoders, \\nrepresenting the latest advancements in deep learning, were used in a smaller number of studies.  \\nIn summary, the studies demonstrated a strong reliance on both traditional supervised \\nmachine learning models like Random Forest and SVM, as well as more advanced deep learning \\nmodels such as ANN and LSTM. These models were particularly effective in detect ing fake news \\nand fake profiles, with high accuracy rates reported across multiple studies. This comprehensive \\nreview highlights both the diversity of approaches and the growing complexity of model usage in \\nthe field of misinformation detection on social m edia platforms.   16 \\nMethodological Quality and Risk of Bias  \\nWe employed the Prediction model Risk Of Bias ASsessment Tool (PROBAST) (Wolff et \\nal., 2019) to systematically evaluate potential biases across four domains: sample selection and \\nrepresentativeness, data preprocessing, model development, and model evaluati on. Each domain \\nwas examined through a series of targeted questions, as shown in Table 2. These questions helped \\nidentify biases that could affect the reliability and generalizability of the machine learning models. \\nKey concerns included the representative ness of samples, the handling of negative words, \\nhyperparameter tuning, and the appropriateness of evaluation metrics, especially in class -\\nimbalanced settings. This structured approach allowed for a clear identification of the strengths \\nand limitations of the studies, ensuring a rigorous assessment of methodological quality.  \\n  \\nInsert Table 2 about here  \\n  \\nSample Selection and Representativeness (Q1 & Q2):  \\nThe reviewed studies employed various sampling methods across multiple social media \\nplatforms, including Twitter, Instagram, Facebook, and Reddit, aiming to investigate phenomena \\nsuch as misinformation, fake accounts, or spam detection. However, only a sma ll fraction of the \\nstudies aimed to provide representative samples of the broader social media user base, with most \\nresearch focusing on particular events, user groups, or trends. This limits the generalizability of \\ntheir findings, as they fail to capture the broader behaviors of social media users. For instance, \\nsome studies (e.g., Study #1) specifically targeted tweets during crisis situations. These approaches \\ndo not reflect the larger population of social media users. Conversely, while others (e.g., Stu dy #4) \\nemployed random sampling methods that were somewhat more representative but remained  17 \\nconfined to Twitter. Studies like these do not reflect the broader social media landscape. A detailed \\nsummary of the sampling approaches used in the studies is provided in Table 3.  \\nAmong the 36 studies reviewed, 20 studies (55%) focused on specific events, user groups, \\nor phenomena without attempting to represent the broader social media population. For example, \\nStudy #1 focused solely on Twitter, collecting 15,952 tweets related to six specific misinformation \\nevents during crisis periods, using data gathered within specific time frames to monitor \\nmisinformation spread. Another study is Study #2, which concentrated on Instagram by web \\nscraping data from the platform, resulting in a da taset of 970 bot accounts, 959 real accounts, and \\n870 fake accounts. Similarly, Study #3 utilized Twitter data collected from over 54 million user \\naccounts but narrowed the focus to tweets associated with three trending topics, specifically \\ntargeting and m anually labeling tweets as spammers based on keyword presence.  \\nAdditionally, 10 studies (28%) employed sampling methods that were more general but \\nremained constrained by platform specificity, typically focusing on a single platform such as \\nTwitter or Instagram. For instance, Study #4 used a dataset of tweets where ph ishing content was \\nmanually identified by security experts, while non -phishing tweets were randomly sampled from \\nthe Twitter stream.  \\nThe remaining 6 studies (17%) attempted more comprehensive or representative sampling \\nacross multiple social media platforms or utilized datasets aimed at being more inclusive, but they \\ntoo faced limitations related to platform -specific constraints like la nguage and geography. Study \\n#22, for instance, utilized  over 15,000 news contents from Facebook, collecting through a custom -\\nbuilt crawler and the Facebook API to gather public user information and posts and analyze both \\nfake and real news. Study #11 took a more inclusive approach, using data from both Reddit and \\nCable News Network (CNN) to improve fake news detection.   18 \\n  \\nInsert Table 3 about here  \\n \\nNon-representative sampling introduces selection bias, which can undermine the \\ngeneralizability of the study‚Äôs findings. Studies that focus exclusively on certain social media \\nplatforms or specific user groups may overlook significant variations in user be havior, interactions, \\ncontent, and platform -specific dynamics across the broader social media landscape. For example, \\na study focusing solely on tweets from specific crisis events or narrowly targets categories like \\nfake accounts may fail to capture the fu ll spectrum of behaviors present on other platforms like \\nFacebook or Reddit. This limitation compromises the validity of the conclusions, especially when \\nattempting to apply the findings to a general population. Fortunately, nearly 95% of the reviewed \\npape rs acknowledge these limitations, demonstrating an awareness of the inherent challenges \\nposed by non -representative sampling, particularly in the field of ML/DL applications for social \\nmedia analysis. In the broader machine learning literature, researchers  such as Goodfellow, \\nBengio, and Courville (2016) underscore the importance of representative sampling to build \\nmodels that generalize effectively and  avoid the risk of overfitting to specific data subsets, thereby \\nenhancing the reliability and applicability of findings across diverse social media contexts.   \\nThis analysis reveals a significant challenge of achieving representative sampling in \\nML/DL studies applied to social media platforms. The prevalent reliance on non -representative \\nsamples introduces selection bias, limiting the validity and generalizability of the findings. Even \\nstudies that incorporate multiple platforms struggle with achieving true representativeness due to \\nconstraints tied to linguistic, geographical, and demographic diversity.  Most authors acknowledge \\nthese limitations, recognizing that fully representative sampling is difficult to achieve given the  19 \\nsheer diversity of the social media landscape. Moving forward, a critical future direction is to \\ndevelop more inclusive sampling strategies that incorporate multiple platforms and strive to \\naccount for factors such as language, geography, and user demograp hic. By doing so, future \\nresearch can more effectively capture the complexities of the social media ecosystem, enhancing \\nthe robustness of ML/DL models and improving the applicability of their conclusions across \\ndiverse social media contexts.  \\nData Preprocessing with Focus of Negative  Words Handling (Q3)  \\nEffective data preprocessing is pivotal in machine learning workflows, particularly in tasks \\nlike fake news detection, where linguistic nuances ‚Äîsuch as negations ‚Äîcan have significant \\ninfluence on model performance. This section evaluates how the reviewed s tudies addressed data \\npreprocessing tasks, focusing specifically on the treatment of negative words, which is crucial in \\naccurately interpreting textual content.  \\nIn the reviewed studies, preprocessing techniques to prepare textual data for machine \\nlearning models followed consistent patterns. One common step involved normalizing text, which \\nincluded converting all characters to lowercase and removing punctuation, U RLs, and special \\nsymbols. Tokenization, the process of breaking text into individual units or tokens, was universally \\nemployed across studies. Some studies also applied stemming and lemmatization techniques to \\nreduce words to their base or root forms, ensu ring uniformity across different grammatical \\nvariants. Another frequently used step was the removal of stop words, eliminating commonly used \\nwords that add little value to modeling. To convert textual data into numerical representations for \\nmodel input, fe ature extraction methods such as Term Frequency -Inverse Document Frequency \\n(TF-IDF), Bag of Words (BoW), and word embeddings were widely adopted (Singh & Singh, \\n2022).   20 \\nOf the 36 reviewed studies, 30 applied traditional machine learning methods using token -\\nbased features like n -grams and TF -IDF. However, 21 of these studies, despite employing \\nalgorithms such as Support Vector Machines (SVM), Decision Trees, Random Forests , and \\nLogistic Regression, did not specify how negative words were handled during preprocessing.  For \\nexample, Studies #1 and #3 utilized Logistic Regression with n -grams and TF -IDF for feature \\nextraction but provided no details on managing negations. Simi larly, Studies #4 and #18 employed \\nSVM with n -grams and TF -IDF for spam detection without addressing the treatment of negative \\nwords, leaving a critical gap in their preprocessing workflows.  \\nAdditionally, six of the reviewed studies employed advanced deep learning methods, such \\nas transformer -based architectures like  Bidirectional Encoder Representations from Transformers \\n(BERT), which inherently manage contextual meanings and linguistic nuances, including \\nnegations, without explicit preprocessing. These models use attention mechanisms and long -range \\ndependencies to ca pture sentence text, allowing them to interpret the contextual meaning of \\nsentences, such as \\'not good,\\' without needing dedicat ed preprocessing for negative words. For \\nexample, Study #35, utilized BERT, which effectively handled negations through its attention \\nmechanism, capturing the contextual meaning of sentences,  without requiring additional \\npreprocessing for negative words. By leveraging these inherent capabilities, the study \\ndemonstrated that BERT can accurately process complex linguistic structures like negations, \\nreducing the need for explicit handling.  \\nIn fake news detection, accurate sentiment analysis is key to identifying misleading \\ncontent. Without proper negation handling, models may overestimate the positivity or negativity \\nof content, resulting in misclassifications and undermining the effectivene ss of detection systems. \\nThe absence of explicit strategies for handling negative words in traditional ML models introduces  21 \\npotential performance biases. Negations, such as \"not good\", can reverse the sentiment of a \\nsentence, resulting in incorrect interpretations if not properly managed. For instance, in Study #1, \\nthe failure to account for negations may have resulted in the m isclassification of tweets during \\ncrisis events, potentially overlooking critical misinformation that uses negations to alter the \\nsentiment. The advantage of negation preprocessing was observed in our reviewed studies. For \\nexample, Study #12 demonstrated that incorporating additional preprocessing steps, such as \\nnegation handling and sentiment correction, significantly improved the accuracy of fake news \\ndetection when combined with machine learning algorithms like SVM and MultinomialNB.  \\nMoreover, research shows that even advanced DL models can benefit from explicit \\npreprocessing steps, particularly when working with domain -specific language or nuanced features \\nlike negations. Kaushik, Hovy, and Lipton (2020)  demonstrated that augmenting training data with \\ncounterfactual examples, including negations, improved accuracy across a range of NLP tasks. \\nThis suggests that even sophisticated models like BERT can be enhanced through preprocessing \\ntechniques, improving  their ability to interpret complex linguistic structures. Summary:   \\nMost  reviewed studies did not address negative word handling during preprocessing, \\nhighlighting a critical gap and potential performance bias in fake news detection research. Notably,  \\nover 72% of all studies, including Study  #3, Study #4, and Study #35, did not mention strategies \\nfor managing negotiations. In particular, among the 30 studies using traditional machine learning \\nmethods,70% failed to integrate  strategies for negation handling. Similarly, 83% (5 out of 6) of \\nthe studies using advanced deep lear ning models did not explicitly address negative words, \\nassuming the models\\' inherent capabilities were sufficient.  \\nTraditional machine learning models are highly reliant on explicit preprocessing to \\neffectively capture linguistic nuances such as negations. Techniques like adding a negation prefix  22 \\n(e.g., transforming \"not good\" to \"NOT_good\") or using sentiment lexicons to adjust scores can \\nsignificantly enhance these models\\' ability to interpret altered sentiments (Pang & Lee, 2008) . \\nWithout such strategies, traditional models often misinterpret negated phrases, leading to \\nmisclassifications and reduced performance, especially in sentiment -driven tasks like fake news \\ndetection. Incorporating sentiment lexicons that adjust scores to account for negations, as well as \\ndeveloping features that specifically detect the presence and impact of negations on sentence \\nmeaning, can further imp rove model accuracy. By adopting these strategies, future research can \\nmitigate potential biases and enhance the effectiveness of machine learning models in tasks such \\nas fake news detection.  \\nWhile deep learning models, such as transformers, are generally better equipped to manage \\nnegations through their attention mechanisms and contextual embeddings (Vaswani et al., 2017) , \\nthey too have limitations. Incorporating explicit preprocessing steps, such as emphasizing \\nnegations, can further boost their performance. Fine -tuning on datasets that contain more examples \\nof negated sentences has been shown to improve the accuracy of t hese models in interpreting \\nnuanced language structures (Kaushik, Hovy, and Lipton, 2020)  .  \\nModel Development  \\nHyperparameter Tuning (Q3, Q4 & Q5):  \\nHyperparameter tuning plays a critical role in optimizing machine learning models and \\nsignificantly impacts their performance. This section evaluates whether the reviewed studies \\nreported hyperparameters, whether these were optimized, and the consistency o f hyperparameter \\ntuning across models. A detailed summary of how these studies report hyperparameters is provided \\nin Table 4.  Out of the 36 reviewed studies, 36.1% of them (13 studies) reported hyperparameters \\nand applied tuning techniques such as grid sea rch for all models. For instance, Study #1 reported  23 \\nhyperparameters for models such as SVM, k -nearest neighbors (KNN), decision tree, random \\nforest, and others, applying grid search for optimization across all models to ensure a consistent \\napproach to hyperparameter tuning. Similarly, Study #2, also used gr id search to optimize \\nhyperparameters for models including Logistic Regression, SVM, Naive Bayes, KNN, decision \\ntree, random forest, and multi -layer perceptron. This consistency in hyperparameter tuning across \\nmodels not only ensures that each model is opt imized, but also allows for fair and accurate \\ncomparisons of their performance.  \\nHowever, 11 studies  did not maintain consistency in hyperparameter tuning. For example, \\nStudy #7 performed hyperparameter tuning for certain models like SVM and logistic regression \\nbut did not apply the same rigor to all models evaluated in their study. Similarly, Study #18 only \\nreported hyperparameters for Random Forest, without tuning for o ther models. This selective \\ntuning introduces inconsistencies in performance evaluation  and model comparison .  \\nIn addition,  12 few studies lacked sufficient details on hyperparameter tuning.  For \\nexample, Studies #6 and #9 neither report hyperparameters nor clarify whether any tuning was \\nperformed. This omission hinders the assessment of model optimization and reproducibility.  \\n  \\nInsert Table 4 about here  \\n \\nThere are two layers of potential biases in hyperparameter tuning: suboptimal tuning for \\ncertain models, and unfair model comparison and selection when not all models have optimized \\nhyperparameters. For example, Study #36 only applied default settings for all models. \\nAdditionally, failure to consistently report or optimize hyperparameters can introduce bias when  24 \\ncomparing machine learning models, as models that are not equally optimized cannot be fairly \\ncompared. For instance, in Studies #3, #7, and #18, only models such as Random Forest and \\nExtreme Gradient Boosting (XGBoost) were fine -tuned by introducing Grid S earch with Cross -\\nvalidation, while other models were left with default settings. This selective tuning can unfairly \\nfavor the optimized models, potentially overstating their effectiveness and underrepresenting the \\ntrue capabilities of the untuned models.  \\nAdditionally, a lack of transparency and reproducibility  in studies such as Study #6 , which \\ndid not provide details on hyperparameter tuning, makes it difficult to discern whether a model\\'s \\npoor performance is due to suboptimal tuning or inherent limitations of the model itself. Missing \\nconsistency, transparency, and reproducibility  can all easily cause biased conclusions about which \\nalgorithms are most effective for tasks such as detecting fake accounts  or fake news.  \\nThe evaluation reveals inconsistency in hyperparameter tuning across the reviewed studies. \\nApproximately 36% of the studies reported hyperparameters and applied tuning techniques \\nconsistently across all models, contributing to fair comparisons, and thus, r eliable results. In \\ncontrast, about 64% of the studies either selectively tuned certain models or failed to report \\nhyperparameter tuning details altogether, introducing potential biases. Models that underwent \\ntuning often appeared to perform better, not ne cessarily due to their inherent superiority, but \\nbecause they were optimized. Untuned models may underperform simply due to lack of \\noptimization.  \\nTo mitigate potential biases and enhance the reliability of findings, it would be beneficial \\nto consistently apply hyperparameter tuning techniques, such as grid search or random search, \\nacross all models evaluated in future research. Providing detailed de scriptions of the \\nhyperparameters used and the tuning processes undertaken can improve reproducibility and allow  25 \\nfor a more accurate assessment of model performance. Additionally, employing standardized \\nevaluation protocols with consistent evaluation metrics and validation strategies, such as cross -\\nvalidation, will enable fairer comparisons across models. Assessing t he impact of hyperparameter \\ntuning on model performance may also provide valuable insights into the importance of tuning for \\ndifferent algorithms. By adopting these practices, it is possible to reduce biases associated with \\nhyperparameter tuning, leading t o more robust and generalizable conclusions about the \\neffectiveness of machine learning models in detecting fake accounts and misinformation on social \\nmedia platforms.  \\nData Partitioning (Q6):  \\nProper data partitioning is a critical aspect of machine learning workflows, helping to \\nensure accurate model evaluation and mitigate overfitting. This section assesses the data \\npartitioning strategies employed in the reviewed studies, particularly whether  datasets were \\nappropriately divided into training, validation, and test sets or if cross -validation techniques were \\nused to ensure robust model evaluation.  \\nAmong the 36 studies reviewed, data partitioning practices fell into three distinct \\ncategories, as outlined in Table 5. A total of 55% of the studies adhered to established machine \\nlearning practices by splitting their data into training, validation, and t est sets, with performance \\nmetrics reported based on separate test sets to evaluate model generalizability  (G√©ron, 2022) . For \\nexample, Study #1 applied this approach by dividing their dataset into distinct sets for training, \\nvalidation, and testing, ensuring an unbiased evaluation of their model. Similarly, Study #2 also \\nimplemented this split, which contributed to the reli ability and validity of their model‚Äôs \\nperformance assessment on new, unobserved data.  \\n   26 \\nInsert Table 5 about here  \\n \\nMoreover, around 28% of the studies employed cross -validation techniques, such as k -fold \\ncross -validation, to enhance the robustness of their evaluations. By employing k -fold cross -\\nvalidation and similar methods, these studies improved the reliability of t heir evaluations by testing \\nthe model across multiple subsets of the data (Kohavi, 1995) . For instance, Study #18 used 10 -\\nfold cross -validation to assess their model\\'s performance across multiple data subsets, thereby \\nimproving the reliability of their evaluation. Similarly, Study #20  applied 10 -fold cross -validation, \\nwhich helped ensure that their model\\'s performance metrics were not biased by a single train -test \\nsplit, leading to a more comprehensive evaluation of model effectiveness.  \\nAmong the reviewed papers, one study (Study #1) explored the impact of different data \\nsplits on model performance. The authors investigated how varying training and testing splits, such \\nas 80 -20 and 70 -30, influenced the model\\'s predictive ability when com bined with five -fold cross -\\nvalidation. This study provided valuable insights into how different training set sizes can affect \\nthe model‚Äôs generalizability and robustness, especially in the context of misinformation detection \\nduring crisis events. By examin ing these data partitioning strategies, the study offered a detailed \\nunderstanding of how different training set sizes affect the model‚Äôs generalizability and robustness, \\nparticularly in the context of misinformation detection during crisis events.  \\nHowever, approximately 17% of the reviewed studies failed to provide sufficient details \\nregarding their data partitioning methods, potentially compromising the reliability and validity of \\ntheir results. For example, Study #25  did not specify how their dataset was divided into training \\nand test sets or whether cross -validation techniques were employed. The absence of explicit \\ninformation on data splitting in these studies raises concerns about the validity of their results, as  27 \\nthe lack of proper data partitioning can compromise the assessment of a model\\'s ability to \\ngeneralize to unobserved data  (Dietterich, 1995) . \\nMost  studies followed best practices in data partitioning, either by clearly dividing data \\ninto training, validation, and test sets or by employing cross -validation techniques, ensuring that \\nmodels were evaluated on unseen data. However, approximately 17% of t he studies did not report \\ntheir data partitioning methods, which poses a significant risk of overfitting. When data \\npartitioning is not clearly defined, models may perform exceptionally well on the training data but \\npoorly on unseen data, leading to in flated performance metrics As Witten and Frank (2002)  \\nhighlighted, models that are not tested on separate or cross -validated data risk learning specific \\npatterns that do not generalize beyond the training set. This failure to report or utilize adequate \\npartitioning methods results in a lack of transparency a nd potentially biased conclusions about \\nmodel performance.  \\nIn summary, most  reviewed studies adhered to best practices in data partitioning, with 55% \\nemploying a clear training/validation/test split and 28% utilizing cross -validation techniques. \\nThese approaches enhanced the credibility and applicability of their machine -learning  models by \\nproviding reliable assessments of model performance and facilitating the development of models \\nthat generalize well to unseen data. However, 17% of the studies did not adequately report their \\ndata partitioning methods, which may have compromised the validity of their results due to \\npotential overfitting, which occurs when a model learns the training data too closely, including \\nnoise and outliers, and fails to perform effectively on new data (Hawkins, 2004; Srivastava et al., \\n2014) . This highlights the importance of proper data partitioning and validation procedures in \\nmachine learning research.   28 \\nMoving forward, it would be beneficial for future research to clearly document data \\npartitioning strategies, including the proportions used for training, validation, and testing. \\nReporting performance metrics based on validation or test sets, rather than s olely on training data, \\ncan provide a more unbiased assessment of model performance. Utilizing and describing cross -\\nvalidation methods where appropriate can further enhance the robustness of model evaluation. \\nAdopting these practices is likely to improve t he transparency, reproducibility, and generalizability \\nof machine learning research, leading to the development of models that are both robust and \\napplicable to real -world scenarios (Cawley & Talbot, 2010; Varma & Simon, 2006).  \\nModel Evaluation: Evaluation Metrics and Class Imbalance (Q8, Q9 & Q10)  \\nModel evaluation is critical in the context of detecting fake news and fake accounts, \\nparticularly due to the significant challenge of class imbalance in the datasets. Typically, real \\ncontent greatly outnumbers fake content, leading to skewed distributions  where standard models \\nmay perform well on majority classes (i.e., real news) but fail to correctly classify minority classes \\n(i.e., fake news). This imbalance necessitates special methods to ensure that models are not biased \\ntowards the majority class. Tw o key techniques for addressing class imbalance in machine learning \\ninclude reweighting and resampling. Reweighting assigns higher penalties for misclassifying \\ninstances from the minority class, thereby forcing the model to focus more on these cases during  \\ntraining. Resampling, on the other hand, involves adjusting the  dataset by either oversampling the \\nminority class or undersampling the majority class to create a more balanced distribution. In \\naddition to these techniques, robust evaluation metrics are cr ucial when dealing with imbalanced \\ndata.  Metrics such as precision, recall, F1 score, or Area Under the Receiver Operating \\nCharacteristic Curve (AUROC) are more informative in such cases than simple accuracy, \\nespecially  in imbalanced scenarios.   29 \\nIn the context of fake identification, precision measures how many of the news items \\nflagged as fake are fake, helping reduce false positives and maintain credibility. Recall, on the \\nother hand, focuses on how well the model identifies actual fake news, ensuring that \\nmisinformation is detected and prevented from spreading. However, balancing recall with \\nprecision  is essential, as high recall alone can lead to many false positives (Bishop, 2006). The F1 \\nscore combines precision and recall into a single metri c, providing a balanced view of the model‚Äôs \\nperformance. Additionally, the AUROC evaluates the model‚Äôs ability to distinguish between fake \\nand real news across different thresholds, making it especially useful for handling imbalanced \\ndatasets and assessing  the overall effectiveness of fake news detection.  \\nAcross the reviewed studies, many recognized the importance of these advanced metrics, \\nwith 86.1% of the studies (Studies #1, #3, #5, #7, #8, #10, and #12 ‚Äî#17, #19 ‚Äî#36) incorporating \\nF1 score, precision, recall, or AUROC in their evaluations, moving beyond  accuracy, which can \\nbe misleading in imbalanced datasets. For example, Study #1 utilized precision, recall, and the \\nmacro -averaged F1 score as evaluation metrics. By doing so, they acknowledged the inherent class \\nimbalance in their dataset and provided a more accurate reflection of the model\\'s ability to handle \\nboth majority and minority classes effectively.  \\nSeveral  studies (Studies #3, #4, #6, #7, #9, #11, #35 ) addressed this issue by balancing \\ntheir datasets, either naturally or through specific data preprocessing techniques. For instance, \\nStudy #35 used a dataset with near -balanced classes, where real news constituted approximately \\n52% and fake news about 48% , mitigating the need for class imbalance -specific preprocessing \\nsteps. In cases of more pronounced imbalance, such as in Study #3, the researchers balanced their \\ndataset by randomly selecting non -spamme r users to match the number of fake accounts (i.e., \\nundersampling the majority class). This approach improved the model\\'s ability to detect both fake  30 \\nand real accounts. In contrast, Study #7 employed oversampling of the minority class using the \\nSMOTE -NC (Synthetic Minority Over -sampling Technique for Nominal and Continuous data) \\nalgorithm. This approach helped mitigate the bias introduced by imbalanced datasets and enhanced \\nthe model\\'s ability to detect both fake and real accounts. In addition to resampling techniques, \\nsome studies also employed reweighting strategies. For example, Study #4 dealt with class \\nimbalance in a phishing detection system by ass igning more weights to the minority phishing class. \\nThe dataset was heavily skewed towards legitimate tweets, but by applying these weights, the \\nresearchers were able to balance the prediction error and minimize the overall error rate. This \\nreweighting app roach ensured that phishing tweets, which were much rarer, were still given \\nadequate attention by the model.  \\nFurthermore, some studies (e.g., Study #7) went beyond data preprocessing techniques by \\nincorporating preferred evaluation metrics such as precision, recall, and F1 scores to provide a \\nmore balanced and informative assessment of model performance.  \\nHowever, not all studies (Studies #2, #9, and #18) adequately addressed the issue of class \\nimbalance. For example, Study #9 relied primarily on accuracy as evaluation metrics, without \\nimplementing any resampling techniques or strategies to address class im balance. This oversight \\nsuggests that researchers did not fully recognize the potential bias introduced by imbalanced data, \\nleading to inflated performance metrics when using accuracy alone. Similarly, Study #18 depend \\nheavily on accuracy as the main evalu ation metric without acknowledging or addressing the \\nskewed class distribution in their dataset. This failure to consider class imbalance undermines the \\nvalidity of their results, as accuracy can be artificially inflated by the overrepresentation of \\nmajori ty-class examples. In contrast, Study 2 did recognize the issue of class imbalance but \\nattempted to address it only through basic normalization techniques like Min -Max scaling, which  31 \\ndoes not solve the imbalance problem. While the researchers were aware of the issue, their \\napproach may not have been sufficient, as more effective methods like resampling or reweighting \\nwould have been necessary to properly handle the imbalanced data, pot entially limiting the \\nreliability of their model\\'s performance evaluation.  \\nOverall, 86.1% of the reviewed studies did not adequately address the class imbalance \\ninherent in fake news datasets. While some researchers effectively employed preprocessing  \\ntechniques and utilized preferred evaluation metrics, others continued to rely on outdated practices, \\nsuch as focusing  on accuracy without considering the effects of class imbalance. To enhance the \\nrobustness and reliability of fake news detection models, future research should adopt more \\ncomprehensive approaches, including  data prepr ocessing methods such as resampling , \\nreweighting, and the consistent use of preferred metrics like recall, precision, F1 score, and \\nAUROC. Incorporating these strategies will lead to a more accurate and nuanced evaluation of \\nmodel performance, especially in scenarios with imbalanced scenari os.  \\nReporting: Transparency and Completeness:  \\nIn evaluating the transparency and completeness of reporting in the reviewed studies, we \\nassessed whether they provided sufficient details about their data collection, preprocessing, model \\ndevelopment, hyperparameter tuning, and evaluation processes. Trans parent reporting is essential \\nfor reproducibility and allows for critical evaluation of methodologies and findings.  \\nOut of the 36 studies reviewed, 20 studies (55.6%) demonstrated a high level of \\ntransparency. These studies provided comprehensive details about their methodologies, including \\ndata sources, sampling methods, preprocessing steps, model architectures, hyperp arameter tuning \\nprocesses, and evaluation metrics. For example, Study #1 meticulously documented their sampling \\nmethods, dataset size, preprocessing techniques, model parameters, and evaluation metrics,  32 \\nfacilitating reproducibility and critical assessment. Study #4 provided detailed descriptions of their \\ndataset collection, feature extraction process, hyperparameter tuning using grid search for all \\nmodels, and provided code snippets or references to repos itories.  \\nConversely, 16 studies (44.4%) lacked transparency in critical areas: 12 studies (33.3%) \\nfailed to fully describe data sources, sampling criteria, or preprocessing steps, limiting assessment \\nof data quality and representativeness (e.g., Study #18 lacked de tails on non -spam user sampling). \\nIn 23 studies (63.9%), hyperparameter tuning was inconsistently applied or undocumented, with \\nsome models optimized (e.g., SVM in Study #7) while others were not. Additionally, 9 studies \\n(25%) did not clearly report evalua tion metrics or justify their choices, particularly in handling \\nclass imbalance, as seen in Study #9, which relied on accuracy without addressing imbalance \\nissues.  \\nThe lack of comprehensive reporting in many studies introduces several biases: \\nPerformance bias arises from inadequate documentation of hyperparameter tuning and model \\ndevelopment, potentially leading to overestimation of a model‚Äôs effectiveness due to ove rfitting. \\nReproducibility issues emerge when insufficient methodological details prevent other researchers \\nfrom replicating or building on the work, hindering scientific progress. Interpretation bias occurs \\nwhen there is insufficient reporting on data prep rocessing and handling of class imbalance, making \\nit difficult to assess the validity of the results and potentially leading to misleading conclusions.  \\nApproximately 55.6% of the reviewed studies provided transparent and comprehensive \\nreporting, enhancing the credibility and reproducibility of their findings. However, 44.4% of the \\nstudies lacked sufficient detail in critical areas such as data collection, preprocessing, \\nhyperparameter tuning, and model evaluation. This lack of transparency introduces potential biases \\nand hampe rs the ability to critically assess and replicate the studies.   33 \\nTo mitigate these issues, future research should prioritize comprehensive methodological \\nreporting by providing detailed descriptions of data sources, sampling methods, preprocessing \\nsteps (including handling of linguistic nuances like negations), model ar chitectures, and \\nhyperparameter tuning processes. Additionally, it is crucial to ensure transparent evaluation \\nprocedures by justifying the choice of evaluation metrics, particularly in imbalanced datasets, and \\nreporting all relevant metrics (precision, re call, F1 score, AUROC). Describe validation methods, \\nincluding data partitioning strategies. Clear descriptions of validation methods and data \\npartitioning strategies are equally important.  Furthermore, studies should include an \\nacknowledgment of potential biases and limitations, such as non -representative sampling or class \\nimbalance issues, and explain the strategies used to mitigate them.  \\nFinally, enhancing reproducibility should be a key goal, with researchers encouraged to \\nshare datasets and code repositories whenever possible. Following established reporting \\nguidelines, such as the Transparent Reporting of a Multivariable Prediction Mode l for Individual \\nPrognosis or Diagnosis (TRIPOD) statement, can further strengthen the reliability and \\ntransparency of research in machine learning applications for fake news detection.  By adopting \\nthese practices, researchers can improve the reliability a nd impact of their studies, contributing to \\nthe advancement of machine learning applications in fake news detection.  \\nResult Summary  \\nThis section provides a systematic review of studies using ML and DL models for fake \\nnews detection on social media. The analysis highlights key insights into model effectiveness \\nwhile identifying significant limitations and biases impacting performance an d generalizability. \\nBiases were found across the machine learning lifecycle, including non -representative sampling,  34 \\ninadequate handling of linguistic nuances, improper hyperparameter tuning, and over -reliance on \\naccuracy for imbalanced datasets, which reduces model effectiveness in real -world scenarios.  \\nTo address these issues, future research should focus on improving representative sampling \\nby incorporating diverse social media platforms, languages, and demographics. Standardizing \\npreprocessing techniques, especially for handling linguistic features and  class imbalance, is \\nessential for model improvement. Consistent hyperparameter tuning and transparent reporting will \\nfurther enhance model optimization and comparability.  \\nAddressing class imbalance with appropriate evaluation metrics like F1 score and recall, \\nrather than accuracy, is critical for robust model performance. Increased transparency and \\nreproducibility, including sharing datasets and code, will support validatio n and advancement in \\nthe field. Lastly, exploring advanced models and integrating multimodal data (text, images, videos) \\ncan enhance detection capabilities.  \\nIn conclusion, overcoming these challenges will lead to more accurate, generalizable \\nmachine learning models for fake news detection, strengthening efforts to combat misinformation \\non social media and fostering trustworthy online environments.  \\nDiscussion  \\nThe proliferation of misinformation and deceptive content on social media platforms has \\nnecessitated the development of automated detection methods. ML and DL models have shown \\nconsiderable promise in identifying fake news, spam, and fake profiles by analyzing vast amounts \\nof data and capturing complex patterns. These models, particularly DL architectures, can handle \\nnuanced language and contextual cues, making  them powerful tools in the fight against \\nmisinformation. However, this systematic review of 36 st udies employing ML and DL techniques  35 \\nreveals several key limitations and biases, such as selection bias, inconsistent data preprocessing, \\nclass imbalance, and poor hyperparameter tuning, which hinder the full potential of these models \\nin real -world applications.  \\nKey Findings and Identified Biases  \\nThe reviewed studies utilized a diverse array of ML and DL models to detect deceptive \\ncontent. Traditional supervised learning algorithms such as Random Forest (used in 17 studies), \\nSupport Vector Machines (SVM) (16 studies), Naive Bayes (12 studies), and Logi stic Regression \\n(14 studies) were prominently featured due to their effectiveness in classification tasks. Deep \\nlearning models, particularly Artificial Neural Networks (ANN) and Long Short -Term Memory \\n(LSTM) networks, were also employed in several studies  (8 and 2 studies, respectively). These \\nmodels demonstrated enhanced capabilities in capturing complex patterns and contextual \\ninformation within unstructured data. These models demonstrated enhanced capabilities in \\ncapturing complex patterns and contextua l information within unstructured data. For example, \\nstudies using LSTM networks reported F1 scores exceeding 90%, showcasing their effectiveness \\nin handling sequential textual data prevalent in social media posts. Overall, these machine learning \\nand deep learning models have proven useful in detecting fake news, fake accounts, and other \\ntypes of misinformation, showing promising results in enhancing accuracy and robustness in a \\nrange of detection tasks across different platforms.  \\nThe majority of studies focused on Twitter (19 studies) and Instagram (9 studies), reflecting \\nthe prominence of these platforms in social discourse and the availability of data. Facebook and \\nReddit were less represented, with 3 studies each, and a few stud ies explored multiple platforms \\nor other social media like Weibo. This platform focus introduces selection bias, limiting the \\ngeneralizability of findings across diverse social media platforms, which exhibit different user  36 \\nbehaviors and content types.  The types of deceptive content analyzed varied, with a significant \\nfocus on fake profiles and fake accounts (15 studies) and general fake news detection (12 studies). \\nSpam and scam detection were addressed in 5 studies, while phishing detection and health -related \\nmisinformation were less commonly explored. Only one study specifically targeted \\nmisinformation during crisis events, suggesting a gap in research for this critical area of public \\nconcern.  \\nBiases were identified throughout the entire lifecycle of machine learning applications, \\nfrom data collection to model evaluation. In data sampling, approximately 55% of the studies \\nexhibited selection bias by focusing on specific events or user groups, li miting the generalizability \\nof their findings. Class imbalance was another major issue, with many studies failing to properly \\naddress the disproportion between real and deceptive content, resulting in models biased toward \\nthe majority class. In data prepro cessing, 86% of the studies did not adequately handle linguistic \\nnuances, such as negations, which can lead to misinterpretation of text and reduced model \\naccuracy. During model development, inconsistent hyperparameter tuning was found in 64% of \\nthe studie s, leading to suboptimal model performance and making fair comparisons between \\nmodels difficult. Finally, model evaluation practices showed bias, as many studies relied heavily \\non accuracy, a metric not suited for imbalanced datasets, rather than using mor e appropriate \\nmeasures like precision, recall, and F1 score. These issues collectively impact the reliability and \\napplicability of the models in detecting deceptive content in real -world scenarios.  \\nLimitations  \\nThis systematic review provides valuable insights into the application of machine learning \\nand deep learning models for fake news detection on social media platforms, but several limitations \\nshould be acknowledged. First, the review focuses on a specific s et of studies that utilized machine  37 \\nlearning for detecting fake news, spam, and fake profiles. This limited scope means it may not \\ncapture all relevant studies, particularly those published in less accessible databases or in \\nlanguages other than English. Moreover, the rapid advancement of ma chine learning techniques, \\nespecially in natural language processing and deep learning, presents another limitation. Some of \\nthe methodologies discussed may already be outdated, with newer approaches potentially offering \\nimproved performance and addressing  challenges like bias and class imbalance more effectively.  \\nAnother limitation lies in the focus of most reviewed studies on textual data. While text \\nanalysis is central to fake news detection, many studies did not explore multimodal approaches \\nthat integrate other forms of content such as images, videos, and metad ata. Given the growing \\nprevalence of multimedia content in online misinformation, this narrow focus may understate the \\npotential of more comprehensive, multimodal detection techniques. Additionally, the variability \\nin evaluation standards across the studie s, including differences in preprocessing techniques, \\nhyperparameter tuning, and performance metrics, makes it difficult to directly compare their \\nresults. This inconsistency hampers the ability to draw broad, definitive conclusions about the most \\neffectiv e models and approaches.  \\nWhile the review focuses primarily on technical issues like bias and class imbalance, it \\ndoes not extensively address the ethical considerations involved in deploying machine learning \\nmodels for fake news detection. Important concerns such as privacy, fair ness, and the potential for \\nunintended societal consequences were outside the scope of this review but are critical factors that \\nshould be considered when developing automated misinformation detection systems. Finally, the \\nreviewed studies mostly focused o n retrospective analyses of pre -collected data, with limited \\nexploration of the challenges involved in real -time detection. Since real -time detection is vital in  38 \\ncurbing the immediate spread of misinformation, this gap highlights an area where future research \\nshould concentrate.  \\nBy acknowledging these limitations, the review underscores the need for more \\ncomprehensive approaches in future research, such as the development of multimodal detection \\ntechniques, the adoption of standardized evaluation methods, and the exploration of re al-time \\ndetection capabilities. Additionally, integrating ethical considerations into research practices will \\nbe essential for ensuring the responsible deployment of machine learning models to combat \\nmisinformation effectively.  \\nConcluding Remark  \\nThis systematic review highlights the potential of machine learning and deep learning \\nmodels in detecting fake news and other forms of deceptive content on social media platforms. \\nHowever, the review also underscores several critical limitations in current  approaches, \\nparticularly in terms of selection bias, class imbalance, inadequate handling of linguistic nuances, \\ninconsistent hyperparameter tuning, and reporting practices. These challenges undermine the \\ngeneralizability and robustness of the models revi ewed, signaling a need for improved \\nmethodologies.  \\nThe limitations outlined in this review emphasize the importance of expanding future \\nresearch to include more representative sampling, better handling of data imbalances, multimodal \\napproaches, and the consistent use of advanced evaluation metrics. Additio nally, as real -time \\ndetection and ethical considerations gain prominence in the fight against misinformation, the \\nintegration of interdisciplinary research and practical solutions is imperative for ensuring the \\neffectiveness and fairness of automated detec tion systems.   39 \\nBy addressing these limitations, future research can advance the field of fake news \\ndetection, making machine learning models more reliable, transparent, and applicable across a \\nbroader range of real -world scenarios. This progress is crucial to curbing the  spread of \\nmisinformation and ensuring a more trustworthy online environment.  \\nAppendix 1. Reviewed Studies on Machine Learning Models for Fake News/Profile Detection \\non Social Media  \\nInsert Table A1 about here  \\n \\nReference:  \\nAbdullah All, T., Mahir, E. M., Akhter, S., & Huq, M. R. (2019, 28 -30 June 2019). \\nDetecting Fake News using Machine Learning and Deep Learning Algorithms. 2019 7th \\nInternational Conference on Smart Computing & Communications (ICSCC),  \\nAggarwal, A., Rajadesingan, A., & Kumaraguru, P. (2012, 2012). PhishAri: Automatic \\nrealtime phishing detection on twitter. 2012 eCrime Researchers Summit,  \\nAkyon, F. C., & Kalfaoglu, M. E. (2019). Instagram fake and automated account detection. \\n2019 Innovations in intelligent systems and applications conference (ASYU),  \\nAlghamdi, J., Lin, Y., & Luo, S. (2023). Towards COVID -19 fake news detection using \\ntransformer -based models. Knowl Based Syst, 274, 110642. \\nhttps://doi.org/10.1016/j.knosys.2023.110642  \\nAllcott, H., & Gentzkow, M. (2017). Social Media and Fake News in the 2016 Election. \\nJournal of Economic Perspectives, 31, 211 -236. https://doi.org/10.1257/jep.31.2.211  \\nAmin, E., Socheanet, L., & Meel, P. (2023). Enhancing the detection of fake news in social \\nmedia based on machine learning models. 2023 2nd International Conference on Applied Artificial \\nIntelligence and Computing (ICAAIC),  \\nAnklesaria, K., Desai, Z., Kulkarni, V., & Balasubramaniam, H. (2021). A survey on \\nmachine learning algorithms for detecting fake instagram accounts. 2021 3rd International \\nConference on Advances in Computing, Communication Control and Networking (ICAC3N),    40 \\nAzer, M., Taha, M., Zayed, H. H., & Gadallah, M. (2021). Credibility detection on twitter \\nnews using machine learning approach. International Journal of Intelligent Systems and \\nApplications, 12(3), 1.  \\nBay, Y. Y., & Yearick, K. A. (2024). Machine Learning vs Deep Learning: The \\nGeneralization Problem. https://arxiv.org/abs/2403.01621  \\nBessi, A., & Ferrara, E. (2016). Social Bots Distort the 2016 US Presidential Election \\nOnline Discussion. First Monday 21(11). https://ssrn.com/abstract=2982233  \\nBharath, G., Manikanta, K. J., Prakash, G. B., Sumathi, R., & Chinnasamy, P. (2021). \\nDetecting Fake News Using Machine Learning Algorithms. 2021 International Conference on \\nComputer Communication and Informatics (ICCCI), 2332 -2335. \\nhttps://doi.org/10.1109/ ICCCI50826.2021.9402470  \\nBhattacharya, A., Bathla, R., Rana, A., & Arora, G. (2021). Application of machine \\nlearning techniques in detecting fake profiles on social media. 2021 9th International Conference \\non Reliability, Infocom Technologies and Optimization (Trends and Future Di rections) (ICRITO),  \\nBishop, C. M. (2006). Pattern Recognition and Machine Learning (Information Science \\nand Statistics). Springer -Verlag.  \\nBraithwaite, S. R., Giraud -Carrier, C., West, J., Barnes, M. D., & Hanson, C. L. (2016). \\nValidating Machine Learning Algorithms for Twitter Data Against Established Measures of \\nSuicidality. JMIR Ment Health, 3(2), e21. https://doi.org/10.2196/mental.4822  \\nCawley, G. C., & Talbot, N. L. C. (2010). On Over -fitting in Model Selection and \\nSubsequent Selection Bias in Performance Evaluation. Journal of Machine Learning Research, 11, \\n2079 -2107  \\nChawla, N., Japkowicz, N., & Ko≈Çcz, A. (2004). Editorial: Special Issue on Learning from \\nImbalanced Data Sets. SIGKDD Explorations, 6, 1 -6. https://doi.org/10.1145/1007730.1007733  \\nChen, X., Chandramouli, R., & Subbalakshmi, K. P. (2014). Scam Detection in Twitter \\n(Vol. 3). Springer International Publishing :. https://doi.org/10.1007/978 -3-642-45252 -9_9  \\nChoi, J., Jeon, B., & Jeon, C. (2024). Scalable Learning Framework for Detecting New \\nTypes of Twitter Spam with Misuse and Anomaly Detection. Sensors (Basel), 24(7). \\nhttps://doi.org/10.3390/s24072263  \\nConroy, N., Rubin, V., & Chen, Y. (2015). Automatic Deception Detection: Methods for \\nFinding Fake News.   41 \\nCueva, E., Ee, G., Iyer, A., Pereira, A., Roseman, A., & Martinez, D. (2020). Detecting \\nFake News on Twitter Using Machine Learning Models. 2020 IEEE MIT Undergraduate Research \\nTechnology Conference (URTC), 1 -5. https://doi.org/10.1109/URTC51696.2020.96688 72 (2020 \\nIEEE MIT Undergraduate Research Technology Conference (URTC))  \\nDan, H. C., Lu, B., & Li, M. (2024). Evaluation of asphalt pavement texture using \\nmultiview stereo reconstruction based on deep learning.  Construction and Building Materials,  412, \\n134837.  \\nDan, H. C., Huang, Z., Lu, B., & Li, M. (2024). Image -driven prediction system: Automatic \\nextraction of aggregate gradation of pavement core samples integrating deep learning and \\ninteractive image processing framework.  Construction and Building Materials,  453, 139056.  \\nDan, H. C., Yan, P., Tan, J., Zhou, Y., & Lu, B. (2024). Multiple distresses detection for \\nAsphalt Pavement using improved you Only Look Once Algorithm based on convolutional neural \\nnetwork.  International Journal of Pavement Engineering,  25(1), 2308169.  \\nDietterich, T. (1995). Overfitting and undercomputing in machine learning. ACM \\ncomputing surveys (CSUR), 27(3), 326 -327.  \\nEkosputra, M. J., Susanto, A., Haryanto, F., & Suhartono, D. (2021). Supervised machine \\nlearning algorithms to detect instagram fake accounts. 2021 4th International Seminar on Research \\nof Information Technology and Intelligent Systems (ISRITI),  \\nEr≈üahin, B., √ñ, A., Kƒ±lƒ±n√ß, D., & Akyol, C. (2017, 5 -8 Oct. 2017). Twitter fake account \\ndetection. 2017 International Conference on Computer Science and Engineering (UBMK),  \\nEzarfelix, J., Jeffrey, N., & Sari, N. (2022). A Systematic Literature Review: Instagram \\nFake Account Detection Based on Machine Learning. Engineering, MAthematics and Computer \\nScience Journal (EMACS), 4(1), 25 -31.  \\nFerrara, E., Varol, O., Davis, C., Menczer, F., & Flammini, A. (2016). The rise of social \\nbots. Commun. ACM, 59(7), 96 ‚Äì104. https://doi.org/10.1145/2818717  \\nG√©ron, A. (2022). Hands -on machine learning with Scikit -Learn, Keras, and TensorFlow.  \\nGupta, A., & Kumaraguru, P. (2012). Credibility ranking of tweets during high impact \\nevents Proceedings of the 1st Workshop on Privacy and Security in Online Social Media, Lyon, \\nFrance. https://doi.org/10.1145/2185354.2185356   42 \\nHan, W., & Mehta, V. (2019). Fake news detection in social networks using machine \\nlearning and deep learning: Performance evaluation. 2019 IEEE international conference on \\nindustrial internet (ICII),  \\nHarris, P., Gojal, J., Chitra, R., & Anithra, S. (2021). Fake instagram profile identification \\nand classification using machine learning. 2021 2nd Global Conference for Advancement in \\nTechnology (GCAT),  \\nHawkins, D. M. (2004). The problem of overfitting. Journal of chemical information and \\ncomputer sciences, 44(1), 1 -12.  \\nHe, H., & Garcia, E. A. (2009). Learning from Imbalanced Data. Knowledge and Data \\nEngineering, IEEE Transactions on, 21, 1263 -1284. https://doi.org/10.1109/TKDE.2008.239  \\nHelmstetter, S., & Paulheim, H. (2018). Weakly Supervised Learning for Fake News \\nDetection on Twitter. 2018 IEEE/ACM International Conference on Advances in Social Networks \\nAnalysis and Mining (ASONAM), 3568 -3277. https://doi.org/10.1109/ASONAM.2018.850852 0 \\n(2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining \\n(ASONAM))  \\nHunt, K., Agarwal, P., & Zhuang, J. (2022). Monitoring Misinformation on Twitter During \\nCrisis Events: A Machine Learning Approach. Risk Anal, 42(8), 1728 -1748. \\nhttps://doi.org/10.1111/risa.13634  \\nJiang, J. -Y., Li, C. -T., Chen, Y., & Wang, W. (2018). Identifying Users behind Shared \\nAccounts in Online Streaming Services. https://doi.org/10.1145/3209978.3210054  \\nKaddoura, S., Chandrasekaran, G., Elena Popescu, D., & Duraisamy, J. H. (2022). A \\nsystematic literature review on spam content detection and classification. PeerJ Comput Sci, 8, \\ne830. https://doi.org/10.7717/peerj -cs.830  \\nKanagavalli, N., & Priya, S. B. (2022). Social networks fake account and fake news \\nidentification with reliable deep learning. Intelligent Automation & Soft Computing, 33(1), 191 -\\n205.  \\nKaplan, A., & Haenlein, M. (2010). Users of the World, Unite! The Challenges and \\nOpportunities of Social Media. Business Horizons, 53, 59 -68. \\nhttps://doi.org/10.1016/j.bushor.2009.09.003   43 \\nKaushik, D., Hovy , E., & Lipton, Z. C. (2020). Learning the difference that makes a \\ndifference with counterfactually -augmented data. International Conference on Learning \\nRepresentations (ICLR),  \\nKodati, S., Pradeep Reddy, K., Mekala, S., Srinivasa Murthy, P., & Chandra Sekhar Reddy, \\nP. (2021). Detection of Fake Profiles on Twitter Using Hybrid SVM Algorithm. E3S Web of \\nConferences, 309, 01046. https://doi.org/10.1051/e3sconf/202130901046  \\nKohavi, R. (1995). A study of cross -validation and bootstrap for accuracy estimation and \\nmodel selection. Morgan Kaufman Publishing.  \\nLauer, D. (2021). Facebook‚Äôs ethical failures are not accidental; they are part of the \\nbusiness model. AI Ethics, 1, 395 ‚Äì403. https://doi.org/10.1007/s43681 -021-00068 -x  \\nLazer, D., Baum, M., Benkler, Y., Berinsky, A., Greenhill, K., Menczer, F., Metzger, M., \\nNyhan, B., Pennycook, G., Rothschild, D., Schudson, M., Sloman, S., Sunstein, C., Thorson, E., \\nWatts, D., & Zittrain, J. (2018). The science of fake news. Science, 359 , 1094 -1096. \\nhttps://doi.org/10.1126/science.aao2998  \\nLee, C. J., & Chua, H. N. (2022, 2022//). Using Linguistics and Psycholinguistics Features \\nin Machine Learning for Fake News ClassificationThrough Twitter. Proceedings of International \\nConference on Data Science and Applications, Singapore.  \\nLiew, S. W., Sani, N. F. M., Abdullah, M. T., Yaakob, R., & Sharum, M. Y. (2019). An \\neffective security alert mechanism for real -time phishing tweet detection on Twitter. Computers \\n& Security, 83, 201 -207. https://doi.org/10.1016/j.cose.2019.02.004  \\nM, H. -W., S, G., A, D., M, R., L, U., HA, S., DH, E., L, L., & B, C. (2021). Bots and \\nMisinformation Spread on Social Media: Implications for COVID -19. J Med Internet Res, 23(5). \\nhttps://doi.org/10.2196/26933 (e26933)  \\nMeda, C., Bisio, F., Gastaldo, P., & Zunino, R. (2014, 13 -16 Oct. 2014). A machine \\nlearning approach for Twitter spammers detection. 2014 International Carnahan Conference on \\nSecurity Technology (ICCST),  \\nMeshram, E. P., Bhambulkar, R., Pokale, P., Kharbikar, K., & Awachat, A. (2021). \\nAutomatic detection of fake profile using machine learning on instagram. International Journal of \\nScientific Research in Science and Technology, 8(1), 117 -127.  \\nNikam, S. S., & Dalvi, R. (2020). Machine Learning Algorithm based model for \\nclassification of fake news on Twitter. 2020 Fourth International Conference on I -SMAC (IoT in  44 \\nSocial, Mobile, Analytics and Cloud) (I -SMAC), 1 -4. https://doi.org/10.1109/I -\\nSMAC49090.2020.9243385 (2020 Fourth International Conference on I -SMAC (IoT in Social, \\nMobile, Analytics and Cloud) (I -SMAC))  \\nPatel, A., & Meehan, K. (2021). Fake news detection on reddit utilising countvectorizer \\nand term frequency -inverse document frequency with logistic regression, multinominalnb and \\nsupport vector machine. 2021 32nd Irish signals and systems conference (ISSC) ,  \\nPatel, K., Agrahari, S., & Srivastava, S. (2020). Survey on fake profile detection on social \\nsites by using machine learning algorithm. 2020 8th international conference on reliability, \\ninfocom technologies and optimization (trends and future directions) ( ICRITO),  \\nPurba, K. R., Asirvatham, D., & Murugesan, R. K. (2020). Classification of instagram fake \\nusers using supervised machine learning algorithms. International Journal of Electrical and \\nComputer Engineering, 10(3), 2763.  \\nRaturi, R. (2018). Machine learning implementation for identifying fake accounts in social \\nnetwork. International Journal of Pure and Applied Mathematics, 118(20), 4785 -4797.  \\nRoy, A., Nikolitch, K., McGinn, R., Jinah, S., Klement, W., & Kaminsky, Z. A. (2020). A \\nmachine learning approach predicts future risk to suicidal ideation from social media data. npj \\nDigital Medicine, 3(1). https://doi.org/10.1038/s41746 -020-0287 -6  \\nRuchansky, N., Seo, S., & Liu, Y. (2017). CSI: A Hybrid Deep Model for Fake News \\nDetection Proceedings of the 2017 ACM on Conference on Information and Knowledge \\nManagement, Singapore, Singapore. https://doi.org/10.1145/3132847.3132877  \\nSahoo, S. R., & Gupta, B. B. (2019). Hybrid approach for detection of malicious profiles \\nin twitter. Computers & electrical engineering, 76, 65 -81. \\nhttps://doi.org/10.1016/j.compeleceng.2019.03.003  \\nSahoo, S. R., & Gupta, B. B. (2021). Multiple features based approach for automatic fake \\nnews detection on social networks using deep learning. Applied Soft Computing, 100, 106983.  \\nShariff, M., Thoms, B., Isaacs, J. T., & Vakilian, V. (2022). Approaches in Fake News \\nDetection: An Evaluation of Natural Language Processing and Machine Learning Techniques on \\nthe Reddit Social Network. CS & IT Conference Proceedings,  \\nShu, K., Sliva, A., Wang, S., Tang, J., & Liu, H. (2017). Fake News Detection on Social \\nMedia: A Data Mining Perspective. SIGKDD Explor. Newsl., 19(1), 22 ‚Äì36. \\nhttps://doi.org/10.1145/3137597.3137600   45 \\nSmith, A. A., M. (2018). Social Media Use in 2018. \\nhttps://www.pewresearch.org/internet/2018/03/01/social -media -use-in-2018/  \\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). \\nDropout: a simple way to prevent neural networks from overfitting. The journal of machine \\nlearning research, 15(1), 1929 -1958.  \\nSUN, Y., WONG, A. K. C., & KAMEL, M. S. (2009). CLASSIFICATION OF \\nIMBALANCED DATA: A REVIEW. International Journal of Pattern Recognition and Artificial \\nIntelligence, 23(04), 687 -719. https://doi.org/10.1142/s0218001409007326  \\nTun√ß, √ú., Atalar, E., Gargƒ±, M. S., & Aydƒ±n, Z. E. (2022). Classification of Fake, Bot, and \\nReal Accounts on Instagram Using Machine Learning. Politeknik Dergisi, 27(2), 479 -488.  \\nUlu≈üan, O., & √ñzejder, ƒ∞. (2024). Faking the war: fake posts on Turkish social media during \\nthe Russia ‚ÄìUkraine war. Humanit Soc Sci Commun, 11(891). https://doi.org/10.1057/s41599 -\\n024-03409 -3  \\nVarma, S., & Simon, R. (2006). Bias in error estimation when using cross -validation for \\nmodel selection. . BMC Bioinformatics 7, 71. https://doi.org/10.1186/1471 -2105 -7-91  \\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, ≈Å., \\n& Polosukhin, I. (2017). Attention is all you need. Advances in Neural Information Processing \\nSystems, 30, 5998 -6008. \\nhttps://papers.nips.cc/paper/2017/file/3f5ee2435 47dee91fbd053c1c4a845aa -Paper.pdf  \\nVosoughi , S., Roy, D., & Aral, S. (2018). The spread of true and false news online. Science, \\n359(6380), 1146 -1151. https://doi.org/doi:10.1126/science.aap9559  \\nWitten, I. H., & Frank, E. (2002). Data mining: practical machine learning tools and \\ntechniques with Java implementations. Acm Sigmod Record, 31(1), 76 -77.  \\nZarocostas, J. (2020). How to fight an infodemic. The Lancet, 395(10225), 676. \\nhttps://doi.org/10.1016/s0140 -6736(20)30461 -x  \\nZhang, X., & Ghorbani, A. (2019). An overview of online fake news: Characterization, \\ndetection, and discussion. Information Processing & Management, 57. \\nhttps://doi.org/10.1016/j.ipm.2019.03.004  \\nZhou, X., & Zafarani, R. (2020). A Survey of Fake News: Fundamental Theories, \\nDetection Methods, and Opportunities. ACM Comput. Surv., 53(5), Article 109. \\nhttps://doi.org/10.1145/3395046     46 \\nTable 1. Key Data Extraction Categories for Systematic Review  \\nCategory  Details  \\nStudy Details  Title, Authors, Year of Publication, Journal or Source, DOI or URL  \\nResearch Objectives  Purpose of the Study, Research Questions or Hypotheses  \\nMethodological Aspects  Study Design, Settings, Sample Sizes, Inclusion and Exclusion Criteria, \\nData Collection Methods, ML/DL Models Employed  \\nCriteria Applied  Data included, e.g., publicly available tweets, specific language posts  \\nData excluded, e.g., private or insufficiently detailed posts  \\nPerformance Metrics  Metrics Used (e.g., Accuracy, Precision, Recall, F1 -score, AUROC , etc.)  \\nBias Evaluation  Data Collection and Preprocessing, Model Development and Tuning, \\nModel Evaluation and Reporting.  \\nAdditional Information  Confounding Factors, Study Limitations, Ethical Considerations, Funding \\nSources  \\n \\nTable 2. Bias Evaluation Questions for Each Domain  \\nDomain  Evaluation Questions  \\nSample Selection and \\nRepresentativeness  Q1. What is the sample used in this study, including the platform, sampling \\ncriteria, and sampling method?  \\nQ2. Does the sample represent the target population of social media users or \\nposts?  \\nData Preprocessing  Q3. Did the study specify its approach to handling negative words when using \\ntraditional or machine learning methods for sentiment analysis?  \\nModel Development  \\n Q4. Did this study report hyperparameters?  \\nQ5. If reported, did this study tune (optimize) hyperparameters or use default \\nsettings?  \\nQ6. If tuned hyperparameters in this study, was this done on all models \\nmentioned in the study?  \\nModel Evaluation  Q7. Did the study divide the dataset into training, validation, and test sets, and \\nwere the reported metrics based only on training data?   47 \\nQ8. What evaluation metric was used in this study?  \\nQ9. Is the evaluation metric appropriate for this context (i.e., class -imbalanced \\nsettings)?  \\nQ10. If the study used accuracy as an evaluation metric, did it mention \\npreprocessing steps to address class imbalance?  \\n \\nTable 3. Sampling Approaches Across Reviewed Studies  \\nSampling Approach  Number (%) of Studies  Studies #  \\nSpecific Events or User \\nGroups  20 (55%)  #1, #2, #3, #5, #7, #8, #9, #10, #12, #13, #14, #16, \\n#17, #18, #20, #21, #27, #31, #33, #34, #36  \\nGeneral Sampling with \\nSingle Platform  10 (28%)  #4, #6, #15, #19, #24, #26, #28, #29, #30, #32  \\nComprehensive Multi -\\nPlatform Sampling  6 (17%)  #11, #22, #23, #25, #35  \\n \\nTable 4. Hyperparameter Reporting and Tuning Practices in Reviewed Studies  \\nHyperparameter Reporting  Number (%) of Studies  Studies #  \\nReported & Tuned for All Models  13 (36.1%)  #1, #2, #10, #11, #12, #13, #17, \\n#21, #22, #23, #24, #33, #35  \\nReported but Partially Tuned  11 (30.6%)  #3, #4, #5, #7, #8, #18, #19, #29, \\n#30, #34, #36  \\nNot Reported or Tuned  12 (33.3%)  #6, #9, #14, #15, #16, #20, #25, \\n#26, #27, #28, #31, #32  \\n \\nTable 5. Summary of Data Partitioning Practices Across Reviewed Studies  \\nData Partitioning Practices  Number (%) of Studies  Studies #  \\nTraining/Validation/Test 20 (55%)  #1, #2, #3, #4, #5, #6, #7, #8, #9, #10,  48 \\nSplit  #11, #12, #13, #14, #15, #16, #17, \\n#29, #31, #35  \\nCross -validation without \\nTraditional Split  10 (28%)  #18, #19, #20, #21, #22, #23, #24, \\n#30, #34, #36  \\nInadequate or Unreported \\nPartitioning  6 (17%)  #25, #26, #27, #28, #32, #33,   \\n \\nTable A.1. Titles and Authors of Reviewed Studies on Machine Learning Models in \\nDetecting Deceptive Activities  on Social Media  \\nIndex  Title of paper  Reference  \\n1 Monitoring Misinformation on Twitter During Crisis Events: A \\nMachine Learning Approach  Hunt et al. (2022)  \\n2 Classification of Fake, Bot, and Real Accounts on Instagram Using \\nMachine Learning  Tun√ß et al. (2022)  \\n3 Scam Detection in Twitter  Chen et al. (2014)  \\n4 PhishAri: Automatic Realtime Phishing Detection on Twitter  Aggarwal et al. (2012)  \\n5 A Survey on Machine Learning Algorithms for Detecting Fake \\nInstagram Accounts  Anklesaria et al. (2021)  \\n6 Application of Machine Learning Techniques in Detecting Fake \\nProfiles on Social Media  Bhattacharya et al. \\n(2021)  \\n7 Instagram Fake and Automated Account Detection  Akyon & Kalfaoglu \\n(2019)  \\n8 Supervised Machine Learning Algorithms to Detect Instagram Fake \\nAccounts  Ekosputra et al. (2021)  \\n9 Fake Instagram Profile Identification and Classification Using \\nMachine Learning  Harris et al. (2021)  \\n10 Fake News Detection in Social Networks Using Machine Learning \\nand Deep Learning: Performance Evaluation  Han & Mehta (2019)  \\n11 Enhancing the Detection of Fake News in Social Media Based on \\nMachine Learning Models  Amin et al. (2023)  \\n12 Fake News Detection on Reddit Utilizing CountVectorizer and \\nTerm Frequency -Inverse Document Frequency with Logistic \\nRegression, MultinomialNB, and Support Vector Machine  Patel & Meehan (2021)  \\n13 Detecting Fake News on Twitter Using Machine Learning Models  Cueva et al. (2020)  \\n14 Machine Learning Algorithm -Based Model for Classification of \\nFake News on Twitter  Nikam & Dalvi (2020)  \\n15 Detection of Fake Profiles on Twitter Using Hybrid SVM \\nAlgorithm  Kodati et al. (2021)   49 \\n16 The Detection of Fake Messages Using Machine Learning  Looijenga (2018)  \\n17 Detecting Fake News Using Machine Learning and Deep Learning \\nAlgorithms  Abdullah All et al. \\n(2019)  \\n18 A Machine Learning Approach for Twitter Spammers Detection  Meda et al.  (2014)  \\n19 Scalable Learning Framework for Detecting New Types of Twitter \\nSpam with Misuse and Anomaly Detection  Choi et al. (2024)  \\n20 Classification of Instagram Fake Users Using Supervised Machine \\nLearning Algorithms  Purba et al. (2020)  \\n21 Approaches in Fake News Detection: An Evaluation of Natural \\nLanguage Processing and Machine Learning Techniques on the \\nReddit Social Network  Shariff et al. (2022)  \\n22 Multiple Features Based Approach for Automatic Fake News \\nDetection on Social Networks Using Deep Learning  Sahoo & Gupta (2021)  \\n23 Social Networks Fake Account and Fake News Identification with \\nReliable Deep Learning  Kanagavalli & Priya \\n(2022)  \\n24 Weakly Supervised Learning for Fake News Detection on Twitter  Helmstetter and \\nPaulheim ( 2018)  \\n25 Survey on Fake Profile Detection on Social Sites by Using Machine \\nLearning Algorithm  Patel et al. (2020)  \\n26 Twitter Fake Account Detection  Er≈üahin et al.  (2017)  \\n27 Using Linguistics and Psycholinguistics Features in Machine \\nLearning for Fake News Classification Through Twitter  Lee & Chua (2022)  \\n28 Detecting Fake News Using Machine Learning Algorithms   Bharath, G. et al. (2021 ) \\n29 Hybrid approach for detection of malicious profiles in twitter  Sahoo and Gupta (2019 ) \\n30 An effective security alert mechanism for real -time phishing tweet \\ndetection on Twitter  Liew  (2019 ) \\n31 Credibility detection on twitter news using machine learning \\napproach  Azer, et al. (2021 ) \\n32 Machine learning implementation for identifying fake accounts in \\nsocial network   Raturi  (2018 ) \\n33 Automatic detection of fake profile using machine learning on \\ninstagram  Meshram  (2021 ) \\n34 Validating Machine Learning Algorithms for Twitter Data Against \\nEstablished Measures of Suicidality  Braithwaite  et al.  (2016 ) \\n35 Towards COVID -19 fake news detection using transformer -based \\nmodels  Alghamdi  et al., (2023 ) \\n36 A machine learning approach predicts future risk to suicidal \\nideation from social media data  Roy et al. (2020 ) \\n  50 \\n \\nFigure 1: PRISMA Flow Diagram of Study Selection Process for Systematic Review on \\nMachine Learning Models in Detecting Deceptive Activities  on Social Media  \\n \\n',\n",
       " 'The 2024 Election Integrity Initiative\\nExposing Cross-Platform Coordinated Inauthentic Activity\\nin the Run-Up to the 2024 U.S. Election\\nFederico Cinus, Marco Minici, Luca Luceri, Emilio Ferrara\\nUniversity of Southern California\\nHUMANS Lab ‚Äì Working Paper No. 2024.7arXiv:2410.22716v1  [cs.SI]  30 Oct 2024Exposing Cross-Platform Coordinated Inauthentic Activity\\nin the Run-Up to the 2024 U.S. Election\\nFederico Cinus¬ß,‚ô°,‚ô†,‚àó, Marco Minici¬ß,‚ô¢,‚ô£,‚àó, Luca Luceri¬ß, Emilio Ferrara¬ß\\n¬ßUniversity of Southern California\\n‚ô¢ICAR-CNR ;‚ô£University of Pisa ;‚ô°Sapienza University of Rome ;‚ô†CENTAI .\\nABSTRACT\\nCoordinated information operations remain a persistent challenge on social media, despite platform\\nefforts to curb them. While previous research has primarily focused on identifying these operations\\nwithin individual platforms, this study shows that coordination frequently transcends platform\\nboundaries. Leveraging newly collected data of online conversations related to the 2024 U.S.\\nElection across X(formerly, Twitter), Facebook, and Telegram, we construct similarity networks\\nto detect coordinated communities exhibiting suspicious sharing behaviors within and across\\nplatforms. Proposing an advanced coordination detection model, we reveal evidence of potential\\nforeign interference, with Russian-affiliated media being systematically promoted across Telegram\\nandX. Our analysis also uncovers substantial intra- and cross-platform coordinated inauthentic\\nactivity, driving the spread of highly partisan, low-credibility, and conspiratorial content. These\\nfindings highlight the urgent need for regulatory measures that extend beyond individual platforms\\nto effectively address the growing challenge of cross-platform coordinated influence campaigns.\\nINTRODUCTION\\nSocial media has evolved into a vital arena for public discourse, serving as a platform where\\nindividuals and communities can converge to discuss political, social, and cultural issues. These\\nplatforms have been instrumental in fostering large-scale movements, such as the Arab Spring and\\nBlack Lives Matter, where activists and citizens alike have used social media to amplify calls for justice\\nand change [ 22,27]. However, social media has also been linked to polarization and radicalization,\\nas exemplified by the events surrounding the U.S. Capitol attack [ 15,37], where online platforms\\nplayed a role in mobilizing and coordinating participants [ 66]. Similar cases include the Christchurch\\nmosque shootings [ 17] and far-right rallies across Europe [ 32,60], where social media content has\\nbeen shown to exacerbate divisive sentiments and mobilize fringe communities [ 13]. The potential\\nfor influence of online discourse has contributed to the establishment of state-sponsored information\\noperations aimed at steering public opinion by introducing false or misleading narratives on popular\\nplatforms [47, 51, 69].\\nCoordinated inauthentic behavior represents a prominent tactic for distorting public discourse by\\namplifying specific viewpoints and giving the illusion of widespread support [ 11,25,31,48,58].\\nThese activities typically involve actions such as synchronized posting and co-activity behavior\\npatterns [ 41,55,58]‚Äî such as similar retweets, hashtag sequences, link sharing‚Äî designed to\\npush particular narratives to the forefront of public attention. Studies have also documented how\\nthese coordinated behaviors contribute to a range of detrimental social effects, such as the spread\\nof propaganda [ 29], conspiracy theories [ 64], and the promotion of disinformation [ 68]. Moreover,\\n*These authors contributed equally to this work.\\nAuthor‚Äôs address:\\n12\\ncoordinated online actions have been linked to heightened toxicity in online conversations [ 34] and\\nthe dissemination of extremist ideologies [33, 66].\\nIn recent years, a growing body of research has focused on the role of social media in the context\\nof political elections. For example, studies on the Brazilian elections have documented the strategic\\nuse of bots and coordinated networks to shape public opinion and influence voter perceptions,\\nhighlighting the risks of computational propaganda in electoral processes [ 56]. Similarly, research\\non European elections has illustrated how misinformation campaigns spread across social media\\ncan alter public perceptions, as seen in countries such as France, Germany, and Italy [ 7,18,35,53].\\nIn the United States, coordinated online activities around elections have garnered significant\\nattention, with investigations revealing how coordinated bot activities and foreign information\\noperations have sought to manipulate voter beliefs and amplify divisive content across multiple\\nelection cycles [8, 28, 39, 64]. Notably, this large body of literature has predominantly focused on\\nidentifying such operations within individual platforms.\\nContribution of this work\\nIn this work, we expand on current research in coordination detection by broadening our scope to\\ninclude the identification of coordinated inauthentic activity (CoIA) spanning multiple platforms.\\nWe build on, adapt, and advance state-of-the-art coordination detection techniques to identify\\nintra- and cross-platform CoIA that promotes external web domains and amplify specific narratives\\nin the context of the 2024 U.S. Presidential Election. We leverage a large-scale dataset covering\\nelection-related online conversations spanning several platforms, including Twitter/ X, Facebook,\\nand Telegram, collected during the run-up months to the Presidential Election. Combining insights\\nfrom computational techniques for CoIA detection and language models for content analysis, we\\naim to answer the following Research Questions (RQs):\\nRQ1:Web Domain Promotion. Do we observe intra- and cross-platform CoIAs aimed at redirecting\\ntraffic towards specific web domains? What are the characteristics of coordinated accounts, and\\nwhich specific domains do they promote?\\nRQ2:Content Amplification. Do we observe CoIAs pushing specific narratives? What are the\\ncharacteristics of coordinated accounts, and which specific topics do they amplify?\\nRQ3:Engagement & Impact. What level of engagement do CoIAs generate across various platforms?\\nAnd how does it compare to the engagement garnered by organic users?\\nLeveraging our multi-platform dataset and advancing CoIA detection techniques, we uncover\\nmultiple networks of coordinated inauthentic accounts. We analyzed the textual content and link-\\nsharing behaviors of these accounts, identifying the narratives they aim to amplify and the external\\ndomains they direct traffic to, assessing both their credibility and ties to foreign and domestic entities.\\nOur findings reveal coordinated efforts to promote Russian-affiliated media across Telegram and X,\\nwith highly partisan, low-credibility content systematically amplified by these networks. Conspiracy\\ntheories surrounding public health, the environment, and political topics, such as immigration and\\ngeopolitical tensions, were prominently featured. Notably, QAnon-related narratives were especially\\nprevalent on Telegram, with coordinated accounts driving much of the discussion. Furthermore,\\nwe assessed the prevalence of AI-generated content produced by coordinated actors and the level\\nof engagement their content attracted. We found that coordinated actors on Telegram relied on\\nAI-generated content significantly more than organic users, while the opposite trend was observed3\\non Facebook. This study sheds new light on cross-platform coordination efforts related to the\\nupcoming U.S. Presidential Election, revealing the complex dynamics of influence operations. These\\nfindings underscore the urgent need for regulatory measures that go beyond individual platforms to\\neffectively tackle the growing challenge of cross-platform coordinated inauthentic activity.\\nRELATED WORK\\nInfluence Campaigns Interfering with U.S. Elections\\nA growing body of research has focused on the influence of coordinated disinformation campaigns\\nduring U.S. elections, particularly through the manipulation of social media platforms [ 21,63].\\nStudies have uncovered significant efforts by foreign and domestic actors to manipulate the political\\nlandscape during both the 2016 and 2020 U.S. Presidential Elections [ 8,66]. For example, the\\nRussian Internet Research Agency (IRA) was linked to a large-scale disinformation campaign during\\nthe 2016 election, deploying thousands of social bots and state-sponsored trolls to promote divisive\\nnarratives and foster discord [ 3,4]. The activity of these inauthentic actors has been extensively\\nstudied on Twitter [ 1,38,40,71], with fewer studies examining their presence on other platforms\\n[72, 73].\\nSimilarly, the 2020 U.S. Presidential Election witnessed the propagation of various false claims\\nand conspiracy theories, including narratives about voter fraud and COVID-19 misinformation [ 21].\\nProminent examples include the ‚ÄúStop the Steal‚Äù movement, which spread across platforms like\\nTwitter and Facebook, inciting allegations that the election results were fraudulent [ 15]. Coordinated\\nactivity surrounding this narrative was widespread, often involving amplification techniques such as\\nautomated retweets or sharing similar content across accounts to create an illusion of widespread\\nconsensus [37].\\nDetection of Coordinated Inauthentic Activity\\nCoordinated inauthentic activity has been identified as a predominant tactic in spreading disinfor-\\nmation and conspiratorial content [ 41,57]. Researchers have developed sophisticated methods\\nto detect coordination. Coordination is not limited to malicious campaigns: it can also encompass\\nlegitimate organizing efforts, social movements, etc. However, in the context of disinformation cam-\\npaigns, coordinated behavior is typically characterized by manipulative actions aimed at amplifying\\nfalse narratives [52, 58].\\nThe detection of such deceptive, orchestrated efforts has evolved to address both automated and\\nhuman-coordinated activities through advanced machine learning and network-based approaches.\\nMachine learning techniques have traditionally focused on identifying automated activities, such\\nas bot-like behavior, and distinguishing them from human actions [ 12,70]. Recently, attention\\nhas shifted toward characterizing suspicious human-operated accounts, with research emphasizing\\ncontent-, behavioral-, and sequence-based methods to detect coordinated actions by state-sponsored\\ntrolls [2, 16, 30, 36, 40, 54].\\nNetwork-based detection, however, has gained prominence due to its ability to reveal coordinated\\nbehavior by constructing networks that highlight similarities in user actions, such as shared content,\\nhashtags, or synchronized posting times [ 41,43,44,52,57,58]. These networks are analyzed using\\nproperties like node centrality and edge weight, which help identify clusters of users involved in\\ncoordinated IOs [ 41,65,66]. This approach has proven effective in uncovering both automated and4\\nhuman-coordinated activities, providing critical insights into the tactics and structure of influence\\ncampaigns.\\nIn contrast to previous cross-platform studies [ 24,50,72], our approach introduces a more\\nadvanced and fine-grained coordination detection framework, specifically crafted to minimize false\\npositives by combining network measures (edge weights and node centralities) to identify a highly\\ncoordinated set of inauthentic accounts across multiple platforms. Unlike existing methods, which\\nhave explored dynamics between mainstream and fringe platforms, our work leverages state-of-the-\\nart language models and AI-generated content detectors to provide a nuanced characterization of\\nusers across a broader selection of mainstream social media platforms.\\nDATA\\nCollection. The rationale behind our data collection is to capture the online discourse surrounding\\nthe 2024 U.S. Presidential Election across multiple social media networks. The data collection for\\nour study spans May and June 2024 and covers three major online platforms: Facebook, X(formerly\\nTwitter), and Telegram. Each platform was queried to obtain posts or messages containing specific\\nelection-related keywords. The full list of keywords used to filter and collect data is provided in\\nAppendix 1.\\nFacebook data was collected through (the now defunct) Crowdtangle, which offered access to\\npublic posts of groups and pages. For X, we gather publicly available information, including original\\ntweets, retweets, replies, and quotes, retrieved via the platform‚Äôs web interface. Finally, Telegram\\ndata was gathered using the Telegram API, allowing the extraction of public chats‚Äô details, meta data,\\nmessages, and message attachments. Details on the Xand Telegram data collection infrastructure\\ncan be found in [5, 10].\\nPlatform Accounts Posts/Tweets/Messages URLs Domains\\nFacebook 6,137 46,310 15,009 5,247\\nX/Twitter 178,379 6,021,428 582,052 35,922\\nTelegram 15,537 4,309,880 2,087,078 183,924\\nTable 1. Datasets statistics.\\nStatistics. For each platform, the collected content consists of posts on Facebook, tweets on X, and\\nmessages on Telegram. To identify potential coordination among accounts operating within each\\nplatform, we analyze the textual similarity of their shared content as well as similarity patterns in\\nco-URL sharing. To establish coordination across platforms, we examine only the co-sharing of URLs,\\nas they represent specific pieces of information that can be easily tracked across different platforms\\n[42]. In fact, URLs can be embedded in user‚Äôs posts on each platform under analysis. These URLs\\nserve as a common thread linking sharing activities across the Web. Thereby, we identify and count\\nunique URLs embedded within the shared content of each dataset. Table 1 presents a summary of\\nthe datasets, including the number of accounts,1posts, and unique URLs for each platform.\\n1We will use the term accounts to also refer to Facebook pages, Facebook groups, and Telegram channels.5\\nMETHODOLOGY\\nExposing CoIA within and across Social Media Platforms\\nCoordinated accounts can execute their campaigns using various strategies; here, we analyze\\ncampaigns that focus on promoting specific URLs (or web domains) and campaigns that produce\\nhighly similar textual content to amplify certain topics. The former strategy is commonly used to\\ncreate the illusion of public consensus around certain viewpoints by artificially amplifying links\\nto external webpages, mock websites, and other social media networks [ 23,45]. The latter is\\noften employed to manipulate platform feed algorithms by pushing specific keywords or hashtags,\\nattempting to boost trending topics, and making content appear more popular than it actually is\\n[58,66]. We refer to these orchestrated efforts as web domain promotion andcontent amplification .\\nSince URL-sharing functions through a consistent mechanism across all observed platforms, we\\nanalyze web domain promotion both within and across platforms. Due to substantial variability in\\ntextual content-sharing‚Äîdriven by platform-specific constraints, such as character limits on some\\nplatforms, and differences in the nature of messaging (e.g., Telegram chats versus Facebook and X\\nposts)‚Äîour analysis of content amplification is focused on within-platform activity.\\nDetection of Web Domain Promotion. To identify orchestrated campaigns promoting specific\\nweb domains, we examine user similarities based on the URLs they share in their social media posts.\\nTo ensure high-quality data and reduce noise, we applied several preprocessing steps. First, we\\nimposed a minimum activity threshold, requiring each user to have shared at least 10 unique URLs.\\nThis criterion is consistent with prior work [ 41,58] and ensures that the analysis focuses on users\\nwith substantial contributions, filtering out those with minimal engagement that could distort the\\ndetection of coordinated accounts. Next, we expanded shortened or obfuscated URLs using a URL\\nexpansion library2.\\nCo-URL similarity network. We constructed a bipartite user-URL network, where users and URLs\\nare connected based on the URLs that users share. In the adjacency matrix of the bipartite graph,\\nusers are represented by the rows, whereas the columns represent the URLs. In accordance with\\nprevious work [ 41,58], we applied the Term Frequency-Inverse Document Frequency (TF-IDF)\\ntransformation to represent the user-URL matrix. To avoid bias towards overly frequent URLs, we\\nset a maximum document frequency (max DF) of the 90th percentile and a minimum document\\nfrequency (min DF) threshold of 5 occurrences per URL. This ensures that both very rare and overly\\ncommon URLs are excluded from the analysis, leaving us with a meaningful set of URLs that can\\nidentify CoIAs.\\nThe similarity between users is built by comparing their pairwise co-shared URL patterns. This is\\nobtained by projecting the bipartite network into a user-to-user similarity network. For network\\nconstruction, we computed exhaustive pairwise cosine similarity between user vectors of the bipartite\\ngraph. This similarity measure captures how similar users are based on the URLs they shared, with\\nhigher similarity scores indicating stronger coordination. This co-URL similarity network forms the\\nfoundation to identify coordinated users potentially driving campaigns aimed at redirecting traffic\\nto specific web domains.\\nIntra-platform and cross-platform detection. Being agnostic to the platform under scrutiny, URLs\\nserve as common entities that can be analyzed across different platforms to identify potential\\n2https://github.com/dfreelon/unspooler6\\ncross-platform campaigns. Therefore, we constructed two types of co-URL networks: intra-platform\\nand cross-platform. For the intra-platform network, we computed cosine similarity between all pairs\\nof users within the same platform. For the cross-platform network, we calculated pairwise similarity\\nonly between users from different platforms, linking users based on their shared URL patterns.\\nExtraction of network properties for coordinated accounts detection. To identify online coordi-\\nnation, we build upon and integrate state-of-the-art strategies that filter either low-weight edges or\\nperipheral nodes from the similarity network to detect CoIAs. The first approach [ 58] prioritizes the\\nstrength of similarity, filtering out low-weight edges likely representing spurious similarities. The\\nsecond approach [ 41] focuses on the breadth of similarities, pruning nodes based on their centrality.\\nLuceri et al. [ 41] demonstrated that eigenvector centrality is an effective network property for\\nidentifying the most suspicious users in a similarity network, where coordinated accounts tend to\\nshare numerous similarities with other highly connected nodes. The assumption is that coordinated\\nactivity typically involves multiple accounts. In a similarity network, coordinated actions manifest as\\na pronounced collective similarity, where a coordinated account is highly connected (i.e., similar) to\\nnumerous other nodes that are themselves well-connected. Consequently, eigenvector centrality has\\nproven to be an effective network property for detecting online coordination [ 41]. Here, we extend\\nthis notion of collective similarity by considering the density of the similarity network. Network\\ndensity is a measure that quantifies how close a network is to being fully connected and, in this\\nscenario, it provides an exact measure of collective similarity.\\nIn particular, we use network density as a variable to modulate the combination of thresholds for\\nboth node and edge filtering. By integrating these strategies, we aim to leverage the strengths of\\nboth filtering techniques, constructing a model that accounts for both the breadth and strength of\\nsimilarities. This integrated approach is particularly important, as these techniques have not yet\\nbeen applied to platforms like Telegram and Facebook, where coordination dynamics may vary\\nsignificantly.\\nDensity-based unsupervised network dismantling. To combine these strategies in a fully un-\\nsupervised manner, we developed a novel method for conducting a grid search of parameters in\\nthe two-dimensional space defined by node centrality and edge weight distributions. Specifically,\\nthe grid search explores two parameters‚Äîthe quantile of node centrality and the quantile of edge\\nsimilarity‚Äîand evaluates network density for each combination and every connected component\\nin the similarity graph. We hypothesize that, as we progressively filter the similarity graph by\\nremoving edges and nodes, a transitional phase in the density of the connected components will\\nsignal potentially coordinated accounts. To ensure robustness, we adopt a conservative approach by\\nevaluating the transitional phase of the smallest density among all connected components in the\\nfiltered similarity graph. By controlling the component with the lowest density, we ensure that all\\nother components exhibit higher densities. This conservative choice is driven by the unsupervised\\nnature of the task, prioritizing the minimization of false positives, i.e., legitimate users misclassified\\nas coordinated. Therefore, filtering thresholds were chosen based on the transitional phase of the\\nsmallest density of a connected component of the similarity graph, thus, focusing on identifying\\nhighly suspicious CoIA. Details of the parameters used for detection are provided in the Appendix.\\nDetection of Content Amplification. Coordinated actors may employ a variety of tactics to\\nachieve their goals. A common tactic is to artificially amplify content on specific topics to create the\\nappearance of widespread grassroots support and manipulate platforms‚Äô feed algorithms [ 44,57,66].7\\nTo uncover content amplification, we construct a Text Similarity Network (TSN), where nodes\\nrepresent users linked by the similarity of the content they share. This TSN is then employed to\\nidentify a subset of users exhibiting suspiciously coordinated behavior.\\nThe initial step in constructing the TSN involves preprocessing the raw data to ensure the results\\nare meaningful. In line with standard practices [ 41], we exclude retweets and remove punctuation,\\nstopwords, emojis, URLs, as well as any content with fewer than four words. To capture the semantic\\nnuances of the content, we embed all text data using the SentenceTransformer model stsb-xlm-r-\\nmultilingual3, and then calculate the average cosine similarity between pairs of users. To avoid\\nspurious correlations, we consider text similarity valid only if it occurs within a one-day sliding\\nwindow. For each window, we calculate the text similarity between the shared content of user pairs,\\nsetting the edge weight in the TSN to the average similarity observed across all windows.\\nTo identify coordinated accounts, we apply established thresholds from the literature [ 41,58].\\nSpecifically, we filter out edges with similarity below 0.95 and classify users as coordinated if their\\nnodes rank in the top 0.5% by eigenvector centrality.\\nCharacterizing Content Pushed by CoIA\\nIn this section, we present an overview of the methods used to characterize textual content amplified\\nby coordinated actors across various social media networks. This characterization encompasses\\ntopic analysis, AI-generated content detection, and an assessment of content credibility.\\nTopic Analysis. Coordinated accounts often amplify specific narratives or themes to steer public\\nattention toward polarized, inflammatory, or misleading discussions. To uncover the agendas these\\naccounts seek to promote, we use BERTopic [ 26], a state-of-the-art tool for topic extraction. For a\\ndetailed description of BERTopic‚Äôs methodology, we refer readers to [ 26]. This approach also helps\\nidentify patterns in shared content, improving our understanding of coordinated activities and their\\nintended impact on public discourse.\\nOnce coordinated accounts are identified using the methodology outlined in Sec. Detection of\\nContent Amplification , we apply BERTopic to the entire content corpus. This allows us to map the\\ntopics promoted by coordinated accounts and compare them with those shared by organic users\\n(i.e., users not classified as coordinated). We choose BERTopic over alternatives such as LDA or\\nGPT-based approaches because it offers an effective balance between accuracy and scalability.\\nAI-Generated Content Detection. To identify AI-generated text, we use the approach introduced\\nby [14], which is designed to identify AI-generated content on X. We initially trained a RoBERTa4\\nmodel on their Xdataset and then evaluated its performance on datasets from other platforms by\\ndeveloping a new validation set containing approximately 2,000 samples. This validation set was\\nconstructed using Llama 3.1 3B Instruct5, GPT-4o6, and open-source social media datasets from\\nTelegram, X, and Facebook, sampled from posts between 2010 and 2015. We assume these older\\ndatasets do not contain AI-generated content and sampled around 1,000 sentences from them.\\nTo create a balanced set of human and AI-generated content, we prompted the language models\\nto generate 1,000 samples that mirrored the original texts in topic distribution and length. This\\n3https://huggingface.co/sentence-transformers/stsb-xlm-r-multilingual\\n4https://huggingface.co/FacebookAI/roberta-base\\n5https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\\n6https://openai.com/index/hello-gpt-4o/8\\napproach ensured that the AI-generated texts were comparable to human-generated samples in both\\nthematic and structural characteristics. Results showed precision values ranging from 0.87 to 0.97\\nin detecting AI-generated content within the validation set, aligning with our goal of prioritizing\\nprecision over recall to minimize false positives (misclassified legitimate users). Full results are\\navailable in the Appendix.\\nCredibility Assessment. We assess the credibility of web domains using Media Bias/Fact Check\\n(MBFC). MBFC is an independent watchdog that rates news outlets on a 6-point factuality scale,\\nranging from Very Low to Very High. For each post in our datasets, we systematically extract, expand,\\nand parse all embedded URLs, checking whether the URL belongs to a low- or high-credibility\\ndomain from our lists.\\nGiven recent news78and prior instances of interference by Russian agencies in U.S. elections [ 3],\\nwe also examined the potential amplification of Russian media outlets. Following a similar approach\\nto previous work [ 61], we utilize the VoynaSlov dataset [ 59] to obtain a list of 23 state-affiliated\\nRussian websites. We then check whether the extracted URLs link to one of these web domains.\\nFinally, we employ a similar approach to identify content linked to conspiracies, such as QAnon,\\ngiven the relevance of fringe theories during the 2020 US Election, and its aftermath [ 63,66].\\nBuilding on the methodology of [ 67], we detect posts sharing QAnon content by utilizing a list of\\nkeywords commonly associated with the conspiracy, as outlined by [63].\\nRESULTS\\nThis section presents the results obtained from the detection of online coordination. We first present\\nthe results for the detection of web domain promotion andcontent amplification . Following the\\nidentification of these campaigns, we examine the content pushed by coordinated networks and the\\nengagement received by suspicious actors focusing on several interaction and engagement metrics.\\nDetecting CoIA for Web Domain Promotion (RQ1)\\nWe detect CoIAs that push web domains within and across platforms as described in Section Detection\\nof Web Domain Promotion . We begin by presenting results for each coordinated network within\\nindividual platforms. Following this, we analyze the inter-platform CoIA.\\nTo provide a comprehensive view, we created a cross-platform network by combining the intra- and\\ninter-platform coordinated networks into a unified network. In this network, nodes are connected if\\nthey are linked in either the intra- or inter-platform coordinated networks. The primary result of\\nthis analysis is displayed in Figure 1, illustrating coordinated activity both within and across the\\nthree platforms under study.\\nIntra-Platform Coordination. We first analyze each intra-platform CoIA based on the co-URL\\nsimilarity networks extracted from each platform separately. We describe intra-platform CoIAs as\\nfollows.\\nTelegram: We identified 33 highly coordinated channels co-sharing URLs to web domains with\\na partisan slant and low factuality. As shown in the Table 2, the most frequently shared domains\\n7https://www.bbc.com/news/articles/c8rx28v1vpro\\n8https://www.state.gov/u-s-department-of-state-takes-actions-to-counter-russian-influence-and-interference-in-u-s-\\nelections/9\\nPeter Boykin\\'s Facebook \\nPages \\nConspiracy & \\nRussian media \\nTelegram channels Partisan Twitter \\naccounts \\nHighly Influential \\nTelegram Channels Mix of conservative / \\npartisan / conspiracy \\nchannels and accounts \\nFig. 1. Cross-platform coordination network showing user coordination across four social media\\nnetworks.\\nare predominantly right-leaning and of low credibility, as assessed by MBFC9. Notably, the table\\nhighlights the presence of a Russian state-controlled media outlet (RT.com) and its video-on-demand\\nsubsidiary (Ruptly.tv), which could indicate a foreign effort to interfere in the election. Both RT.com\\nand Ruptly.tv have previously been accused of orchestrating campaigns to influence U.S. elections\\nvia social media10.\\nAdditionally, this CoIA promotes a far-right website (thegatewaypundit.com) and an extremist-\\nfriendly video platform (odysee.com), suggesting ties to fringe ideas. A manual review of these\\nTelegram channels, including their profile descriptions (see Table 8) and shared messages (see\\nTable 12), reveals a strong prevalence of content and accounts promoting conspiracy theories,\\nparticularly those related to COVID vaccines, 5G, and alternative news media.\\n9https://mediabiasfactcheck.com/\\n10https://www.nbcnews.com/politics/2020-election/facebook-blocks-russia-backed-accounts-other-sites-keep-\\nchurning-out-n124268310\\nDomain Shares Factuality Leaning\\nruptly.tv 2,117 Mixed RIGHT-CENTER\\nrt.com 1,941 Very Low RIGHT-CENTER\\nodysee.com 1,602 Low RIGHT CONSPIRACY\\ndailymail.co.uk 1,159 Low RIGHT\\nthegatewaypundit.com 746 Very Low EXTREME RIGHT\\nTable 2. Top-5 domains shared in Telegram by coordinated accounts. Columns represent, from\\nleft to right: domain, the number of shares among coordinated accounts, and the factuality and\\npolitical leaning scores based on data from MBFC.\\nDomain Shares Factuality Leaning\\nfoxnews.com 880 Mixed RIGHT\\nfoxbusiness.com 13 Mixed RIGHT-CENTER\\nnewsbreakapp.com 5 NA NA\\ngo.shr.lc 2 NA NA\\nlifenews.com 2 Low FAR RIGHT\\nTable 3. Top-5 domains shared in Twitter by coordinated accounts. Columns represent, from left to\\nright: domain, the number of shares among coordinated accounts, and the factuality and political\\nleaning scores based on data from MBFC.\\nX/Twitter: We identified a network of 19 coordinated accounts predominantly promoting content\\nfrom right-leaning domains, as listed in Table 3. A manual review of this Twitter CoIA reveals that\\nmany of these coordinated accounts share similar profile descriptions, reflecting narratives tied\\nto religious and conservative principles (see Table 9). Additionally, an analysis of the messages\\n(see Table 13) shows a strong presence of politically partisan content, predominantly aligned with\\nright-leaning ideologies.\\nFacebook: On Facebook, no well-known media outlets dominate the shared content. However,\\nthe most shared domain is a partisan news outlet, as suggested by the semantics of its title:\\ngorightnews.com. Additionally, there is a direct connection to a public activist known for political\\ncampaigns11and their website: peterboykin.com. These campaigns and related Facebook accounts\\nprimarily focus on the ‚ÄúGays for Trump\" movement, an American LGBTQ organization that advocates\\nfor former U.S. President Donald Trump and his administration12. The top-engagement messages\\n(see Table 14) and bios (see Table 10) indicate strong support for right-leaning ideologies.\\nCross-Platform Coordination. The analysis of the cross-platform network reveals distinct patterns.\\nThe unified cross-platform network, displayed in Fig. 1, highlights a giant component connecting\\ncoordinated accounts from both Telegram and Twitter, while the Facebook coordination network\\nremains largely disconnected. A few noteworthy observations:\\nFirst, we observe that most cross-platform connections occur between Twitter accounts and\\nTelegram channels, likely due to their higher representation in the dataset. Examining the bios of\\nthe top users by degree, as shown in the Appendix (see Table 15), we note a prominent presence of\\nnon-mainstream news outlets and conspiracy theories, such as the flat earth one.\\n11https://en.wikipedia.org/wiki/Peter_Boykin\\n12https://en.wikipedia.org/wiki/Gays_for_Trump11\\nDomain Shares Factuality Leaning\\nthegatewaypundit.com 23,513 Very Low EXTREME RIGHT\\nzerohedge.com 8,331 Low RIGHT CONSPIRACY\\ntruthsocial.com 7,800 NA NA\\nnypost.com 6,797 Mixed RIGHT-CENTER\\ntheepochtimes.com 6,241 Mixed RIGHT\\nTable 4. Top domains shared by cross-platform coordinated accounts. Columns represent, from\\nleft to right: domain, the number of shares among coordinated accounts, and the factuality and\\npolitical leaning scores based on data from MBFC.\\nSecond, when focusing on the bridge nodes between the Telegram-coordinated cluster and the\\ncross-platform giant component, we find highly influential channels with up to 24,000 subscribers.\\nThese channels often feature bios referencing free speech and religion, such as:\\n‚Ä¢\"The FIGHT for FREEDOM - :Aron TRUTH Social\"\\n‚Ä¢\"Q Reee-searchers Watchers\"\\n‚Ä¢\"‚ÄôArise and shine, for your light has come, and the glory of the LORD rises upon you.‚Äô Isa.\\n60:1 WE RISE beyond the challenges of yesterday to become better stewards of tomorrow!\"\\nSimilarly, the bridge node between the Twitter-coordinated cluster and the cross-platform giant\\ncomponent is a highly influential account with 32,000 followers. The bio reads: \"NO DM‚ÄôS. Beau-\\ntiful disaster. Self-proclaimed arbiter of great ideas. Here to annoy the dumb asses. NO LISTS!!\\n#Imvotingforafelon #animallover\", indicating a strong partisan flavor.\\nFinally, the largest component of the cross-platform coordination network is dominated by a\\nmix of Telegram and Twitter accounts promoting domains such as magapac.com , QAnon-related\\nnarratives (e.g., \"WWG1WGA\"13), and accounts with partisan bios like:\\n‚Ä¢\"We The Ultra Patriots is made up of patriotic Americans dedicated to exposing crimes\\nagainst humanity, false flags, lies, and corruption. We The Ultra Patriots are...\"\\nA closer look at the top shared domains within the coordinated network (see Table 4) reveals\\nthat most are partisan, including truthsocial.com , an alternative non-mainstream social me-\\ndia platform14. Additionally, seven of the shared domains are Russian state-affiliated websites15,\\nincluding tv5,rbc,gazeta ,ruptly , and mil, further suggesting potential Russian information\\noperations to interfere with the election discourse. Examining the posts and messages with the\\nhighest engagement (see Table 15 in the Appendix), we observe the prevalence of ultra-MAGA\\nnarratives, conservative religious themes, and environmental news.\\nDetecting CoIA for Content Amplification (RQ2)\\nIn this section, we present our findings on the specific topics amplified by coordinated users within\\ncontent amplification campaigns. Following the method in Sec. Detection of Content Amplification ,\\nwe extract the Text Similarity Network and identify a seed set of coordinated users. We then apply\\nBERTopic, as described in Sec. Topic Analysis , to identify the themes these users aim to promote\\nand compare them to the themes that emerge organically on each platform. For each platform, we\\n13https://en.wiktionary.org/wiki/WWG1WGA\\n14https://en.wikipedia.org/wiki/Truth_Social\\n15https://github.com/chan0park/VoynaSlov/tree/master12\\n0 1 2 3\\nPrevalence (%)vaccine-skepticism\\nholistic-health\\npresidential-debate\\nflat-earth-conspiracy\\nsinn-f√©in\\ntrump-trial\\nbible-citations\\nbird-flu-conspiracy\\nukraine-russia\\nantisemitic-conspiracyT opicsOrganic Users\\nCoordinated UsersT elegram\\n(a) Telegram\\n0 1 2 3\\nPrevalence (%)rfk\\ndebate\\ncovid-vaccine\\nborder-security\\nbowman-lattimer\\nepstein-files\\nstudent-loan\\nhunter-biden\\nblack-against-biden\\noil-reserveT opicsOrganic Users\\nCoordinated UsersT witter (b) Twitter\\n0 5 10 15\\nPrevalence (%)late-show\\nagainst-trump\\nbarron-trump-rnc\\n2020-election-acceptance\\ntrump-trial-dayoff\\njacked-up-biden\\ntrump-classified-documents\\nrfk\\ncoulter-ramaswamy\\ntrump-falsifying-records\\nbannon-sentenceT opics\\nOrganic Users\\nCoordinated UsersFacebook (c) Facebook\\nFig. 2. Content analysis of coordinated channels on Telegram, Twitter, and Facebook. We report\\nthe top-10 most recurring topics and compare their prevalence against the organic discourse.\\nKeyword Count / Prevalence\\nCoordinated Organic\\nwwg1wga 1260 (0.59%) 520 (0.05%)\\nplandemic 619 (0.29%) 102 (0.01%)\\nadrenochrome 448 (0.21%) 118 (0.01%)\\nqanon 260 (0.12%) 107 (0.01%)\\ndeepstate 193 (0.09%) 87 (0.008%)\\nTable 5. Keyword statistics in coordinated and organic content related to QAnon conspiracies on\\nTelegram.\\nreport the top-10 most prevalent topics within the coordinated cohort and compare their prevalence\\nto that within the organic population.\\nThis approach highlights themes promoted by coordinated users, with large differences indicating\\ntopics characteristic of coordinated activity. In the remainder of this section, we detail our findings\\non coordination-driven content amplification across the three platforms analyzed.\\nTelegram: Based on the significantly high similarity of their shared content, we identified 57\\ncoordinated Telegram channels. Among these channels, public health is a prominent topic of\\ndiscussion, as can be seen in Figure 2a, with skepticism around the COVID-19 vaccine emerging as\\nthe most frequently discussed theme. Additionally, many messages promote alternative medicine,\\nand there is notable discussion surrounding a conspiracy theory related to bird flu vaccination.\\nTelegram‚Äôs coordinated channels also focus on strictly political discussions, particularly on the U.S.\\npresidential debate, the Russia-Ukraine conflict, and Donald Trump‚Äôs legal challenges. Interestingly,\\nwe observe discourse around immigration issues in Ireland, which appears to be echoed by accounts\\nassociated with the MAGA movement. Notably, three out of the ten topics involve conspiracy theories\\nrelated to flat-earth beliefs, bird flu, and antisemitism. The most representative content for each\\ntopic, as identified by BERTopic, is provided in the Appendix.\\nWe also assess the presence of QAnon-related keywords in content shared by both coordinated\\nand organic channels. Approximately 1.66% of content from coordinated users contains at least\\none QAnon-related keyword, compared to only 0.12% of content from the organic population. This\\nsubstantial difference is evident in Table 5, where we list the top five keywords present in the\\ncoordinated group, also compared to keywords shared by organic channels. Although coordinated\\nchannels make up only 0.36% of the total Telegram dataset, they show a marked prominence,\\nboth in absolute and relative terms, in sharing QAnon-related keywords compared to the organic\\nchannels. These findings indicate a propensity for Telegram CoIAs to disseminate conspiracies.13\\nX/Twitter: On Twitter, we identified 221 coordinated users. Figure 2b displays the top 10 topics\\nshared by these coordinated accounts, all of which pertain to social, economic, or political issues.\\nThe most prevalent topic ‚Äî and one that diverges significantly from those shared by the organic\\ngroup ‚Äî focuses on the Independent candidate Robert F. Kennedy, with a subset of coordinated\\naccounts promoting him as a serious contender in the U.S. Presidential Election. Coordinated users\\nalso concentrate on the Democratic Primary race in Westchester County (labeled as ‚Äúbowman-\\nlattimer‚Äù in the figure), where the most representative tweets endorse candidate Lattimer. This\\ndiscussion appears to be framed around the anti-Israel stance of the incumbent, Bowman. The\\nremaining topics revolve around polarizing issues such as COVID-19 vaccines, U.S. border security,\\nand various scandals, including the Epstein files and the second son of Joe Biden (Hunter Biden).\\nAlso, these users discuss perceived opposition within the Black community to President Biden,\\ncritiques of the student loan forgiveness policy, and concerns over the Biden administration‚Äôs sale of\\noil reserves. No conspiracy theories were detected among the coordinated users, and only a minimal\\namount of QAnon-related content was found within the organic group. The most frequent keyword,\\n‚Äúqanon‚Äù, appeared just five times.\\nFacebook: We identify 16 coordinated users in the Facebook discourse. As shown in Figure 2c,\\nmost topics promoted by these users are left-leaning. A manual inspection reveals that all 16\\naccounts are associated with The Young Turks network16, which MBFC classifies as left-leaning17.\\nApart from a news item quoting a GOP representative accusing Biden of using ‚Äúsupplements‚Äù (i.e.,\\n‚Äújacked-up Biden‚Äù), we found no trace of misleading, conspiratorial, or inflammatory content.\\n101103105107\\nGenerated Engagement0.00.20.40.60.81.0ECDFCoordinated UsersOrganic UsersT elegram\\n(a) Telegram\\n100101102103104105106\\nGenerated Engagement0.00.20.40.60.81.0ECDFCoordinated UsersOrganic UsersT witter (b) Twitter\\n100101102103104105\\nGenerated Engagement0.00.20.40.60.81.0ECDFCoordinated UsersOrganic UsersFacebook (c) Facebook\\nFig. 3. Empirical Cumulative Distribution of the generated engagement by coordinated and organic\\nusers.\\nGenerated Engagement and AI-Generated Content\\nTo comprehensively assess the impact of coordination strategies in the 2024 U.S. election online\\ndebate, we analyze the level of engagement generated by coordinated actors and the extent of\\ntheir reliance on AI-generated content (AIGC). For each platform, we begin by merging the sets\\nof coordinated actors identified through co-URL sharing (web domain promotion) and content\\nsimilarity patterns (content amplification). We observe that the intersection between these two\\npatterns is modest, with overlap rates of 1.3%, 0.5%, and 0% for Telegram, Twitter, and Facebook,\\nrespectively. This is in line with previous findings showing that different strategies are used to push\\ndiverse agendas through distinct coordinated networks [ 41,49]. Aggregating coordinated actors\\nfrom distinct CoIA networks, we obtain 233 coordinated actors on Telegram, 764 on Twitter, and 25\\non Facebook.\\n16https://en.wikipedia.org/wiki/The_Young_Turks\\n17https://mediabiasfactcheck.com/the-young-turks/14\\nT elegram T witter Facebook\\nPlatform0246AIGC Prevalence (%)Coordinated UsersOrganic Users\\nFig. 4. Distribution of AIGC shared by coordinated and organic users across the three different\\nplatforms.\\nFigure 3 shows the distribution of engagement generated by coordinated and organic users across\\nall platforms. In each case, coordinated users tend to generate less engagement than their organic\\ncounterparts. This is expected, as the organic group includes major media outlets and influencers\\nthat significantly boost overall engagement. However, it is noteworthy that the engagement from\\ncoordinated actors is substantial, and particularly on Twitter and Facebook, the engagement\\ndistributions of coordinated and organic users are quite similar.\\nWe also assess the prevalence of AIGC that was shared across the three platforms by both\\ncoordinated and organic users. These results are shown in Figure 4. Facebook is the social media\\nplatform where most AIGC is produced by both coordinated and organic accounts. While the AIGC\\nprevalence is limited, we observe a striking difference in the activity between coordinated and\\norganic users on Telegram and Facebook, whereas it is quite balanced on Twitter. Interestingly, AI-\\ngenerated content is predominantly diffused by coordinated actors on Telegram, while an opposite\\ntrend is observed on Facebook.\\nCONCLUSIONS\\nThis paper introduces a novel, network-based framework for detecting coordinated inauthentic\\nactivity (CoIA) across multiple social media platforms. Unlike most traditional methods that fo-\\ncus on isolated platform-specific analyses, our approach emphasizes cross-platform behaviors by\\nconstructing similarity networks that capture community-wide patterns of content sharing. By\\nprioritizing both intra- and cross-platform connections, we are able to detect subtle yet significant\\ncoordination signals that remain hidden when platforms are analyzed in isolation.\\nOur unsupervised, network-based methodology allows us to identify a range of influence cam-\\npaigns aimed at directing traffic toward specific narratives and domains. These campaigns frequently\\npromote content that is partisan, low-credibility, and conspiratorial in nature. The model not only\\ndetects domestic actors but also reveals the systematic promotion of foreign-affiliated media, partic-\\nularly Russian state-sponsored outlets, across platforms like Telegram and X. This evidence points to\\na coordinated effort to amplify certain messages and domains across distinct platforms, particularly\\nin the context of the 2024 U.S. Election, underscoring the necessity of a unified, cross-platform\\napproach to tackle such influence operations [24].15\\nLimitations. While our study demonstrates substantial strengths, a few considerations remain. First,\\nalthough the model leverages data from Telegram, Facebook, and X, the specific timeframe under\\nobservation and platform-specific mechanisms may influence the broader applicability of our model\\nand the generalizability of our findings to other scenarios and social media networks. Second,\\ncombining similarity networks with differing similarity distributions across platforms may introduce\\nbiases in the overall detection of coordination, potentially favoring certain platforms. Finally, this\\nwork does not examine multimedia content, thereby overlooking coordination across different data\\nmodalities and the potential use of generative AI techniques for producing synthetic media.\\nFuture work should address these limitations by incorporating sophisticated statistical models to\\nassess the significance of coordination patterns, thereby providing a more robust and statistically\\nsound methodology to ensure that identified coordination signals are not merely artifacts of\\nunderlying data distributions. Expanding the framework to encompass additional platforms, data\\nmodalities, and a broader range of behavioral signals would further strengthen its capability to\\ndetect diverse forms of coordinated inauthentic activity.\\nABOUT THE TEAM\\nThe 2024 Election Integrity Initiative is led by Emilio Ferrara and Luca Luceri and carried out by\\na collective of USC students and volunteers whose contributions are instrumental to enable these\\nstudies. The authors are grateful to the following HUMANS Lab‚Äôs members for their tireless efforts\\non this project: PhD students: Charles ‚ÄôDuke‚Äô Bickham, Leonardo Blas, Eun Cheol Choi, Priyanka\\nDey, Gabriela Pinto, Siyi Zhou. Masters‚Äô students: Ashwin Balasubramanian, Sneha Chawan,\\nVishal Reddy Chintham, Srilatha Dama, Yashvi Ashok Hiranandani, Joyston Menez, Jinhu Qi,\\nAmeen Qureshi, Namratha Sairam, Tanishq Salkar, Srivarshan Selvaraj, Kashish Atit Shah, Gokulraj\\nVaratharajan, Reuben Varghese. Undergraduate students: Isabel Epistelomogi, Collin Hargreaves,\\nSaborni Kundu, Grace Li, Richard Peng, Brian Ramirez-Gonzalez, Christina You, Vito Zou. Other\\ncollaborators: Dr. Keith Burghardt. Financial Support: This work has not been supported by any\\nfunding agency, private organization, or political party. Previous memos: [6, 9, 19, 20, 46, 62]\\nREFERENCES\\n[1]Aseel Addawood, Adam Badawy, Kristina Lerman, and Emilio Ferrara. 2019. Linguistic cues to deception: Identifying\\npolitical trolls on social media. In Proceedings of the international AAAI conference on web and social media , Vol. 13.\\n15‚Äì25. (Cited on 3)\\n[2]Meysam Alizadeh, Jacob N Shapiro, Cody Buntain, and Joshua A Tucker. 2020. Content-based features predict\\nsocial media influence operations. Science advances 6, 30 (2020), eabb5824. (Cited on 3)\\n[3]Adam Badawy, Aseel Addawood, Kristina Lerman, and Emilio Ferrara. 2019. Characterizing the 2016 Russian IRA\\ninfluence campaign. Social Network Analysis and Mining 9 (2019), 1‚Äì11. (Cited on 3, 8)\\n[4]Adam Badawy, Emilio Ferrara, and Kristina Lerman. 2018. Analyzing the digital traces of political manipulation:\\nThe 2016 Russian interference Twitter campaign. In 2018 IEEE/ACM international conference on advances in social\\nnetworks analysis and mining (ASONAM) . IEEE, 258‚Äì265. (Cited on 3)\\n[5]Ashwin Balasubramanian, , Vito Zou, Hitesh Narayana, Christina You, Luca Luceri, and Emilio Ferrara. 2024. A\\nPublic Dataset Tracking Social Media Discourse about the 2024 U.S. Presidential Election on Twitter/X (Under\\nReview). HUMANS Lab ‚Äì Working Paper No. 2024.6 (2024). (Cited on 4)\\n[6]Ashwin Balasubramanian, Vito Zou, Hitesh Narayana, Christina You, Luca Luceri, and Emilio Ferrara. 2024. A\\nPublic Dataset Tracking Social Media Discourse about the 2024 U.S. Presidential Election on Twitter/X . Technical\\nReport. HUMANS Lab ‚Äì Working Paper No. 2024.6.16\\n[7]Anees Baqir, Alessandro Galeazzi, and Fabiana Zollo. 2024. News and misinformation consumption: A temporal\\ncomparison across European countries. Plos one 19, 5 (2024), e0302473. (Cited on 2)\\n[8]Alessandro Bessi and Emilio Ferrara. 2016. Social bots distort the 2016 US Presidential election online discussion.\\nFirst monday 21, 11-7 (2016). (Cited on 2, 3)\\n[9]Leonardo Blas, Luca Luceri, and Emilio Ferrara. 2024. Unearthing a Billion Telegram Posts about the 2024 U.S.\\nPresidential Election: Development of a Public Dataset . Technical Report. HUMANS Lab ‚Äì Working Paper No. 2024.5.\\n[10] Leonardo Blas, Luca Luceri, and Emilio Ferrara. 2024. Unearthing a Billion Telegram Posts about the 2024 U.S.\\nPresidential Election: Development of a Public Dataset (Under Review). HUMANS Lab ‚Äì Working Paper No. 2024.5\\n(2024). (Cited on 4)\\n[11] Lorenzo Cima, Lorenzo Mannocci, Marco Avvenuti, Maurizio Tesconi, and Stefano Cresci. 2024. Coordinated\\nbehavior in information operations on Twitter. IEEE Access (2024). (Cited on 1)\\n[12] Stefano Cresci, Roberto Di Pietro, Marinella Petrocchi, Angelo Spognardi, and Maurizio Tesconi. 2016. DNA-\\ninspired online behavioral modeling and its application to spambot detection. IEEE Intelligent Systems 31, 5 (2016),\\n58‚Äì64. (Cited on 3)\\n[13] Jacob Davey and Julia Ebner. 2019. The Great Replacement‚Äô: The violent consequences of mainstreamed\\nextremism. Institute for Strategic Dialogue 7 (2019), 1‚Äì36. (Cited on 1)\\n[14] Alphaeus Dmonte, Marcos Zampieri, Kevin Lybarger, and Massimiliano Albanese. 2024. Classifying Human-\\nGenerated and AI-Generated Election Claims in Social Media. arXiv preprint arXiv:2404.16116 (2024). (Cited on 7,\\n19)\\n[15] Election Integrity Partnership. 2020. The Long Fuse: Misinformation and the 2020 Election. (2020). https:\\n//purl.stanford.edu/tr171zs0069 (Cited on 1, 3)\\n[16] Fatima Ezzeddine, Luca Luceri, Omran Ayoub, Ihab Sbeity, G Nogara, Emilio Ferrara, and Silvia Giordano. 2023.\\nExposing influence campaigns in the age of LLMs: a behavioral-based AI approach to detecting state-sponsored\\ntrolls. EPJ Data Science 12, 46 (2023). (Cited on 3)\\n[17] Hanif Fakhrurroja, Muhammad Nashir Atmaja, Joe Nathan CG Panjaitan, Andry Alamsyah, and Aris Munandar.\\n2019. Crisis Communication on Twitter: a social network analysis of christchurch terrorist attack in 2019. In 2019\\nInternational Conference on ICT for Smart Society (ICISS) , Vol. 7. IEEE, 1‚Äì6. (Cited on 1)\\n[18] Emilio Ferrara. 2017. Disinformation and social bot operations in the run up to the 2017 French presidential\\nelection. First Monday (2017). (Cited on 2)\\n[19] Emilio Ferrara. 2024. Charting the Landscape of Nefarious Uses of Generative Artificial Intelligence for Online Election\\nInterference . Technical Report. HUMANS Lab ‚Äì Working Paper No. 2024.1. https://arxiv.org/abs/2406.01862.\\n[20] Emilio Ferrara. 2024. What Are The Risks of Living in a GenAI Synthetic Reality? Technical Report. HUMANS Lab ‚Äì\\nWorking Paper No. 2024.2. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4883399.\\n[21] Emilio Ferrara, Herbert Chang, Emily Chen, Goran Muric, and Jaimin Patel. 2020. Characterizing social media\\nmanipulation in the 2020 US presidential election. First Monday (2020). (Cited on 3)\\n[22] Deen Freelon, Charlton D McIlwain, and Meredith Clark. 2016. Beyond the hashtags:# Ferguson,# Blacklivesmat-\\nter, and the online struggle for offline justice. Center for Media & Social Impact, American University, Forthcoming\\n(2016). (Cited on 1)\\n[23] Nicholas A Gabriel, David A Broniatowski, and Neil F Johnson. 2023. Inductive detection of influence operations\\nvia graph learning. Scientific Reports 13, 1 (2023), 22571. (Cited on 5)\\n[24] Valerio La Gatta, Luca Luceri, Francesco Fabbri, and Emilio Ferrara. 2023. The interconnected nature of online\\nharm and moderation: investigating the cross-platform spread of harmful content between youtube and Twitter. In\\nProceedings of the 34th ACM conference on hypertext and social media . 1‚Äì10. (Cited on 4, 14)\\n[25] Fabio Giglietto, Nicola Righetti, and Giada Marino. 2020. Detecting Coordinated Link Sharing Behavior on\\nFacebook during the Italian Coronavirus Outbreak. AoIR Selected Papers of Internet Research (2020). (Cited on 1)\\n[26] Maarten Grootendorst. 2022. BERTopic: Neural topic modeling with a class-based TF-IDF procedure. arXiv preprint\\narXiv:2203.05794 (2022). (Cited on 7)\\n[27] Philip N Howard and Muzammil M Hussain. 2013. Democracy‚Äôs fourth wave?: digital media and the Arab Spring .\\nOxford University Press. (Cited on 1)\\n[28] Philip N Howard, Samuel Woolley, and Ryan Calo. 2018. Algorithms, bots, and political communication in the US\\n2016 election: The challenge of automated political communication for election law and administration. Journal of17\\ninformation technology & politics 15, 2 (2018), 81‚Äì93. (Cited on 2)\\n[29] Kristina Hristakieva, Stefano Cresci, Giovanni Da San Martino, Mauro Conti, and Preslav Nakov. 2022. The spread\\nof propaganda by coordinated communities on social media. In Proceedings of the 14th ACM Web Science Conference\\n2022 . 191‚Äì201. (Cited on 1)\\n[30] Kristina Hristakieva, Stefano Cresci, Giovanni Da San Martino, Mauro Conti, and Preslav Nakov. 2022. The Spread\\nof Propaganda by Coordinated Communities on Social Media. In 14th ACM Web Science Conference 2022 . ACM.\\nhttps://doi.org/10.1145/3501247.3531543 (Cited on 3)\\n[31] Tuja Khaund, Baris Kirdemir, Nitin Agarwal, Huan Liu, and Fred Morstatter. 2021. Social bots and their coordination\\nduring online campaigns: a survey. IEEE Transactions on Computational Social Systems 9, 2 (2021), 530‚Äì545. (Cited\\non 1)\\n[32] Ofra Klein and Jasper Muis. 2019. Online discontent: Comparing Western European far-right groups on Facebook.\\nEuropean societies 21, 4 (2019), 540‚Äì562. (Cited on 1)\\n[33] Ugur Kursuncu, Hemant Purohit, Nitin Agarwal, and Amit Sheth. 2021. When the bad is good and the good is\\nbad: understanding cyber social health through online behavioral change. IEEE Internet Computing 25, 1 (2021),\\n6‚Äì11. (Cited on 2)\\n[34] Edoardo Loru, Matteo Cinelli, Maurizio Tesconi, and Walter Quattrociocchi. 2024. The influence of coordinated\\nbehavior on toxicity. Online Social Networks and Media 43 (2024), 100289. (Cited on 2)\\n[35] Alessandro Lovari. 2020. Spreading (dis) trust: Covid-19 misinformation and government intervention in Italy.\\nMedia and Communication 8, 2 (2020), 458‚Äì461. (Cited on 2)\\n[36] Luca Luceri, Eric Boniardi, and Emilio Ferrara. 2024. Leveraging Large Language Models to Detect Influence\\nCampaigns on Social Media. In Companion Proceedings of the ACM on Web Conference 2024 . 1459‚Äì1467. (Cited on\\n3)\\n[37] Luca Luceri, Stefano Cresci, and Silvia Giordano. 2021. Social media against society. The Internet and the 2020\\nCampaign 1 (2021). (Cited on 1, 3)\\n[38] Luca Luceri, Ashok Deb, Adam Badawy, and Emilio Ferrara. 2019. Red bots do it better: Comparative analysis of\\nsocial bot partisan behavior. In Companion proceedings of the 2019 world wide web conference . 1007‚Äì1012. (Cited\\non 3)\\n[39] Luca Luceri, Ashok Deb, Silvia Giordano, and Emilio Ferrara. 2019. Evolution of bot and human behavior during\\nelections. First Monday (2019). (Cited on 2)\\n[40] Luca Luceri, Silvia Giordano, and Emilio Ferrara. 2020. Detecting troll behavior via inverse reinforcement learning:\\nA case study of russian trolls in the 2016 us election. In Proceedings of the international AAAI conference on web and\\nsocial media , Vol. 14. 417‚Äì427. (Cited on 3)\\n[41] Luca Luceri, Valeria Pant√®, Keith Burghardt, and Emilio Ferrara. 2024. Unmasking the web of deceit: Uncovering\\ncoordinated activity to expose information operations on twitter. In Proceedings of the ACM on Web Conference 2024 .\\n2530‚Äì2541. (Cited on 1, 3, 5, 6, 7, 13)\\n[42] Luca Luceri, Jinyi Ye, Julie Jiang, and Emilio Ferrara. 2024. The Susceptibility Paradox in Online Social Influence.\\narXiv preprint arXiv:2406.11553 (2024). (Cited on 4)\\n[43] Thomas Magelinski, Lynnette Ng, and Kathleen Carley. 2022. A synchronized action framework for detection of\\ncoordination on social media. Journal of Online Trust and Safety 1, 2 (2022). (Cited on 3)\\n[44] Lorenzo Mannocci, Michele Mazza, Anna Monreale, Maurizio Tesconi, and Stefano Cresci. 2024. Detection and\\ncharacterization of coordinated online behavior: A survey. arXiv preprint arXiv:2408.01257 (2024). (Cited on 3, 6)\\n[45] Marco Minici, Luca Luceri, Federico Cinus, and Emilio Ferrara. 2024. Uncovering Coordinated Cross-Platform\\nInformation Operations Threatening the Integrity of the 2024 US Presidential Election Online Discussion. arXiv\\npreprint arXiv:2409.15402 (2024). (Cited on 5)\\n[46] Marco Minici, Luca Luceri, Federico Cinus, and Emilio Ferrara. 2024. Uncovering Coordinated Cross-Platform\\nInformation Operations Threatening the Integrity of the 2024 US Presidential Election Online Discussion . Technical\\nReport. HUMANS Lab ‚Äì Working Paper No. 2024.4. https://arxiv.org/abs/2409.15402.\\n[47] Vanessa Molter and Renee DiResta. 2020. Pandemics & propaganda: How Chinese state media creates and\\npropagates CCP coronavirus narratives. Harvard Kennedy School Misinformation Review 1, 3 (2020). (Cited on 1)\\n[48] Monica Murero. 2023. Coordinated inauthentic behavior: An innovative manipulation tactic to amplify COVID-19\\nanti-vaccine communication outreach via social media. Frontiers in Sociology 8 (2023), 1141416. (Cited on 1)18\\n[49] Lynnette Hui Xian Ng and Kathleen M Carley. 2022. Online coordination: methods and comparative case studies\\nof coordinated groups across four events in the united states. In Proceedings of the 14th ACM Web Science Conference\\n2022 . 12‚Äì21. (Cited on 13)\\n[50] Lynnette Hui Xian Ng, Iain J Cruickshank, and Kathleen M Carley. 2022. Cross-platform information spread during\\nthe January 6th capitol riots. Social Network Analysis and Mining 12, 1 (2022), 133. (Cited on 4)\\n[51] Erik C Nisbet and Olga Kamenchuk. 2019. The psychology of state-sponsored disinformation campaigns and\\nimplications for public diplomacy. The Hague Journal of Diplomacy 14, 1-2 (2019), 65‚Äì82. (Cited on 1)\\n[52] Leonardo Nizzoli, Serena Tardelli, Marco Avvenuti, Stefano Cresci, and Maurizio Tesconi. 2021. Coordinated\\nbehavior on social media in 2019 UK general election. In Proceedings of the International AAAI Conference on Web\\nand Social Media , Vol. 15. 443‚Äì454. (Cited on 3)\\n[53] Gianluca Nogara, Francesco Pierri, Stefano Cresci, Luca Luceri, and Silvia Giordano. 2024. Misinformation and\\nPolarization around COVID-19 vaccines in France, Germany, and Italy. In Proceedings of the 16th ACM Web Science\\nConference . 119‚Äì128. (Cited on 2)\\n[54] Alexander C Nwala, Alessandro Flammini, and Filippo Menczer. 2023. A language framework for modeling social\\nmedia account behavior. EPJ Data Science 12, 1 (2023), 33. (Cited on 3)\\n[55] Sarah Oates and John Gray. 2019. # Kremlin: Using hashtags to analyze Russian disinformation strategy and\\ndissemination on twitter. Available at SSRN 3445180 (2019). (Cited on 1)\\n[56] Diogo Pacheco. 2024. Bots, Elections, and Controversies: Twitter Insights from Brazil‚Äôs Polarised Elections. In\\nProceedings of the ACM on Web Conference 2024 . 2651‚Äì2659. (Cited on 2)\\n[57] Diogo Pacheco, Alessandro Flammini, and Filippo Menczer. 2020. Unveiling Coordinated Groups Behind White\\nHelmets Disinformation. In Companion Proceedings of the Web Conference 2020 . ACM. https://doi.org/10.1145/\\n3366424.3385775 (Cited on 3, 6)\\n[58] Diogo Pacheco, Pik-Mai Hui, Christopher Torres-Lugo, Bao Tran Truong, Alessandro Flammini, and Filippo\\nMenczer. 2021. Uncovering coordinated networks on social media: methods and case studies. In Proceedings of the\\ninternational AAAI conference on web and social media , Vol. 15. 455‚Äì466. (Cited on 1, 3, 5, 6, 7)\\n[59] Chan Young Park, Julia Mendelsohn, Anjalie Field, and Yulia Tsvetkov. 2022. Challenges and opportunities in\\ninformation manipulation detection: An examination of wartime Russian media. arXiv preprint arXiv:2205.12382\\n(2022). (Cited on 8)\\n[60] Concha P√©rez-Curiel. 2020. Trend towards extreme right-wing populism on Twitter. An analysis of the influence\\non leaders, media and users. Communication & Society (2020), 175‚Äì192. (Cited on 1)\\n[61] Francesco Pierri, Luca Luceri, Nikhil Jindal, and Emilio Ferrara. 2023. Propaganda and misinformation on\\nFacebook and Twitter during the Russian invasion of Ukraine. In Proceedings of the 15th ACM web science conference\\n2023 . 65‚Äì74. (Cited on 8)\\n[62] Gabriela Pinto, Charles Bickham, Tanishq Salkar, Luca Luceri, and Emilio Ferrara. 2024. Tracking the 2024 US\\nPresidential Election Chatter on Tiktok: A Public Multimodal Dataset . Technical Report. HUMANS Lab ‚Äì Working\\nPaper No. 2024.3. https://arxiv.org/abs/2407.01471.\\n[63] Karishma Sharma, Emilio Ferrara, and Yan Liu. 2022. Characterizing online engagement with disinformation and\\nconspiracies in the 2020 US presidential election. In Proceedings of the international AAAI conference on web and\\nsocial media , Vol. 16. 908‚Äì919. (Cited on 3, 8)\\n[64] Serena Tardelli, Leonardo Nizzoli, Marco Avvenuti, Stefano Cresci, and Maurizio Tesconi. 2024. Multifaceted\\nonline coordinated behavior in the 2020 US presidential election. EPJ Data Science 13, 1 (2024), 33. (Cited on 1,\\n2)\\n[65] Serena Tardelli, Leonardo Nizzoli, Maurizio Tesconi, Mauro Conti, Preslav Nakov, Giovanni Da San Martino, and\\nStefano Cresci. 2023. Temporal Dynamics of Coordinated Online Behavior: Stability, Archetypes, and Influence.\\narXiv preprint arXiv:2301.06774 (2023). (Cited on 3)\\n[66] Padinjaredath Suresh Vishnuprasad, Gianluca Nogara, Felipe Cardoso, Stefano Cresci, Silvia Giordano, and Luca\\nLuceri. 2024. Tracking fringe and coordinated activity on Twitter leading up to the US Capitol attack. In Proceedings\\nof the international AAAI conference on web and social media , Vol. 18. 1557‚Äì1570. (Cited on 1, 2, 3, 5, 6, 8)\\n[67] Emily L Wang, Luca Luceri, Francesco Pierri, and Emilio Ferrara. 2023. Identifying and characterizing behavioral\\nclasses of radicalization within the qanon conspiracy on Twitter. In Proceedings of the international AAAI conference\\non web and social media , Vol. 17. 890‚Äì901. (Cited on 8)19\\n[68] Derek Weber and Frank Neumann. 2021. Amplifying influence through coordinated behaviour in social networks.\\nSocial Network Analysis and Mining 11, 1 (2021), 111. (Cited on 1)\\n[69] Tom Wilson, Kaitlyn Zhou, and Kate Starbird. 2018. Assembling strategic narratives: Information operations as\\ncollaborative work within an online community. Proceedings of the ACM on Human-Computer Interaction 2, CSCW\\n(2018), 1‚Äì26. (Cited on 1)\\n[70] Kai-Cheng Yang, Onur Varol, Clayton A. Davis, Emilio Ferrara, Alessandro Flammini, and Filippo Menczer. 2019.\\nArming the public with artificial intelligence to counter social bots. Human Behavior and Emerging Technologies 1, 1\\n(Jan 2019), 48‚Äì61. https://doi.org/10.1002/hbe2.115 (Cited on 3)\\n[71] Savvas Zannettou, Tristan Caulfield, Barry Bradlyn, Emiliano De Cristofaro, Gianluca Stringhini, and Jeremy\\nBlackburn. 2020. Characterizing the use of images in state-sponsored information warfare operations by Russian\\ntrolls on Twitter. In Proceedings of the International AAAI Conference on Web and Social Media , Vol. 14. 774‚Äì785.\\n(Cited on 3)\\n[72] Savvas Zannettou, Tristan Caulfield, Emiliano De Cristofaro, Michael Sirivianos, Gianluca Stringhini, and Jeremy\\nBlackburn. 2019. Disinformation warfare: Understanding state-sponsored trolls on Twitter and their influence on\\nthe web. In Companion proceedings of the 2019 world wide web conference . 218‚Äì226. (Cited on 3, 4)\\n[73] Savvas Zannettou, Tristan Caulfield, William Setzer, Michael Sirivianos, Gianluca Stringhini, and Jeremy Blackburn.\\n2019. Who let the trolls out? towards understanding state-sponsored trolls. In Proceedings of the 10th acm conference\\non web science . 353‚Äì362. (Cited on 3)\\nAPPENDIX\\n1 DATASET COLLECTION\\nKeywords. Joe Biden, Donald Trump, 2024 US Elections, US Elections, 2024 Elections, 2024\\nPresidential Elections, Biden, Joe Biden, Joseph Biden, Biden2024, Donald Trump, Trump2024,\\ntrumpsupporters, trumptrain, republicansoftiktok, conservative, MAGA, KAG, GOP, CPAC, Nikki\\nHaley, Ron DeSantis , RNC, democratsoftiktok, thedemocrats, DNC, Kamala Harris, Marianne\\nWilliamson, Dean Phillips, williamson2024, phillips2024, Democratic party, Republican party, Third\\nParty, Green Party, Independent Party, No Labels, RFK Jr, Roberty F. Kennedy Jr. , Jill Stein, Cornel\\nWest, ultramaga, voteblue2024, letsgobrandon, bidenharris2024, makeamericagreatagain, Vivek\\nRamaswamy, JD Vance, Assassination, Tim Walz, WWG1WGA.\\nAdditional dataset statistics. We present the distribution of the number of posts per user and post‚Äôs\\nlength for each platform.\\n2 AI DETECTION CLASSIFIER VALIDATION\\nWe utilized the AI-generated text detection classifier from [ 14], originally trained to identify AI-\\ngenerated content in tweets. To extend the applicability of this classifier to other platforms, we\\nconstructed an external validation set using diverse, older datasets. Specifically, we built upon four\\ndatasets: Twitter-201018,Facebook19, and Telegram .\\nFrom each of these datasets, we sampled 500 texts ranging in length from 125 to 1000 characters.\\nGiven the release years of these datasets, we assume that all texts are non-AI generated. To create\\nthe AI-generated class, we used GPT-4o and Llama 3.1 3B Instruct, prompting them to generate\\nnew texts with similar topics and lengths as the original non-AI texts. We performed this generation\\nfor each non-AI text and sampled 250 AI-generated texts from GPT and 250 from Llama.\\n18https://archive.org/details/twitter_cikm_2010\\n19https://www.kaggle.com/datasets/sheenabatra/facebook-data20\\n0 200 400 600 800 1000 1200 1400\\nNumber of Messages100101102103104Number of User\\n0 1000 2000 3000 4000 5000\\nMessage Length (characters)103104Frequency\\n(a) Facebook posts (b) Facebook characters\\n0 100000 200000 300000 400000 500000\\nNumber of Messages100101102103104Number of Channel\\n0 2000 4000 6000 8000 10000 12000\\nMessage Length (characters)100101102103104105106107Frequency\\n(c) Telegram posts (d) Twitter characters\\n0 500 1000 1500 2000 2500 3000 3500\\nNumber of Messages100101102103104105106Number of User\\n0 1000 2000 3000 4000\\nMessage Length (characters)104105106107Frequency\\n(e) Twitter posts (f) Telegram characters\\nTable 6. Distributions of posts and characters across platforms.\\nThus, for each platform, the validation dataset consists of 500 non-AI generated texts paired with\\n500 AI-generated texts: 250 from GPT and 250 from Llama, with both AI-generated sets matching\\nthe original texts in topic and length.\\nTo further enhance the generalizability of the validation dataset, we incorporated non-AI generated\\ncontent from additional sources: Reddit comments20, IMDB reviews21, movies corpus22, and Yelp\\n201323. AI-generated content for these datasets was sourced from tweetHunter24and GPT, which\\nwas prompted to generate texts on specific topics such as American football, climate change, the\\nSoccer World Cup, the Gaza conflict, U.S. politics, and vaccines.\\n20https://zissou.infosci.cornell.edu/convokit/datasets/reddit-coarse-discourse-corpus/\\n21https://ai.stanford.edu/~amaas/data/sentiment/\\n22https://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/\\n23https://www.yelp.com/dataset/download\\n24https://tweethunter.io/21\\nThe validation results are displayed in Table 7.\\nPlatform Precision Recall\\nGeneral 0.9064 0.5927\\nFacebook 0.8723 0.6686\\nTwitter 0.8935 0.5613\\nTelegram 0.9598 0.5956\\nReddit 0.895 0.5368\\nAdditional set 0.9697 0.5\\nTable 7. Precision and Recall for each platform\\n(a) Facebook.\\n (b) Twitter.\\n (c) Telegram.\\nFig. 5. Thresholds grid search for filtering similarity graph: x-axis edge similarity quantile, y-axis\\nnode centrality quantile. z-axis is the minimum graph density across all connected components of\\nthe filtered co-url similarity graph. The green square corresponds to the selected thresholds.\\n3 GRID SEARCH FOR SIMILARITY AND CENTRALITY THRESHOLDS\\nTo identify coordinated accounts in the co-URL similarity network, we filter edges and nodes based\\non cosine similarity and eigenvector centrality. Two nodes are connected if they co-share URLs,\\nand the strength of these connections is represented by the cosine similarity between their TF-IDF\\nvectors across the space of unique URLs.\\nWe employ two filtering techniques commonly found in the literature: edge filtering, based on\\ncosine similarity, and node filtering, based on eigenvector centrality. The thresholds for these filters\\nare determined by the percentiles of the distributions of edge similarity and node centrality.\\nAssuming that coordinated accounts manifest as dense components in the similarity graph, we use\\nthe density of each connected component as a key indicator of coordination. To ensure robustness,\\nwe adopt a conservative approach, using the minimum density across all connected components\\nafter filtering as a comprehensive quality measure of the graph.\\nFigure 5 presents the results of our grid search across these parameters. The x-axis represents the\\npercentile of edge similarity, while the y-axis corresponds to the percentile of eigenvector centrality.\\nThe z-axis indicates the minimum density of the similarity graph after filtering based on the selected\\nx-y percentiles.\\nFor each platform, we identified the optimal thresholds by observing sharp changes in minimum\\ndensity and high overall density values. These threshold combinations define the coordinated\\naccounts on each platform. Specifically:22\\nFig. 6. Thresholds grid search for filtering the Cross-platform similarity graph: x-axis edge similarity\\nquantile, y-axis node centrality quantile. z-axis is the minimum graph density across all connected\\ncomponents of the filtered co-url similarity graph. The green square corresponds to the selected\\nthresholds.\\n‚Ä¢For Facebook, we selected (50%, 45%) as the minimum density increased from 0.73 and\\n0.80 to 0.90.\\n‚Ä¢For Twitter, we chose (85%, 99%), consistent with previous studies, leading to an increase\\nfrom 0.74 to 0.99.\\n‚Ä¢For Reddit, we selected (85%, 80%), where the minimum density shifted from 0.81 and\\n0.89 to 0.96.\\n‚Ä¢For Telegram, we used (99%, 99%), focusing on a significant jump from 0.18 to 0.87.\\nOther threshold combinations were evaluated qualitatively, yielding similar results. More quanti-\\ntative methods for threshold selection are left for future work.23\\nBio\\nCV Lies & 5G Dangers - Discussions\\nScammers may try to impersonate Real World News Channel.\\nDon‚Äôt trust any messages you get from them.\\nThey are not us. Block and report them immediately.\\nCovering News, Military information, across Wiltshire & the\\nSouthwest areas and for connecting people together uk_flag\\nCredence - Breaking News\\nTruth. Faith. Freedom.\\nTable 8. Current bios of top-5 coordinated channels in Telegram sorted by degree.\\nBio\\nHave integrity. Be Just. Precinct Chairman.\\nChristian. Pro-Life. Conservative Political Activist. #CruzCrew\\nNO DM‚ÄôS. Beautiful disaster.\\nSelf proclaimed arbiter of great ideas.\\nHere to annoy the dumb asses.\\nNO LIST!! #Imvotingforafelon #animallover\\nCHRISTIAN, ULTRA MAGA, PATRIOT, CONSERVATIVE,\\n2A, FJB, TRUMP WON, TRUMP2016, TRUMP2020, TRUMP2024,\\nELON MUSK, YELLOWSTONE, DALLAS COWBOYS\\njust a opinionated old cowgirl from WY\\nTraditional work ethic, traditional values\\nstudied spirituality under a minister study 15 years worked 30+years.\\nI write as I see them, past present and maybe future.\\ndon‚Äôt expect you to agree but think\\nTable 9. Current bios of top-5 coordinated accounts in Twitter sorted by degree.\\nBio\\n#GayRightNews Bringing You Gay(and other)\\nNews with a Right Leaning View. #GaysGoRight\\n#GaysForTrump Support Our President #GaysGoRight\\nThis is the OFFICIAL Page of the #GoRight Movement\\n(due to loss of the previous pages we were forced to create a new page)\\nThis is not the time to abandon the GOP it‚Äôs time to #GoRight and\\ncreate the Great American Revival of the GOP and America.\\n#GoRight and Join Us on #TheQiew\\nWhen Gays Go Right We Make America Great\\nPolitical organization https://gorightnews.com/\\nTable 10. Current bios of top-5 coordinated pages in Facebook sorted by degree.\\n4 CHARACTERIZATION OF COORDINATED USERS\\nWe report the bios of the top 5 coordinated accounts by degree, at the time of writing, and the top 5\\nmessages by total engagement written by coordinated accounts in each platform.24\\nBio\\nAll of the articles and opinions shared here are not the opinions of HVU.\\nUse your own discernment. All of HVU‚Äôs links in one olace\\nhttps://linktr.ee/highvibesup Family - Love - Learn\\nAll of the articles and opinions shared here are not the opinions of HVU.\\nUse your own discernment. All of HVU‚Äôs links in one olace\\nhttps://linktr.ee/highvibesup Family - Love - Learn\\nKarli Channel The Real Karli and X only KarluskaP is real\\neveryone else is being sued for using copyright avi artwork\\n‚Äò‚Äô\\nPlease post Tartaria architecture, flat earth\\nTable 11. Current bios of top-5 cross-platform coordinated accounts sorted by degree.\\nTop-5 Messages\\n\"US Secretary of State Antony Blinken Walking the Congressional Office\\nBuilding Halls Surrounded by Police\\n‚ÄôWar criminal‚Äô, ‚Äôgenocide secretary‚Äô\\nProtestors reminding him of his complicity in crimes against humanity.\\nRealWorldNewsChannel RealWorldNewsChat\"\\n\"Its sickening, disgusting money laundering MF‚Äôs. In a meantime the\\npeople of United States are suffering! Can‚Äôt afford basic necessities!!\"\\n\"I like how zoinists shout poor me when they get caught out for there evil\\ncrimes Normal Jews hate zoinist Jews People of the world don‚Äôt hate Jews\\nPeople of the world hate zoinists Jews n zoinists in general n what they\\nstand for They think there the chosen ones and that gives them the rite\\nto kill n genecide inercent people Zoinisium is hated by all races of the\\nworld And has no place for peace in humanity \"\\n\"Ukrainians in Galway are being advised to vote for two Nigerians and a\\nLabour candidate to best serve their interests.\\n‚ÄôUnder no circumstances vote for radicals - Irish Freedom Party, Indepen-\\ndent Ireland, The Irish People and all others who have the slogans ‚ÄôIreland\\nfor the Irish‚Äô.\\n#ForeignInterference \"\\n\"Former CNN Anchor Chris Cuomo Admits to Suffering from a COVID\\nVaccine Injury\\nICYMI: There‚Äôs been a major shift in the official narrative.\\nFollow Vigilant_News \"\\nTable 12. Top-5 messages by total engagement for coordinated channels in Telegram\\n5 TOPIC ANALYSIS\\nWe report the most representative posts, tweets and messages‚Äîi.e., Facebook, Twitter/ X, Telegram‚Äî\\nfor each topic in Table 16, 17 and 18, as computed by BERTopic.25\\nTop-5 Posts\\n\"‚ÄúWE THE PEOPLE‚Äù don‚Äôt play by your dictator rules. Just answer the\\ndamn question, crybaby. White House correspondents fire back after Biden\\nsnaps at reporter for refusing to ‚Äôplay by the rules‚Äô\"\\n\"If Biden wins there will no more arguing because the will not be anything\\nleft to argue about. He‚Äôs handing the keys over to illegals and the new\\nworld order. In the end, the joke will be on the democrats. The WHO/WEF\\nhates them too and knows they‚Äôre imbeciles. Everyone knows\"\\n\"‚ÄôShameful‚Äô: GOP lawmaker shreds ‚ÄôAWOL‚Äô Biden for throwing Jews ‚Äôunder\\nthe bus‚Äô amid anti-Israel protests If this president, so-called president\\ndoesn‚Äôt personify evil destruction division of this country, I don‚Äôt know\\nwhat or who would !\"\\n\"Biden DHS docs suggested Trump supporters, military and religious\\npeople are likely violent terror threats. HaHaHa! I guess we also have\\nSanta Claus/Easter bunny over here in training. Give me a break, we are\\ntrying to save this country from the bad guys!\"\\n\"DeSantis spox dunks on NYT ‚Äôfact-check‚Äô on terrorists entering southern\\nborder: ‚ÄôAwaiting your correction‚Äô They‚Äôve been entering for last 3 years.\\nBiden admin has no damn idea who is coming in. Even ones they think\\nthey know are using others‚Äô identity\"\\nTable 13. Top-5 posts by total engagement for coordinated accounts in Twitter26\\nTop-2 Posts\\n\"Youth Vote Shifts Toward Trump in 2024 Election https://gorightnews.com/youth-vote-shifts-toward-trump-in-2024-election/\\nMaybe the kids are alright... #GoRightNews Recent polls indicate a surprising trend: young voters are warming to Donald Trump\\nin the 2024 presidential election. The question arises: Are these voters aligning with Trump‚Äôs policies, or is President Biden driving\\nthem away? The answer might lie in a combination of both factors. Polling Data Reveals Shift According to the latest New York\\nTimes poll, young voters aged 18 to 29 favor Biden by a slim margin of two points, 47% to 45%. A Quinnipiac poll shows Trump\\nleading Biden among voters aged 18 to 34, 48% to 47%. This is a stark contrast to the 2020 presidential election, where Joe Biden\\nsecured the youth vote by a significant 24%. The last time a Republican won this demographic was in 1988. Biden‚Äôs Struggling\\nMessage Aidan Kohn-Murphy, founder of Gen Z for Change‚Äîa group that supported Biden in 2020‚Äîstated in the Washington\\nPost, ‚ÄúBiden is out of step with young people on a number of key issues.\" Key issues where Biden seems to be losing support\\ninclude: The War in Gaza: A majority of young voters, 51%, support the Palestinians, while only 15% support Israel. TikTok Ban:\\nBiden‚Äôs support for a TikTok ban is perceived as an attack on free speech. Consequently, 67% of Gen Z voters say this makes them\\nless likely to vote for him. Economic Challenges: High inflation and interest rates have made essential costs like food and housing\\nunaffordable for young people entering the workforce or trying to purchase their first home. Trump‚Äôs Resonating Message The 45th\\npresident‚Äôs message appears to be resonating with young voters for several reasons: Gaza Conflict: While Trump supports Israel, he\\npromises to bring a peaceful end to the conflict, citing his administration‚Äôs four years of peace as evidence of his capability. TikTok\\nEngagement: Trump recently joined TikTok, emphasizing that Biden wants to shut the platform down, thus appealing to younger\\nusers. Economic Performance: When asked about Trump‚Äôs handling of the economy, 65% of young voters approved, compared to\\njust 33% for Biden. Social Media Dynamics According to CredoIQ, a social media analytics firm, nearly 25% of the top left-leaning\\ncontent creators on TikTok have posted anti-Biden content in the first four months of 2024, garnering over 100 million views. This\\ncontent is often created by young, non-white liberals who share the belief that the U.S. Government, and specifically Joe Biden,\\naims to restrict free speech and information flow. Trump‚Äôs Adaptation to New Media In 2016, Trump broke political norms with an\\n‚Äôanyplace, anytime‚Äô approach, dominating cable TV. For the 2024 campaign, he has adapted this strategy to new media, appearing\\non podcasts, YouTube shows, and attending live events such as UFC and Formula 1. He has also made campaign stops at local\\nvenues, including bodegas, firehouses, and even in the South Bronx. The shift in the youth vote suggests a significant realignment\\nin political affiliations, one that underscores the importance of addressing the issues most pertinent to young Americans.\\nThis analysis highlights the potential impact of these changing dynamics on the upcoming presidential election. [Source:\\nWashington Post, Axios] https://archive.is/kk3HU https://www.axios.com/2024/06/13/trump-election-young-voters-polling\\n#GoRightNews Shared by Peter Boykin - American Political Commentator / Citizen Journalist / Activist / Constitutionalist for\\nLiberty Web: https://PeterBoykin.com Kick: http://Kick.com/PeterBoykin YouTube: https://youtube.com/PeterBoykinForAmerica\\nTwitter: https://twitter.com/GoRightNews Telegram: http://t.me/realpeterboykin Rumble: http://Rumble.com/GoRightNews\\nLike the Content? Please Support! - Go Right News: Stripe: https://gorightnews.com/donations/support-gorightnews/ Cash App:\\nhttp://cash.app/$PeterBoykin1\"\\n\"Rising Costs Highlight Challenges for American Families Cost of rent, energy, and other essentials surged in May In an alarming\\ntrend, the cost of essentials such as rent, energy, and groceries continues to surge, underscoring the persistent financial challenges\\nfaced by American families. While the overall Consumer Price Index (CPI) showed a slight stabilization, the specifics reveal a\\nstark reality of escalating living expenses in our Constitutional Republic. Analyzing the Numbers The CPI indicated a 3.3% rise\\nin overall inflation compared to the previous year. Although this marks a slight decrease from April‚Äôs 3.4% and a significant\\ndrop from the 9.1% peak in June 2022, it remains well above the Federal Reserve‚Äôs target rate of 2%. This persistent inflation\\nunderscores the ongoing economic strain on American households. Historical Context It‚Äôs noteworthy that during the four\\nyears of Donald Trump‚Äôs presidency, the average inflation rate was maintained at a modest 1.9%. This comparison highlights\\na more stable economic period and suggests a need for policies that can effectively manage inflation without compromising\\nthe financial well-being of citizens. Essential Expenses on the Rise A closer examination of the May report reveals substantial\\nincreases in essential costs: Rent: Up by 5.4% Mortgage: Up by 5.6% Hospital services: Up by 7.2% Car insurance: Up by 20.3%\\nElectricity: Up by 5.9% Ground beef: Up by 4.9% Steak: Up by 5.7% Bacon: Up by 6.9% Hot dogs: Up by 7.3% These increases in\\nessential goods and services strain the budgets of American families, making everyday living increasingly unaffordable. Public\\nSentiment A recent poll reflects the public‚Äôs discontent, with only 31% of voters approving of President Biden‚Äôs handling of\\ninflation, while a significant 61% disapprove. This sentiment underscores the urgent need for effective economic policies that\\naddress the real concerns of the populace. Administration‚Äôs Response The Biden administration continues to assert progress\\nin combating inflation. A statement from the White House on social media claimed: \"Today‚Äôs report shows continued progress\\nin lowering inflation. President Biden knows that costs are still too high for many families and we still have a lot more to\\ndo. That‚Äôs why he will keep fighting to lower drug costs, grocery prices, and energy bills.\" Critical Perspective However, many\\nAmericans find these assurances lacking. The everyday experience at grocery stores and gas stations starkly contrasts with\\nthe administration‚Äôs optimistic declarations, leading to a disconnect between government rhetoric and public reality. As a\\nConstitutional Republic dedicated to ensuring democracy and the well-being of its citizens, it is imperative that our government\\nimplements policies that stabilize the economy and reduce the financial burden on American families. The rising costs of essential\\ngoods and services are a pressing concern that requires immediate and effective action to safeguard the economic future of our\\nnation. [Source: Poll, Whitehouse on X, BLS] https://d3nkl3psvxxpe9.cloudfront.net/documents/econTabReport_maqVHQt.pdf\\nhttps://x.com/WhiteHouse/status/1800957041390792843 https://www.bls.gov/news.release/pdf/cpi.pdf #GoRightNews\\nShared by Peter Boykin - American Political Commentator / Citizen Journalist / Activist / Constitutionalist for Liberty\\nWeb: https://PeterBoykin.com Kick: http://Kick.com/PeterBoykin YouTube: https://youtube.com/PeterBoykinForAmerica Twit-\\nter: https://twitter.com/GoRightNews Telegram: http://t.me/realpeterboykin Rumble: http://Rumble.com/GoRightNews Like\\nthe Content? Please Support! - Go Right News: Stripe: https://gorightnews.com/donations/support-gorightnews/ Cash App:\\nhttp://cash.app/$PeterBoykin1\"\\nTable 14. Top-2 posts by total engagement for coordinated pages in Facebook27\\nTop-5 Messages\\n\"!You are a Golden Child!\\nA Golden Child 94 Unicorn 94 Third Eye 94 Covenant 94 Harmony 94\\nPraise God 94 Divine Gene 94 Nine Six 94 Blue Eyes 94 Indigo Child 94\\nUltra Maga 94 John John 94 White Hat 94 American Eagle 94 Carry On\\n94 New Earth 94 Pineapple 94\\nPassionForFruit\"\\n\"#CAWildfireSituationUpdate as of 6-16-2024, 5:03 P.M. PST.\\n#PostFire - 12,266 acres, 2% containment #HesperiaFire - 1,330, 7%\\n#JunesFire - 1,076, 70% #JacksonFire - 876, ?% #HernandezFire - 600,\\n25% #MaxFire - 500, 0% #LisaFire - 350, 0% #PointFire 150, 0%\"\\n\"What is it like to be wiser than your Creator? Job tried that and later\\nrepented in dust and ashes. Job was a wise and blessed man of faith.\"\\n\"What was the true history behind all of these melted ruins? Certainly not\\nthe result of erosion, but perhaps a plasma storm? The results of a sudden\\nflip in our polar magnetic electric field perhaps.\"\\n\"WAVELAND, Mississippi A boil advisory was issued Mon afternoon dt\\nburst in the main water line.\\nNASHVILLE, Indiana A boil advisory was issued Mon after a water main\\nbreak on Honeysuckle Ln.\\nMONTICELLO, Kentucky A boil advisory was issued Mon for the East Hwy\\n92 area dt a water main break.\\nMILAN, Ohio Seminary Rd bw Perrin Rd/Broad St in Milan is closed dt a\\nwater main break.\\nSASKATOON, SK - Canada North Park Wilson School is closed dt a water\\nmain break.\\nNEW CASTLE CO., Delaware Shipley Rd at Foulk Rd was closed Mon night\\ndt a water main break.\\nMONTGOMERY CO., Maryland Dt a water main break Mon, a portion of\\nthe NW Branch Stream Valley Park near Highwood Terrace was undergoing\\nrepairs.\\nSIOUX FALLS, South Dakota A water main break Mon afternoon caused\\nflooding in downtown.\\nBOURBON CO., Kansas There is a water main break at the Bourbon Co\\nTransfer Station.\\nFORTUNA, California Water will be shut off Wed to repair a broken water\\nmain on S Fortuna Blvd.\"\\nTable 15. Top-5 posts by total engagement for cross-platform coordinated accounts28\\nTopic Representative Posts\\nlate-show\\nagainst-trump \"Stephen Colbert Speaks Truth to Trump‚Äôs Most Damaging Lie - #stephencolbert #The-\\nLateShow #Trump #DonaldTrump #MAGA #immigrants\"\\n\"Stephen Colbert Speaks Truth to Trump‚Äôs Most Damaging Lie - Rick Strom #stephencolbert\\n#TheLateShow #Trump #DonaldTrump #MAGA #immigrants\"\\nbarron-trump-rnc \"Former President Donald Trump said his youngest son Barron Trump was 17-years-old in\\nan interview, when in fact he is 18 and a nominate delegate for the RNC.\"\\n\"Former President Donald Trump‚Äôs 18-year-old son, Barron Trump, is serving as a delegate\\nto the Republican National Convention.\"\\n2020-election-acceptance \"Republican National Committee co-chair Lara Trump said it‚Äôs ‚Äôobvious‚Äô that Donald Trump\\nhas accepted the 2020 election results, when in fact, he has not.\"\\n\"Republican National Committee co-chair Lara Trump said it‚Äôs ‚Äôobvious‚Äô that Donald Trump\\nhas accepted the 2020 election results, when in fact, he has not.\"\\ntrump-trial-dayoff \"Judge Juan Merchan gave Donald Trump the day off from his hush money trial to attend\\nBarron Trump‚Äôs graduation ceremony, but evidence suggests that he is attending political\\nfundraiser.\"\\n\"Judge Juan Merchan gave Donald Trump the day off from his hush money trial to attend\\nBarron Trump‚Äôs graduation ceremony, but evidence suggests that he is attending political\\nfundraiser.\"\\njacked-up-biden \"Rep. Greg Murphy, a Republican, says he has evidence that President Joe Biden was using\\n\"something\" before his popular State of the Union address.\"\\n\"Rep. Greg Murphy, a Republican, says he has evidence that President Joe Biden was using\\n\"something\" before his popular State of the Union address.\"\\ntrump-classified-documents \"More bombshell evidence dropped against former President Donald Trump in his classified\\ndocuments case, just as Judge Aileen Cannon delayed the trial indefinitely.\"\\n\"More bombshell evidence dropped against former President Donald Trump in his classified\\ndocuments case, just as Judge Aileen Cannon delayed the trial indefinitely.\"\\nrfk \"Exciting Announcement! Join us for a special interview with independent presidential\\ncandidate RFK Jr. on Monday, June 17, immediately following The Young Turks! Tune in on\\ntyt.com/live YouTube, Facebook, or Twitch! TYT members get exclusive access to a members-\\nonly Bonus Episode after the interview, where Cenk will ask even more personal questions\\nand dive deeper into the issues! Sign up as a member today at to join the discussion! Don‚Äôt\\nmiss this unique opportunity to engage with crucial political dialogue and watch TYT hold a\\npresidential candidate accountable! See you there!\"\\n\"Exciting Announcement! Join us for a special interview with independent presidential\\ncandidate RFK Jr. on Monday, June 17, immediately following The Young Turks! Tune in on\\ntyt.com/live YouTube, Facebook, or Twitch! TYT members get exclusive access to a members-\\nonly Bonus Episode after the interview, where Cenk will ask even more personal questions\\nand dive deeper into the issues! Sign up as a member today at to join the discussion! Don‚Äôt\\nmiss this unique opportunity to engage with crucial political dialogue and watch TYT hold a\\npresidential candidate accountable! See you there!\"\\ncoulter-ramaswamy \"Conservative Ann Coulter told former Republican presidential candidate Vivek Ramaswamy\\nsaid while she agrees with him on many issues, she wouldn‚Äôt vote for him because he‚Äôs ‚Äúan\\nIndian.‚Äù\"\\n\"Conservative Ann Coulter told former Republican presidential candidate Vivek Ramaswamy\\nsaid while she agrees with him on many issues, she wouldn‚Äôt vote for him because he‚Äôs ‚Äúan\\nIndian.‚Äù\"\\ntrump-falsifying-records \"Fox News‚Äô Jeanine Pirro and conservative commentator Charlie Kirk are melting down over\\nformer President Donald Trump‚Äôs 34-count conviction for falsifying business records in his\\nhush money trial.\"\\nbannon-sentence \"Donald Trump adviser Steve Bannon proclaimed that ¬¥\"no one\" will shut him up as he was\\nordered to report to prison for refusing to testify before congress.\"\\nTable 16. Topics and their representative posts on Facebook29\\nTopic Representative Tweets\\nrfk \"If Kennedy iselected it will cost the evil doers over a trillion dollars in the first year. If Kennedy is in the debate\\nit will show Biden as sinile, Trump as weak and Bobby will be elected. So they won‚Äôt let Bobby in but will\\ncompensate CNN and anyone that might go to jail.\"\\n\"I‚Äôve never been more convinced that both Trump and Biden fear Kennedy. The uniparty has no soul, and they\\nhate democracy. We will never forget their attempts to silence us. Kennedy FTW\"\\ndebate \"Biden‚Äôs really not up to debate is what this means. Whenever a question is asked, Biden‚Äôs promoters will either\\ntell the answer in an earplug or use a prompter screen on the podium so Biden can read the answer. This is\\nfunny.\"\\n\"Trump has guts though. They tried to make it as unfriendly for him as possible, and he still is going to do\\nit. I‚Äôm sure they were hoping he would say no way, and then Biden could brag about Trump being afraid to\\ndebate him.\"\\ncovid-vaccine \"Remember this on election day, remember All the \"suddenly died\" family and friends! Biden did this, he made\\nvaccines mandatory! Just one of many bad Biden decisions!\"\\n\"Trump was during Pandemic; FAILED to protect the American people when he knew damn well how infectious;\\ndeadly Covid Virus was, instead Trump told America \"Covid was a Hoax\"; failed to create a National Suppression\\nPlan to save lives 700KAmericans DIED under Trump!\"\\nbowman-latimer \"Bowman called Biden a liar. Bowman voted against the Infrastructure Act. Bowman even voted against the\\nDebt Limit increase that could have jeopardized Social Security. Bowman did not earn another term as a\\nDemocrat.\"\\n\"Bowman is his worst enemy. His and Rape denying has turned his district against him. His Votes AGAINST the\\nBiden Administration‚Äôs Progressive policies are NOT helping his district! Vote !\"\\nborder-security \"with your lies. It is that BLOCKED Bipartisan Border Security Bill bc told GOP to \"Kill The Bill\". publicly stated\\nit. The Border Crisis is NOW all on for Blocking Border Bill to SECURE THE BORDER!\"\\n\"This bill was a sham and would not close border,but would allow more to take American jobs!! Donalds\\nreiterates why GOP rejected ‚Äòbipartisan‚Äô border bill to head off potential debate talking point\"\\nepstein-files \"His flights with Epstein were with family present and years before he had an island. He‚Äôs also the only one\\nsaying anything about releasing the logs, Biden sure isn‚Äôt, and Trump took umbrage when DeSantis started\\nadvocating for their release for some strange reason...\"\\n\" just another day in ‚ÄôMurica, home of the child sex slaves. Trump made these folks feel they deserved Matt\\nGaetz‚Äô job, Trumps‚Äô job or Epstein‚Äôs job if they just ‚Äônetworked‚Äô enough. Investigate the real ‚Äôsatanic‚Äô sex cult\\nthat police and politicians hide.\"\\nstudent-loan \"This administration and president Biden seem to be answerable to nobody between giving money to student\\nloan forgiveness even after the court ruled against them not getting Kennedy secret service protectiin, the list\\ngoes on no accountability.\"\\n\"\"Summary: We estimate that President Biden ¬¥s recently announced \"New Plans\" to provide relief to student\\nborrowers will cost $84 billion, in addition to the $475 billion that we previously estimated for President\\nBiden ¬¥s SAVE plan.\"\"\\nhunter-biden \"Hunter Biden‚Äôs laptop has nothing to do with the false Electoral College votes sent in for the 2020 election.\\nAlso, Alexander Smirnov (an FBI informant) was INDICTED for his false accusations about Barisma and the\\nBidens.\"\\n\"Actions speak louder than how they phrased something ‚Äì especially when the actions are repeated over and\\nover again. Their lies are proven in this week‚Äôs Hunter Biden trial where the laptop is used as evidence.\"\\nblacks-against-biden \"People aren‚Äôt paying attention and don‚Äôt expect him to be the President and don‚Äôt know about Biden‚Äôs\\naccomplishments. That‚Äôs what the polls show now but that‚Äôs gonna change. Trump isn‚Äôt gonna win 20% of the\\nblack vote. No were here near it\"\\n\"‚ÄôAt this stage in his presidency (3.5 years in) Biden must stop trying to convince Black voters his policies\\nhave made the US a significantly better place. Instead he should spend everything he‚Äôs got highlighting how\\nanti-Black Trump is and how quickly any past gains will be lost.\"\\nTable 17. Topics and their representative posts on Twitter30\\nTopic Representative Messages\\nvaccine-skepticism \"BREAKING: US doctors and scientists are currently investigating whether the COVID-19 virus is to blame for an ‚Äúun-\\nusual pattern‚Äù of rare and deadly cancers that have been popping up in the wake of the pandemic.GeneralMCNews\"\\n\"You, the unvaccinated are a special group of people. You stood your ground. You did not let the pressures of\\nsociety sway you. You withstood the harshest discrimination seen in modern times. You were excluded from society.\\nSome of you lost friendships and relationships with family members. You are the best that society has to offer.\\nYou used your analytical judgment and common sense. You are the tree that withstood the hurricane. You are\\nsuperheroes! CovidVaccineTruth CovidVaccineTruthChat\"\\nholistic-health \"Ginger Lemonade for Immunity and Weight Loss \\\\n\\\\n We must: \\\\n- two large lemons \\\\n- A piece of ginger root\\n(about 10-15 cm) \\\\n- two liters of chilled drinking water. \\\\n \\\\n Wash the lemon well and peel the ginger. Cut the\\nlemon and ginger into larger pieces and grind them in a blender. Put everything in a jug, fill it with water and\\nleave it in the fridge overnight.\"\\n\"HACKS: CAR SCRATCHES \\\\n\\\\n ‚ÄúA life hack you‚Äôre gonna wish you knew sooner!!‚Äù \\\\n\\\\n I LOVE that our NATURAL\\nREMEDIES are now in the category of MIND BLOWING, seriously who knew that VINEGAR & COCONUT OIL\\ncould eliminate car scratches. \\\\n\\\\n Follow: DrBarbara_ONeill\"\\npresidential-debate \"CNN is devastated after the debate. \\\\n\"It¬¥s not just panic, it ¬¥s pain.\" \\\\nhahahah. \\\\n\\\\n \"I think there ¬¥s a lot of\\npeople who are going to want to see him consider taking a different course now. There is time for this party to\\nfigure out a different way forward if he will allow us to do that. Um. But that was not what we needed from Joe\\nBiden. it ¬¥s personally painful for a lot of people. It ¬¥s not just panic, it ¬¥s pain. From what we saw tonight.\" \\\\n\"\\n\"The Biden regime, and their lapdogs at CNN, are going extreme lengths to control all aspects of the debate. \\\\n\\\\n No\\noutside media. No audience. \\\\n\\\\n They are trying to completely control public perception of the event. \\\\n\\\\n They\\nwant you to reject the evidence of your eyes and ears.\"\\nflat-earth-conspiracy \"The Encyclopedia of Freemasonry reveals that the Earth is indeed flat. Organizations like The Freemasons know\\nthe truth about God and our flat earth but they choose to keep it hidden.\"\\n\"Flat Earth was the cosmology of our ancestors who had direct contact with the Divine because they were closer to\\nthe Divine than we are now. That is how the ancient‚Äôs knew things despite not having access to technology we\\nhave now.\"\\nsinn-f√©in \"Foreign ‚Äôpolice‚Äô mercenaries led by an ex-RUC thug against Irish Citizens? \\\\n\\\\n This is only going to end one\\nway.\\\\n\\\\n Pack your bags Harris. The game is up. \\\\n\\\\n The Irish political class can pack their bags as well. This\\nland is our land, it can never be sullied again by the treacherous dogs currently in office.\"\\n\"Just watch this \\\\n\\\\n COOLE COUNTY WEST MEATH BEING PLANTED AT 5 AM IN THE MORNING - LOOK\\nAT THE MONEY PUMPED INTO THIS INVASION/ PLANTATION YOU WILL BEGIN TO UNDERSTAND THE\\nFINANCIAL REWARD FOR SELLING YOUR SOUL BEING A TRAITOROUS, SELL OUT PIG \\\\n\\\\n JUDGEMENT DAY\\nAPPROACHES \\\\n\\\\n √©iReGoBragh \\\\n\\\\n Fergus (Ferg) Power (@FergusPower1) \\\\n\\\\n These treacherous bastards\\nwon‚Äôt do this for the Irish people . Out children will have to work for years to afford a home yet LOOK AT THIS.\\nMy blood is boiling. \\\\n\\\\n Good to see that lad using the word invasion & not using unvetted. We need to keep\\npressing to change the language around this. \\\\n\\\\n It is an invasion, we are being replaced, legal immigration is\\nworse, we are at war.\"\\ntrump-trial \"If this is legit, it should wipe out Trump‚Äôs conviction. \\\\nJudge Juan Merchan has alerted Trump‚Äôs attorneys to a\\nFacebook post by a supposed cousin of a Trump juror who spilled the beans that he had inside info that Trump was\\nabout to be convicted \\\\n\"My cousin is a juror and says Trump is getting convicted. Thank you folks for all your\\nhard work!‚Äù \\\\n\\\\n T edit: BY LAW, it would immediately be tossed. Remember those lessons in Constitutional Law\\nour REAL POTUS/CiC promised us.... \\\\n\\\\n \"\\n\"The jurors were divided into 3 groups of 4. They found Trump gulity on 34 counts. I think we‚Äôve entered the final\\nstages in this sting operation.\"\\nbible-citations \"Repent ye therefore, and be converted, that your sins may be blotted out, when the times of refreshing shall come\\nfrom the presence of the Lord. \\\\nFor more, please download \\\\n\"\\n\"2 Chronicles 7:14 \\\\n \\\\n ‚ÄúIf my people, which are called by my name, shall humble themselves, and pray, and\\nseek my face, and turn from their wicked ways; then will I hear from heaven, and will forgive their sin, and will\\nheal their land.\"\\nbird-flu-conspiracy \"NIH-Funded Scientists Develop mRNA Bird Flu Vaccine ‚Äòto Prevent Human Infections‚Äô \\\\n\\\\n The likelihood of\\npeople getting H5N1 is very small, Dr. Robert Malone said. ‚ÄúThe thing is, it doesn‚Äôt readily infect humans.‚Äù \\\\n\\\\n\\\\n \"\\n\"Former coronavirus coordinator Deborah Birx wants to test millions of U.S. cows every week and screen dairy\\nworkers for \"asymptomatic\" cases of bird flu! \\\\n\\\\n You‚Äôve got to be kidding me, this woman is nuts! They want to\\ntry and pull the same asymptomatic bs again with more fraudulent PCR tests! \\\\n\\\\n\\\\n \"\\nukraine-russia \"JUST IN: Russia says it‚Äôs ready to start arming enemies of the United States since the US is arming Ukraine. \\\\n\\\\n\\n@BRICSNews\"\\n\"They want to reassure Zelensky with the promise of a ‚Äúbridge to the alliance‚Äù before the summit. The New York\\nTimes writes about this. \\\\n\\\\n NATO wants to give Kyiv ‚Äúsomething significant‚Äù and at the same time maintain its\\nposition that it is not time for Ukraine to join the unification, the newspaper writes.\"\\nantisemitic-conspiracy \"The left and right paradigm Israel is president is from Poland they‚Äôre not real Jews most of them and even the\\nmasonic pedophile boys do Jewish rituals and then you got the Kabbalah which leads to Jewish mysticism black\\nmagic\"\\n\"The Jews admit to controlling the world and they want to kill you and your children. \\\\n\\\\n It doesn‚Äôt matter\\nwhat you are, European, Arabian, African, Asian, Christian, Muslim or even atheist. No matter what you are,\\nthey want to kill you all. People must wake up and stop finghting with each other and realize who‚Äôs their actual\\nenemy!!! \\\\n\\\\n Follow us >>News_Without_Lies\"\\nTable 18. Topics and their representative posts on Telegram',\n",
       " 'Automatic Identification of Political Hate Articles\\nfrom Social Media using Recurrent Neural\\nNetworks\\nSultan Ahmed\\nDepartment of Information Systems\\nUniversity of Maryland Baltimore County\\nMaryland, USA\\nIL66977@umbc.eduSalman Rakin\\nDepartment of CSE\\nBangladesh University\\nof Engineering & Technology\\nDhaka, Bangladesh\\n0417052033@grad.cse.buet.ac.bdKhadija Urmi\\nDepartment of CSE\\nBRAC University\\nDhaka, Bangladesh\\nkhadija.urmi.cse@gmail.com\\nChandan Kumar Nag\\nDepartment of CSE\\nSoutheast University\\nDhaka, Bangladesh\\ncknag11@gmail.comDr. Md. Mostofa Akbar\\nDepartment of CSE\\nBangladesh University\\nof Engineering & Technology\\nDhaka, Bangladesh\\nmostofa@cse.buet.ac.bd\\nAbstract ‚ÄîThe increasing growth of social media provides us\\nwith an instant opportunity to be informed of the opinions of\\na large number of politically active individuals in real-time. We\\ncan get an overall idea of the ideologies of these individuals\\non governmental issues by analyzing the social media texts.\\nNowadays, different kinds of news websites and popular social\\nmedia such as Facebook, YouTube, Instagram, etc. are the most\\npopular means of communication for the mass population. So the\\npolitical perception of the users toward different parties in the\\ncountry is reflected in the data collected from these social sites. In\\nthis work, we have extracted three types of features, such as the\\nstylometric feature, the word-embedding feature, and the TF-\\nIDF feature. Traditional machine learning classifiers and deep\\nlearning models are employed to identify political ideology from\\nthe text. We have compared our methodology with the research\\nwork in different languages. Among them, the word embedding\\nfeature with LSTM outperforms all other models with 88.28%\\naccuracy.\\nIndex Terms ‚ÄîOpinion Mining, Political Ideology, Machine\\nLearning, Social Media Text, Word Embedding, Stylometric\\nFeature\\nI. I NTRODUCTION\\nText is the most important means of communication in\\ntoday‚Äôs world. Popular online social networking sites such\\nas Facebook, X, LinkedIn, etc. are mainly text-based. The\\nrapid growth of social media has created enough opportunities\\nto share information across time and space. Users are now\\nmore comfortable contributing to the content of social media\\nwebsites and posting their own material.\\nWith the constant flow of information on the internet,\\nindividuals are inclined to consume a greater amount of\\ncontent from various social media posts and the accompanying\\ncomments. Nowadays, the people of Bangladesh heavily relyon social media [ tasnim2021political ]. They have a huge\\namount of political information at their disposal. Individuals\\nhave the ability to share and articulate their opinions or critique\\npolitical news from their own perspective using a global\\nplatform. They express their critical opinions by commenting\\non the news articles. An individual‚Äôs political ideology may\\noften be discerned by examining the comments in the comment\\nsections of political news articles, as an individual‚Äôs words\\noften reveal their political ideology [ iyyer2014political ].\\nThe classical approach to feature extraction for opinion\\nmining from textual data is to identify unique stylometric\\nfeatures of written texts. The underlying assumption here is\\nthat each author has unique writing styles that are relatively\\nfixed and barely change with time. So we can use stylometric\\nfeatures to uniquely identify the writing style of the author [3].\\nAlong with the stylometric feature, we will use the TF-IDF\\nvectorizer and Word Embedding approach to identify political\\nideology from textual data.\\nIn this work, we are interested in implementing an intel-\\nligent system that can analyze and predict political ideology\\non the basis of the political debate online on social media\\nwebsites. The system can take any political comment and\\npredict if it is a positive or negative comment. Based on\\nthe prediction, the political ideology of that user can be\\ndetected easily. We are interested in addressing the political\\nideology problem from social media Bangla text. To the best\\nof our knowledge, only one paper has previously addressed\\nthis problem.\\nAlthough political ideology(PI) identification has been\\nwidely studied in different languages, it is still understudied\\nin the Bangla language. Bangla language is one of the mostarXiv:2411.04542v1  [cs.HC]  7 Nov 2024Fig. 1: Problem Statement\\nwidely spoken and culturally rich languages. This language is\\nthe 7thmost spoken language [4] of the world and the native\\nlanguage of Bangladesh. However, this is not the only reason\\nto study PI problems in the Bangla language. The problems\\nassociated with the Bangla language and the relatively under-\\ndeveloped field of Bangla Natural Language Processing (NLP)\\nmakes it more challenging to study such problems for Bangla.\\nIn this work, we will follow the following steps to study\\nthe political ideology identification problem. We will create a\\ndataset from Facebook containing political posts and neutral\\nposts. We will then pre-process the data and extract features\\nfrom the data in 3 ways. One is called the Bag-of-words\\ntechnique. Another one is computing stylometric features to\\ncapture the writing style of the author. The third one is using a\\nword embedding approach to convert text into a feature vector.\\nIn recent years, deep learning-based recurrent neural models\\nhave been used to automate political ideology extraction due\\nto their performance in building models. These models do not\\nrequire to be provided with pre-defined handpicked features.\\nInstead, they can learn useful features from the data by\\nthemselves [6].\\nIn this work, we will use deep learning recurrent models\\nto automate political ideology determination from Facebook\\ntextual data. Specifically, we will use LSTM and GRU models\\nfrom deep learning recurrent neural network models. From the\\ntraditional model, we will use SVM and NB models. Then\\nthe performance of the deep learning model is compared to\\ntraditional machine learning models.\\nThe possible contributions to this work are as follows:\\n‚Ä¢We will create a dataset that contains user political text\\nor neutral text in the Bangla language.\\n‚Ä¢We will design a system that aims to identify political\\nideology from social media comments or text in the\\nBangla language.\\n‚Ä¢We will propose several types of stylometric features as\\npolitical post indicators for the Bangla language. We will\\ndesign a set of measures to infer political ideology from\\nshort writings through extensive experiments.\\n‚Ä¢We will apply machine learning classifiers and deep\\nlearning models to identify political bias in short writing.\\nThe rest of this paper is organized as follows: Section\\nII overviews the related works of the political ideology de-termination problem. Section III discusses the mathematical\\nbackground of the problem. In section IV , we have proposed\\nour detailed solution. Section V presents the experimental\\nresults. Finally, in Section VI, we provide project planning\\nand schedule.\\nII. R ELATED WORKS\\nIn [21], the authors conducted a depression analysis in Chi-\\nnese. In their endeavor, Psychological and Machine Learning\\nknowledge were combined. The authors opted for Psycholo-\\ngists who assisted 90 depressive and 90 non-depressed Micro-\\nblog users in collecting a total of 6013 micro-blogs. Their\\nmodel‚Äôs precision was 80\\nAbdul et al. [19] proposed using a Long Short-Term\\nMemory Recurrent Neural Network (LSTM-RNN) to ana-\\nlyze Bangla social media posts for depression. They gathered\\nBangla tweets from Twitter in order to compile the dataset\\nrequired for this endeavor. This dataset contained 1968 tweets,\\nwhich was insufficient for a deep learning model to perform\\nadequately. In order to improve the performance of the model\\nwith this short dataset, the data were stratified so that one\\ndepressive text was followed by one non-depressive text. They\\nexperimented with dividing the dataset into 80 percent for\\ntraining, 10 percent for validation, and 10 percent for model\\ntesting. They optimized four hyperparameters of the trained\\nmodel (LSTM size, batch size, number of epochs, and number\\nof layers) for maximum classification accuracy. The evaluation\\nof the model revealed an accuracy of 86.3%.\\nAuthors performed the Gated Recurrent Neural Network\\nalgorithm on the same dataset in another work [20] to predict\\ndepressive Bangla text. As in previous work, They worked\\nwith the hyper-parameters of the GRU model and achieved\\napproximately 75% classification for this task.\\nBillah et al. [5] collected depressed and non-depressed\\nposts from Facebook manually and applied SGD classifier,\\nMultinomial Na ¬®ƒ±ve Bayes, Logistic Regression, and Linear\\nSVC to detect social media post whether it was depressive\\nor not. During the treatment of patients, Psychologists prefer\\nsome linguistic features that may help to detect depression.\\nFor example, depressed people usually use words like ‚Äúme‚Äô,\\n‚ÄúI‚Äù, ‚Äúmyself‚Äù etc. which actually represent their self-centered\\nthinking focusing on themselves rather than other people.\\nThe authors collected these types of posts to enrich their\\ndataset which consisted of 1000 texts having depressive and\\nnot depressive texts. They applied several pre-processing steps\\nto clean the data like punctuation removal, normalization, tok-\\nenization, etc. They applied Unigram, Bigram, and Emoticon\\nfeatures in their dataset. They had achieved the highest 77.9%\\nclassification accuracy for the SGDC classifier.\\nHassan et al. [9] developed an automated system to detect\\ndepression levels of people from social media posts. They re-\\nmoved the stop words and applied N-grams, POS tagging, and\\nNegation feature extraction techniques to transform the text\\ninto a word vector. Finally, SVM, Na ¬®ƒ±ve Bayes, and Maximum\\nEntropy are applied to the dataset to classify depressive tweets\\nwhere SVM showed the highest 91% accuracy.\\n2III. M ETHODOLOGY\\nThis section presents a detailed overview of three feature\\nextraction techniques. One is the Bag-Of-Words feature and\\nanother is the Stylometric feature approach and the third one\\nis the word embedding approach. We first label the dataset\\nand then compute the feature after pre-processing the data.\\nThe feature is then fed into the machine learning model. Then\\nwe provide the architecture of the model. Fig 2 presents an\\noverview of our proposed solution.\\nFig. 2: Steps followed in our solution\\nA. Dataset Creation\\nLike any other TC problem, the first step in the political hate\\nspeech detection problem is to build a large dataset. We have\\ncollected 1980 texts from this work [ karim2020BengaliNLP ]\\nwhere the number of political class labels is 814 and the\\nnumber of neutral class labels is 1166 which is listed in\\nTable I.\\nTABLE I: Data Distributions\\nClass Label Number of posts\\nPolitical Hate Post 814\\nNeutral Post 1166\\nB. Pre-processing\\nThe post obtained from this work [ karim2020BengaliNLP ]\\nis noisy and often contains a lot of unnecessary information.\\nSo we have applied various pre-processing steps and filtered\\nout all characters except Bangla characters. Then we tokenize\\nour texts and remove stop words from the text. We collect\\nBangla stop words from GitHub repository [17] as mentioned\\nin research work [18].\\nElongated words often contain some context to identify the\\npolitical inclination of the text. We express our feelings in\\nelongated words. For example, ‚ÄùGreaaat news!!!‚Äù has more\\nfeelings than ‚ÄùGreat news!!!‚Äù. To maintain the context of the\\ntext, we do not apply lemmatization.C. Word Vector Formation\\nWe use deep learning models and traditional machine learn-\\ning models in our proposed solution. To use these models, we\\nneed to convert our text to a word vector. Conversion to a\\nword vector from text is done using stylometric features, TF-\\nIDF features, and word embedding features.\\n1)Stylometric Features Approach: :Stylometric features\\ncapture the writing style of both political and neutral posts. We\\nhave already computed a large set of stylometric features based\\non existing works of [7, 14]. These features are categorized\\ninto four types: lexical features, structural features, syntactic\\nfeatures, and content-specific features. These four categories\\nof stylometric features have been widely used in research work\\nof [1, 2].\\nLexical features : Lexical features are the most common\\nset of stylometric features that are intended for stylistics and\\ntext readability analysis. These features also signify language\\nassessment and first and second language acquisition. Lexical\\nfeatures consist of word-based and character-based features.\\nThese features are concerned with the usage frequency of\\nindividual letters, vocabulary richness, entropy measure, con-\\nsecutive occurrence of words, etc.\\nSyntactic features: These features are primarily intended\\nfor identifying writing formation patterns, such as the usage\\nof punctuation marks. These features include the total number\\nof commas, colons, question marks, and exclamations marks\\netc.\\nStructure-based features: These features are concerned\\nwith how an author organizes the layout of a text; the organiza-\\ntion of articles represents different habitual facts of an author,\\nsuch as paragraph length and use of greetings. Online texts\\nhave less content information but richer stylistic information,\\nso these habits are seen to be more prominent in these texts,\\nbearing strong authorial evidence of personal writing styles.\\nContent-specific features: These features represent\\ndomain-specific terms. From the study of [12], it has been\\nshown that these features are important in the author‚Äôs writing\\npattern formation. For these features, we have first got the\\nfeature words as suggested for the Arabic language in [3].\\nThen we prepared the Bangla feature words by translating\\nthese Arabic words using Google Translator service API. We\\nhave translated Arabic words into 5 categories: Economy,\\nPolicy, Social, Sport, and Negative.\\nThis translation resulted in many duplicates, flaws, and\\ninconsistencies in the translated lexicons. We have cleared all\\nof these issues by manually inspecting the lexicons.\\nFor each text of the user, the feature extractor produced\\na 141 dimension vector to represent the values of the 141\\nfeatures. As these feature sets contain information on the\\nwriting style of a user measured by various methods, the\\nfeature values we computed could range from 0 to any positive\\nvalue.\\nAs we want to ensure all features are treated equally in the\\nclassification process, we have normalized the features using\\nthe max-min normalization method to ensure all feature values\\nare between 0 and 1:\\n3x‚Ä≤\\nij=xij‚àímin (xj)\\nmax (xj)‚àímin (xj)\\nwhere xijis the jth feature in the ith example, min( xj) and\\nmax( xj) are the minimum and maximum feature values of the\\njth feature separately.\\n2)TF-IDF Count Vectorizer Approach :TF-IDF (term\\nfrequency-inverse document frequency) is an approach that\\nconverts the text of the user into a feature vector. We get\\nterms after tokenizing the text of the user. Term frequency\\nrepresents how many times a specific term occurs in the\\ntext. On the other hand, document frequency is the number\\nof documents containing that term. Term frequency indicates\\nthe importance of a specific term in a document. Document\\nfrequency indicates how common the term is in [11]. We have\\nimplemented the TF-IDF count vectorizer from the scikit-learn\\nlibrary. We set the number of extracted terms to 1000 to extract\\nfeatures from any text. Suppose we have some text from users.\\nTo convert these texts into feature vectors, we have to first\\nidentify unique words and count how many times these words\\noccur in each text. Then we have to compute inverse document\\nfrequency (IDF) using the following formula.\\nid f i=logn\\nd fi\\nwhere d firepresents how many documents contain the term\\ni and n is the total number of documents. Now we will multiply\\nthe TF matrix with IDF score to get the vectorized form of\\neach text. All texts are converted into feature vectors using this\\nTF-IDF approach. These vectors can be fed into any machine-\\nlearning algorithm.\\n3)Word Embedding Representation Approach :Tradition-\\nally, in text classification, the bag-of-words (BOW) model is\\nused to extract features from the text. At the time of extracting\\nfeatures from text, the BOW model does not consider gram-\\nmar, context, or even word order. This model only keeps track\\nof the multiplicity of tokens in text. Authors in [3] applied the\\nBOW model with a set of hand-crafted rules to prepare the\\nfeature set.\\nHowever, with the advancement of deep learning models in\\ntext classification, the word embedding approach has evolved\\nto capture syntactic and semantic regularities. In this approach,\\nindividual words are represented as real-valued vectors in a\\npredefined vector space. In the following, we explain: 1) the\\nmethods of the word2vec algorithm, and 2) how the word2vec\\nalgorithm can generate word embedding. Finally, we discuss\\nthe classification approach after getting word embedding from\\nuser text.\\n1) Word2vec:\\nWord2vec, an efficient algorithm proposed by Google [13],\\ncan learn a standalone word embedding from a text cor-\\npus efficiently maintaining the contextual meaning of words.\\nWord2vec has two model architectures to produce an em-\\nbedding representation of words. One is Continuous Bag of\\nWords (CBOW), and another is Skip Gram (SG). CBOW\\nModel takes the context of each word as the input and triesto predict the word corresponding to the context. SG predicts\\nthe surrounding window of context words based on the current\\nsingle word. The word vector prediction is not influenced by\\nthe order of the context words.\\n2) Word Embedding using Word2vec:\\nThe Word2vec model can capture a lot of information\\nmaintaining semantic, conceptual, and contextual relations. We\\nhave learned the embedding vector of each word from a user\\npost on Facebook of our dataset using the CBOW and Skip-\\nGram model.\\nFor example, let us have two sentences in our dataset. [I\\nhave a book, I love to eat mango]. So first we split the\\nsentence and generated a two-dimensional vector. The two-\\ndimensional vector will be [ [I, have, a, book], [I, love, to,\\neat, mango]]. Then we pass this two-dimensional vector to\\nthe word2vec model. Skip-Gram and CBOW models generate\\nthe word vector from this two-dimensional dataset using a\\nwindow size of 5. The size of the word vector is 300. We\\nhave used the Gensim package to implement the Word2vec\\nmodel. This model returns a 300-dimensional vector for each\\nof these words: I, have, a, book, love, to, eat, and mango. We\\nsave these word embeddings and later use these embeddings\\nas the weight of the embedding layer.\\n3) Classification:\\nFor classifying a text, we first encode each word of the\\ntext using a unique number. Then we multiply this encoding\\nvector with the embeddings of words present in s to form\\nthe hidden representation of s. These sentence representations\\nare used to train a linear classifier. Specifically, we use the\\nsoftmax function to compute the probability distribution over\\nthe classes in C.\\nD. Model Architecture\\nWe implement two different approaches to identify political\\nideology from the text. The first one is a deep learning model,\\nand the other is a traditional machine learning model classifier.\\nIn the deep learning model architecture, we implement LSTM\\nand GRU models. In the traditional machine learning model,\\nwe implement SVM and NB models.\\n1) Model with Stylometric Features:\\nWe have prepared word vectors based on the stylometric\\nfeatures. We pass these vectors to the LSTM layer, which\\nhas 300 nodes. The output of the LSTM layer is passed\\nto a dense layer. Softmax [15] is used as an activation\\nfunction. The optimizer is RMSprop, and binary cross\\nentropy [16] is used as a loss function. The same process\\nis repeated for Gated Recurrent Unit(GRU), SVM, and\\nNB models. We have noted down each model‚Äôs accuracy,\\nF1-score. Fig. 3(a) shows the architecture of the LSTM,\\nGRU, SVM, and NB model with the stylometric feature.\\n2) Model with TF-IDF feature:\\nWe have extracted feature vector from text using the\\nTerm Frequency Inverse Document Frequency (TF-IDF)\\napproach to feed into deep learning and traditional ma-\\nchine learning models. We have initialized the TF-IDF\\nvectorizer using n = 2 grams and maximum vocabulary\\n4is set to 1000. The user text is converted to feature\\nvectors using this TF-IDF approach. We have shown\\nthe architecture of traditional machine learning and deep\\nlearning models with TF-IDF features in figure 3(b).\\n3) Model with Word Embedding feature:\\nIn this architecture, we have first tokenized the user text\\nand taken only the first 100 words. Shorter text is padded\\nwith zeros. At the same time, the word embedding of\\nthe top 1000 vocabulary words is generated using the\\nword2vec algorithm. These embeddings are set as the\\nweight of the embedding layer. The tokens are converted\\ninto sequences and fed into LSTM and GRU models.\\nThe output of these models is passed to a dense layer,\\nwhich is used to detect political ideology from the text.\\nSoftmax [15] is used in the dense layer as an activation\\nfunction. The optimizer is RMSprop and binary cross\\nentropy [16] is used as a loss function. The same process\\nis repeated for all the remaining deep-learning models.\\nFigure 3(c) shows the architecture of our models with\\nthe word embedding features.\\nIV. E XPERIMENTAL EVALUATION\\nIn this section, we have evaluated the performance of our\\nproposed methods for depression detection on Facebook data\\nsets. We compare the performance of deep learning algorithms\\nwith traditional machine learning algorithms like Support\\nVector Machine(SVM) and Naive Bayes(NB).\\nA. Experimental Setup\\nWe have employed Python Keras framework with Tensor-\\nflow as a framework to implement deep learning models for\\ntraining, tuning, and testing. We have also used Gensim pack-\\nage for word2vec model implementation. Scikit-learn package\\nis used to implement traditional machine learning algorithms.\\nExperimental evaluation was conducted on a machine with an\\nIntel Core i7 processor with 1.8GHz clock speed and 8GB\\nRAM. The machine has also an Nvidia GeForce MX150 with\\n2GB memory and therefore Tensorflow based experiments\\nhave fully utilized GPU instructions. Considerable speed can\\nbe achieved in Tensorflow based experiments by adding a GPU\\nas shown in [8].\\nB. Performance Evaluation and Parameter Tuning\\nWe have studied the efficiency and scalability of our pro-\\nposed methods by varying model architectures and feature set\\nvectors. We have measured the performance of the recurrent\\nneural network, LSTM model by generating word vectors\\nusing the stylometric feature method.\\nC. Result Analysis\\n1) Performance of stylometric feature:\\nWe present the performance of stylometric features along\\nwith traditional machine learning and deep learning al-\\ngorithms in Table II. From Table II, we can see that\\ntraditional machine learning algorithms such as SVMperforms better than deep learning algorithm like LSTM.\\nWe have used stylometric features that can capture the\\nwriting style of different authors. SVM outperforms all\\nthe models because the SVM model with stylometric fea-\\ntures can predict political ideology by creating a decision\\nboundary. Again, LSTM having feedback connections\\ncan process the entire sequence of data. Thus LSTM\\nwith stylometric features outperforms traditional machine\\nlearning algorithms like SVM and NB.\\n2) Performance of word embedding feature: The perfor-\\nmance of word embedding using the word2vec algorithm\\nis shown in Table III. From this table, we can see that\\nLSTM and GRU models outperform SVM and NB mod-\\nels. The LSTM model can learn sequential information\\nfrom feature vectors. Again, word embedding vectorizers\\ncan capture the semantic relationships of words. Thus,\\nLSTM with the word embedding approach outperforms\\nall other models.\\n3) Performance of TF-IDF feature:\\nWe have shown the performance of TF-IDF feature with\\ndeep learning and traditional machine learning models\\nin Table IV. Here, the GRU model outperforms the\\nLSTM, SVM, GRU, and NB models. TF-IDF feature\\nassigns weight to a word based on the number of times\\nit appears in the text. GRU model can capture long-term\\ndependencies in sequential data. So, GRU model with the\\nTF-IDF feature outperforms other models. The highest\\naccuracy is 69.94% obtained by the GRU model with the\\nTF-IDF feature.\\n4) Performance among different models: The performance\\nof different machine learning models associated with\\nfeatures is presented in Figure 4. From Figure 4, we\\ncan see that the LSTM model outperforms traditional\\nmachine learning models such as SVM & NB. LSTM\\nwith the word embedding feature outperforms all other\\nmodels. This is expected, as LSTMs can preserve the\\nprevious state. SVM performs slightly better than NB\\nin terms of accuracy since SVM works based on the\\ncomputation of hyperplane equations, which separates\\ndata into classes perfectly [10]. The highest accuracy and\\nF1-score obtained by our proposed methods are 88.28%\\nand 85.41% respectively.\\n5) Comparison of Proposed Method with Previous Works:\\nWe have presented a comparison of our methods with\\nother works on determining political ideology from\\nthe text in Table V. As the number of previ-\\nous works done on political post detection from so-\\ncial media text in the Bangla language is very low,\\namong [ tasnim2021political ], [rahman2018datasets ],\\n[baly2020we ] and our proposed methodology, which\\nworks are done for Bangla language, our work is showing\\ncomparatively better performance.\\nV. C ONCLUSION & F UTURE WORK\\nResearch on identifying political ideology from text is done\\nin various languages. However, there is a lack of research in\\n5(a) Model Architecture for Stylometric Feature\\n(b) Model Architecture for TF-IDF\\n(c) Model Architecture for Word Embedding\\nFig. 3: Model Architecture\\nTABLE II: Performance measure for Stylometric Feature:\\nModel Feature Vector Accuracy F1-score\\nLSTM Styometric Feature 73.18% 61.96%\\nGRU Styometric Feature 72.67% 64.21%\\nSVM Styometric Feature 78.88% 71.35%\\nNB Styometric Feature 61.46% 37.52%\\nthe Bangla language. Again, there is no publicly available\\ndataset to systematically value the ideology from the text.\\nWe have made our dataset public so that future work on\\nthis domain can be done using this dataset. In this study, we\\nhave carried our research by applying the stylometric feature,\\nthe word embedding feature, and the TF-IDF feature. Various\\ntypes of machine learning classifiers and deep learning models\\nare used to identify the ideology in the text. In the future, this\\nideology will be used to deduce which party is better for the\\ncountry. Again, we can predict the political party affiliation\\nfrom text using these methods. We will also increase the size of\\nthe dataset and implement more models so that better accuracy\\ncan be obtained.REFERENCES\\n[1] A. Abbasi and H. Chen. ‚ÄúApplying Authorship Analysis\\nto Arabic Web Content‚Äù. In: Intelligence and Security\\nInformatics, Berlin, Heidelberg . 2005.\\n[2] A. Abbasi and H. Chen. ‚ÄúApplying authorship analysis\\nto extremist-group Web forum messages‚Äù. In: IEEE\\nIntelligent Systems 20.5 (2005), pp. 67‚Äì75.\\n[3] K. Alsmearat, M. Al-Ayyouba, R. Al-Shalabi, and G.\\nKanaanbt. ‚ÄúAuthor gender identification from Arabic\\ntext‚Äù. In: Journal of Information Security and Appli-\\ncations 35.8 (2017), pp. 85‚Äì95.\\n[4] Bengali language. Bengali language ‚Äî Wikipedia,\\nThe Free Encyclopedia . https : / / en . wikipedia . org /\\n6TABLE III: Performance measure for Word Embedding Feature:\\nModel Feature Vector Accuracy F1-score\\nLSTM Word Embedding Feature 88.28% 85.41%\\nGRU Word Embedding Feature 88.23% 85.17%\\nSVM Word Embedding Feature 72.72% 62.75%\\nNB Word Embedding Feature 65.40% 16.96%\\nTABLE IV: Performance measure for TF-IDF feature:\\nModel Feature Vector Accuracy F1-score\\nLSTM TF-IDF Feature 69.69% 42.30%\\nGRU TF-IDF Feature 69.94% 46.15%\\nSVM TF-IDF Feature 66.16% 37.38%\\nNB TF-IDF Feature 67.42% 43.67%\\nTABLE V: Comparison of Proposed Method with Previous Works\\nLanguage Proposed by Methodology Feature Extraction Accuracy\\nBangla [tasnim2021political ] None CBOW, Skip-Gram 76.22%\\nBangla [rahman2018datasets ] KNN, SVM, RF TF-IDF 71.21%\\nEnglish [baly2020we ] LSTM, BERT Word Embedding 72.37%\\nBangla Proposed Method LSTM, GRU, SVM, NB Stylometric Feature,\\nTF-IDF Feature, Word\\nEmbedding Feature88.28%\\nFig. 4: Performance compare among different models\\nwiki/Bengali language. [Online; accessed 04-February-\\n2020]. 2020.\\n[5] Masum Billah and Enamul Hassan. ‚ÄúDepression de-\\ntection from Bangla Facebook status using machine\\nlearning approach‚Äù. In: Int. J. Comput. Appl 975 (2019),\\np. 8887.\\n[6] B. Bsir and M. Zrigui. ‚ÄúEnhancing Deep Learning\\nGender Identification with Gated Recurrent Units Ar-\\nchitecture in Social Text‚Äù. In: Computaci ¬¥on y Sistemas\\n22.3 (2018), pp. 757‚Äì766.\\n[7] M. Corney, O. de Vel, A. Anderson, and G. Mohay.\\n‚ÄúGender-preferential text mining of e-mail discourse‚Äù.\\nIn:Computer Security Applications Conference(CSAC),\\nLas Vegas, USA, Dec 9-13 . 2002.[8] Alexander G. de G. Matthews, Mark van der Wilk,\\nTom Nickson, Keisuke Fujii, Alexis Boukouvalas, Pablo\\nLe¬¥on-Villagr ¬¥a, Zoubin Ghahramani, and James Hens-\\nman. ‚ÄúGPflow: A Gaussian Process Library using Ten-\\nsorFlow‚Äù. In: Journal of Machine Learning Research\\n18.40 (2017), pp. 1‚Äì6. URL: http://jmlr.org/papers/v18/\\n16-537.html.\\n[9] Anees Ul Hassan, Jamil Hussain, Musarrat Hussain,\\nMuhammad Sadiq, and Sungyoung Lee. ‚ÄúSentiment\\nanalysis of social networking sites (SNS) data using\\nmachine learning approach for the measurement of\\ndepression‚Äù. In: 2017 international conference on in-\\nformation and communication technology convergence\\n(ICTC) . IEEE. 2017, pp. 138‚Äì140.\\n[10] R. Kusumawati, A. D‚Äôarofah, and P. A. Pramana.\\n‚ÄúComparison Performance of Naive Bayes Classifier\\nand Support Vector Machine Algorithm for Twitter‚Äôs\\nClassification of Tokopedia Services‚Äù. In: Journal of\\nPhysics Conference Series 1320 (2019), p. 012016.\\n[11] Luthfi Ramadhan. TF-IDF Simplified . https : / /\\ntowardsdatascience . com / tf - idf - simplified -\\naba19d5f5530. [Online: accessed 30-October-2022].\\n[12] C. Martindale and D. McKenzie. ‚ÄúOn the utility of con-\\ntent analysis in author attribution:The Federalist‚Äù. In:\\nComputers and the Humanities 29.04 (1995), pp. 259‚Äì\\n270. URL: https://link.springer.com/article/10.1007/\\nBF01830395.\\n7[13] T. Mikolov, K. Chen, G. Corrado, and J. Dean. ‚ÄúEf-\\nficient Estimation of Word Representations in Vector\\nSpace‚Äù. In: arXiv preprint arXiv:1301.3781 . 2013.\\n[14] F. A. Otoom, E. E. Abdullah, S. Jaafer, A. Hamdallh,\\nand D. Amer. ‚ÄúTowards author identification of Arabic\\ntext articles‚Äù. In: International Conference on Infor-\\nmation and Communication Systems (ICICS), Irbid,\\nJordan, April 1-3 . 2014.\\n[15] O. Sharma. ‚ÄúA New Activation Function for Deep\\nNeural Network, Faridabad, India‚Äù. In: International\\nConference on Machine Learning, Big Data, Cloud and\\nParallel Computing (COMITCon), Feb 14-16 . 2019.\\n[16] Shipra Saxena. Binary Cross Entropy/Log Loss for\\nBinary Classification . https : / / www . analyticsvidhya .\\ncom / blog / 2021 / 03 / binary - cross - entropy - log - loss -\\nfor-binary-classification/. [MARCH 3, 2021].\\n[17] stopwords-iso. Stopwords Bengali . https://github.com/\\nstopwords - iso / stopwords - bn. [Online: accessed 31-\\nMay-2021].\\n[18] N. I. Tripto and M. E. Ali. ‚ÄúDetecting Multilabel Senti-\\nment and Emotions from Bangla YouTube Comments,\\nSylhet, Bangladesh‚Äù. In: International Conference on\\nBangla Speech and Language Processing (ICBSLP),\\nSept 21-22 . 2018.\\n[19] Abdul Hasib Uddin, Durjoy Bapery, and Abu Shamim\\nMohammad Arif. ‚ÄúDepression Analysis from Social\\nMedia Data in Bangla Language using Long Short Term\\nMemory (LSTM) Recurrent Neural Network Tech-\\nnique‚Äù. In: 2019 International Conference on Computer,\\nCommunication, Chemical, Materials and Electronic\\nEngineering (IC4ME2) . 2019, pp. 1‚Äì4. DOI: 10.1109/\\nIC4ME247184.2019.9036528.\\n[20] Abdul Hasib Uddin, Durjoy Bapery, and Abu Shamim\\nMohammad Arif. ‚ÄúDepression analysis of bangla social\\nmedia data using gated recurrent neural network‚Äù. In:\\n2019 1st International Conference on Advances in Sci-\\nence, Engineering and Robotics Technology (ICASERT) .\\nIEEE. 2019, pp. 1‚Äì6.\\n[21] Xinyu Wang, Chunhong Zhang, Yang Ji, Li Sun, Leijia\\nWu, and Zhana Bao. ‚ÄúA depression detection model\\nbased on sentiment analysis in micro-blog social net-\\nwork‚Äù. In: Trends and Applications in Knowledge Dis-\\ncovery and Data Mining: PAKDD 2013 International\\nWorkshops: DMApps, DANTH, QIMIE, BDM, CDA,\\nCloudSD, Gold Coast, QLD, Australia, April 14-17,\\n2013, Revised Selected Papers 17 . Springer. 2013,\\npp. 201‚Äì213.\\n8',\n",
       " 'RetrieveGPT: Merging Prompts and Mathematical Models\\nfor Enhanced Code-Mixed Information Retrieval\\nAniket Deroy1,*,‚Ä†, Subhankar Maity1\\n1IIT Kharagpur, Kharagpur, India\\nAbstract\\nCode-mixing, the integration of lexical and grammatical elements from multiple languages within a single\\nsentence, is a widespread linguistic phenomenon, particularly prevalent in multilingual societies. In India, social\\nmedia users frequently engage in code-mixed conversations using the Roman script, especially among migrant\\ncommunities who form online groups to share relevant local information. This paper focuses on the challenges of\\nextracting relevant information from code-mixed conversations, specifically within Roman transliterated Bengali\\nmixed with English. This study presents a novel approach to address these challenges by developing a mechanism\\nto automatically identify the most relevant answers from code-mixed conversations. We have experimented with\\na dataset comprising of queries and documents from Facebook, and Query Relevance files (QRels) to aid in this\\ntask. Our results demonstrate the effectiveness of our approach in extracting pertinent information from complex,\\ncode-mixed digital conversations, contributing to the broader field of natural language processing in multilingual\\nand informal text environments. We use GPT-3.5 Turbo via prompting alongwith using the sequential nature of\\nrelevant documents to frame a mathematical model which helps to detect relevant documents corresponding to a\\nquery.\\nKeywords\\nGPT, Relevance, Code Mixing, Probability, Prompt Engineering\\n1. Introduction\\nCode-mixing, where elements from multiple languages are blended within a single sentence, is a natural\\nand widespread phenomenon in multilingual societies [ 1,2,3]. It is particularly prevalent in India, a\\ncountry with a rich linguistic diversity where speakers often switch between languages depending on\\ncontext, audience, and medium of communication [ 4,5]. With the rapid rise of online social networking,\\nthis practice has become increasingly common in digital conversations, where users frequently combine\\ntheir native languages with others, often using foreign scripts [6, 7].\\nOne notable trend in India is the use of the Roman script to communicate in native languages on\\nsocial media platforms [ 8,9]. This practice is especially common among migrant communities who\\nform online groups to share information and experiences relevant to their unique circumstances [ 10,11].\\nFor instance, Bengali speakers from West Bengal who have migrated to urban centers like Delhi or\\nBangalore often establish groups such as \"Bengali in Delhi\" on platforms like Facebook and WhatsApp.\\nThese groups serve as vital hubs for exchanging advice on a wide range of local issues, from housing\\nand employment to navigating new social environments.\\nThe COVID-19 pandemic highlighted the importance of these online communities as critical sources\\nof information [ 12,13]. During this period, these groups became essential for sharing experiences,\\nseeking support, and keeping up with the frequently changing government guidelines. However, the\\ninformal and often colloquial nature of the language used in these code-mixed conversations, typically\\ntransliterated into Roman script, presents significant challenges for information retrieval. The lack of\\nstandardization, combined with the blending of languages, makes it difficult to identify and extract\\nrelevant answers, especially for those who might seek similar information at a later time [14, 15].\\nForum for Information Retrieval Evaluation, December 12-15, 2024, India\\n*Corresponding author.\\n/envel‚å¢pe-‚å¢penroydanik18@kgpian.iitkgp.ac.in (A. Deroy); subhankar.ai@kgpian.iitkgp.ac.in (S. Maity)\\n/orcid0000-0001-7190-5040 (A. Deroy); 0009-0001-1358-9534 (S. Maity)\\n¬©2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).arXiv:2411.04752v1  [cs.CL]  7 Nov 2024This paper addresses the challenge of extracting relevant information from code-mixed digital\\nconversations, with a specific focus on Roman transliterated Bengali mixed with English. While\\ncode-mixing is a well-recognized phenomenon in natural language processing (NLP), the unique\\ncharacteristics of transliterated text‚Äîsuch as variations in spelling, grammar, and syntax‚Äîcomplicate\\nthe task of effective information retrieval [ 16,17]. To tackle this issue, we have developed a mechanism\\nthat identifies the most relevant answers from these complex, multilingual discussions.\\nWe begin experimenting with a dataset of code-mixed conversations collected from Facebook, which\\nhas been carefully annotated to reflect query relevance (QRels). This dataset forms the basis of our\\nstudy and is crucial for evaluating the effectiveness of our approach.\\nWe leverage GPT-3.5 Turbo [ 18,19] by employing carefully designed prompts that guide the model\\nto evaluate the relevance of documents with respect to a given query. This involves not only the\\nsemantic understanding capabilities of GPT-3.5 Turbo but also the strategic use of the sequential nature\\nof documents. Often, documents are part of a series or a conversation where the relevance to a query\\ncan be influenced by preceding or succeeding documents. By acknowledging this sequence, we can\\nbetter capture contextual relationships that might be missed if documents were considered in isolation.\\nTo formalize this process, we integrate GPT-3.5 Turbo‚Äôs outputs into a mathematical model. This\\nmodel takes into account the sequential dependencies among documents, treating the task of relevance\\ndetection as a problem of finding the optimal path or chain of relevance across the sequence.\\n2. Related Work\\nCode-mixing and transliteration have gained increasing attention in the field of natural language\\nprocessing (NLP), especially as global communication becomes more digital and multilingual [ 20,21,\\n22,23]. This section reviews key studies related to code-mixing, information retrieval from code-mixed\\ntext, and the challenges of processing Roman transliterated languages, particularly in the context of\\nIndian languages. Code-mixing, where speakers blend elements from multiple languages within a single\\nutterance, is a common linguistic phenomenon in multilingual societies [ 22,24,25]. Early studies on\\ncode-mixing focused primarily on sociolinguistic aspects, examining how and why speakers switch\\nlanguages within conversations [ 20,21,22,26,27]. However, with the advent of digital communication,\\nresearchers have increasingly turned their attention to computational methods for processing and\\nunderstanding code-mixed text [28, 29, 30].\\nSeveral studies have explored various NLP tasks, such as part-of-speech tagging, language identi-\\nfication, and sentiment analysis, in code-mixed settings [ 31,32]. [33,34] provided one of the earliest\\ncomprehensive analyses of code-mixed text, highlighting the unique challenges it poses for traditional\\nNLP pipelines, such as non-standard spelling, syntax variations, and the blending of multiple languages\\nwithin a single text. More recent work by [ 35,36,37] introduced a code-mixed dataset, spanning\\nmultiple Indian languages, which has become a benchmark for evaluating NLP models in this domain.\\nInformation retrieval (IR) in code-mixed settings is relatively underexplored compared to other\\nNLP tasks [ 38,39,40]. However, the need for effective IR systems that can handle multilingual and\\ncode-mixed queries has become increasingly important, particularly in the context of digital information\\nexchange on social media platforms. [ 41,40] investigated the problem of query-focused summarization\\nin code-mixed social media data, emphasizing the complexity of extracting relevant information from\\nnoisy, informal text. Work by [ 42] addressed code-mixed question answering, where the goal is to\\nidentify correct responses from a mixed-language corpus. Their approach involved using translation\\nmodels to standardize the text before applying traditional IR techniques, demonstrating that even simple\\ntranslation-based methods can significantly improve performance. However, these methods often fail\\nto capture the nuances of code-mixed language, such as cultural context and colloquial expressions.\\nRoman script transliteration of Indian languages, commonly referred to as \"Romanagari\" for languages\\nlike Hindi, is a widespread practice in digital communication. Transliteration introduces additional\\nchallenges for NLP, as it often involves non-standard spellings and inconsistent usage. For instance,\\nmultiple transliterations may exist for the same word, depending on the speaker‚Äôs regional accent,literacy in the original script, or personal preference.\\nNotable efforts in this area include the work by [ 43,44], which explored transliteration normalization\\nfor Hindi-English code-mixed text. They developed algorithms to map Romanized text back to its\\noriginal script, enabling more accurate processing by traditional NLP models. However, normalization\\nremains a challenging task due to the inherent variability in transliterated text. In the context of\\nBengali, the Roman script transliteration is less standardized than for Hindi, leading to even greater\\nvariability in spelling and grammar. [ 45,46] addressed this issue by creating a Roman Bengali dataset and\\nproposed methods for transliteration normalization and language identification. Their work highlights\\nthe difficulties of processing Roman Bengali and the need for specialized approaches tailored to the\\ncharacteristics of the language.\\nWhile these studies provide valuable insights into code-mixing, transliteration, and information\\nretrieval, there is a noticeable gap in addressing the specific challenges of extracting relevant information\\nfrom code-mixed conversations in Roman transliterated Bengali. Our work builds on the foundations\\nlaid by previous research but focuses on the unique intersection of these challenges in a real-world\\ncontext. By developing a mechanism to identify relevant answers in code-mixed discussions, we aim\\nto contribute to the growing body of research on multilingual NLP and enhance the accessibility of\\ninformation in linguistically diverse online communities.\\nLarge Language Models (LLMs) [ 46,47,48,49,50,51,52] like GPT-3 have shown promise in various\\nNLP tasks, including LI. Previous works have demonstrated the capability of GPT-3 in performing\\nzero-shot and few-shot learning, making it a potentially powerful tool for LI in resource-constrained\\nsettings. However, the application of LLMs [ 53,54,55,56,57,58,59] to code-mixed and morphologically\\nrich languages remains underexplored. Recent studies, have started to explore the use of transformers\\nand pre-trained models for multilingual LI, but the effectiveness of these models in Bengali languages\\nrequires further investigation.\\nThis section places our work within the context of existing research, highlighting the contributions\\nof prior studies while identifying gaps that our research aims to fill.\\n3. Dataset\\nThis shared task consists of a single dataset for code mixed information retrieval. The corpus consists\\nof 107900 documents in the training set and 20 queries in the training set. There are 30 queries in the\\ntesting set. The dataset is in roman transliterated bengali mixed with english language.\\n4. Task Definition\\nThe task is to automatically determine the relevance of a query to a document within code-mixed data,\\nspecifically focusing on English and Roman transliterated Bengali.\\nGiven a query and a document, the goal is to classify whether the query is relevant or not relevant\\nto the document. Based on the relevance we have to rank the documents. This involves handling\\nthe complexities of code-mixing, where elements from both languages are used within the same\\ntext, and dealing with the informal and non-standardized nature of the language. The system must\\naccurately capture the semantic relationship between the query and the document despite these linguistic\\nchallenges.\\n5. Methodology\\n5.1. Why Prompting?\\nPrompting [ 60] for Information Retrieval is a burgeoning approach that leverages large language models\\n(LLMs) to enhance the retrieval of relevant information from complex, unstructured data, such ascode-mixed text or informal online conversations [ 60]. Below are several reasons why prompting is\\nbecoming an effective strategy in information retrieval (IR):\\n-Handling Ambiguity and Contextual Nuances: Traditional IR systems often struggle with\\nunderstanding the nuanced language, ambiguity, and context found in unstructured or informal\\ntext, such as code-mixed conversations. Prompting LLMs allows these models to interpret context\\nmore effectively by guiding them to generate or rank responses that are contextually appropriate,\\neven when dealing with code-mixing or informal language structures [ 61]. By crafting specific\\nprompts, users can elicit more relevant and accurate results that account for the complexities of\\nthe input text.\\n-Enhanced Language Understanding: Large language models like GPT-3.5 are pre-trained on\\nvast datasets that include a variety of languages and dialects [ 62]. This extensive training enables\\nthem to understand and generate text across different languages and contexts [ 62]. By using\\nprompting, these models can be directed to focus on the most relevant aspects of a query or\\ndocument, improving the retrieval process even in multilingual and code-mixed scenarios. For\\nexample, when retrieving information from Roman transliterated Bengali mixed with English, an\\nLLM can be prompted to recognize and process the code-mixed language more effectively than\\ntraditional IR systems.\\n-Adaptability to Informal and Unstructured Text: Prompting allows LLMs to adapt to the\\ninformal and often unstructured nature of social media text [ 63], which is common in online com-\\nmunities. This flexibility is particularly beneficial when dealing with code-mixed or transliterated\\ntext, where the lack of standardization poses a challenge to conventional IR techniques. Prompted\\nlanguage models can generate or filter responses that align more closely with the informal tone\\nand style of the original text, thereby improving the relevance of the retrieved information.\\n-Reduction of Noise and Irrelevance: One of the major challenges in IR is filtering out irrelevant\\nor noisy data, especially in informal online conversations where off-topic or redundant information\\nis common. By using targeted prompts, LLMs can be instructed to prioritize certain types of\\ninformation, such as direct answers to specific questions, while de-emphasizing or ignoring\\nirrelevant content [ 64]. This leads to a more efficient and effective retrieval process, particularly\\nin environments where users are seeking specific answers within a sea of mixed and informal\\nlanguage.\\n-Scalability and Customization: Prompting for information retrieval offers scalability and\\ncustomization that traditional IR systems might lack. By designing prompts tailored to specific\\ncontexts or types of queries, LLMs can be dynamically adjusted to meet the needs of different\\nretrieval tasks [ 64,65]. This customization is particularly useful in handling domain-specific\\nlanguage or code-mixed scenarios, where standard IR systems might require extensive re-training\\nor re-configuration.\\n-Real-Time Processing and Interaction: In real-time communication platforms, the ability\\nto quickly retrieve relevant information based on ongoing conversations is crucial. Prompting\\nenables LLMs to process and respond to queries in real-time, enhancing the interactivity and\\nresponsiveness of the IR system [ 64]. This is especially beneficial in scenarios where users are\\nengaged in active discussions and require immediate, contextually relevant information.\\n5.2. Merging Prompt and Mathematical Model-Based Approaches\\nWe used the GPT-3.5 Turbo model via prompting through the OpenAI API1to solve the document\\nretrieval task. We used the following prompt:\\n\"Given the query <query> and the document <document>, find how relevant is the query to the document\\nbased on semantic similarity. Provide a relevance score between 0 and 1. Only state the score .\"\\nAfter the prompt is provided to the LLM, the following steps happen internal to the LLM while\\ngenerating the output. The following outlines the steps that occur internally within the LLM,\\n1https://platform.openai.com/docs/models/gpt-3-5-turbosummarizing the prompting approach using GPT-3.5 Turbo:\\nStep 1: Tokenization\\n‚Ä¢Prompt: ùëã= [ùë•1, ùë•2, . . . , ùë• ùëõ]\\n‚Ä¢The input text (prompt) is first tokenized into smaller units called tokens. These tokens are often\\nsubwords or characters, depending on the model‚Äôs design.\\n‚Ä¢Tokenized Input: ùëá= [ùë°1, ùë°2, . . . , ùë° ùëö]\\nStep 2: Embedding\\n‚Ä¢Each token is converted into a high-dimensional vector (embedding) using an embedding matrix\\nùê∏.\\n‚Ä¢Embedding Matrix: ùê∏‚ààR|ùëâ|√óùëë, where|ùëâ|is the size of the vocabulary and ùëëis the embedding\\ndimension.\\n‚Ä¢Embedded Tokens: ùëáemb= [ùê∏(ùë°1), ùê∏(ùë°2), . . . , ùê∏ (ùë°ùëö)]\\nStep 3: Positional Encoding\\n‚Ä¢Since the model processes sequences, it adds positional information to the embeddings to capture\\nthe order of tokens.\\n‚Ä¢Positional Encoding: ùëÉ(ùë°ùëñ)\\n‚Ä¢Input to the Model: ùëç=ùëáemb+ùëÉ\\nStep 4: Attention Mechanism (Transformer Architecture)\\n‚Ä¢Attention Score Calculation: The model computes attention scores to determine the importance\\nof each token relative to others in the sequence.\\n‚Ä¢Attention Formula:\\nAttention (ùëÑ, ùêæ, ùëâ ) =softmax(Ô∏ÇùëÑùêæùëá\\n‚àöùëëùëò)Ô∏Ç\\nùëâ (1)\\n‚Ä¢where ùëÑ(query), ùêæ(key), and ùëâ(value) are linear transformations of the input ùëç.\\n‚Ä¢This attention mechanism is applied multiple times through multi-head attention, allowing the\\nmodel to focus on different parts of the sequence simultaneously.\\nStep 5: Feedforward Neural Networks\\n‚Ä¢The output of the attention mechanism is passed through feedforward neural networks, which\\napply non-linear transformations.\\n‚Ä¢Feedforward Layer:\\nFFN(ùë•) = max(0 , ùë•ùëä 1+ùëè1)ùëä2+ùëè2 (2)\\n‚Ä¢where ùëä1, ùëä 2are weight matrices and ùëè1, ùëè2are biases.\\nStep 6: Stacking Layers\\n‚Ä¢Multiple layers of attention and feedforward networks are stacked, each with its own set of\\nparameters. This forms the \"deep\" in deep learning.\\n‚Ä¢Layer Output:\\nùêª(ùëô)=LayerNorm (ùëç(ùëô)+Attention (ùëÑ(ùëô), ùêæ(ùëô), ùëâ(ùëô))) (3)\\nùëç(ùëô+1)=LayerNorm (ùêª(ùëô)+FFN(ùêª(ùëô))) (4)Step 7: Output Generation\\n‚Ä¢The final output of the stacked layers is a sequence of vectors.\\n‚Ä¢These vectors are projected back into the token space using a softmax layer to predict the next\\ntoken or word in the sequence.\\n‚Ä¢Softmax Function:\\nùëÉ(ùë¶ùëñ|ùëã) =exp(ùëçùëñ)\\n‚àëÔ∏Ä|ùëâ|\\nùëó=1exp(ùëçùëó)(5)\\n‚Ä¢where ùëçùëñis the logit corresponding to token ùëñin the vocabulary.\\n‚Ä¢The model generates the next token in the sequence based on the probability distribution, and\\nthe process repeats until the end of the output sequence is reached.\\nStep 8: Decoding\\n‚Ä¢The predicted tokens are then decoded back into text, forming the final output.\\n‚Ä¢Output Text: ùëå= [ùë¶1, ùë¶2, . . . , ùë¶ ùëò]\\nAfter obtaining the relevance score, we used the following mathematical formulation to account for\\nthe sequential presence of relevant documents. This can be written as follows:\\nùëÉ(ùê∑ùëõ+1|ùê∑ùëõ) =‚éß\\n‚é™‚é™‚é™‚é™‚é®\\n‚é™‚é™‚é™‚é™‚é©Score (ùê∑ùëõ+1) if Score (ùê∑ùëõ+1)<0.3, ùê∑ùëõ=ùëüùëíùëôùëíùë£ùëéùëõùë°\\nScore (ùê∑ùëõ+1) ifùëõ=‚àí1\\n0.2 +Score (ùê∑ùëõ+1)if Score (ùê∑ùëõ+1)>= 0.3, ùê∑ùëõ=ùëüùëíùëôùëíùë£ùëéùëõùë°\\nScore (ùê∑ùëõ+1) ùëúùë°‚Ñéùëíùëüùë§ùëñùë†ùëí\\nThis equation now reflects that if the score of the current document ùê∑ùëõis less than 0.3 and the\\nprevious document is relevant, the probability of the current document being relevant is simply equal\\nto the relevance score of current document.\\nIf the previous document is relevant and if the score of the current document ùê∑ùëõis greater than equal\\nto 0.3 then the probability that the current document is relevant is 0.2 + Score for the current document.\\nFor the first document, the probability is equal to the relevance score of current document. In all other\\nsituations, the probability is equal to the relevance score of current document. If the probability score\\nof a particular document is greater than 0.5, we consider the document to be relevant to the query. Like\\nthis we found out all documents which are relevant to a query.\\nFor the five results reported, we ran the GPT model at different temperature values namely 0.5, 0,6,\\n0.7, 0.8, and 0.9. The diagram for GPT-3.5 Turbo is shown in Figure 1. The figure representing the\\nmethodology is shown in Figure 2.\\n6. Results\\nTable 1 presents the evaluation metrics for different submissions for the team named \"TextTitans\".\\nThe metrics used to assess the performance are MAP Score, ndcg Score, p@5 Score, and p@10 Score.\\nHere‚Äôs what these results imply. MAP is a common metric in information retrieval that measures the\\nprecision of results across multiple queries. A higher MAP score indicates that relevant documents are\\nconsistently ranked higher across all queries. In the table, the MAP scores for the first four submissions\\nare identical (0.701773), while the fifth submission slightly improves to 0.703734. This indicates that the\\nfifth submission is marginally better in terms of ranking relevant results across multiple queries. The\\nndcg score measures the quality of the ranking based on the position of relevant documents. A higher\\nndcg score suggests that relevant documents are placed higher in the ranking. The scores are also very\\nsimilar across submissions, with the first four submissions having an ndcg score of 0.797937, and the\\nfifth submission showing a slight improvement to 0.799196. This suggests a minor improvement inFigure 1: An overview of the GPT-3.5 Turbo architecture.\\nFigure 2: Overview diagram of the methodology followed for GPT-3.5 Turbo.\\nMAP Score ndcg Score p@5 Score p@10 Score Team Name Submission File Rank\\n0.701773 0.797937 0.793333 0.766667 TextTitans submit_cmir 5\\n0.701773 0.797937 0.793333 0.766667 TextTitans submit_cmir_1 4\\n0.701773 0.797937 0.793333 0.766667 TextTitans submit_cmir_2 3\\n0.701773 0.797937 0.793333 0.766667 TextTitans submit_cmir_3 2\\n0.703734 0.799196 0.793333 0.766667 TextTitans submit_cmir_4 1\\nTable 1\\nA Comparison of MAP, NDCG, P@5, and P@10 Scores for the TextTitans Team.\\nranking relevant documents for the fifth submission. p@5 measures how many of the top 5 ranked\\ndocuments are relevant. A score of 1 would mean that all 5 of the top-ranked documents are relevant.\\nAll submissions have the same p@5 score of 0.793333, indicating that the top 5 results are equally\\naccurate across all submissions. Precision@10 measures how many of the top 10 ranked documents are\\nrelevant. Like p@5, a higher score is better. Similar to p@5, all submissions have the same p@10 score\\nof 0.766667, showing no variation in the top 10 results across the different submissions. The metrics\\nare very consistent across all submissions, with only minor improvements in MAP and NDCG scores\\nfor the fifth submission. The fifth submission shows a slight improvement in ranking and retrievalperformance, but the changes are minimal. The p@5 and p@10 scores indicate that the precision of the\\ntop 5 and top 10 results is identical across all submissions, suggesting that the models are performing\\nsimilarly in identifying the most relevant documents. Overall, while there is a slight improvement in\\nthe last submission, the models generally perform similarly across all metrics.\\n7. Conclusion\\nIn conclusion, this study addresses the critical challenges of extracting relevant information from\\ncode-mixed conversations, specifically within Roman transliterated Bengali mixed with English. This\\nlinguistic phenomenon is prevalent among migrant communities in India, who often rely on social media\\nplatforms to share and seek vital information, especially during crises like the COVID-19 pandemic.\\nThe informal and non-standardized nature of these conversations presents unique difficulties for\\ninformation retrieval. To tackle these challenges, we developed a novel approach that leverages the\\nGPT-3.5 Turbo model in conjunction with a sequential engineering approach, achieving notable success\\nin retrieving pertinent answers from complex, code-mixed digital conversations. The effectiveness of\\nour method is demonstrated through the results on the test set documents and queries, which provides\\na valuable resource for future research in natural language processing within multilingual and informal\\ntext environments. This work contributes to enhancing information accessibility for marginalized\\ncommunities, underscoring the potential of advanced AI models in bridging communication gaps in\\ndiverse linguistic landscapes. We observe that the GPT-3.5 model along with mathematical formulation\\napproach performs well for the task of Code mixed information retrieval, though there is scope for\\nimprovement.\\nReferences\\n[1]E. Sippola, Multilingualism and the structure of code-mixing, in: The Routledge handbook of\\nPidgin and Creole languages, Routledge, 2020, pp. 474‚Äì489.\\n[2]E. O. Aboh, Lessons from neuro-(a)-typical brains: universal multilingualism, code-mixing,\\nrecombination, and executive functions, Frontiers in psychology 11 (2020) 488.\\n[3]A. Deroy, K. Ghosh, S. Ghosh, How ready are pre-trained abstractive models and llms for legal\\ncase judgement summarization?, arXiv preprint arXiv:2306.01248 (2023).\\n[4] A. De Swaan, Words of the world: The global language system, John Wiley & Sons, 2013.\\n[5]A. Deroy, K. Ghosh, S. Ghosh, Ensemble methods for improving extractive summarization of legal\\ncase judgements, Artificial Intelligence and Law 32 (2024) 231‚Äì289.\\n[6]C. Lee, Multilingual resources and practices in digital communication, in: The Routledge handbook\\nof language and digital communication, Routledge, 2015, pp. 118‚Äì132.\\n[7]K. S. Rao, et al., A novel approach to unsupervised pattern discovery in speech using convolutional\\nneural network, Computer Speech & Language 71 (2022) 101259.\\n[8]S. Shekhar, H. Garg, R. Agrawal, S. Shivani, B. Sharma, Hatred and trolling detection transliteration\\nframework using hierarchical lstm in code-mixed social media text, Complex & Intelligent Systems\\n9 (2023) 2813‚Äì2826.\\n[9]A. Deroy, P. Bhattacharya, K. Ghosh, S. Ghosh, An analytical study of algorithmic and expert\\nsummaries of legal cases, in: Legal Knowledge and Information Systems, IOS Press, 2021, pp.\\n90‚Äì99.\\n[10] L. Komito, Social media and migration: Virtual community 2.0, Journal of the American society\\nfor information science and technology 62 (2011) 1075‚Äì1086.\\n[11] S. Maity, A. Deroy, S. Sarkar, A novel multi-stage prompting approach for language agnostic\\nmcq generation using gpt, in: European Conference on Information Retrieval, Springer, 2024, pp.\\n268‚Äì277.\\n[12] M. M. Meurer, M. Waldkirch, P. K. Schou, E. L. Bucher, K. Burmeister-Lamp, Digital affordances:How entrepreneurs access support in online communities during the covid-19 pandemic, Small\\nBusiness Economics (2022) 1‚Äì27.\\n[13] S. Maity, A. Deroy, S. Sarkar, Harnessing the power of prompt-based techniques for generating\\nschool-level questions using large language models, in: Proceedings of the 15th Annual Meeting\\nof the Forum for Information Retrieval Evaluation, 2023, pp. 30‚Äì39.\\n[14] D. D. Lewis, K. S. Jones, Natural language processing for information retrieval, Communications\\nof the ACM 39 (1996) 92‚Äì101.\\n[15] S. Maity, A. Deroy, S. Sarkar, How ready are generative pre-trained large language models for\\nexplaining bengali grammatical errors?, in: B. Paa√É≈∏en, C. D. Epp (Eds.), Proceedings of the 17th\\nInternational Conference on Educational Data Mining, International Educational Data Mining\\nSociety, Atlanta, Georgia, USA, 2024, pp. 664‚Äì671. doi: 10.5281/zenodo.12729912 .\\n[16] M. Janse, N. Vassalou, D. Papazachariou, Variation in the vowel system of mi≈°√≥tika cappadocian:\\nFindings from two refugee villages in greec, in: 13th International Conference on Greek Linguistics,\\nUniversity of Westminster, 2017.\\n[17] S. Maity, A. Deroy, S. Sarkar, Exploring the capabilities of prompted large language models in\\neducational and assessment applications, in: B. Paa√É≈∏en, C. D. Epp (Eds.), Proceedings of the 17th\\nInternational Conference on Educational Data Mining, International Educational Data Mining\\nSociety, Atlanta, Georgia, USA, 2024, pp. 961‚Äì968. doi: 10.5281/zenodo.12730013 .\\n[18] T. B. Brown, Language models are few-shot learners, arXiv preprint ArXiv:2005.14165 (2020).\\n[19] A. Deroy, S. Maity, Questioning biases in case judgment summaries: Legal datasets or large\\nlanguage models?, arXiv preprint arXiv:2312.00554 (2023).\\n[20] T. Jauhiainen, H. Jauhiainen, K. Linden, A survey on automatic language identification in written\\ntexts, in: Journal of Artificial Intelligence Research, volume 65, 2019, pp. 675‚Äì782.\\n[21] Y. Muthusamy, R. A. Cole, B. T. Oshika, Automatic language identification: A review/tutorial, in:\\nIEEE Signal Processing Magazine, volume 11, 1994, pp. 33‚Äì41.\\n[22] G. I. Ahmad, J. Singla, Sentiment analysis of code-mixed social media text (sa-cmsmt) in indian-\\nlanguages, in: 2021 International Conference on Computing Sciences (ICCS), IEEE, 2021, pp.\\n25‚Äì33.\\n[23] S. K. Nigam, A. Deroy, N. Shallum, A. K. Mishra, A. Roy, S. K. Mishra, A. Bhattacharya, S. Ghosh,\\nK. Ghosh, Nonet at semeval-2023 task 6: Methodologies for legal evaluation, arXiv preprint\\narXiv:2310.11049 (2023).\\n[24] A. Deroy, K. Ghosh, S. Ghosh, Applicability of large language models and generative models for\\nlegal case judgement summarization, Artificial Intelligence and Law (2024) 1‚Äì44.\\n[25] S. K. Nigam, A. Deroy, Fact-based court judgment prediction, in: Proceedings of the 15th Annual\\nMeeting of the Forum for Information Retrieval Evaluation, 2023, pp. 78‚Äì82.\\n[26] A. Deroy, S. Maity, S. Ghosh, Prompted zero-shot multi-label classification of factual incorrectness\\nin machine-generated summaries., in: FIRE (Working Notes), 2023, pp. 734‚Äì746.\\n[27] S. Maity, A. Deroy, S. Sarkar, How effective is gpt-4 turbo in generating school-level questions\\nfrom textbooks based on bloom‚Äôs revised taxonomy?, 2024. URL: https://arxiv.org/abs/2406.15211.\\narXiv:2406.15211 .\\n[28] A. F. Hidayatullah, A. Qazi, D. T. C. Lai, R. A. Apong, A systematic review on language identification\\nof code-mixed text: techniques, data availability, challenges, and framework development, IEEE\\naccess 10 (2022) 122812‚Äì122831.\\n[29] A. Deroy, S. Maity, Multi-label classification of covid-tweets using large language models, arXiv\\npreprint arXiv:2312.10748 (2023).\\n[30] A. Deroy, N. K. Bailung, K. Ghosh, S. Ghosh, A. Chakraborty, Artificial intelligence (ai) in legal\\ndata mining, arXiv preprint arXiv:2405.14707 (2024).\\n[31] G. I. Ahmad, J. Singla, A. Anis, A. A. Reshi, A. A. Salameh, Machine learning techniques for\\nsentiment analysis of code-mixed and switched indian social media text corpus: A comprehensive\\nreview, International Journal of Advanced Computer Science and Applications 13 (2022).\\n[32] A. Deroy, S. Maity, Ai-powered answer assessment: A comprehensive overview, Authorea\\nPreprints (2024).[33] A. F. Hidayatullah, A. Qazi, D. T. C. Lai, R. A. Apong, A systematic review on language identification\\nof code-mixed text: techniques, data availability, challenges, and framework development, IEEE\\naccess 10 (2022) 122812‚Äì122831.\\n[34] A. Deroy, S. Maity, A short case study on understanding the capabilities of gpt for temporal\\nreasoning tasks, Authorea Preprints (2024).\\n[35] A. Pratapa, G. Bhat, M. Choudhury, S. Sitaram, S. Dandapat, K. Bali, Language modeling for\\ncode-mixing: The role of linguistic theory based synthetic data, in: Proceedings of the 56th Annual\\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2018, pp.\\n1543‚Äì1553.\\n[36] A. Deroy, S. Maity, Question generation: Past, present & future, Authorea Preprints (2024).\\n[37] A. Deroy, S. Maity, Exploring the mathematical reasoning capabilities of gemini, Authorea\\nPreprints (2024).\\n[38] U. Barman, Automatic processing of code-mixed social media content, Ph.D. thesis, Dublin City\\nUniversity, 2019.\\n[39] S. Ghosh, K. Ghosh, D. Ganguly, A. Bhattacharya, P. P. Chakrabarti, S. Guha, A. Pal, K. Rudra,\\nP. Majumder, D. Roy, et al., Report on the 2nd symposium on artificial intelligence and law (sail)\\n2022, in: ACM SIGIR Forum, volume 56, ACM New York, NY, USA, 2023, pp. 1‚Äì7.\\n[40] S. Maity, A. Deroy, Natural language correction with an emphasis on bangla (2023).\\n[41] D. Gupta, A. Ekbal, P. Bhattacharyya, A semi-supervised approach to generate the code-mixed\\ntext using pre-trained encoder and transfer learning, in: T. Cohn, Y. He, Y. Liu (Eds.), Findings\\nof the Association for Computational Linguistics: EMNLP 2020, Association for Computational\\nLinguistics, Online, 2020, pp. 2267‚Äì2280. URL: https://aclanthology.org/2020.findings-emnlp.206.\\ndoi:10.18653/v1/2020.findings-emnlp.206 .\\n[42] K. R. Chandu, A. W. Black, Style variation as a vantage point for code-switching, arXiv preprint\\narXiv:2005.00458 (2020).\\n[43] K. Bali, J. Sharma, M. Choudhury, Y. Vyas, ‚Äúi am borrowing ya mixing?\" an analysis of english-hindi\\ncode mixing in facebook, in: Proceedings of the first workshop on computational approaches to\\ncode switching, 2014, pp. 116‚Äì126.\\n[44] A. Deroy, Exploiting Machine Learning Techniques for Unsupervised Clustering of Speech Utter-\\nances, Ph.D. thesis, Indian Institute of Technology Kharagpur, 2019.\\n[45] B. Sarkar, N. Sinhababu, M. Roy, P. K. D. Pramanik, P. Choudhury, Mining multilingual and\\nmultiscript twitter data: unleashing the language and script barrier, International Journal of\\nBusiness Intelligence and Data Mining 16 (2020) 107‚Äì127.\\n[46] A. Deroy, S. Maity, S. Sarkar, Mirror: A novel approach for the automated evaluation of open-ended\\nquestion generation, arXiv preprint arXiv:2410.12893 (2024).\\n[47] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, Language models are unsupervised\\nmultitask learners, in: OpenAI Blog, volume 1, 2019.\\n[48] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, P. J. Liu, Exploring\\nthe limits of transfer learning with a unified text-to-text transformer, Journal of Machine Learning\\nResearch 21 (2020) 1‚Äì67.\\n[49] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep bidirectional transformers\\nfor language understanding, in: Proceedings of the 2019 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long\\nand Short Papers), 2019, pp. 4171‚Äì4186.\\n[50] S. K. Nigam, A. Deroy, S. Maity, A. Bhattacharya, Rethinking legal judgement prediction in a\\nrealistic scenario in the era of large language models, arXiv preprint arXiv:2410.10542 (2024).\\n[51] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, V. Stoyanov,\\nRoberta: A robustly optimized bert pretraining approach, in: arXiv preprint arXiv:1907.11692,\\n2019.\\n[52] S. Maity, A. Deroy, S. Sarkar, Exploring the capabilities of prompted large language models in\\neducational and assessment applications (2024).\\n[53] S. Maity, A. Deroy, Generative ai and its impact on personalized intelligent tutoring systems,arXiv preprint arXiv:2410.10650 (2024).\\n[54] W. X. Zhao, K. Zhou, J. Li, X. Tang, J. J. Wang, J. Liu, T. Wang, Y. Bao, J.-R. Wen, A survey of large\\nlanguage models, in: arXiv preprint arXiv:2303.18223, 2023.\\n[55] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polosukhin,\\nAttention is all you need, Advances in neural information processing systems 30 (2017) 5998‚Äì6008.\\n[56] A. Deroy, S. Maity, Code generation and algorithmic problem solving using llama 3.1 405b, arXiv\\npreprint arXiv:2409.19027 (2024).\\n[57] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, Fine-tuning gpt-2 for human-like\\ntext generation, in: arXiv preprint arXiv:1907.11692, 2019.\\n[58] R. Zellers, A. Holtzman, H. Rashkin, Y. Bisk, A. Farhadi, F. Roesner, Y. Choi, Defending against\\nneural fake news, in: Advances in Neural Information Processing Systems, volume 32, 2019, pp.\\n9054‚Äì9065.\\n[59] S. Maity, A. Deroy, The future of learning in the age of generative ai: Automated question\\ngeneration and assessment with large language models, arXiv preprint arXiv:2410.09576 (2024).\\n[60] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, G. Neubig, Pre-train, prompt, and predict: A systematic\\nsurvey of prompting methods in natural language processing, ACM Computing Surveys 55 (2023)\\n1‚Äì35.\\n[61] P. Singh, M. Patidar, L. Vig, Translating across cultures: Llms for intralingual cultural adaptation,\\narXiv preprint arXiv:2406.14504 (2024).\\n[62] G. Yenduri, M. Ramalingam, G. C. Selvi, Y. Supriya, G. Srivastava, P. K. R. Maddikunta, G. D.\\nRaj, R. H. Jhaveri, B. Prabadevi, W. Wang, et al., Gpt (generative pre-trained transformer)‚Äìa\\ncomprehensive review on enabling technologies, potential applications, emerging challenges, and\\nfuture directions, IEEE Access (2024).\\n[63] M. Johnsen, Large Language Models (LLMs), Maria Johnsen, 2024.\\n[64] J. Kaddour, J. Harris, M. Mozes, H. Bradley, R. Raileanu, R. McHardy, Challenges and applications\\nof large language models, arXiv preprint arXiv:2307.10169 (2023).\\n[65] S. Maity, A. Deroy, Human-centric explainable ai in education, 2024. URL: https://arxiv.org/abs/\\n2410.19822. arXiv:2410.19822 .',\n",
       " ' \\n 1 \\nARTICLE INFORMATION  \\nArticle title  \\nMulti -language Video Subtitle Dataset for Image -based Text Recognition  \\nAuthors  \\nThanadol Singkhornarta, Olarik Surintaa,*   \\nAffiliations  \\n a Multi -agent Intelligent Simulation Laboratory (MISL) Research Unit, Department of Information \\nTechnology, Faculty of Informatics, Mahasarakham University, Mahasarakham 44150, Thailand  \\nCorresponding author‚Äôs email address  \\n Email address: olarik.s@msu.ac.th  (O. Surinta)  \\nKeywords  \\nText recognition; Deep learning; Feature extraction; Transformer; Convolutional neural network;  \\nLong short -term memory network; Connectionist temporal classification  \\n \\nAbstract  \\nThe Multi -language Video Subtitle Dataset is a comprehensive collection designed to support research \\nin text recognition across multiple languages. This dataset includes 4,224 subtitle images extracted \\nfrom 24 videos sourced from online platforms. It featu res a wide variety of characters, including Thai \\nconsonants, vowels, tone marks, punctuation marks, numerals, Roman characters, and Arabic \\nnumerals. With 157 unique characters, the dataset provides a resource for addressing challenges in \\ntext recognition w ithin complex backgrounds. It addresses the growing need for high -quality, \\nmultilingual text recognition data, particularly as videos with embedded subtitles become increasingly \\ndominant on platforms like YouTube and Facebook. The variability in text lengt h, font, and placement \\nwithin these images adds complexity, offering a valuable resource for developing and evaluating deep \\nlearning models. The dataset facilitates accurate text transcription from video content while providing \\na foundation for improving c omputational efficiency in text recognition systems. As a result, it holds \\nsignificant potential to drive advancements in research and innovation across various computer \\nscience disciplines, including artificial intelligence, deep learning, computer vision , and pattern \\nrecognition.  \\n \\n   \\n 2 \\nSPECIFICATIONS TABLE  \\nSubject  Computer Science  \\nSpecific subject \\narea   The multi -language video subtitle image dataset consists of images containing \\ntext in various languages, specifically designed to facilitate text recognition within \\nimages. This dataset is relevant to several disciplines, including artificial \\nintelligence, deep learning, computer science applications, computer vision, and \\npattern recognition .  \\nType of data  Image (JPG format )  \\nData collection   The multi -language video subtitle images were sourced from online platforms \\ncontaining content in various languages, including Thai characters, English \\ncharacters, Thai numerals, Arabic numerals, and special symbols. The dataset was \\ncreated by extracting frames from videos, followed by manually annotating the \\nsubtitle locations within each frame. As a result, a single frame may contain \\nmultiple subtitle regions .   \\nData source \\nlocation   Online platforms: YouTube and Facebook   \\nData accessibility  Repository name: Mendeley Data  \\nData identification number:  10.17632/gj8d88h2g3.2  \\nDirect URL to data:  https://data.mendeley.com/datasets/gj8d88h2g3/2  \\nRelated research \\narticle  T. Singkhornart, O. Surinta, Multi -language video subtitle recognition with \\nconvolutional neural network and long short -term memory networks, ICIC \\nExpress Letters 16 (2022) 647 ‚Äì655. https://doi.org/ 10.24507/icicel.16.06.647  \\n \\nVALUE OF THE DATA  \\n‚Ä¢ The Multi -language Video Subtitle Dataset provides a substantial sample size of 4,224 subtitle \\nimages, focusing on two primary languages, Thai and English, as well as two numeral systems \\n(Thai and Arabic) and special characters, encompassing a total of 157  distinct characters. \\nLabels corresponding to the subtitle content are embedded within the filenames, enabling \\nefficient referencing and organization. A single sample image may contain a mix of Thai, \\nEnglish, and numerals, presenting challenges in recogniz ing multiple languages \\nsimultaneously.  \\n‚Ä¢ The text within the images varies significantly, with some appearing against complex \\nbackgrounds. In this dataset, the shortest text length is one character, while the longest \\nextends to approximately 80 characters. Additionally, around 80 samples contain texts ranging \\nfrom 10 to 40 characters.  \\n‚Ä¢ Researchers in computer science and related fields can  utilize  this dataset to advance research \\nand enhance performance in terms of accuracy and computational efficiency. Moreover,  \\n 3 \\nvarious deep learning methodologies can be applied to further explore this dataset and push \\nthe boundaries of text recognition research.  \\n \\nBACKGROUND  \\n  Massive numbers of videos are uploaded to online platforms like YouTube and Facebook, \\nmaking them mainstream channels, particularly among teenagers. Many popular TV channels \\nworldwide have embraced this trend, offering high -quality content through these pl atforms. \\nNumerous videos also provide subtitles, which are necessary for individuals with hearing impairments, \\nenabling them to comprehend the video content. Subtitles further assist audiences in learning the \\nspelling of names, brands, acronyms, and abbr eviations and understanding various accents. \\nAdditionally, subtitles bridge the accessibility gap between content and audiences. Further supporting \\nnon-native speakers, subtitles serve as essential tools for individuals with hearing impairments, \\noffering a  textual representation of spoken dialogue that transcends language barriers and audio \\nlimitations.  \\nIn this dataset, the research team focused on capturing subtitles from online platforms like \\nYouTube and Facebook that feature multiple languages, including Thai and English. The text within \\nthese images consists of a mix of Thai, English, and numerals, often displayed against complex \\nbackg rounds, making recognition particularly challenging. It emphasizes the requirement for high -\\nquality datasets that provide comprehensive data for developing deep learning models and enhancing \\ntext recognition systems. Advanced recognition systems rely on ro bust models capable of accurately \\ntranscribing text from videos, even when encountered with font, size, and language variations.  \\n \\nDATA DESCRIPTION  \\n  The Multi -language Video Subtitle Dataset is a comprehensive collection of images containing \\ntext in multiple languages, referred to as subtitle images. These images were extracted from 24 videos \\nsourced from online platforms. The details of the dataset a re presented as follows.  \\n \\nCharacter Collection  \\nThe subtitle images were extracted from 24 videos containing a diverse range of characters \\nacross multiple languages. These include Thai consonants, vowels, tone marks, punctuation marks, \\nand numerals, as well as Roman characters and Arabic numerals. The M ulti-language Video Subtitle \\nDataset comprises 157 unique characters, as detailed in Table 1.  \\n \\n \\n \\n \\n  \\n 4 \\nTable 1.  Character collection in the Multi -language Video Subtitle Dataset.  \\nCharacter Types  Characters  \\nThai  Consonant  ‡∏Å  ‡∏Ç  ‡∏É  ‡∏Ñ  ‡∏Ö  ‡∏Ü  ‡∏á  ‡∏à  ‡∏â  ‡∏ä  ‡∏ã  ‡∏å  ‡∏ç  ‡∏é  ‡∏è   \\n‡∏ê  ‡∏ë  ‡∏í ‡∏ì  ‡∏î  ‡∏ï  ‡∏ñ  ‡∏ó  ‡∏ò  ‡∏ô  ‡∏ö  ‡∏õ  ‡∏ú  ‡∏ù  ‡∏û  \\n‡∏ü  ‡∏†  ‡∏°  ‡∏¢  ‡∏£  ‡∏•  ‡∏ß  ‡∏®  ‡∏©  ‡∏™  ‡∏´  ‡∏¨  ‡∏≠  ‡∏Æ \\nVowel  ‡∏∞  ‡∏±   ‡∏≤  ‡πÖ   ‡∏±    ‡∏±    ‡∏±    ‡∏±   ‡∏±   ‡∏±   ‡∏± ‡∏≤   ‡∏±    \\n‡πÉ   ‡πÑ  ‡πÇ   ‡∏±    ‡πÄ   ‡πÅ   ‡∏§  ‡∏¶ \\nTone  ‡∏±   ‡∏±   ‡∏±   ‡∏±  \\nPunctuation mark  ‡πÜ  ‡∏Ø  ‡∏±  \\nNumeral  ‡πë  ‡πí  ‡πì  ‡πî  ‡πï  ‡πñ  ‡πó  ‡πò  ‡πô  ‡πê \\nRoman  Character  A  B  C  D  E  F  G  H  I  J  K  L  M   \\nN  O P  Q  R  S  T  U  V  W  X  Y Z \\na  b  c  d  e  f  g  h  i  j  k  l  m   \\nn o  p  q  r  s  t  u  v  w  x  y  z \\nArabic  Numeral  1  2  3  4  5  6  7  8  9  0  \\nSpecial character  .   ,   !   (   )   -   $   ‡∏ø   &   :   ?   *   ‚Äú   ‚Äù   ‚Äò   ‚Äô   (space ) \\n \\nLabelling  \\nThe research team split the 24 videos into frames, resulting in 4,224 images with a resolution \\nof 1,280√ó720 pixels per frame. Out of the total, 2,700 images containing subtitles were selected and \\nannotated. However, the dataset does not include annotation files (XML); only the image files (JPG) \\nare freely accessible via the Mendeley Data repository. Examples of subtitle images and their labels \\nare illustrated in Table 2.  \\n \\nTable 2.  Examples of images with subtitles labels.  \\nImages with subtitles  Labels  \\n \\n \\n  \\n‡πÄ ‡∏Ç  ‡∏≤ ‡∏°‡∏≤ ‡πÅ ‡∏ï  ‡∏á ‡∏ö ‡∏≤ ‡∏ô ‡πÉ ‡∏´  ‡∏™  ‡∏î ‡∏≠ ‡∏¢ ‡∏≤ ‡∏á ‡∏ù  ‡∏ô ‡∏ó  ‡πà  \\n \\n 5 \\nImages with subtitles  Labels  \\n \\n \\n  \\nYOU NEVER SHARE YOUR TOYS OR \\nCOMMUNICATE  \\n \\nI GUESS I‚ÄôM JUST A PLAY DATE TO YOU  \\n \\n \\n  \\nONCE I WAS SEVEN YEARS OLD  \\n \\nFile format and conversion  \\nAfter the annotation and labelling process, a Python program was developed to extract the \\nsubtitle images based on the coordinates provided in the XML files. The extracted subtitle images are \\nstored in format with RGB channels, ensuring compatibility with widely used image processing \\nlibraries and too ls. \\nFurthermore, the filenames of each subtitle image follow a consistent and standardized \\nnaming convention, embedding both a unique identifier and a label corresponding to the subtitle \\ncontent. Examples of subtitle images and their labels are presented in Ta ble 3.  \\n \\n \\nTable 3.  Examples of subtitle images and labels.  \\nSubtitle Images  Filenames  \\n \\n \\n  \\n3798_1.jpg  \\n \\n \\n 6 \\nSubtitle Images  Filenames  \\n \\n \\n  \\n3148 _ ‡∏õ‡∏è  ‡∏ö  ‡∏ï  ‡∏Å ‡∏≤ ‡∏£ .jpg \\n \\n \\n \\n  \\n1001_ ‡πÇ ‡∏£ ‡∏á ‡∏´ ‡∏ô  ‡∏á   ‡∏ô  ‡∏ß‡∏¢‡∏≠ ‡∏£  ‡∏Ñ   ‡∏™ ‡∏∞ ‡∏û ‡∏≤ ‡∏ô ‡∏Ñ ‡∏ß‡∏≤ ‡∏¢ .jpg \\n \\n \\n 1002_ ‡∏ñ  ‡∏≠ ‡πÄ ‡∏õ  ‡∏ô ‡πÇ ‡∏£ ‡∏á ‡∏´ ‡∏ô  ‡∏á ‡∏™  ‡∏î ‡∏Æ  ‡∏ï ‡πÄ ‡∏°  ‡πà‡∏≠ ‡∏£ ‡∏≤ ‡∏ß  40 ‡∏õ  ‡∏Å  ‡∏≠ ‡∏ô .jpg \\n \\n 105_ ‡∏Ç‡∏≠ ‡∏á Memory Foam.jpg  \\n \\n 1149_ ‡∏Å ‡∏•  ‡∏ö‡πÑ ‡∏° ‡πÄ ‡∏õ  ‡∏ô ‡∏à ‡∏£  ‡∏á !.jpg \\n \\n 2190_ ‡∏ó ‡∏£ ‡∏û  ‡∏© ‡∏•‡∏á ‡∏ß  ‡∏ô ‡∏ó  ‡πà  ‡πî ‡∏ï  ‡∏•‡∏≤ ‡∏Ñ ‡∏°  ‡∏û ‡∏£ ‡∏∞ ‡∏û  ‡∏ó ‡∏ò ‡∏®  ‡∏Å ‡∏£ ‡∏≤ ‡∏ä  \\n‡πí ‡πï ‡πñ ‡πë  ‡∏ß ‡∏≤ ‡∏î  ‡∏ß‡∏¢‡∏Å ‡∏£ ‡∏°‡∏®  ‡∏Ç‡∏≤ ‡∏†  ‡∏ö‡∏≤ ‡∏• .jpg \\n \\n 3374_ ‡πÄ ‡∏£ ‡∏≤ ‡πÄ ‡∏£  ‡∏¢ ‡∏Å ‡∏ß ‡∏≤  M18A1 ‡∏ú ‡∏•  ‡∏ï ‡∏ó  ‡πà‡∏õ‡∏£ ‡∏∞ ‡πÄ ‡∏ó ‡∏® ‡∏™ ‡∏´ ‡∏£  ‡∏ê ‡∏Ø .\\njpg \\n \\n 4205_dream_.jpg  \\n \\n 4041_I DON_T WANT TO PLAY NO \\nGAMES.jpg  \\n \\n \\n \\n \\n 7 \\nDistribution of characters and subtitle images  \\nAfter generating the subtitle images and labels, we calculated basic statistics for the \\ncharacters, as presented in Fig. 1 and Fig. 2.  \\nAs depicted in Fig. 1, the distribution of characters shows the number of subtitle images \\nrelative to the number of characters appearing in each image. It reveals that more than 80 subtitle \\nimages contain between 10 and 40 characters, which represents the standard length for subtitles. \\nHowever, a small number of images contain fewer than 10 characters or more than 40 characters, \\nunderscoring the importance of balanced training to ensure models can effectively handle both \\nshorter and longer subtitles.  \\n \\nFig. 1.   illustrates the number of subtitle images (y -axis) relative to the number of characters \\nappearing in each image (x -axis).  \\n \\nFig. 2.  illustrates the number of pixels in both the width (blue) and height (red) of the subtitle images \\n(x-axis) relative to the number of subtitle images (y -axis). It is recommended to view this graph in \\ncolor for better clarity.  \\n \\n \\n 8 \\nAdditionally, Fig. 2 presents the frequency distribution of image dimensions. The red line, \\nrepresenting image height, shows a prominent peak at lower pixel values, indicating that most images \\nhave a shorter height. The blue line, representing image width, displays a broader distribution, with \\nwidths extending up to 1,200 pixels. Moreover, Fig. 3 highlights the maximum width and height of the \\nsubtitle images in the Multi -language Video Subtitle Dataset.  \\n \\n \\n(a) 1,279 √ó229 pixels  \\n \\n(b) 675√ó624 pixels  \\nFig. 3. illustrates subtitle images with width√óheight dimensions, showing (a) a maximum width of \\n1,279 pixels and (b) a maximum height of 624 pixels collected in the dataset.  \\n \\nEXPERIMENTAL DESIGN, MATERIALS AND METHODS  \\nMaterials  \\nThe subtitle images were collected from YouTube and Facebook between October 2020 and \\nJanuary 2021 by selecting videos that featured embedded subtitles. In the initial phase of data \\ncollection, the research team focused solely on subtitles appearing at the  bottom of the video. \\nHowever, it was later observed that many videos, particularly those from news channels, also show \\ngraphical text on the screen. Consequently, lyric videos featuring multiple lines of text in the middle of \\nthe screen were also included . \\n The video sources were gathered from the following Facebook pages: ‡πÑ ‡∏ó ‡∏¢‡∏£  ‡∏ê ‡∏≠ ‡∏≠ ‡∏ô ‡πÑ ‡∏•‡∏ô   (3 ‡∏ô ‡∏≤ ‡∏ó  ‡∏Ñ ‡∏î  ‡∏î  ‡∏á ) \\nand TEP - Thailand Education Partnership ‡∏† ‡∏≤ ‡∏Ñ  ‡πÄ ‡∏û  ‡πà‡∏≠ ‡∏Å ‡∏≤ ‡∏£ ‡∏®  ‡∏Å ‡∏© ‡∏≤ ‡πÑ ‡∏ó ‡∏¢ . The YouTube channels included Bearhug, \\nKLUAYTHAI, KRIT Eighth Floor, Genierock, Taj Tracks, Cakes & Eclairs, 7clouds, DopeLyrics, Equilanora, \\nJEMIN Apollo, San Ko, Tangerine JJY , SNH48 Lyrics, and Lemoring. After downloading the videos, they \\nwere split int o frames, with a frame captured every 5 seconds, providing a comprehensive snapshot \\nof the video content. An example of images with subtitle labels is shown in Fig. 4.  \\n \\n \\n 9 \\n \\n \\nFig. 4.  Example images with subtitles. Note that the green rectangle boxes represent the locations of \\nthe subtitles.  \\n \\nThe LabelImg software was used to annotate the subtitles in the images (see Fig. 5), \\ngenerating XML files with precise subtitle locations and corresponding labels. A Python program was \\nthen developed to extract the subtitle images and labels based on the i nformation provided in the \\nXML files.  \\n \\n  \\n (a)  (b) \\nFig. 5.  Examples of (a) the LabelImg software and (b) the XML file, which provides the location of \\nsubtitles.  \\n \\nMoreover, the subtitle images include text with various fonts, sizes, and colors. The number \\nof characters in each subtitle image also varies. As a result, some subtitle images contain both Thai \\nand English characters. Example subtitle images and their dim ensions are shown in Table 4.  \\n \\n 10 \\nTable 4.  Example subtitle images and their dimensions.  \\nSubtitle images and dimensions  \\n \\n \\n623√ó39 pixels  \\n \\n \\n878√ó68 pixels  \\n \\n \\n598√ó96 pixels  \\n \\n \\n92√ó99 pixels  \\n \\n \\n624√ó25 pixels  \\n \\n \\n997√ó61 pixels  \\n \\n \\nExperimental design  \\nDeep learning methods, such as convolutional neural networks (CNNs) and long short -term \\nmemory (LSTM) networks, have been proposed to evaluate model performance using the Multi -\\nlanguage Video Subtitle Dataset. In 2022, Singkhornart and Surinta [1] introduced a CNN -LSTM \\narchitecture where the output utilized connectionist temporal classification (CTC) as the loss function \\nto compute the loss value and d ecode the predictions. The modified VGG19 was employed as the \\nCNN backbone, resulting in a character error rate (CER) of 9.36%.  \\n \\n 11 \\nAdditionally, Gonwirat, Surinta, & Pawara [2] proposed the FusionCNNs -LSTM architecture to \\nextract more robust temporal features from the subtitle images. Their approach integrated the fusion \\nof VGG -s1 and VGG -s2 architectures, where s represents the stride number, using an additive \\noperation. They also compared two decoding algorithms, CTC and word beam search (WBS), for \\ndecoding the output. Their experimental results indicated that WBS outperformed the CTC algorithm, \\nwith the FusionCNNs -LSTM achieving a CER of 5.29% on the test set.  \\nBoth of these methods focus on the fusion of deep learning algorithms, particularly CNN and \\nLSTM architectures, to extract both spatial and temporal features. Additionally, to enhance feature \\nrobustness, a multi -layer adaptive spatial -temporal feature fusi on network (ASTFF) [3] can be \\nemployed to extract spatial -temporal features before passing them to the decoding algorithms.  \\nTo further advance text recognition, several methods, such as LISTER [4], CLIPTER [5], and \\nClass -Aware Mask -guided [6] have been developed to address varying text lengths and challenging \\nconditions, such as handwritten, blurred, and noisy text. These appro aches not only prioritize accuracy \\nbut also emphasize computational efficiency, with a strong focus on minimizing processing costs.  \\n \\nLIMITATIONS  \\nNot applicable  \\n \\nETHICS STATEMENT  \\nThe authors have read and follow the ethical requirements and confirming that the current work \\ndoes not involve human subjects, animal experiments, or any data collected from social media \\nplatforms.  \\n \\nCRediT AUTHOR STATEMENT  \\nThanadol Singkhornart:  Conceptualization, Data Curation, Investigation, Methodology, Resources, \\nValidation, Writing ‚Äì Original Draft; Olarik Surinta: Supervision, Conceptualization, Experimental \\nDesign, Writing ‚Äì Review & Editing, Funding Acquisition.  \\n \\nACKNOWLEDGEMENTS  \\nThis research project was financially supported by Mahasarakham University, Thailand  \\n \\nDECLARATION OF COMPETING INTERESTS  \\nThe authors declare that they have no known competing financial interests or personal relationships \\nthat could have appeared to influence the work reported in this paper.   \\n 12 \\nREFERENCES  \\n[1] T. Singkhornart, O. Surinta, Multi -language video subtitle recognition with convolutional neural \\nnetwork and long short -term memory networks, ICIC Express Letters 16 (2022) 647 ‚Äì655.  \\n[2] S. Gonwirat, O. Surinta, P . Pawara, Fusion convolutional recurrent neural networks for Thai and \\nEnglish video subtitle recognition, ICIC Express Letters 16 (2022) 1331 ‚Äì1339.  \\n[3] S. Phiphitphatphaisit, O. Surinta, Multi -layer adaptive spatial -temporal feature fusion network \\nfor efficient food image recognition, Expert Syst Appl 255 (2024) 124834. \\nhttps://doi.org/10.1016/j.eswa.2024.124834 . \\n[4] C. Cheng, P . Wang, C. Da, Q. Zheng, C. Yao, LISTER: Neighbor decoding for length -insensitive \\nscene text recognition, in: IEEE/CVF International Conference on Computer Vision (ICCV), IEEE, \\n2023: pp. 19484 ‚Äì19494. https://doi.org/10.1109/ICCV51070.2023.01790 . \\n[5] A. Aberdam, D. Bensa√Ød, A. Golts, R. Ganz, O. Nuriel, R. Tichauer, S. Mazor, R. Litman, CLIPTER: \\nLooking at the bigger picture in scene text recognition, in: IEEE/CVF International Conference on \\nComputer Vision (ICCV), IEEE, 2023: pp. 21649 ‚Äì21660. \\nhttps://doi.org/10.1109/ICCV51070.2023.01984 . \\n[6] M. Yang, B. Yang, M. Liao, Y . Zhu, X. Bai, Class -aware mask -guided feature refinement for scene \\ntext recognition, Pattern Recognit 149 (2024) 110244. \\nhttps://doi.org/10.1016/j.patcog.2023.110244 . \\n \\n \\n ',\n",
       " 'News-Driven Stock Price Forecasting in Indian\\nMarkets: A Comparative Study of Advanced Deep\\nLearning Models\\nKaushal Attaluri‚àó, Mukesh Kumar Tripathi‚Ä†, Srinithi Reddy‚Ä°and Shivendra¬ß\\n‚àóSoftware Engineer-Machine Learning and AI ,Pegasystems India Worldwide Pvt Ltd , Hyderabad, India\\niamkaushal49@gmail.com\\n‚Ä†Department of Computer Science and Engineering ,Vardhaman College of Engineering , Hyderabad, India\\nmukeshtripathi016@gmail.com\\n‚Ä°Department of Information Technology ,Vasavi College of Engineering , Hyderabad, India\\nsrinithireddy214@gmail.com\\n‚Ä°Department of Computer Application ,D. K. College , Dumaraon, Bihar, India\\nsrivastavashivendra29@gmail.com\\nAbstract ‚ÄîForecasting stock market prices is a challenging\\ntask for traders, analysts, and engineers due to the myriad of\\nvariables influencing stock prices. However, the advent of artifi-\\ncial intelligence (AI) and natural language processing (NLP) has\\nsignificantly advanced stock market forecasting. AI‚Äôs ability to\\nanalyze complex data sets allows for more informed predictions.\\nDespite these advancements, stock price forecasting remains an\\narea where AI has not yet achieved optimal results. In this\\npaper, we forecast stock prices using 30 years of historical data\\nfrom various national banks in India sourced from the National\\nStock Exchange. We employ advanced deep learning models,\\nincluding multivariate multi-step Long Short-Term Memory\\n(LSTM), Facebook Prophet with LightGBM and Optuna, and\\nSeasonal Auto-Regressive Integrated Moving Average (SARIMA).\\nAdditionally, we analyze news data from tweets and reliable\\nsources like Business Standard and Reuters, recognizing their\\nsignificant impact on stock price movements.\\nIndex Terms ‚ÄîStock Market Forecasting, Data preprocess-\\ning, Advanced Algorithms, Deep Learning Models, Multivari-\\nate Multistep LSTM, SARIMA, LightGBM, Optuna, acebook\\nProphet,Sentiment Analysis.\\nI. I NTRODUCTION\\nIt has always been challenging and highly competitive\\nfor traders and investors to optimize their earnings in the\\nstock market by making sound judgments based on market\\ntrends and projections. Creating efficient prediction models is\\nvery important since these projections‚Äô precision significantly\\ninfluences investment decisions. Various techniques, such as\\nquantitative, fundamental, and technical analysis, have been\\ndeveloped to forecast stock prices[1-3]. Time series analysis\\nis one of these that has become a potent tool for stock\\nprice prediction. Deep learning RNN architectures such as\\nLSTM-based networks and conventional time series models\\nlike ARIMA have been used to forecast stock prices. Recently,\\na new forecasting tool that has become very popular in data\\nscience is Facebook Prophet. Facebook created the Prophet\\ntime series forecasting model, which aims to produce preciseand effective projections for various time series data, including\\nstock prices [4-5].\\nThe Indian stock market, like many other global financial\\nmarkets, is susceptible to a variety of factors, including\\neconomic indicators, international events, and, increasingly,\\nthe vast amount of information disseminated through news\\nmedia. In the digital age, news spreads rapidly, influencing\\ninvestor sentiment and stock prices. The ability to accurately\\nforecast stock prices based on news data has become a critical\\narea of interest for researchers and market practitioners [6-7].\\nThrough this comparative study, we aim to provide insights\\ninto the most effective deep-learning models for news-driven\\nstock price forecasting in the Indian market. The findings\\nwill contribute to the growing body of knowledge in financial\\nforecasting and could serve as a valuable tool for investors\\nand financial analysts seeking to enhance their decision-\\nmaking processes in a market heavily influenced by news\\nand information flows. This study aims to evaluate how well\\nthe enhanced Facebook Prophet, SARIMA, and Multivariate\\nmultistep LSTM models predict share value. The three models\\nwill be trained and assessed using historical stock price data\\nfrom a selected company, with their performance measured\\nby metrics such as Root Mean Squared Error (RMSE). In\\naddition, we‚Äôll examine how these models stack up against\\nLSTMs, which are attention-based and are trained not only\\non stock price data but also on news data from sources,\\nincluding tweets and news articles from the Business Standard\\nand Reuters. These methods will give us an idea of how the\\nmodels work for Indian Stock Market Prices; this paper will\\nalso act as an understanding of Finance and Machine Learning\\nEnthusiasts. The proposed work has been discussed and will\\nshow the several advantages, which will Section 3 of this\\npaper, but before that, Section 2 deals with the Related Work\\nthat was carried out in the field of stock price prediction.\\nThe other part of Section 3 will also deal with the model‚Äôs\\narchitecture, process flow, and the rest of the paper fromarXiv:2411.05788v1  [q-fin.ST]  14 Oct 2024Section 4 deals with the Results, Summary, Future Work and\\nReferences.\\nII. RELATED WORK\\nIn recent years, news-driven stock price prediction has\\ndrawn much attention as researchers try to use the abundance\\nof data supplied by news stories to raise the accuracy of stock\\nprice forecasts as the judgments vary with the daily news.\\nSeveral research papers have explored various deep learning\\nmodels and techniques to address this difficult task and ana-\\nlyze the market accurately. This section offers a comparative\\nanalysis of prominent documents that have been presented at\\ninternational conferences and have significantly advanced the\\nfield of news-driven stock price forecasting in Indian markets.\\nThe influence of news on stock price movements has been\\na topic of extensive research, particularly as the availability\\nof digital news sources has surged [8]. Traditional models,\\nsuch as the Efficient Market Hypothesis (EMH), suggest that\\nall available information, including news, is quickly reflected\\nin stock prices, leaving little room for predictive modeling.\\nHowever, the complexity and volume of news data have\\nchallenged this notion, paving the way for more sophisticated\\napproaches like deep learning. Recent studies have increas-\\ningly focused on applying deep learning models to analyze\\nthe impact of news on stock prices [9-10]. Among these, the\\nBidirectional Long Short-Term Memory (Bi-LSTM) model has\\nemerged as a powerful tool due to its ability to capture the\\ntemporal dependencies in sequential data and its proficiency\\nin processing the context from past and future time steps. This\\nbidirectional processing capability allows the model to under-\\nstand better the sentiment and relevance of news articles about\\nstock price movements. Several researchers have explored the\\neffectiveness of Bi-LSTM models in stock price forecasting.\\nFor instance, some studies have demonstrated that Bi-LSTM\\nmodels outperform traditional unidirectional LSTM models\\nand other machine learning techniques when predicting the\\ndirection of stock price changes based on news data. This is\\nattributed to the Bi-LSTM‚Äôs ability to learn complex patterns\\nand dependencies in the data, which simpler models often miss\\n[11-12].\\nThe literature [13-14]indicates that sentiment analysis in\\nconjunction with Bi-LSTM models further enhances prediction\\naccuracy. Sentiment analysis is a technique that involves\\ndetermining the emotional tone behind a series of words used\\nto understand how people feel about a particular topic or\\nissue. By incorporating sentiment scores derived from news\\narticles, these models can better gauge the market‚Äôs reaction\\nto news, leading to more accurate forecasts. The combination\\nof Bi-LSTM with sentiment analysis has been shown to im-\\nprove the model‚Äôs performance, particularly in volatile markets\\nwhere news plays a crucial role in driving price changes[15].\\nOverall, the literature highlights the significant potential of\\nBi-LSTM models in capturing the impact of news on stock\\nprices. These models represent a significant advancement in\\nfinancial forecasting, offering a more nuanced understanding\\nof how information flows influence market trends. The ongoingdevelopment and refinement of these models, which includes\\nimproving the model‚Äôs ability to handle larger and more di-\\nverse datasets, continue to contribute to the evolving landscape\\nof stock price prediction, especially in markets susceptible to\\nnews, such as the Indian stock market.\\nIn this paper [15-16], the authors focused on forecasting\\nthe stock price for the next day, specifically targeting the\\nclosing price of the S P 500 index. To achieve this, they\\nemployed Long Short-Term Memory (LSTM) models, a type\\nof recurrent neural network (RNN) well-suited for time series\\nprediction due to its ability to capture long-term dependencies\\nin sequential data. A notable aspect of this study is using nine\\nunique parameters as predictors, including fundamental market\\ndata, macroeconomic data, and technical indicators. These\\nparameters were carefully selected to provide a comprehensive\\nview of the factors influencing stock market behavior. The\\nfindings of this study revealed that the single-layer LSTM\\nmodel outperformed the multi-layer LSTM model in terms of\\nfit and prediction accuracy [17]. Specifically, the single-layer\\nmodel achieved lower RMSE and MAPE values, indicating a\\ncloser match between predicted and actual stock prices. The\\nhigher correlation coefficient in the single-layer LSTM model\\nalso suggested a stronger relationship between the expected\\nand actual values.\\nMuch of the paper is dedicated to an in-depth discussion of\\nLSTM networks, which are particularly well-suited for time\\nseries forecasting due to their ability to capture long-term\\ndependencies in sequential data. The authors explained the\\narchitecture and functioning of LSTM networks, highlighting\\ntheir advantages over traditional models in handling the com-\\nplexities of financial data. The authors [18] employed other\\nmachine learning algorithms, such as Random Forests, deep\\nneural networks, and logistic regression. By integrating these\\ndiverse models, they developed a comprehensive trading ap-\\nproach that leverages the strengths of each method. The feature\\nspace and target variables for both training and prediction\\nwere identified. This step involved selecting relevant financial\\nindicators and metrics that could serve as inputs to the model,\\nenabling it to learn patterns and trends influencing stock prices.\\nA unique aspect of this study is employing a CNN model,\\nwhere the authors utilized image data representing stock price\\ntrends. By converting time series data into images, the CNN\\nmodel could capture latent dynamics and patterns within the\\ndata that might not be easily discernible through traditional\\nnumerical analysis. This innovative approach allowed CNN to\\nextract features from the visual representation of stock price\\ntrends, providing a new dimension to financial data analysis.\\nThe study also incorporated a sliding window algorithm, a\\ntechnique used to predict short-term future values [19]. This\\nmethod uses a fixed-size window of past data to predict the\\nnext value in the sequence. The sliding window approach\\nis efficient for capturing short-term trends and making near-\\nfuture predictions, which are crucial for traders and investors\\nlooking for timely insights. The findings underscore the po-\\ntential of deep learning models, particularly when innovative\\nmethodologies such as the sliding window algorithm andimage-based analysis are integrated into the prediction process.\\nIII. P ROPOSED WORK\\nOur research suggests a thorough and complex method for\\nstock price prediction in Indian markets, taking the data from\\nthe news and combining several deep learning models, con-\\nventional time series analysis approaches, and natural language\\nprocessing methods. The combination of several deep learning\\nmodels, such as Prophet, Multistep LSTM, and SARIMA,\\nforms the foundation of our idea. These architectures ef-\\nfectively identify the complex relationships and nonlinear\\npatterns present in the Indian stock market data, improving\\nour predictions‚Äô precision and robustness.\\nTo improve our model‚Äôs accuracy even more, we provide\\na text segmentation module that uses the Hidden Markov\\nModel (HMM) to identify and extract the most noteworthy and\\ninformative sections from news stories about the Indian stock\\nmarket. By utilizing the Viterbi algorithm, we achieve accurate\\nsegmentation by concentrating only on crucial textual data that\\nsubstantially influences changes in stock prices. In addition,\\nwe incorporate a Bidirectional LSTM-based text emotion clas-\\nsification module to fully assess news item emotional context,\\nallowing for a more sophisticated comprehension of market\\nmood and its possible impact on stock prices. Our model uses\\nthe rich contextual data that Bidirectional LSTMs gather to\\nclassify news articles into several emotion classes ( positive,\\nnegative, and neutral), increasing our method‚Äôs potential for\\nprediction. We aim to empirically validate the effectiveness\\nand superiority of our proposed approach, highlighting its\\ncapacity to provide precise and valuable stock price predictions\\namidst the complexities and dynamics inherent in the Indian\\nmarket landscape through rigorous experimentation and care-\\nful evaluation of real-world data from the Indian stock market.\\nFig. 1. Block diagramIV. M ETHODOLOGY\\nA. Data Preprocessing\\nThe Dataset used in this study will take the parameters\\nOLHCV (Open High Low Close V olume) prices of the Stock.\\nAll the values have been taken from Yahoo Finance. We have\\nchosen 4 different banks, 2 public sector and 2 private banks.\\nThe period taken for this share price was from the last 20\\nyears.\\nWe were initially using Pandas datareader and quandl APIs\\nto get our data, but there were some issues with it while\\nparsing so we have used yfinance which will easily fetch us\\nthese banks‚Äô National Stock Exchange prices.\\nB. Multivariate Multistep LSTM MODEL\\nLong Short-Term Memory (LSTM) is an advanced variant\\nof the recurrent neural network (RNN) introduced by Hochre-\\niter and Schmidhuber. In general, Time-series forecasting. we\\nare predicting future dependent variables (y) based on the\\nprevious independent variables(x). In univariate forecasting, y\\nis predicted on that independent variable (x). In multivariate,\\nthere are many independent variables (x1,x2,x3,...xn), and here\\nwe employ multistep forecast, which means predicting a few\\ntimes-steps. So in our research, we employ four independent\\nvariables (x1,x2,x3,x4).\\nWe are rearranging our data in split sequences, with two\\narguments: how much data we have to look back for our\\nprediction and how much multistep data we want to forecast.\\n(Let us denote them by input and output). We are marking our\\ninput and output to be 8 and 9. Here, we look back at the last\\n30 (x1,x2,x3,x4) and forecast the future for 30 multi-steps\\nahead. So, in total, we have 72 batches of dependent variables\\nand independent variables. We are splitting them into 69 for\\ntraining and 3 for testing. The shapes of our training data and\\ntesting data should be (104820, 30, 4) and (16480, 30, 3).\\nThe author has to start our LSTM architecture by initializing\\nthe optimizer learning rate and number of layers. For this\\nresearch, we have decided to go with the Adam optimizer.\\nft=œÉ(Wf¬∑[ht‚àí1, xt] +bf)\\n-ft: The forget gate has an output entry, which determines\\nwhich portions of the cell state should be discarded.\\n-Wf: Weight matrix for the forget gate operation.\\n-[ht‚àí1, xt]: Combined vector of the prior hidden state and\\nthe current input.\\n-bf: Bias term associated with the forget gate.\\nit=œÉ(Wi¬∑[ht‚àí1, xt] +bi)\\n-it: The input gate has an output, which decides which new\\ninformation is incorporated into the cell state.\\n-Wi: Weight matrix for the input gate operation.\\n-[ht‚àí1, xt]: Combined vector of the prior hidden state and\\nthe current input.Fig. 2. LSTM Basic Architecture\\n-bi: Bias term associated with the input gate.\\nÀúCt= tanh( WC¬∑[ht‚àí1, xt] +bC)\\n-ÀúCt: Candidate value for the state cell, representing potential\\nupdates.\\n-WC: Weight matrix used for generating the candidate cell\\nstate.\\n-[ht‚àí1, xt]: Combined vector of the prior hidden state and\\nthe current input.\\n-bC: Bias term associated with the candidate cell state.\\nCt=ft‚äôCt‚àí1+it‚äôÀúCt\\n-Ct: Updated cell state, combining retained information from\\nthe previous state with new inputs.\\n-ft: forget gate‚Äôs output.\\n-‚äô: Element-wise multiplication operator.\\n-Ct‚àí1: the previous time step‚Äôs cell state.\\n-it: Output of the input gate.\\n-ÀúCt: Candidate cell state value.\\not=œÉ(Wo¬∑[ht‚àí1, xt] +bo)\\n-ot: The output gate‚Äôs output controls the information output\\nfrom the cell state.\\n-Wo: The output gate‚Äôs weight matrix.\\n-[ht‚àí1, xt]: Combined vector of the prior hidden state andthe current input.\\n-bo: Bias term for the output gate.\\nThe role of the forget gate tells how much data for the\\nprevious cell state should be preserved for the current state\\ncell. It picks the sigmoid activation function by default,\\nensuring the output is constrained to 0 and 1 [21]. The role of\\nthe input gate is to check what elements or data for the new\\ninput should be assigned for the cell state. Here, the activation\\nfunction is also sigmoid by default, where the output range is\\nbetween 0 and 1, which will signify the data that is coming\\nfor updating the cell state.The role of the candidate cell is to\\nintroduce all the new data that may or may not be added to\\nthe state cell. Here, we use a tanh activation function ranging\\nbetween -1 and 1. The role of the output gate is to finally\\nproduce the output for each time step, which becomes the\\nhidden state for the following time step. Here, the sigmoid\\nwill control all the output values from 0 to 1 and indicate the\\nportion of the state cell‚Äôs data available for the output.\\nC. Facebook Prophet\\nProphet has been a widely used forecasting tool in time\\nseries developed by the Meta AI team . It is designed to\\nhandle a lot of data and assist businesses in understanding\\nand predicting market trends. This open-source tool employs\\na decomposable additive model that accommodates non-linear\\ntrends, seasonality, and the impact of holidays. Before diving\\ninto practical applications, it‚Äôs essential to familiarize yourself\\nwith some key concepts:\\n‚Ä¢Trend : This is the main component of the Prophet\\narchitecture; this will represent the path of the underlying\\ninformation over a while, effectively filtering out short-\\nterm fluctuations and seasonal effects.\\n‚Ä¢Seasonality : Seasonality refers to repetitive variations\\nover shorter intervals. These variations are less persistent\\nthan trends and are characterized by regular patterns\\nwithin specific periods.\\ny(t) =g(t) +s(t) +h(t) +œµt\\ng(t) = (k+a(t)TŒ¥)t+ (m+a(t)TŒ≥)\\ng(t) =C\\n1 + exp( ‚àí(k+a(t)TŒ¥)(t‚àí(m+a(t)TŒ≥)))\\ns(t) =NX\\nk=1\\x14\\nakcos\\x122œÄkt\\nP\\x13\\n+bksin\\x122œÄkt\\nP\\x13\\x15\\n-N: Seasonal components.\\n-ak, bk: Fourier coefficients.\\n-P: Period.\\nh(t) =LX\\nj=1[Œ≤jDj(t)]-L: Number of holidays.\\n-Œ≤j: Holiday effects.\\n-Dj(t): Holiday indicators.\\nThe author also combines the prophet model with the Light-\\nGBM, an open-source boosting framework that Microsoft has\\ndeveloped. This is based on decision trees, which will improve\\nthe model‚Äôs efficiency and accuracy. In this method, we are\\nusing LightGBM as a wrapper integrated with the Optuna\\nLibrary, a hyperparameter optimization framework. Here, our\\noverall method is a stepwise approach where we optimize the\\nhyperparameters at a single time and employ a pre-defined\\nsearch space. This approach is much faster than Grid search,\\nwhere a combination of all parameters training takes place at\\nonce. In gridwisee the parameters will be h1*h2*h3*h4...hn\\nbut in our approach we‚Äôll have the total number of parameters\\nto be h1+h2+h3+...hn, where hi value represents values for\\nthat parameter, Overall this approach will reduce the number\\nof parameters and has increased our efficiency and accuracy.\\nD. SARIMA\\nThe author also employs a seasonal autoregressive inte-\\ngrated moving average (SARIMA), an extended version of\\nARIMA and a popular time series forecasting tool. It analyzes\\nthe relationship between a dependent variable and its past\\nvalues to predict future trends. Unlike methods that use actual\\nvalues, ARIMA models focus on the differences between\\nsuccessive observations to forecast future financial or market\\nmovements. SARIMA is denoted as ARIMA(a,b,c)(A, B, C)s.\\nHere, the hyperparameters are denoted as a: This parameter\\nindicates the Trend autoregression order b: This parameter\\nindicates the Trend difference in order c: This parameter\\nindicates the average order of the moving Trend\\nSome seasonal elements are generally not configured in\\nARIMA but must be configured for our model A: Order of\\nSeasonal autoregression B: Order of Seasonal difference C:\\nOrder of Seasonal moving average m: Time steps for a single\\nseasonal interval\\nE. Sentiment Recognition\\nIn addition to traditional time-series forecasting models,\\nsentiment recognition is crucial in enhancing the prediction\\naccuracy of Indian bank stock prices. The sentiment data were\\nmeticulously gathered from various sources, including promi-\\nnent news outlets like Business Standard andReuters , as well\\nas real-time Twitter tweets, ensuring comprehensive coverage\\nof market sentiment. The author used the Hidden Markov\\nModel (HMM) to analyze these sentiment data. The HMM\\nframework allowed us to model the latent sentiment states,\\ndenoted by the set L={l1, l2, . . . , l n}, which could not be\\nobserved directly but influenced the observable data sequences,\\nrepresented by ‚åà‚à´={ds1, ds 2, . . . , ds T}. The model param-\\neters included the transition probability matrix Ax, which\\ngoverns the probability of moving from one sentiment state\\nto another, and the observation probability matrix Bx, which\\ndefines the likelihood of observing a particular sentiment\\nindicator given a specific state. The vector œÄdenoted theinitial state distribution. We utilized the Viterbi algorithm to\\ndecode the most probable sequence of hidden sentiment states\\nthat could have generated the observed data. This algorithm\\nefficiently computes the most likely path of sentiment states,\\nL‚àó={l‚àó\\n1, l‚àó\\n2, . . . , l‚àó\\nT}, by maximizing the posterior probability\\nP(L|‚åà‚à´)over all possible state sequences. This path reveals\\nthe temporal evolution of sentiment in the market, which is\\na critical factor in influencing stock price movements. The\\nresulting sentiment scores, derived from the Viterbi-decoded\\nstate sequence, were then incorporated as exogenous variables\\ninto the stock price forecasting models. This integration allows\\nthe models to account for market sentiment, providing a\\nricher and more informed prediction of future stock prices.\\nBy combining sentiment analysis with multivariate time-series\\nmodels, we aim to capture both the quantitative and qualitative\\naspects of market dynamics, leading to more robust and\\naccurate forecasting outcomes.\\nV. RESULTS\\nA. Results using LSTM\\nThe study calculated results for the LSTM model using the\\nstock‚Äôs closing prices, focusing initially on a univariate model.\\nThis univariate approach relied solely on the closing prices as\\nthe input data, providing a straightforward yet effective method\\nfor predicting future stock prices. The simplicity of this model\\nmakes it easy to implement, though it may need to capture\\nthe broader market dynamics that influence price movements.\\nThe authors also considered a multivariate model to enhance\\nthe predictive power, where a multistep multivariate LSTM\\nwas employed to predict stock prices over five future time\\nsteps. By incorporating these extra features, the model was\\nable to capture more complex patterns and interactions within\\nthe stock market data. Including multiple variables in the\\nmultivariate model provided a more holistic view of the market\\nconditions. The opening price and stock volume, in particular,\\nare crucial indicators of market sentiment and trading activity,\\nand their inclusion allowed the LSTM model to understand\\nbetter and predict the factors driving stock price movements.\\nThis comprehensive approach aimed to improve the prediction\\naccuracy compared to the univariate model by leveraging the\\ninterdependencies among different market indicators.\\nFig. 3. : Split 1 Multistep LSTMFig. 4. : Split 2 Multistep LSTM\\nFig. 5. : Split 3 Multistep LSTM\\nB. Results using Facebook Prophet\\nIn the study, the same period used for the LSTM model\\nwas also considered for evaluating the performance of the\\nProphet model, a popular tool for time series forecasting. The\\nvisualization of the results included the following elements.\\nBlack Dots Represent the actual data points of the stock prices.\\nThese dots provide a reference for the exact values against\\nwhich the model‚Äôs predictions can be compared. The blue\\nline shows the central tendency of the projections made by\\nthe Prophet model over the selected time, providing a clear\\nindication of the predicted trend. Sky Blue Area Illustrates\\nthe variability in the model‚Äôs predictions, often called the\\nuncertainty interval or confidence interval.\\nC. Results using SARIMA Model\\nHere, the nonseasonal order is set as (1,1,1), and the\\nseasonal order is set as (3,3,1,15). One thing we have observed\\nwith SARIMA is that it can predict stock prices well, even\\nduring seasonal trends. During COVID-19, around April-\\nSeptember 2020, SARIMA came up with the best forecasting\\nas it quickly adjusted to the seasonal trend compared to other\\nmodels.\\nFig. 6. : Split 4 Multistep LSTM\\nFig. 7. : Split 5 Multistep LSTM\\nFig. 8. Prophet Prediction\\nD. Evaluation Metrics\\na) Root Mean Square Error (RMSE): Root Mean Square\\nError (RMSE) is a widely utilized metric for assessing the\\naccuracy of predictions. It quantifies the deviation between\\npredicted and actual observed values by calculating the Eu-\\nclidean distance between them. To determine RMSE, follow\\nthese steps: first, compute the residuals for each data point,\\nwhich is the difference between the predicted value and the\\nactual value. Next, calculate the square of each residual, then\\nfind the average of these squared residuals. Finally, take the\\nsquare root of this average. RMSE is particularly useful in\\nsupervised learning contexts as it requires valid values for each\\nFig. 9. Prophet Predictionprediction.\\nThe following table presents the RMSE values for four stock\\nprices sourced from the National Stock Exchange, evaluated\\nusing various models.\\nTABLE I\\nMODEL PERFORMANCE ON DIFFERENT STOCKS\\nMODEL Stock 1 Stock 2 Stock 3 Stock 4\\nUnivariate LSTM 4.89 3.14 8.12 2.34\\nMultivariate Multi-step\\nLSTM (Global Average)3.91 2.990 5.96 1.98\\nSARIMA 11.28 10.281 14.37 9.87\\nFacebook Prophet +\\nLGBM + Optuna6.47 7.252 6.98 5.90\\nCONCLUSION\\nIn conclusion, the study provides valuable insights into\\nthe effectiveness of various models for stock price fore-\\ncasting, highlighting each approach‚Äôs relative strengths and\\nweaknesses. The attention-based Multistep LSTM, which in-\\ntegrates news sources and Twitter data, emerged as the top\\nperformer, demonstrating superior accuracy compared to other\\nmodels. This suggests incorporating real-time sentiment and\\ninformation from diverse sources can significantly enhance\\npredictive performance. However, it is also noteworthy that\\nthe Multivariate LSTM model, which considers multiple finan-\\ncial indicators, performed consistently well across all cases,\\nmaking it a reliable alternative when real-time data sources\\nare unavailable. The findings indicate that each model has\\nits strengths. They also emphasize the importance of contin-\\nuous improvement and refinement in stock price forecasting\\nmethodologies. The results contribute to a broader understand-\\ning of how different models perform in real-world scenarios,\\noffering valuable guidance for future research and practical\\napplications in financial forecasting.\\nREFERENCES\\n[1] Yinghao Rena, Fangqing Liaoa, andYongjing Gong, Impact of News on\\nthe Trend of Stock Price Change: an Analysis based on the Deep Bidirec-\\ntional LSTM Model,‚Äù 2019 International Conference on Identification,\\nInformation and Knowledge in the Internet of Things (IIKI2019).\\n[2] Hum Nath Bhandari, Binod Rimal Nawa Raj Pokhrel , Predicting stock\\nmarket index using LSTM,V olume 9, 15 September 2022, 100320.\\n[3] Tripathi, Mukesh Kumar, Dhananjay Maktedar, D. N. Vasundhara,\\nCH VKNSN Moorthy, and Preeti Patil. ‚ÄùResidual Life Assessment\\n(RLA) Analysis of Apple Disease Based on Multimodal Deep Learning\\nModel.‚Äù International Journal of Intelligent Systems and Applications in\\nEngineering 11, no. 3 (2023): 1042-1050.\\n[4] Sumedh Kaninde, Manish Mahajan and Aditya Janghale ‚ÄúStock Price\\nPrediction using Facebook Prophet,‚Äù January 2022 ITM Web of Con-\\nferences 44(4):03060.\\n[5] Taewook Kim and Ha Young Kim ‚ÄúForecasting stock prices with a\\nfeature fusion LSTM-CNN model using different representations of the\\nsame data,‚Äù Published: February 15, 2019.\\n[6] R Vinayakumar, E. A Gopalakrishnan and Vijay Krishna Menon, ‚ÄúStock\\nprice prediction, using LSTM, RNN, and CNN-sliding window model‚Äù\\nPublished in 2017 International Conference on Advances in Computing,\\nCommunications and Informatics (ICACCI).\\n[7] Tripathi, Mukesh Kumar, M. Neelakantapp, Anant Nagesh Kaulage,\\nKhan Vajid Nabilal, Sahebrao N. Patil, and Kalyan Devappa Bamane.\\n‚ÄùBreast Cancer Image Analysis and Classification Framework by Apply-\\ning Machine Learning Techniques.‚Äù International Journal of Intelligent\\nSystems and Applications in Engineering 11, no. 3 (2023): 930-941.[8] Mehar Vijh, Deeksha Chandola, Vinay Anand Tikkiwal Stock Closing\\nPrice Prediction using Machine Learning Techniques , Published in :\\nICCIDS 2019.\\n[9] Masoud Najeb MH The impact of stock market performance upon\\neconomic growth. International Journal of Economics and Financial\\nIssues, 3 (4) (2017), pp. 788-798.\\n[10] Tripathi, Mukesh Kumar, Praveen Kumar Reddy, and Madugundu Nee-\\nlakantappa. ‚ÄùIdentification of mango variety using near-infrared spec-\\ntroscopy.‚Äù Indonesian Journal of Electrical Engineering and Computer\\nScience 31, no. 3 (2023): 1776-1783.\\n[11] Hur Jung, Manoj Raj, Yohanes E. Riyanto Finance and trade: A cross-\\ncountry empirical analysis on the impact of financial development and\\nasset tangibility on international trade. World Development, 34 (10)\\n(2006), pp. 1728-1741.\\n[12] Li, Lei, Yabin Wu, Yihang Ou, Qi Li, Yanquan Zhou, and Daoxin\\nChen. (2017) ‚ÄúResearch on machine learning algorithms and feature\\nextraction for time series.‚Äù IEEE 28th Annual International Symposium\\non Personal, Indoor, and Mobile Radio Communications (PIMRC): 1-5.\\n[13] Liu, Wen-Jie, Ye-Bo Ge, and Yu-Chen Gu. ‚ÄùNews-driven stock mar-\\nket index prediction based on trellis network and sentiment attention\\nmechanism.‚Äù Expert Systems with Applications 250 (2024): 123966.\\n[14] Tripathi, Mukesh Kumar, and Shivendra. ‚ÄùImproved deep belief network\\nfor estimating mango quality indices and grading: A computer vision-\\nbased Neutroscophic approach.‚Äù Network: Computation in Neural Sys-\\ntems (2024): 1-29.\\n[15] Roy, Ranjan Kumar, Koyel Ghosh, and Apurbalal Senapati. ‚ÄùStock\\nPrice Prediction: LSTM Based Model.‚Äù In Proceedings of Intelligent\\nComputing and Technologies Conference. 2021.\\n[16] Zunic, Emir , Korjeni ¬¥c, Kemal,Hod Àázi¬¥c, Kerim ,Donko,\\nDzenana.‚ÄùApplication of Facebook‚Äôs ProphetAlgorithm for Successful\\nSales Forecasting Based onReal-world Data‚Äù in Info Studio d.o.o.\\nSarajevo, Bosniaand Herzegovina.V ol 12,May 2020.\\n[17] Karim Elbahloul.‚ÄùStock Market Predictions UsingVarious Statistical\\nMethods V olume II‚Äù, August2019,DOI:10.13140/RG.2.2.19926.40005.\\n[18] Jiake Li.‚ÄùResearch on Market Stock Index PredictionBased on Net-\\nwork Security and Deep Learning‚Äù inSecurity and Communication\\nNetworks.vol:2021,April2021.\\n[19] Tripathi, Mukesh Kumar, and Shivendra. ‚ÄùImproved deep belief network\\nfor estimating mango quality indices and grading: A computer vision-\\nbased Neutroscophic approach.‚Äù Network: Computation in Neural Sys-\\ntems (2024): 1-29.\\n[20] Subba Rao Polamuri,Kudipudi Srinivas,A. Krishna Mohan.‚ÄùA Survey\\non Stock Market Prediction Using Machine Learning Techniques‚Äù in\\nInternational Conference on Data Science, Machine Learning and Ap-\\nplications, Springer Link. pp.923-931,May2020,DOI:10.1007/978-981-\\n15-1420-3-101.\\n[21] Zolfagharinia, Hossein, Mehdi Najafi, Shamir Rizvi, and Aida Haghighi.\\n‚ÄùUnleashing the Power of Tweets and News in Stock-Price Prediction\\nUsing Machine-Learning Techniques.‚Äù Algorithms 17, no. 6 (2024): 234.',\n",
       " 'Characteristics of Political Misinformation Over the\\nPast Decade\\nErik J. Schlicht\\nMisinformation-Monitor\\nmisinfo-monitor.org\\nerik@misinfo-monitor.org\\nAbstract\\nAlthough misinformation tends to spread online, it can have serious real-world\\nconsequences. In order to develop automated tools to detect and mitigate the\\nimpact of misinformation, researchers must leverage algorithms that can adapt\\nto the modality (text, images and video), the source, and the content of the false\\ninformation. However, these characteristics tend to change dynamically across\\ntime, making it challenging to develop robust algorithms to fight misinformation\\nspread. Therefore, this paper uses natural language processing to find common\\ncharacteristics of political misinformation over a twelve year period. The results\\nshow that misinformation has increased dramatically in recent years and that it has\\nincreasingly started to be shared from sources with primary information modalities\\nof text and images (e.g., Facebook and Instagram), although video sharing sources\\ncontaining misinformation are starting to increase (e.g., TikTok). Moreover, it\\nwas discovered that statements expressing misinformation contain more negative\\nsentiment than accurate information. However, the sentiment associated with\\nboth accurate and inaccurate information has trended downward, indicating a\\ngenerally more negative tone in political statements across time. Finally, recurring\\nmisinformation categories were uncovered that occur over multiple years, which\\nmay imply that people tend to share inaccurate statements around information they\\nfear or don‚Äôt understand (Science and Medicine, Crime, Religion), impacts them\\ndirectly (Policy, Election Integrity, Economic) or Public Figures who are salient in\\ntheir daily lives. Together, it is hoped that these insights will assist researchers in\\ndeveloping algorithms that are temporally invariant and capable of detecting and\\nmitigating misinformation across time.\\n1 Introduction\\nMisinformation is a statement that contains false or misleading information, and can result in serious\\nconsequences, including the erosion of civil discourse, political paralysis, uncertainty, in addition to\\nalienation and disengagement (Kavanagh & Rich, 2018; Hook & Verdeja, 2022). Despite its serious\\nimpact on individuals and society, misinformation is known to be shared more than valid information\\n(V osoughi, et al, 2018), and the reasons that misinformation is propagated are diverse and include\\ncognitive factors (Del Vicario, et al , 2016; Ecker, et al, 2022), socio-affective factors (Ecker, et al,\\n2022), incentives (Ceylan, et al, 2023) and changes in the information system (Kavanagh & Rich,\\n2018, Chen et al, 2023).\\nAs a result, finding scalable solutions to detect and mitigate the impact of misinformation has proven\\nchallenging, although many efforts have demonstrated some promise (Conroy, et al., 2015; ,Aldwairi\\n& Aldwairi, 2018, RAND, 2018). Part of the challenge facing researchers is that misinformation\\ncan be propagated through many modalities (e.g., text, image and video), thereby increasing the\\nBEA Research Symposium: The Impact of Disinformation and Misinformation on a Democratic Society (Las\\nVegas, NV , April, 2024).arXiv:2411.06122v1  [cs.SI]  9 Nov 2024algorithmic complexity and computational resources necessary to deploy scalable solutions. Moreover,\\nthe sources from which misinformation is spread can effortlessly adapt to any mitigation attempts\\n(e.g., account removal), which can quickly become a real-life version of the Whac-A-Mole game.\\nIn order for robust and effective solutions to be deployed, first we must understand the features of\\nmisinformation that are temporally invariant, since the content, sources and modalities associated\\nwith misinformation are likely to change across time. Although some researchers have explored\\ntemporal patterns of misinformation on social media (Allcott, et al, 2019), they focused on relatively\\nnarrow timelines that were less than five years in range. Therefore, this effort explores the trends\\nassociated with political misinformation over a twelve year period, leveraging tools from natural\\nlanguage processing to uncover common themes in misinformation across time. By uncovering these\\ninsights, it may allow researchers to develop more robust tools to detect and mitigate misinformation,\\nand the next section will detail the data used to this end.\\n2 PolitiFact Data\\nIn order to investigate trends in political misinformation across time, twelve years of fact-checked\\nPolitiFact data was obtained, ranging between 2011 through 2023. PolitiFact is owned by the\\nnonprofit Poyneter Institute for Media with the objective of improving the relevance, ethical practice\\nand value of journalism. As part of that objective, PolitiFact verifies the accuracy of claims made\\nthroughout various online and media sources. By using this fact-checked information, it offers a\\nsource of ground-truth for information accuracy with relatively high confidence.\\nApproximately sixteen thousand statements were collected across twelve years, and further classified\\ninto three categories:\\n‚Ä¢Accurate: this information class was given to fact-checked ratings with TRUTH-O-METER\\nscores of TRUE or MOSTLY-TRUE.\\n‚Ä¢Misinformation: this information class was given to fact-checked ratings with TRUTH-O-\\nMETER scores of PANTS-ON-FIRE or FALSE.\\n‚Ä¢Mixed-Accuracy: this information class was given to fact-checked ratings with all other\\nTRUTH-O-METER ratings not contained in the classes above.\\nThis statement classification resulted in the following distribution of information accuracy in the\\nsample:\\nTable 1: Information Classes Across Data\\nPolitiFact Sample\\nClass Count\\nAccurate Statements 3,353\\nMisinformation 7,720\\nMixed-Accuracy Statements 4,921\\nTotal 15,994\\nUsing this data and classification system, the following section will show the trends in misinformation\\nacross time.\\n3 Trends in Information Accuracy\\nFigure 1 shows how political information accuracy has changed across time. It is apparent that\\npolitical misinformation has increased over time, starting around 2017 (red dashed line), when it first\\nbecame more frequent than accurate or mixed-accuracy information. It is interesting to note that this\\nis eleven years after the public launch of Facebook and Twitter (2006), and according to Oberlo.com,\\nthis was a time when Facebook had approximately two billion users.\\nThe approximately ten year window that it took for misinformation to surface on PolitFact following\\nthe launch of major social media sites can be due to a number of reasons including the time it takes\\n22012 2014 2016 2018 2020 2022020040060080010001200\\nInfo Type Accurate Misinfo MixedPolitical Information Type Over Time\\nYearTotal Politifact StatementsFigure 1: Trends in PolitiFact statement accuracy across time.\\nfor misinformation to be noticed by journalists and fact-checkers on these platforms, or even a lack\\nof partnerships that enabled PolitiFact to obtain misinformation data at scale.\\nRegardless, it is apparent that the increase in misinformation on PolitiFact is significant, and the next\\nsections will focus on misinformation specifically and explore the primary source modality from\\nwhich it was shared across time.\\n4 Misinformation Source Modality Trends\\nSince the modality in which misinformation is conveyed (text, images or video) impacts the algo-\\nrithmic complexity and computational resources necessary to deploy solutions, it is important to\\nunderstand trends across time. Figure 2 shows the primary information modality associated with\\nmisinformation sources that were responsible for at least five PollitiFact statements for a given\\nyear. For example, Facebook and Twitter (now Meta and X) both have a primary modality of text,\\nsince most of the posts shared on these sites are written in text, whereas Instagram and TikTok\\nwould be examples of sources with a primary modality of image and video, respectively. Finally,\\nmisinformation statements associated with sources that are entities are labeled with a source modality\\nof individual.\\nIt is apparent that misinformation from sources that are primarily in text form increased around\\n2017, eleven years after the launch of Facebook and Twitter, whereas misinformation from sources\\nassociated with images started to increase around 2019, only nine years after the public launch of\\nInstagram. Moreover, misinformation from sources with a primary modality of video started to\\nappear in 2020. If misinformation associated with videos follow similar trends as text and images, it\\nwould predict an uptick in this modality in the next few years, as TikTok was launched internationally\\nin 2017.\\nAs a result of the multimodal nature of contemporary misinformation, researchers should no longer\\nrely exclusively on algorithms associated with text (natural language processing) or images/video\\n(computer vision) alone, and transition to multimodality versions available models. Alternatively,\\nthey may choose to deploy independent models to detect and mitigate for each modality, but this may\\nrequire additional complexity.\\nRegardless of the solution, these results demonstrate that political misinformation now spans the\\nmodality spectrum (Figure 2) and that misinformation has been trending upward in recent years\\n(Figure 1). Next, tools from natural language processing will be used to explore the sentiment\\nassociated with both accurate and inaccurate information across time.\\n32012 2014 2016 2018 2020 20220100200300400500600\\nSource Modality Indivudal Text Image VideoPrimary Modality of Political Misinformation Source Over Time\\nYearTotal Politifact StatementsFigure 2: Primary source modality of misinformation across time.\\n5 Information Sentiment Trends\\nPrevious research found that misinformation relies on emotional content, such as appealing to morality\\nand statements with negative sentiment (Carrasco-Farre, 2022). Therefore, this section explores the\\nsentiment associated with political statements sampled in this study by leveraging the compound\\nscore produced by V ADER (Hutto & Gilbert, 2014). This score ranges between -1 and +1, with scores\\nless than 0 corresponding to a statement with negative sentiment, scores greater than 0 corresponding\\nto statements with positive sentiment, and scores around 0 corresponding to neutral statements.\\nAccurate Misinformation‚àí0.1‚àí0.0500.050.1\\nInformation Type Accurate MisinformationAverage Compound Sentiment Across Information Type\\nInformation TypeMean Compound Sentiment Score (VADER)\\nFigure 3: Average statement sentiment for each information type (error bars correspond to ¬±1 SEM).\\nBy averaging the compound sentiment scores across years (Figure 3), similar trends were realized\\nwhere misinformation was associated with more negative sentiment (Mean = -.08, SEM = .004) than\\naccurate statements (Mean = -.03, SEM = .007). A Mann-Whitney U-test found these differences to\\nbe significant (p<.001). This supports the notion that misinformation relies on emotional content to\\npropagate at higher rates than valid information (e.g., clickbait).\\n4Table 2: Examples of Statement Sentiment\\nPolitiFact Statement Sentiment\\nStatement V ADER Compound Score\\nSays Newt Gingrich aligned with Nancy Pelosi on global warming. 0.1531\\nIn New York City, \"an entry level janitor gets paid twice as much as an entry level teacher.\" 0\\nSays it cost Massachusetts taxpayers $100,000 when Mitt Romney and his staff purchased computer hard drives. -0.1027\\nIn oder to investigate trends in misinformation across time, average sentiment was calculated sep-\\narately within each year and information type (Figure 4). The results show a slight negative trend\\nin PolitiFact statements over time for both accurate and inaccurate statements. This is somewhat\\nsurprising, since the objective of accurate information is to relay facts rather than propagate sensa-\\ntional claims. Therefore, this may suggest that accurate information has adopted similar hyperbolic\\nreporting tactics over time as misinformation in order to compete for the attention of readers.\\nDespite the fact that the results show significant differences in sentiment between information type,\\nthey are very small and do not correspond to large perceptible differences in statements (Table 2).\\nTherefore, although these trends are interesting, they do not correspond to major tendencies towards\\nnegative dialog over years.\\n2010 2012 2014 2016 2018 2020 2022 2024‚àí0.15‚àí0.1‚àí0.0500.050.10.15\\nInfo Type Accurate\\n Misinfo\\n Mixed\\nAverage Statement Sentiment Over Time\\nYearMean Compound Sentiment (VADER)\\nFigure 4: Average statement sentiment for each year (marker size proportional to SEM).\\n6 Misinformation Topics Across Years\\nAnother line of investigation explored in this effort was to use a new technique in natural language\\nprocessing (BERTopic) to discover topics associated with PolitiFact statements (Grootendorst, 2022).\\nThe model uses document embeddings to cluster into coherent topics in order to uncover themes in\\ntext. Misinformation statements from each year were input into the algorithm, allowing it to converge\\non the best number and type of clusters for each year independently (Table 3).\\nSubjective descriptions were provided for each cluster/topic to better understand common topics\\nacross years. Table 3 shows each of these topic descriptions with the number of misinformation\\nstatements that are associated with each topic. The number of topics BERTopic converged on\\nincreased across time, suggesting that not only is there more misinformation in recent years (Figure\\n1), but that it spans a greater number of topics (Table 3). Note that the topic description corresponds to\\nthe primary theme or entity associated with each cluster. In the case of entities, it does not discriminate\\nbetween those where the entity was the target of misinformation or if they were responsible for stating\\nthe misinformation.\\n5Table 3: Misinformation Topics Across Years.\\nPolitiFact Statement Topics\\nYear Topic Description (Number of Statements in Topic) Number\\n2023 Joe Biden (359), Space (79), COVID (72), Spanish Content (62), Elon Musk (33), Walmart Refund (32) 11\\nState Politics (31), Wildfires (30), Medicine (15), Celebrities (15), Mass Shootings (14)\\n2022 COVID (160), Ukraine War (152), Election Fraud (118), Universal Healthcare (84), Joe Biden (61) 21\\nCelebrities (57), Mass Shootings (43), Nancy Pelosi (36), Gas Prices (26), Elon Musk (25), Hillary Clinton (23)\\nQueen Elizabeth (31), Space (19), Adolf Hitler (18), Monkeypox (16), Climate Change (15),\\nAlkaline Foods (14), Defund Police (14), Electric Vehicles (12), Baby Formula (11), Corporate Conspiracy (10)\\n2021 COVID (359), Election Fraud (94), Joe Biden (58), Hillary Clinton (55), Capital Riot (54) 17\\nTaxes (43), Meghan Markle (23), Afghanistan (20), Gas Prices (20), Climate Change (20), Immigration (19),\\nNancy Pelosi (17), Facebook (14), George Floyd (13), Mass Shootings (13), Electric Vehicles (13), Racism (10)\\n2020 Joe Biden (417), COVID (300), BLM (89), PPE Masks (29), Mass Shootings (26), Barack Obama (20) 9\\nIranian Rockets (16), Kobe Bryant (15), Gretchen Whitmer (10)\\n2019 Donald Trump (96), Political Quotes (94), State Politics (58), Joe Biden (28), Border Wall (27) 11\\nImmigration (21), Vaccines (20), Alexandria Ocasio-Cortez (18), Abortion (18), Democratic Policy (12), Muslims (12)\\n2018 Democratic Policy (118), Barack Obama (65), Donald Trump (25), Second Amendment (18) 7\\nImmigration (18), Mass Shootings (14), Airline (11)\\n2017 Barack Obama (110), Universal Healthcare (27), Republican Policy (27), State Politics (24), Muslims (22) 11\\nSpace (21), Mass Shootings (20), Hurricanes (16), Celebrities (15), Donald Trump (13), Abortion (12)\\n2016 Hillary Clinton (111), Taxes (88), Barack Obama (55), Abortion (13) 4\\n2015 Democratic Politicians (224), Abortion (13) 2\\n2014 Universal Healthcare (49), Barack Obama (48), Taxes (29), Scott Walker (24), Economic Disparity (18) 6\\nMass Shootings (11)\\n2013 Taxes (36), State Politics (48), Scott Walker (11) 3\\n2012 Universal Healthcare (35), Scott Walker (31), Barack Obama (17), Taxes (16), State Politics (13) 6\\nMitt Romney (13)\\n2011 Barack Obama (20), State Politics (13) 2\\nExploring topics within each year, it is apparent that some topics are heavily aligned with current\\nevents and themes that are temporally localized (e.g., Capital Riot). However, there are some topics\\nthat recur across years and the next section will explore those topics further.\\n7 Recurring Misinformation Topics\\nIn order to facilitate insights into topics that occur across multiple years, Figure 5 depicts recurring\\ntopics and the number of statements that correspond to each recurring topic. The number of years\\ntopics recur ranges between two and seven years, with Mass Shootings and Barack Obama occurring\\nmost frequently across years. However, when focusing on the total number of statements that\\ncorrespond to each recurring topic, Joe Biden and COVID are the topics that correspond to the\\ngreatest number of misinformation statements.\\nMass ShootingsBarack ObamaState PoliticsJoe BidenTaxesCOVIDAbortionUniversal HealthcareNancy PelosiCelebritiesSpaceImmigrationHillary ClintonScott WalkerElection FraudElectric VehiclesDonald TrumpElon MuskDemocratic PolicyMuslimsClimate ChangeGas Prices 0246\\n200400600800Total PostsRecurring Misinformation Topics\\nTopic DescriptionNumber of Years Topic Appears\\nFigure 5: Recurring misinformation topics.\\n6Table 4: Examples of Statements for Recurring Misinformation Topic Categories,\\nRecurring Misinformation Topic Categories\\nCategory Topic Description Example Statement\\nPublic Figures Joe Biden, Joe Biden just resigned from the White House.\\nBarack Obama Says President Barack Obama is a socialist.\\nHillary Clinton Navy SEALs arrested Hillary Clinton.\\nNancy Pelosi Says Nancy Pelosi has been executed.\\nScott Walker Says Gov. Scott Walker dropped or was kicked out of college short of a degree.\\nElon Musk Elon Musk is dead.\\nCelebrities U.S. comedian Kevin Hart in critical condition after gory car crash.\\nDonald Trump Says Shaquille O‚ÄôNeal said Donald Trump is possibly the best president.\\nScience and Medicine COVID COVID-19 vaccines have killed 676,000 Americans.\\nSpace NASA faked footage of astronauts on the International Space Station.\\nAbortion Almost 95 % of all (Planned Parenthood) pregnancy services were abortions.\\nClimate Change Says Greta Thunberg said, the climate crisis doesn‚Äôt exist.\\nElectric Vehicles When electric cars get in accidents, they explode, they catch fire very very badly\\nbecause of the lithium batteries\\nPolicy Taxes The yearly cost of religious tax exemptions: $71 billion. If the church paid\\ntaxes, everyone would only have to pay 3% taxes.\\nUniversal Healthcare President Obama‚Äôs health care law is a government takeover of healthcare.\\nState Politics Campaign finance disclosures showed Fulton County District Attorney Fani\\nWillis took part in a massive money laundering and election fraud scheme.\\nDemocratic Policy V oting for any Democrat gets you socialism, undefended open borders, immedi-\\nate tax increases, 100% government-run health care.\\nImmigration 2,000 illegal aliens were arrested by ICE in 2017 for murders committed here\\nin the United States!\\nElection Integrity Election Fraud Fluctuating Georgia U.S. Senate runoff vote count is evidence of election fraud.\\nCrime Mass Shootings The mass shooting in Highland Park, Illinois, was a false flag.\\nEconomic Gas Prices In April 1997, there was a gas out conducted nationwide in protest of gas prices.\\nGasoline prices dropped 30 cents a gallon overnight.\\nReligion Muslims Says Islamic studies professor Tariq Ramadan said Muslims are here to colonize\\nthe U.S. and Canada and spread Sharia law and won‚Äôt hesitate to use violent\\nJihad if they have to.\\nTaking this aggregation a level higher, subjective categories were provided across topics in an attempt\\nto understand the high-level categories of recurring misinformation topics (Table 4). The categories\\nspan seven separate areas with Public Figures, Science and Medicine, and Policy containing the most\\nrecurring topics, whereas other topic categories only contain one recurring topic within each.\\n8 Conclusions\\nOverall, these results suggest that the advent of social media has allowed misinformation to propagate\\nmore rapidly, spanning a greater number of topics and modalities (Figures 1 & 2, Table 3). Moreover,\\nthe sentiment results (Figure 3) support the idea that misinformation appeals to people‚Äôs emotions\\nthrough negative claims (Grootendorst, 2022) that are often shared at higher rates than accurate\\ninformation (V osoughi, et al, 2018). What‚Äôs more, the sentiment trends across time show that accurate\\ninformation has also adopted a slightly more negative tone, which may imply that valid information\\nis starting to be portrayed in an emotional context to complete for the attention of readers (Figure 4).\\nThis paper also uncovered several recurring topics of misinformation (Figure 5) spanning seven\\ncategories (Table 4), which provides insight into temporally invariant themes of misinformation.\\nTaken in the context that misinformation is emotionally charged, these categories could suggest that\\npeople tend to share inaccurate statements around information they fear or don‚Äôt understand (Science\\nand Medicine, Crime, Religion), impacts them directly (Policy, Election Integrity, Economic) or\\nPublic Figures who are salient in their daily lives.\\nThe impact of misinformation on our democracy has been well studied and includes the erosion of\\ncivil discourse, political paralysis, uncertainty, in addition to alienation and disengagement (Kavanagh\\n& Rich, 2018; Hook & Verdeja, 2022). A necessary first condition to mitigating these outcomes\\nis to reduce the availability of misinformation. Since social media provides the opportunity for\\nmisinformation to be spread rapidly, automated methods for detecting and removing inaccurate\\nmisinformation is one approach to accomplish this objective.\\nAs a result, it is hoped that these insights will help researchers and engineers to develop robust\\nalgorithms to detect and mitigate misinformation. Whatever the solution, these algorithms will need\\n7to span multiple modalities and capture that major topic categories that correspond to misinformation\\nthat were uncovered in this investigation.\\nReferences\\nAldwairi, M. and Aldwairi, A. (2018). Detecting fake news in social media networks. Procedia Computer\\nScience , 141, 215-222.\\nAllcott, H., Gentzkow, M. and Yu, C. (2019). Trends in the diffusion of misinformation on social media.\\nResearch and Politics , 10, 1-8.\\nCarrasco-Farr√©, C. The fingerprints of misinformation: how deceptive content differs from reliable sources in\\nterms of cognitive effort and appeal to emotions. Humanities and Social Sciences Communications , 162 (2022).\\nhttps://doi.org/10.1057/s41599-022-01174-9.\\nCeylan, G., Anderson, I. and Wood, W. (2023). Sharing of misinformation is habitual, not just lazy or biased/\\nPNAS , 120, https://doi.org/10.1073/pnas.2216614120.\\nChen, S., Lu, X., and Akit, K (2023). Spread of misinformation on social media: What contributes to it and how\\nto combat it. Computers in Human Behavior , 141, https://doi.org/10.1016/j.chb.2022.107643\\nConroy, N.J., Rubin, V ., and Chen, Y . (2015). Automatic deception detection: methods for finding fake news.\\nASIST 2015: Proceedings of the 78th ASIST Annual Meeting: Information Science with Impact: Research in and\\nfor the Community , 82, 1-4.\\nDel Vicario, et al. (2016). The spreading of misinformation online. PNAS , 113, https://doi.org/10.1073/\\npnas.1517441113 .\\nEcker, U.K.H., et al. (2022). The psychological drivers of misinformation belief and its resistance to correction.\\nNature Reviews Psychology , 1, 13-29.\\nGrootendorst, M. (2022). BERTopic: Neural topic modeling with a class-based TF-IDF procedure. arXiv:\\nComputation and Language ,https://doi.org/10.48550/arXiv.2203.05794\\nHook, K., Verdeja, E. (2022). Social media misinformation and the prevention of political instability and mass\\natrocities. Stimson: Human Security & Governance .\\nHutto, C., and Gilbert, E. (2014). V ADER: A Parsimonious Rule-Based Model for Sentiment Analysis of Social\\nMedia Text. Proceedings of the International AAAI Conference on Web and Social Media , 8(1), 216-225.\\nKavanagh, J., and Rich, M.D. (2018). Truth Decay: An initial exploration of the diminishing role of facts and\\nanalysis in American public life. RAND Corporation . ISBN: 978-0-8330-9994-5.\\nRAND Corporation Website (n.d.). Tools that fight disinformation online. https://www.rand.org/\\nresearch/projects/truth-decay/fighting-disinformation/search.html\\nV osoughi, S., Roy, D., and Aral, S. (2018). The spread of true and false news online. Science , 359, 1146-1151.\\n8',\n",
       " 'HateGPT: Unleashing GPT-3.5 Turbo to Combat Hate\\nSpeech on X\\nAniket Deroy1,*,‚Ä†, Subhankar Maity1\\n1IIT Kharagpur, Kharagpur, India\\nAbstract\\nThe widespread use of social media platforms like Twitter and Facebook has enabled people of all ages\\nto share their thoughts and experiences, leading to an immense accumulation of user-generated content.\\nHowever, alongside the benefits, these platforms also face the challenge of managing hate speech and\\noffensive content, which can undermine rational discourse and threaten democratic values. As a result,\\nthere is a growing need for automated methods to detect and mitigate such content, especially given the\\ncomplexity of conversations that may require contextual analysis across multiple languages, including\\ncode-mixed languages like Hinglish, German-English, and Bangla. We participated in the English task\\nwhere we have to classify English tweets into two categories namely Hate and Offensive and Non\\nHate-Offensive. In this work, we experiment with state-of-the-art large language models like GPT-3.5\\nTurbo via prompting to classify tweets into Hate and Offensive or Non Hate-Offensive. In this study, we\\nevaluate the performance of a classification model using Macro-F1 scores across three distinct runs. The\\nMacro-F1 score, which balances precision and recall across all classes, is used as the primary metric for\\nmodel evaluation. The scores obtained are 0.756 for run 1, 0.751 for run 2, and 0.754 for run 3, indicating\\na high level of performance with minimal variance among the runs. The results suggest that the model\\nconsistently performs well in terms of precision and recall, with run 1 showing the highest performance.\\nThese findings highlight the robustness and reliability of the model across different runs.\\nKeywords\\nGPT, Hate Speech, Classification, English, Prompt Engineering\\n1. Introduction\\nThe advent of social media platforms such as Twitter (currently known as X) and Facebook has\\nrevolutionized the way individuals communicate, enabling people from diverse backgrounds\\nto share their thoughts, experiences, and opinions freely [ 1]. This democratization of content\\ncreation has led to an exponential increase in user-generated data. While these platforms have\\nfacilitated global connectivity and discourse, they have also become hotbeds for hate speech [ 2,\\n3,4,5,6] and offensive content. Such content not only disrupts meaningful communication but\\nalso poses significant threats to social cohesion and democratic values.\\nAddressing the proliferation of hate speech [ 7,8,9,10] on social media is a complex challenge.\\nThe nature of online communication, where context and nuance often play a crucial role,\\nmakes it difficult to detect offensive language accurately [ 11,12,13]. This challenge is further\\nForum for Information Retrieval Evaluation, December 12-15, 2024, India\\n*Corresponding author.\\n/envel‚å¢pe-‚å¢penroydanik18@kgpian.iitkgp.ac.in (A. Deroy); subhankar.ai@kgpian.iitkgp.ac.in (S. Maity)\\n/orcid0000-0001-7190-5040 (A. Deroy); 0009-0001-1358-9534 (S. Maity)\\n¬©2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).\\nCEUR\\nWorkshop\\nProceedingshttp://ceur-ws.org\\nISSN 1613-0073\\nCEUR Workshop Proceedings (CEUR-WS.org)arXiv:2411.09214v1  [cs.CL]  14 Nov 2024compounded by the multilingual nature of online communities, where users frequently employ\\ncode-mixed languages such as Hinglish (a mix of Hindi and English), German-English, and\\nBangla, among others [ 14,15]. As these languages blend cultural and linguistic elements, the\\ntask of identifying hate speech becomes even more intricate.\\nIn response to this growing concern, technology companies and social media platforms have\\nbegun to invest in automated methods to detect and manage offensive content [ 2,16,17,18].\\nThe goal is to strike a balance between preserving open and free dialogue while preventing\\nthe spread of harmful speech. In this work, we focus on the classification of English tweets\\ninto two categories: Hate and Offensive and Non Hate-Offensive. By leveraging state-of-the-art\\nlarge language models such as GPT-3.5 Turbo, we experiment with prompting techniques to\\nclassify tweets accurately.\\nRun 1 achieved the highest Macro-F1 score at 0.756, indicating it balanced precision and recall\\nacross different classes better than the other runs. Run 2 had a slightly lower score of 0.751,\\nsuggesting a small decline in performance, either in precision, recall, or both. Run 3 scored\\n0.754, which was slightly lower than Run 1 but higher than Run 2, indicating its performance\\nwas similar to Run 2‚Äôs with a minor improvement.\\n2. Related Work\\nThe detection of hate speech and offensive content on social media has garnered significant\\nattention in recent years, driven by the growing need to maintain safe and constructive online\\nenvironments [ 19,20]. Researchers have explored a variety of approaches to address this issue,\\nranging from traditional machine learning techniques [ 21,22,23,24] to the application of\\nadvanced deep learning models [25, 2, 26, 27].\\nEarly approaches to hate speech detection [ 21,28] used simple machine learning algorithms.\\nThese models [ 22,29] used manually crafted features, including word n-grams, part-of-speech\\ntags, and sentiment scores, to classify text. For instance, [ 30,31,32,33] employed a logistic\\nregression model with n-grams and part-of-speech features to classify tweets into hate speech,\\noffensive language, and neither. However, the performance of these models was often limited\\nby their reliance on surface-level features, which could not fully capture the complexities of\\nlanguage and context.\\n[7, 34] experimented with LSTM networks and Gradient Boosted Decision Trees (GBDT) to\\nclassify hate speech on Twitter, demonstrating improvements over traditional machine learning\\nmethods. Similarly, [ 35,8,36] utilized a CNN-LSTM architecture to detect offensive language,\\nshowing that deep learning models could capture both local and sequential patterns in text.\\n[37,25,9,38] marked the advancement in the field of BERT and transformer. These models,\\npre-trained on large corpora, allowed for contextual understanding of text, leading to more\\naccurate classification. [ 39,40,2,41,42] leveraged BERT for hate speech detection, achieving\\nstate-of-the-art performance by fine-tuning the model on labeled datasets. The success of\\ntransformer models paved the way for further research into leveraging large language models\\nfor offensive language detection.\\nMore recently, the focus has shifted toward leveraging even more sophisticated large language\\nmodels (LLMs), such as GPT-3 and its successors. These LLMs, with their ability to generate andunderstand text in a nuanced manner, have shown promise in detecting offensive content. For\\ninstance, [ 43,32,44,45,46,47] explored the use of GPT-3 for hate speech detection through\\nfew-shot learning, highlighting the model‚Äôs ability to generalize across different datasets with\\nminimal task-specific training. However, challenges remain in applying these models to code-\\nmixed languages and in ensuring that they can handle the subtleties of context-dependent hate\\nspeech.\\n[48,46,49,50] investigated the detection of hate speech in Hinglish using deep learning\\nmodels, while organized a shared task on multilingual hate speech detection [ 4,51,52], focusing\\non languages such as English, German, and Hindi. These studies underscore the importance of\\ndeveloping language-agnostic models or approaches that can effectively deal with code-mixing\\nand multilingual content. However, no work has explored the capabilities of GPT-3.5 Turbo for\\nhate speech detection. In this work, we explored the capabilities of GPT-3.5 Turbo to detect\\nhate speech in English social media posts on X.\\n3. Dataset\\nThe English testing dataset consists of 888 tweets collected from a popular social media platform,\\nX. Since we used effectively only the test dataset for our predictions, we only mentioned the\\nstatistics corresponding to the test dataset.\\n4. Task Definition\\nThe task in this study involves the automated classification of social media content, specifically\\ntweets, into two distinct categories: Hate and Offensive (i.e., HOF) and Non Hate-Offensive (i.e.,\\nNOT). The objective is to develop a model that can accurately identify whether a given tweet\\ncontains hate speech or offensive language (i.e., HOF), or if it does not (i.e., NOT).\\n5. Methodology\\nPrompting, especially with large language models such as GPT-3.5 Turbo, offers a powerful\\napproach to solving the problem of hate speech and offensive content detection for several\\nreasons:\\n-Contextual Understanding: Large language models are pretrained [ 53] on vast amounts\\nof text data, enabling them to understand language nuances, context, and semantic\\nrelationships between words and phrases. This deep understanding allows them to\\ndiscern whether a piece of content is offensive or hateful, even when the language is\\nsubtle or context-dependent.\\n-Flexibility and Adaptability: Prompting allows for flexibility [ 54] in how the task is\\nframed and tackled. By carefully designing prompts, the model can be directed to focus\\non specific aspects of the content, such as detecting harmful language or distinguishing\\nbetween different forms of offense. This adaptability is crucial in handling the diverse\\nand evolving nature of hate speech on social media.-Multilingual and Code-Mixed Language Handling: Prompting large language models\\nis beneficial for dealing with multilingual content [ 55], including code-mixed languages,\\nwhich are common on social media. The model‚Äôs extensive training on diverse text sources\\nhelps it understand and classify content that blends languages or uses non-standard\\nlinguistic forms.\\n-Efficiency in Deployment: Prompting does not require the traditional pipeline of data\\npreprocessing [ 56], feature extraction, and model training. Instead, the model can be used\\ndirectly to classify content by providing it with well-crafted prompts. This reduces the\\ntime and resources needed to deploy hate speech detection systems.\\n-Scalability: With prompting, the same model can be applied to a wide range of tasks\\nwithout significant modifications. This scalability [ 57] is important for social media\\nplatforms that need to monitor vast amounts of content in real time and across different\\nlanguages.\\n-Handling Ambiguity and Subjectivity: Hate speech and offensive content often involve\\nsubjective judgments [ 58]. Prompting a large language model allows for more nuanced\\ndecision-making, as the model can consider context, intent, and the subtleties of language\\nthat might be missed by simpler models.\\n-Rapid Iteration and Improvement: Prompting [ 59] enables quick adjustments and\\nrefinements based on feedback, making it easier to improve the model‚Äôs performance\\nover time. As new forms of offensive language emerge, the prompts can be updated or\\nrefined to ensure the model remains effective.\\n5.1. Prompt Engineering-Based Approach\\nWe used the GPT-3.5 Turbo1model [ 60] via prompting to solve the classification task. We used\\nGPT-3.5 Turbo in zero-Shot mode via prompting. After the prompt is provided to the LLM,\\nthe following steps happen internal to the LLM while generating the output. The following\\noutlines the steps that occur internally within the LLM, summarizing the prompting approach\\nusing GPT-3.5 Turbo:\\nStep 1: Tokenization\\n‚Ä¢Prompt: ùëã= [ùë•1, ùë•2, . . . , ùë• ùëõ]\\n‚Ä¢The input text (prompt) is first tokenized into smaller units called tokens. These tokens\\nare often subwords or characters, depending on the model‚Äôs design.\\n‚Ä¢Tokenized Input: ùëá= [ùë°1, ùë°2, . . . , ùë° ùëö]\\nStep 2: Embedding\\n‚Ä¢Each token is converted into a high-dimensional vector (embedding) using an embedding\\nmatrix ùê∏.\\n‚Ä¢Embedding Matrix: ùê∏‚ààR|ùëâ|√óùëë, where |ùëâ|is the size of the vocabulary and ùëëis the\\nembedding dimension.\\n1https://platform.openai.com/docs/models/gpt-3-5-turbo‚Ä¢Embedded Tokens: ùëáemb= [ùê∏(ùë°1), ùê∏(ùë°2), . . . , ùê∏ (ùë°ùëö)]\\nStep 3: Positional Encoding\\n‚Ä¢Since the model processes sequences, it adds positional information to the embeddings to\\ncapture the order of tokens.\\n‚Ä¢Positional Encoding: ùëÉ(ùë°ùëñ)\\n‚Ä¢Input to the Model: ùëç=ùëáemb+ùëÉ\\nStep 4: Attention Mechanism (Transformer Architecture)\\n‚Ä¢Attention Score Calculation: The model computes attention scores to determine the\\nimportance of each token relative to others in the sequence.\\n‚Ä¢Attention Formula:\\nAttention (ùëÑ, ùêæ, ùëâ ) =softmax(Ô∏ÇùëÑùêæùëá\\n‚àöùëëùëò)Ô∏Ç\\nùëâ (1)\\n‚Ä¢where ùëÑ(query), ùêæ(key), and ùëâ(value) are linear transformations of the input ùëç.\\n‚Ä¢This attention mechanism is applied multiple times through multi-head attention, allowing\\nthe model to focus on different parts of the sequence simultaneously.\\nStep 5: Feedforward Neural Networks\\n‚Ä¢The output of the attention mechanism is passed through feedforward neural networks,\\nwhich apply non-linear transformations.\\n‚Ä¢Feedforward Layer:\\nFFN(ùë•) = max(0 , ùë•ùëä 1+ùëè1)ùëä2+ùëè2 (2)\\n‚Ä¢where ùëä1, ùëä 2are weight matrices and ùëè1, ùëè2are biases.\\nStep 6: Stacking Layers\\n‚Ä¢Multiple layers of attention and feedforward networks are stacked, each with its own set\\nof parameters. This forms the \"deep\" in deep learning.\\n‚Ä¢Layer Output:\\nùêª(ùëô)=LayerNorm (ùëç(ùëô)+Attention (ùëÑ(ùëô), ùêæ(ùëô), ùëâ(ùëô))) (3)\\nùëç(ùëô+1)=LayerNorm (ùêª(ùëô)+FFN(ùêª(ùëô))) (4)\\nStep 7: Output Generation\\n‚Ä¢The final output of the stacked layers is a sequence of vectors.\\n‚Ä¢These vectors are projected back into the token space using a softmax layer to predict the\\nnext token or word in the sequence.‚Ä¢Softmax Function:\\nùëÉ(ùë¶ùëñ|ùëã) =exp(ùëçùëñ)\\n‚àëÔ∏Ä|ùëâ|\\nùëó=1exp(ùëçùëó)(5)\\n‚Ä¢where ùëçùëñis the logit corresponding to token ùëñin the vocabulary.\\n‚Ä¢The model generates the next token in the sequence based on the probability distribution,\\nand the process repeats until the end of the output sequence is reached.\\nStep 8: Decoding\\n‚Ä¢The predicted tokens are then decoded back into text, forming the final output.\\n‚Ä¢Output Text: ùëå= [ùë¶1, ùë¶2, . . . , ùë¶ ùëò]\\nFigure 1: An overview of GPT-3.5 Turbo for hate speech detection using social media posts.\\nThe process begins with tokenization, where the input text is broken down into smaller units\\ncalled tokens, which could be subwords or characters depending on the model. Next, in the\\nembedding step, each token is converted into a high-dimensional vector using an embedding\\nmatrix ,resulting in embedded tokens. To capture the order of tokens in the sequence, positional\\nencoding is added to the embedded tokens, producing the input for the model. The attention\\nmechanism in the transformer architecture then computes attention scores to determine the\\nimportance of each token relative to others. Following attention, the output is passed through\\nfeedforward neural networks that apply non-linear transformations to enhance the model‚Äôs\\nlearning capacity. The feedforward process involves weight matrices and biases, introducing\\nnon-linearity. These attention and feedforward layers are then stacked to form the deep layers of\\nthe model. Each layer processes the input and adds its contribution to the overall understanding\\nof the sequence. The output from the stacked layers is a sequence of vectors. In output\\ngeneration, these vectors are projected back into the token space using a softmax layer to\\npredict the next token in the sequence. The softmax function produces a probability distribution\\nover the vocabulary, and the model selects the most likely token. Finally, in decoding, thepredicted tokens are converted back into text, forming the final output sequence. This process\\nrepeats until the entire output sequence is generated, resulting in the final text produced by the\\nmodel.\\nWe used the following prompt for English language for the purpose of classification: \" Please\\nCheck whether the Tweet-<Tweet> is Hate and Offensive or Non Hate-Offensive. Only state Hate\\nand Offensive or Non Hate-Offensive \". The figure representing the methodology is shown in\\nFigure 1.\\nWe run the GPT model at 3 different temperature values- 0.7, 0.8, and 0.9.\\nAll the labels are converted from Hate and Offensive to HOF and from Non Hate-Offensive\\nto NOT and then submitted.\\n6. Results\\nRun Number Macro-F1 Score\\nRun 1 0.7560535301\\nRun 2 0.7518319164\\nRun 3 0.7543135973\\nTable 1\\nMacro-F1 Scores for the Three Runs for Hate Speech Detection in English.\\nOur Team TextTitans ranked 5th in the English Task for Hate Speech detection. Table 1\\nshows the Macro-F1 scores for the three runs for Hate Speech Detection in English.\\nRun 1 (0.756): This run has the highest Macro-F1 score among the three runs. This suggests\\nthat the model performed slightly better in balancing precision and recall across different classes\\ncompared to the other runs.\\nRun 2 (0.751): This run‚Äôs Macro-F1 score is a bit lower than Run 1. The decrease in score might\\nindicate a slight drop in performance, either in precision, recall, or both, across the classes.\\nRun 3 (0.754): The score for this run is also slightly lower than Run 1 but higher than Run 2.\\nThis suggests performance is similar to Run 2 but with a slight improvement.\\nThe scores are relatively close to each other, indicating that the model‚Äôs performance across\\nthese runs is quite consistent. Differences between the scores are minor, which might be due to\\nvariations in the hyperparameter i.e. temperature. These differences, while small, might still be\\nsignificant depending on the context of the task or the scale of the evaluation. In summary, all\\nthree runs show strong performance with Macro-F1 scores in the range of approximately 0.75\\nto 0.76. Run 1 shows the best performance among the three, but the differences are minimal,\\nsuggesting that the model‚Äôs performance is relatively stable across these runs.\\n7. Conclusion\\nIn this study, we explored the application of large language models, specifically GPT-3.5 Turbo,\\nfor the task of detecting hate speech and offensive content on social media. The increasingvolume and complexity of online communication, especially in multilingual and code-mixed\\nlanguages, present significant challenges for maintaining a safe and constructive digital envi-\\nronment. Our work focused on classifying English tweets into Hate and Offensive and Non\\nHate-Offensive categories, while also extending our analysis to other languages.\\nThe Macro-F1 scores across the three runs of the classification model demonstrate strong\\nand consistent performance. With scores of 0.756, 0.751, and 0.754, respectively, the results\\nindicate that the model effectively balances precision and recall across different classes. The\\nslight variations observed among the runs are minimal, reflecting the model‚Äôs stability and\\nreliability in various testing scenarios. These findings affirm the model‚Äôs capability to perform\\nwell in a balanced manner across all classes, reinforcing its utility in practical applications\\nwhere class performance consistency is critical. Future work may explore further refinements\\nto enhance performance or investigate additional metrics for a more comprehensive evaluation.\\nWhile the results are promising, they also highlight areas for further improvement. The\\ncomplexity of language, the nuances of context, and the evolving nature of online discourse\\nrequire continuous refinement of models and approaches. Future research should focus on\\nenhancing the model‚Äôs ability to handle multilingual and code-mixed content more effectively,\\nas well as on developing strategies to address the subjectivity inherent in detecting offensive\\nlanguage.\\nReferences\\n[1] V. Taprial, P. Kanwar, Understanding social media, Bookboon, 2012.\\n[2]Y. Zhu, H. Xu, S. Wang, X. Zhu, M. Zeng, Hate speech detection based on sentiment\\nknowledge sharing in multi-task learning, in: Proceedings of the 58th Annual Meeting of\\nthe Association for Computational Linguistics, 2020, pp. 3456‚Äì3467.\\n[3]Y. Jin, L. Wanner, A. Shvets, Gpt-hatecheck: Can llms write better functional tests for hate\\nspeech detection?, arXiv preprint arXiv:2402.15238 (2024).\\n[4]S. S. Aluru, B. Mathew, P. Saha, A. Mukherjee, Deep learning models for multilingual hate\\nspeech detection, arXiv preprint arXiv:2004.06465 (2020).\\n[5]B. R. Chakravarthi, N. Sripriya, B. Bharathi, K. Nandhini, S. C. Navaneethakrishnan,\\nT. Durairaj, R. Ponnusamy, P. K. Kumaresan, K. K. Ponnusamy, C. Rajkumar, Overview of\\nthe shared task on sarcasm identification of dravidian languages (malayalam and tamil) in\\ndravidiancodemix, in: Forum of Information Retrieval and Evaluation FIRE-2023, 2023.\\n[6]A. Deroy, K. Ghosh, S. Ghosh, How ready are pre-trained abstractive models and llms for\\nlegal case judgement summarization?, arXiv preprint arXiv:2306.01248 (2023).\\n[7]Z. Zhang, F. Luo, Detecting hate speech on twitter using a convolution-gru based deep\\nneural network, in: Proceedings of the 5th International Workshop on Natural Language\\nProcessing for Social Media, 2018, pp. 17‚Äì18.\\n[8]F. Liu, D. Avci, Nuli at semeval-2019 task 6: Transfer learning for offensive language\\ndetection using bidirectional transformers, in: Proceedings of the 13th International\\nWorkshop on Semantic Evaluation, 2019, pp. 87‚Äì91.\\n[9]H. M. Saleem, K. P. Dillon, S. Benesch, D. Ruths, A web of hate: Tackling hateful speech inonline social spaces, in: Proceedings of the 1st Workshop on Abusive Language Online,\\n2017, pp. 1‚Äì10.\\n[10] A. Deroy, K. Ghosh, S. Ghosh, Ensemble methods for improving extractive summarization\\nof legal case judgements, Artificial Intelligence and Law 32 (2024) 231‚Äì289.\\n[11] S. MacAvaney, H.-R. Yao, E. Yang, K. Russell, N. Goharian, O. Frieder, Hate speech detection:\\nChallenges and solutions, PloS one 14 (2019) e0221152.\\n[12] K. S. Rao, et al., A novel approach to unsupervised pattern discovery in speech using\\nconvolutional neural network, Computer Speech & Language 71 (2022) 101259.\\n[13] A. Deroy, P. Bhattacharya, K. Ghosh, S. Ghosh, An analytical study of algorithmic and\\nexpert summaries of legal cases, in: Legal Knowledge and Information Systems, IOS Press,\\n2021, pp. 90‚Äì99.\\n[14] T. K. Bhatia, W. C. Ritchie, Multilingualism and forensic linguistics, The Handbook of\\nbilingualism and multilingualism (2012) 671‚Äì699.\\n[15] S. Maity, A. Deroy, S. Sarkar, A novel multi-stage prompting approach for language\\nagnostic mcq generation using gpt, in: European Conference on Information Retrieval,\\nSpringer, 2024, pp. 268‚Äì277.\\n[16] M. Anzovino, E. Fersini, P. Rosso, Automatic identification and classification of misog-\\nynistic language on twitter, in: Proceedings of the 23rd International Conference on\\nApplications of Natural Language to Information Systems, 2018, pp. 57‚Äì64.\\n[17] N. Sripriya, T. Durairaj, K. Nandhini, B. Bharathi, K. K. Ponnusamy, C. Rajkumar, P. K.\\nKumaresan, R. Ponnusamy, C. Subalalitha, B. R. Chakravarthi, Findings of shared task on\\nsarcasm identification in code-mixed dravidian languages, FIRE 2023 16 (2023) 22.\\n[18] S. Maity, A. Deroy, S. Sarkar, Harnessing the power of prompt-based techniques for\\ngenerating school-level questions using large language models, in: Proceedings of the\\n15th Annual Meeting of the Forum for Information Retrieval Evaluation, 2023, pp. 30‚Äì39.\\n[19] F. Poletto, V. Basile, M. Sanguinetti, C. Bosco, V. Patti, Resources and benchmark corpora\\nfor hate speech detection: a systematic review, Language Resources and Evaluation 55\\n(2021) 477‚Äì523.\\n[20] S. Maity, A. Deroy, S. Sarkar, How ready are generative pre-trained large language models\\nfor explaining bengali grammatical errors?, in: B. Paa√É≈∏en, C. D. Epp (Eds.), Proceedings of\\nthe 17th International Conference on Educational Data Mining, International Educational\\nData Mining Society, Atlanta, Georgia, USA, 2024, pp. 664‚Äì671. doi: 10.5281/zenodo.\\n12729912 .\\n[21] J. Smith, J. Doe, A study on hate speech detection using machine learning, Journal of\\nComputational Social Science 12 (2018) 123‚Äì145. doi: 10.1007/s12345-018-1234-5 .\\n[22] C. Nobata, J. Tetreault, A. Thomas, Y. Mehdad, Y. Chang, Abusive language detection in\\nonline user content, in: Proceedings of the 25th International Conference on World Wide\\nWeb, 2016, pp. 145‚Äì153.\\n[23] S. Maity, A. Deroy, S. Sarkar, Exploring the capabilities of prompted large language models\\nin educational and assessment applications, in: B. Paa√É≈∏en, C. D. Epp (Eds.), Proceedings of\\nthe 17th International Conference on Educational Data Mining, International Educational\\nData Mining Society, Atlanta, Georgia, USA, 2024, pp. 961‚Äì968. doi: 10.5281/zenodo.\\n12730013 .\\n[24] A. Deroy, S. Maity, Questioning biases in case judgment summaries: Legal datasets orlarge language models?, arXiv preprint arXiv:2312.00554 (2023).\\n[25] M. Mozafari, R. Farahbakhsh, N. Crespi, A bert-based transfer learning approach for hate\\nspeech detection in online social media, in: Complex Networks and Their Applications\\nVIII: Volume 1 Proceedings of the Eighth International Conference on Complex Networks\\nand Their Applications COMPLEX NETWORKS 2019 8, Springer, 2020, pp. 928‚Äì940.\\n[26] S. K. Nigam, A. Deroy, N. Shallum, A. K. Mishra, A. Roy, S. K. Mishra, A. Bhattacharya,\\nS. Ghosh, K. Ghosh, Nonet at semeval-2023 task 6: Methodologies for legal evaluation,\\narXiv preprint arXiv:2310.11049 (2023).\\n[27] A. Deroy, K. Ghosh, S. Ghosh, Applicability of large language models and generative\\nmodels for legal case judgement summarization, Artificial Intelligence and Law (2024)\\n1‚Äì44.\\n[28] S. K. Nigam, A. Deroy, Fact-based court judgment prediction, in: Proceedings of the 15th\\nAnnual Meeting of the Forum for Information Retrieval Evaluation, 2023, pp. 78‚Äì82.\\n[29] A. Deroy, S. Maity, S. Ghosh, Prompted zero-shot multi-label classification of factual\\nincorrectness in machine-generated summaries., in: FIRE (Working Notes), 2023, pp.\\n734‚Äì746.\\n[30] A. Deroy, S. Maity, Code generation and algorithmic problem solving using llama 3.1 405b,\\narXiv preprint arXiv:2409.19027 (2024).\\n[31] P. Badjatiya, S. Gupta, M. Gupta, V. Varma, Deep learning for hate speech detection\\nin tweets, in: Proceedings of the 26th International Conference on World Wide Web\\nCompanion, 2017, pp. 759‚Äì760.\\n[32] K.-L. Chiu, A. Collins, R. Alexander, Detecting hate speech with gpt-3, arXiv preprint\\narXiv:2103.12407 (2021).\\n[33] S. Maity, A. Deroy, S. Sarkar, How effective is gpt-4 turbo in generating school-level\\nquestions from textbooks based on bloom‚Äôs revised taxonomy?, 2024. URL: https://arxiv.\\norg/abs/2406.15211. arXiv:2406.15211 .\\n[34] A. Deroy, S. Maity, Multi-label classification of covid-tweets using large language models,\\narXiv preprint arXiv:2312.10748 (2023).\\n[35] S. Maity, A. Deroy, Generative ai and its impact on personalized intelligent tutoring\\nsystems, arXiv preprint arXiv:2410.10650 (2024).\\n[36] A. Deroy, N. K. Bailung, K. Ghosh, S. Ghosh, A. Chakraborty, Artificial intelligence (ai) in\\nlegal data mining, arXiv preprint arXiv:2405.14707 (2024).\\n[37] S. Maity, A. Deroy, The future of learning in the age of generative ai: Automated question\\ngeneration and assessment with large language models, arXiv preprint arXiv:2410.09576\\n(2024).\\n[38] A. Deroy, S. Maity, Ai-powered answer assessment: A comprehensive overview, Authorea\\nPreprints (2024).\\n[39] M. Mozafari, R. Farahbakhsh, N. Crespi, Hate speech detection and racial bias mitigation\\nin social media based on bert model, PloS one 15 (2020) e0237861.\\n[40] S. K. Nigam, A. Deroy, S. Maity, A. Bhattacharya, Rethinking legal judgement prediction\\nin a realistic scenario in the era of large language models, arXiv preprint arXiv:2410.10542\\n(2024).\\n[41] A. Deroy, S. Maity, A short case study on understanding the capabilities of gpt for temporal\\nreasoning tasks, Authorea Preprints (2024).[42] S. Maity, A. Deroy, S. Sarkar, Exploring the capabilities of prompted large language models\\nin educational and assessment applications (2024).\\n[43] A. Deroy, S. Maity, S. Sarkar, Mirror: A novel approach for the automated evaluation of\\nopen-ended question generation, arXiv preprint arXiv:2410.12893 (2024).\\n[44] M. Mozafari, R. Farahbakhsh, N. Crespi, Cross-lingual few-shot hate speech and offensive\\nlanguage detection using meta learning, IEEE Access 10 (2022) 14880‚Äì14896.\\n[45] S. Maity, A. Deroy, Human-centric explainable ai in education, 2024. URL: https://arxiv.\\norg/abs/2410.19822. arXiv:2410.19822 .\\n[46] H. Thapliyal, Sarcasm Detection System for Hinglish Language (SDSHL), Ph.D. thesis, IIIT\\nHyderabad, 2020.\\n[47] A. Deroy, S. Maity, Question generation: Past, present & future, Authorea Preprints (2024).\\n[48] S. Yadav, A. Kaushik, K. McDaid, Leveraging weakly annotated data for hate speech\\ndetection in code-mixed hinglish: A feasibility-driven transfer learning approach with\\nlarge language models, arXiv preprint arXiv:2403.02121 (2024).\\n[49] A. Deroy, S. Maity, Exploring the mathematical reasoning capabilities of gemini, Authorea\\nPreprints (2024).\\n[50] A. Deroy, Exploiting Machine Learning Techniques for Unsupervised Clustering of Speech\\nUtterances, Ph.D. thesis, Indian Institute of Technology Kharagpur, 2019.\\n[51] S. Ghosh, K. Ghosh, D. Ganguly, A. Bhattacharya, P. P. Chakrabarti, S. Guha, A. Pal,\\nK. Rudra, P. Majumder, D. Roy, et al., Report on the 2nd symposium on artificial intelligence\\nand law (sail) 2022, in: ACM SIGIR Forum, volume 56, ACM New York, NY, USA, 2023, pp.\\n1‚Äì7.\\n[52] S. Maity, A. Deroy, Natural language correction with an emphasis on bangla (2023).\\n[53] J. Chen, Z. Liu, X. Huang, C. Wu, Q. Liu, G. Jiang, Y. Pu, Y. Lei, X. Chen, X. Wang,\\net al., When large language models meet personalization: Perspectives of challenges and\\nopportunities, World Wide Web 27 (2024) 42.\\n[54] P. Dillenbourg, P. Tchounikine, Flexibility in macro-scripts for computer-supported\\ncollaborative learning, Journal of computer assisted learning 23 (2007) 1‚Äì13.\\n[55] K. Shanmugavadivel, V. Sathishkumar, S. Raja, T. B. Lingaiah, S. Neelakandan, M. Subra-\\nmanian, Deep learning based sentiment analysis and offensive language identification on\\nmultilingual code-mixed data, Scientific Reports 12 (2022) 21557.\\n[56] J. Heit, J. Liu, M. Shah, An architecture for the deployment of statistical models for the big\\ndata era, in: 2016 IEEE International Conference on Big Data (Big Data), IEEE, 2016, pp.\\n1377‚Äì1384.\\n[57] B. Lester, R. Al-Rfou, N. Constant, The power of scale for parameter-efficient prompt\\ntuning, arXiv preprint arXiv:2104.08691 (2021).\\n[58] A. C. Curry, G. Abercrombie, Z. Talat, Subjective isms? on the danger of conflating hate\\nand offence in abusive language detection, in: Proceedings of the 8th Workshop on Online\\nAbuse and Harms (WOAH 2024), 2024, pp. 275‚Äì282.\\n[59] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prab-\\nhumoye, Y. Yang, et al., Self-refine: Iterative refinement with self-feedback, Advances in\\nNeural Information Processing Systems 36 (2024).\\n[60] T. B. Brown, Language models are few-shot learners, arXiv preprint arXiv:2005.14165\\n(2020).',\n",
       " 'SpiderDAN: Matching Augmentation in Demand-Aware Networks\\nAleksander Figiel‚àóDarya Melnyk‚àóAndr¬¥ e Nichterlein‚àóArash Pourdamghani‚àó\\nStefan Schmid‚àó\\nAbstract\\nGraph augmentation is a fundamental and well-studied\\nproblem that arises in network optimization. We consider\\na new variant of this model motivated by reconfigurable\\ncommunication networks. In this variant, we consider a\\ngiven physical network and the measured communication\\ndemands between the nodes. Our goal is to augment the\\ngiven physical network with a matching, so that the shortest\\npath lengths in the augmented network, weighted with the\\ndemands, are minimal. We prove that this problem is NP-\\nhard, even if the physical network is a cycle. We then\\nuse results from demand-aware network design to provide\\na constant-factor approximation algorithm for adding a\\nmatching in case that only a few nodes in the network\\ncause almost all the communication. For general real-world\\ncommunication patterns, we design and evaluate a series\\nof heuristics that can deal with arbitrary graphs as the\\nunderlying network structure. Our algorithms are validated\\nexperimentally using real-world traces (from e.g., Facebook)\\nof data centers.\\n1 Introduction\\nThis paper considers a network augmentation problem\\nthat is motivated by emerging data center technolo-\\ngies [16, 26, 42, 44]. The idea of these technologies is to\\nenable demand-aware networks by using reconfigurable\\noptical switches, on top of an existing demand-oblivious\\ndata center topology based on electrical switches. An\\noptical circuit switch allows one to directly connect (i.e.,\\nmatch) each data center rack to at most one other rack\\nvia an optical link, thus forming a (perfect) matching on\\ntop of an existing electrical network [15, 36, 47]. This\\nallows to reduce the number of hops traversed by com-\\nmunicated bit, and hence increases the bandwidth. See\\nfor example Jupiter Evolving for a recent solution de-\\nployed by Google [42].\\nFrom a theoretical perspective, network augmenta-\\ntion has been studied under two optimization criteria\\nso far: minimizing the diameter of the network (i.e.,\\nthe worst-case communication cost) [1, 2, 7, 8, 22], and\\n‚àóTechnische Universit¬® at Berlin, Germany\\n‚Ä†This work is supported by the European Research Council\\n(ERC) under grant agreement No. 864228 (AdjustNet), 2020-2025.minimizing average shortest path length between any\\ntwo nodes [21, 31, 37, 39, 40, 48]. However, previous\\nwork has mostly assumed that the communication de-\\nmand between the nodes is evenly distributed, i.e., that\\nany two nodes are equally likely to communicate, inde-\\npendent of their role in the network. We drop this as-\\nsumption in this work and arrive at the following graph\\nproblem: Given a graph (the existing infrastructure net-\\nwork) and a demand matrix (encoding the communica-\\ntion demand between all pairs of nodes), our goal is to\\ncompute a matching to add so that the weighted av-\\nerage path length in the augmented network is mini-\\nmized where the weights are given by the demand ma-\\ntrix. We call this problem Minimizing Weighted Av-\\nerage Shortest Path Length via Matching Ad-\\ndition (MWASP ), see Section 2 for a formal definition.\\nThis approach can be used to reconfigure the network\\nafter some time to adjust to the new demand.\\nOur Contribution. We analyze the complexity\\nand approximability of MWASP . We start by showing\\nthat it is NP-hard, already if the underlying infrastruc-\\nture graph is a ring and each node communicates to at\\nmost two other nodes (i. e., the demand matrix is ex-\\ntremely sparse).\\nWe further propose a constant-factor approxima-\\ntion algorithm for MWASP on connected underlying in-\\nfrastructure graphs of constant maximum degree. This\\nalgorithm groups small segments of nodes that are in\\nclose proximity into super-nodes and then connects\\nthese super-nodes with a known construction from de-\\nmand aware networks [4]. The idea behind the grouping\\nis that nodes can help their neighbors of high demand\\nconnect to other high-demand nodes. This construc-\\ntion limits us to highly skewed demand matrices where\\nonly a few nodes in the network cause almost all the\\ncommunication.\\nWe consider synthetically generated communication\\ndemands, as well as real-world datasets; in particular\\na dataset collected from Facebook [43]. We further\\ntest our algorithms on various infrastructure graphs,\\nincluding symmetric structures like rings, 2D and the\\n3D torus, as they are often used in distributed archi-\\ntectures [35]. Our heuristics and approximation provearXiv:2411.11426v1  [cs.DS]  18 Nov 2024highly efficient and effective on these datasets: They dis-\\nplay good approximation factors on the small synthetic\\ninstances where we do know the optimum. Moreover,\\nthe approximation and our heuristics scale well on the\\nreal-world datasets.\\nRelated Work. Graph augmentation has been\\nwidely studied from the perspective of optimizing com-\\nmunication in a peer-to-peer network. One goal is to\\nminimize the diameter of a graph [8], i.e., to improve the\\nworst-case cost of routing. Schoone et al. [45] showed\\nthat the problem of minimizing the number of edges\\nthat one needs to add to a graph in order to reduce\\nits diameter to dis NP-complete. Following this result,\\nlower and upper bounds on the number of additional\\nedges have been derived for cycles [2, 7, 22], paths [7],\\n(t+1)-edge connected graphs [7], and graphs of bounded\\nmaximum degree [2, 7]. In another variant, the goal is\\nto add a fixed number of edges such that the diameter\\nis minimized [31]. Adriaens and Gionis [1] generalize\\nthis setting to adding up to Œ¥edges per node, which\\ncorresponds to adding a matching for Œ¥= 1. Both vari-\\nants have been shown to be NP-hard and algorithms\\nwith logarithmic approximation ratios (in the number\\nof added edges) have been proposed in the respective\\npapers. This problem has also been considered from\\nthe viewpoint of opinion polarization in social networks.\\nHere, the goal is to reduce polarization, i.e., the distance\\nbetween pairs of nodes located in different groups, by\\nadding a small number of edges to the graph [20, 24, 30].\\nGraph augmentation has also been studied in the\\ncontext of small-world networks. In this setting, the\\nmain question is how many edges one needs to add to\\na graph to minimize the average shortest path length.\\nWatts and Strogatz [48] proposed a first model where\\nthe nodes are placed on a circle, and edges representing\\n‚Äúlong-range‚Äù contacts are added randomly to each node.\\nKleinberg [31] introduced a formal model where the\\nnodes are placed on a grid, and each node is allowed\\nto add one shortcut edge following the inverse power\\ndistribution. Kleinberg showed that such a small-world\\nnetwork has an average path length that is logarithmic\\nin the number of nodes. Meyerson and Tagiku [37]\\nintroduce edge weights in their model and allow the\\naddition of a constant number of shortcut edges to the\\ngraph. They show that this problem is NP-complete\\nand provide constant approximation algorithms for the\\nweighted and the unweighted cases. Since many of the\\nproblem variants have been proven to be hard, different\\nheuristics for adding a small number of edges to a\\nnetwork have been considered in the literature [21, 39,\\n40].\\nIn this paper, we look at a demand-aware average\\nshortest path length as the quality measure of our gen-erated network. Our motivation to study an augmenta-\\ntion variant with a matching comes from demand-aware\\ndata center networks [26, 42]: emerging optical switch-\\ning technologies make it possible to enhance a given\\nnetwork with a demand-aware matching. This model\\nhas been considered before also in the theoretical litera-\\nture: Kulkarni et al. [34] consider a setting where pack-\\nets need to be routed from sources to destinations via a\\nmatching. The authors present a stable matching that\\nis updated over time in an online fashion. Hanauer et al.\\n[28] presents a dynamic setting in which a weighted k-\\ndisjoint matching is recomputed dynamically based on\\nchanging demand. Other algorithms aim at minimizing\\nthe load or the congestion of routing in demand-aware\\nnetworks [10, 11].\\nAlso, demand-aware network design has been con-\\nsidered in the literature, however, not with a single\\nmatching. Avin et al. [4] propose designing demand-\\naware networks from scratch by building a bounded de-\\ngree graph. They provide a lower bound on the average\\npath length based on the entropy of the demand ma-\\ntrix, and present a constant approximation algorithm\\nfor sparse demand graphs. Hanauer et al. [27] study al-\\ngorithms to find kdisjoint heavy matchings in a graph.\\nThey show that this problem is NP-hard, and propose\\ndifferent approximation algorithms for the problem.\\n2 Model\\nInfrastructure graph. We are given a set of nodes\\nV={v1, . . . , v n}that communicate over an underlying\\ninfrastructure graph G, this graph corresponds to the\\nphysical network. The infrastructure graph is assumed\\nto have a non-constant diameter and a large number of\\nnodes n, where nis even (so that there always exists a\\nperfect matching).\\nDemand matrix. The communication pattern is\\ndescribed by an n√óndemand matrix D. In this\\nmatrix, Du,vindicates the probability with which a\\nnode ucommunicates to a node v. Observe that the\\ndemand matrix is normalized, i.e.,P\\nu,v‚ààV,uÃ∏=vDu,v=\\n1, and the demand from one node to itself is 0, i.e.,\\nDv,v= 0‚àÄv‚ààV. For simplicity, we assume that the\\ndemand matrix is symmetric. Thus, the demand matrix\\nencodes an edge-weighted, simple, undirected graph\\n(zero-weight edges are omitted). We use dist G(u, v)\\nto denote the distance (the length of a shortest path\\nbetween) uandvinG. Our objective is to minimize\\nthe weighted average shortest path length ObjD(G) =P\\nu,v‚ààVDu,v¬∑distG(u, v) in Gfor the given demand\\nmatrix D.\\nOptimization objective. Our goal is to add a\\nperfect matching Mto the set of edges of Gthat\\nminimizes the weighted average shortest path lengthin this augmented graph G+M. We consider the\\ncase where the added matching edges behave the same\\nas the edges of the underlying graph, i.e., they are\\nindistinguishable from the edges of the infrastructure\\ngraph in terms of their weight and length. We now\\ndefine Minimizing Weighted Average Shortest\\nPath Length via Matching Addition ( MWASP )\\nas finding for a given graph Ga matching Mthat\\nminimizes ObjD(G+M).\\n3 NP-Hardness\\nBefore discussing the approximation algorithm in the\\nnext section, we state that even restricted to very sim-\\nplistic underlying infrastructure graphs and sparse de-\\nmand matrices, the problem remains NP-hard. Thus,\\nthere is (probably) no polynomial-time algorithm com-\\nputing optimal solutions for more interesting real-world\\ninfrastructure graphs and demand matrices.\\nTheorem 3.1. MWASP is NP-hard, even if the under-\\nlying graph is a cycle and every row and column of D\\nhas at most two non-zero elements.\\nProof. We reduce from Vertex Cover , which remains\\nNP-hard on graphs of maximum degree three [19].\\nGiven a graph G= (V, E) of maximum degree three\\nand an integer k, the question is whether there is\\na vertex cover of size at most k, that is, whether\\nthere is a vertex subset S‚äÜV,|S| ‚â§k, such that\\nfor each e‚ààEwe have e‚à©SÃ∏=‚àÖ? Considering\\nsuch a Vertex Cover instance ( G, k), we build an\\ninstance ( G‚Ä≤= (V‚Ä≤, E‚Ä≤), D, b ) of the decision version of\\nMWASP where G‚Ä≤is a cycle and bis the cost bound,\\nthat is, the question is if there is a matching Mso\\nthat ObjD(G‚Ä≤+M)‚â§b?\\nTo better distinguish the graphs GandG‚Ä≤we use\\nthe term vertices forGandnodes forG‚Ä≤. Throughout\\nour construction we want to enforce certain edges to\\nbe in a solution; denote with qthe number of these\\nedges. For each of these edges, we set the demand\\ninDto a high number Œ≤and set the cost bound bto\\nsatisfy qŒ≤ < b < (q+ 1)Œ≤. (For ease of presentation, we\\ndo not normalize D. By dividing band each entry in D\\nby the sum of entries in Dwe could normalize Dwithout\\nchanging the problem). Thus, the budget constraint\\nenforces that the endpoints of each of the qedges need\\nto have a distance of one in the resulting graph.\\nSubsequently, we describe our gadgets, each of\\nwhich forms a path of nodes in G‚Ä≤. In the end, these\\npaths will be connected to form the ring G‚Ä≤. Refer\\nto Figure 1 for a sketch of the gadgets and their\\ninteractions.\\nVertex gadget. Consider a vertex v‚ààVincident to\\nbetween one and three edges (vertices of degree zero\\nEdge GadgetVertex GadgetVertex GadgetChooser GadgetFigure 1: A schematic picture of our construction.\\nThe green and solid green edges incident to the vertex\\ngadgets and the black edges within the edge gadget are\\nforced to be in the matching. One vertex in the chooser\\ngadget has a demand to the middle diamond vertex\\nin the edge gadget in the bottom but cannot reach it\\ndirectly. The only two options, indicated by the dotted\\nedges: Connect a vertex in the chooser gadget to the\\nhelper vertex hv(denoted by the squares) in one of the\\ntwo vertex gadgets that represent the endpoints of the\\nedge. In the example, the left option is chosen and the\\npath to the diamond vertex in the bottom goes via the\\nvertex gadget into the edge gadget. Note that the long\\npath between the left circle node e‚Ñìand the diamond\\nnode emidin the edge gadget prevents traversing the\\nedge gadget from one circle node ( e‚Ñì) to the other ( er).\\ncan be ignored). Add a path on deg( v) + 1 nodes to\\nG‚Ä≤. These deg( v) + 1 nodes are a helper node hv(first\\nposition in the path) and vertex nodes v1, . . . , vdeg(v).\\nEach vertex node vicorresponds to one edge incident\\ntov.\\nEdge gadget. For each edge e‚ààE, add a path Pe\\non 4¬∑Œ±‚àí1 nodes e1, . . . , e 4Œ±‚àí1toG‚Ä≤(Œ±will be specified\\nlater). The unique middle node in this path is emid=\\ne2Œ±and the nodes at distance exactly Œ±from emidare\\ntheedge nodes e‚Ñì=eŒ±ander=e3Œ±(for left and\\nright). The remaining 4( Œ±‚àí1) nodes are dummy nodes\\nwhose purpose is to ensure the distances between emid\\nande‚Ñì, er. To ensure the matching cannot ‚Äúdisrupt‚Äù the\\ndistances on the path, we set for each i‚àà[Œ±‚àí1] the\\ndemand of the pairs D(eŒ±‚àíi, eŒ±+i) =D(e3Œ±‚àíi, e3Œ±+i) =\\nŒ≤. Add a dummy node e0at the beginning of the\\npath Peand set a demand D(emid, e0) =Œ≤. Let u, v\\nbe the endpoints of e, that is, e={u, v}. Add the\\ndemand D(e‚Ñì, vi) =D(er, uj) =Œ≤where vi, ujare the\\nvertex nodes corresponding to e. Here the mapping of e‚Ñì\\ntoviandertoujis arbitrarily chosen, but fixed.\\nChooser gadget. Add m =|E|demand\\nnodes d1, . . . , d m, connected on a path. For an edge ei‚àà\\nEthe demand node dihas a demand of 1 to emid\\ni, that is,D(di, emid\\ni) = 1. For all but kof these demand nodes we\\nforce edges in the matching as follows. To this end, as-\\nsume without of generality that kandmhave the same\\nparity (otherwise double Gandk). Thus m‚àíkis even.\\nFori‚àà[(m‚àík)/2] add the demand D(di, dm+1‚àíi) =Œ≤.\\nThe remaining kdemand nodes are free to be connected\\nto the vertex nodes (the idea is that they select a vertex\\ncover).\\nGap gadget. To ensure that different gadgets are far\\napart on the ring, we add a gap paths . Each gap path\\nconsists of 4 Œ≤gapnodes g1, . . . , g 4Œ≤. For each i‚àà[Œ≤]\\nthe demands are D(g4i‚àí3, g4i‚àí1) =D(g4i‚àí2, g4i) =Œ≤.\\nThe overall construction. Put all vertex, edge\\ngadgets and the chooser gadget on the ring (in arbitrary\\norder), ensuring that between any two of these gadgets\\na gap gadget is placed. Set all the demands that were\\nnot mentioned to zero. Note that b < (q+ 1)Œ≤and\\nall numbers are polynomially bounded in the input.\\nCorrectness. We show that ( G, k) is a yes-instance of\\nVertex Cover if and only if ( G‚Ä≤, D, b ) is a yes-instance\\nofMWASP .\\n‚Äú‚áí‚Äù: Let S‚äÜVbe a vertex cover of size at most k\\nforG. Denote S={s1, . . . , s k} ‚äÜV. Then add to G‚Ä≤\\nthe following matching M: For each pair {u, v}with\\ndemand D(u, v) =Œ≤add{u, v}toM. Moreover, for\\neach vertex si‚ààSadd the edge {d(m‚àík)/2+i, hsi}. We\\nclaim that this matching incurs a cost of at most b. By\\nconstruction, each pair {u, v}with demand D(u, v) =\\nŒ≤has distance one in G‚Ä≤‚à™M. Thus, these pairs\\ncontribute qŒ≤to the cost. The only other non-zero\\ndemands are between the demand nodes and the middle\\nnodes in the edge gadgets. Consider demand node d\\nhaving demand one to emid. Since Sis a vertex cover,\\nthere is an s‚ààSwith e‚à©sÃ∏=‚àÖ. Hence, the edge {d‚Ä≤, s}\\nis in Mfor some demand node d‚Ä≤. Moreover, Malso\\ncontains the edge {si, e‚Ñì}or the edge {si, er}for some\\ni‚àà[3]. Thus, in G‚Ä≤‚à™Mthere is the path Pfrom d\\ntoemidviad‚Ä≤,hs,si, and either erore‚Ñì. Every demand\\nnode has a distance less than mto each other demand\\nnode in G‚Ä≤. Moreover, erande‚Ñìhave distance Œ±toemid.\\nHence, Phas distance at most m+Œ±+ 3. This gives a\\ncost of at most m(m+Œ±+ 3) for the demands of value\\none. Thus, the overall cost is at most b.\\n‚Äú‚áê‚Äù: Let Mbe a matching added to G‚Ä≤such that\\nthe cost is at most b. We claim that in Gthere is a\\nvertex cover S‚äÜV,|S| ‚â§k, formed by the vertices\\nwhose helper nodes are matched to demand nodes in M,\\nformally, S:={v‚ààV| {di, hv} ‚ààM‚àßi‚àà[m]}.\\nDenote with M‚Ä≤the pairs {u, v}with de-\\nmand D(u, v) = Œ≤. Since b < (q+ 1)Œ≤, it follows\\nthatM‚Ä≤‚äÜM. The only nodes not matched in M‚Ä≤arek\\ndemand nodes di,i‚àà {(m‚àík)/2+1, . . . , m ‚àí(m‚àík)/2},\\nand all helper nodes. Thus, |S| ‚â§k. It remains to showthatSis indeed a vertex cover.\\nThe remaining budget for the demands between the\\ndemand nodes and the middle nodes is m(Œ±+m+ 3).\\nNote that in G‚à™M‚Ä≤each middle node emidfor the\\nedge e={u, v} ‚ààEhas distance ( Œ±+ 1) to the two\\nhelper nodes hvandhuand distance more than 2 Œ±to\\nall other helper nodes and the demand nodes. Thus,\\ninG‚à™Mthe distance between any demand node and\\nany middle node is more than Œ±. By the choice of Œ±, we\\nhave that ( m+1)Œ± > m (Œ±+m+3). Thus, each demand\\nnode dhas a path of length strictly less than 2 Œ±to the\\nmiddle node emidwith D(d, emid) = 1. Hence, for each\\nedge e={u, v} ‚ààEat least one of hvandhuneeds\\nto be matched to a demand node. Thus, Sis indeed a\\nvertex cover in G.\\n4 Approximation Algorithm\\nIn this section, we discuss the main algorithm of this\\npaper, which provably retains a constant factor approx-\\nimation for a critical class of demand matrices. The\\nmain idea behind our algorithm is to benefit from the\\nfact that shifting our view to design a higher degree net-\\nwork first and then transferring that result into match-\\ning opens up new possibilities.\\nWe first define new terminologies that we need to\\ndescribe our approximation algorithm, then go over the\\nalgorithm and at the end, we show the approximation\\nfactor of our algorithm.\\n4.1 Preliminaries. We now go over terminologies\\nthat we need in the design of our algorithm.\\nDemand graph. The demand graph is a weighted\\ngraph. It is built by considering the demand matrix\\nas the adjacency matrix of the graph. However, if two\\nendpoints have already an edge in the infrastructure,\\nwe do not consider an additional edge in the demand\\ngraph.\\nSuper-graph and super-node. The super-graph is\\nan undirected simple graph, consisting of super-nodes.\\nA super-node is a collection of nodes of an underlying\\ngraph. We consider the case that each super-node\\ncontains exactly Œ±nodes of the infrastructure graph,\\nfor a fixed 1 < Œ±‚â§n.‚àó\\nSimilarly, we can define the demand graph for the\\nsuper-graph. We call that super-demand-graph.\\n4.2 Algorithm. Now we introduce our algorithm,\\nSpiderDAN‚Ä†. We start by discussing how super-nodes are\\n‚àóWe point out that the fixed super-node size gives us the\\nbounds that we are looking for, but acknowledge that one can\\nconsider a variable with different super-node sizes.\\n‚Ä†Similar to how a spider creates its web from a combination\\nof threads (matchings). DAN refers to Demand-Aware Network.1\\n2\\n3\\n46\\n7 89101314\\n1112\\n5(a)\\n1\\n2\\n3\\n46\\n7 89101314\\n1112\\n5 (b)\\n1\\n2\\n3\\n46\\n7 89101314\\n1112\\n5 (c)\\n14131210\\n119876\\n5\\n4\\n3\\n2\\n1 (d)\\n14131210\\n119876\\n5\\n4\\n3\\n2\\n1 (e)\\nFigure 2: Steps of SpiderDAN considering a graph with 14 nodes (Figure 2a), and super-nodes of size Œ±= 3.\\nWe first run a depth-first search (DFS) from the node in the upper-right, shown in blue (Figure 2b). We then\\ndo a pre-order traversal of the tree from one of the deepest leaves, collecting nodes in groups of 3(Figure 2c, we\\nconsider the last group to have size only 2). Figure 2d shows a DAN is built on top of the super-graph, and lastly\\nin Figure 2e we show how this DAN can be transformed back into matchings.\\nformed, and then discuss how a demand-aware network\\n(DAN) can be built on top of it. We conclude by\\ndiscussing how the DAN on the super-graph can be\\ntransformed into matchings.\\nSuper-node creation. From our infrastructure graph\\nG(that can be any connected graph, an example in\\nFigure 2a) we create a super-graph Sin two steps:\\n1. Our algorithm first creates a spanning tree on the\\ninfrastructure graph (e.g. by running a Depth First\\nSearch ) from a node of G(Figure 2b).\\n2. To form each super-node, we take the deepest node,\\nand we consider the subtree of its grand-parent at\\ndistance Œ±. Then we select and delete Œ±arbitrary\\ndeepest nodes from this subtree one by one, and\\nadd them to the super-node and then remove them\\nfrom the sub-tree. We repeat this procedure until\\nwe group all nodes into super-nodes (Figure 2c).\\nLet us assume the output of the above steps as the\\nnode mapping fS. For the super-graph Sthe node\\nset is S(V) = fS(V(G)) and the edge set E(S) =\\n{{fS(u), fS(v)} |fS(u)Ã∏=fS(v),{u, v} ‚ààE(G)}. For\\nsimplicity, in the rest of this section we assume nto\\nbe divisible by Œ±(later on, we will discuss how we\\novercome this assumption in practice, as our super-node\\ncreation algorithm can run without this assumption).\\nThe entries of the demand DSfora, b‚ààV(S) are\\nDS(a, b) =X\\nu,v‚ààV\\nfS(u)=a,fS(v)=bDu,v\\nDAN on super-graph. After creating the super-\\ngraph, we use the construction of Avin et al. [4] to\\nbuild a demand-aware network (DAN) on top of the\\nsuper-nodes. We consider a degree at most 12 ¬∑‚àÜavgon\\ntop of the super-graph, where ‚àÜ avgdenotes the average\\ndegree of the super-demand-graph, that is the number\\nof non-zeros per row/column in the matrix.Matching assignment inside a super-node. We\\nnow transform the edges of the DAN on top of super-\\nnodes into a matching on the original graph. Thus, for\\neach super-node vwe can map each incident edge to a\\ndifferent node in f‚àí1\\nS(v) and obtain a matching in G.\\nSee a visualization in Figure 2e. In this transformation,\\nwe try to not add an edge that already exists in the\\ninfrastructure graph.\\n4.3 Analysis. In this section, we prove that\\nSpiderDAN on very sparse demands and constant de-\\ngree infrastructure graphs computes a constant factor\\napproximation, and discuss its running time at the end.\\nFor analytical purposes, we restrict ourselves to demand\\ngraphs with an average degree of at most 1 /12 (we dis-\\ncuss how this assumption can be improved later). Note\\nthat the constructed DAN has a maximum degree of\\n12 ‚àÜ avg‚â§12, by assumption.‚Ä°\\nLemma 4.1. (‚ãÜ) Any two nodes within a super-node\\nhave a distance of at most 2Œ±in the infrastructure\\ngraph.\\nApproximation ratio . To compute the approxima-\\ntion ratio of SpiderDAN , we first require a few relations\\nbetween adding a matching to a graph and demand-\\naware networks for the super-graph.\\nLemma 4.2 establishes a lower bound on the match-\\ning cost using demand aware networks for the demand\\nDS.\\nLemma 4.2. (‚ãÜ) Given a graph Gwith demand D,\\nand a super-graph Swith corresponding demand DS\\nobtained by merging Œ±nodes into super-nodes as defined\\nby the node mapping fS. For any matching Mon\\nV(G), there exists a demand-aware network HSforDS\\nwith maximum degree at most Œ±(1 + ‚àÜ( G))such that\\nObjD(G+M)‚â•ObjDS(HS).\\n‚Ä°The proofs of statements marked by ‚ãÜare deferred to an\\nappendix.Given a demand-aware network for the super-graph\\nwith maximum degree at most Œ±, then a matching on\\nthe infrastructure graph can be found with cost only a\\nconstant higher than that of the demand-aware network.\\nLemma 4.3. Given a graph Gand demand D, and a\\nsuper-graph Swith corresponding demand DSobtained\\nby merging Œ±nodes into super-nodes as defined by the\\nnode mapping fS. For any demand-aware network HS\\nwith maximum degree at most Œ±there exists a matching\\nMonV(G)such that ObjD(G+M)‚â§7Œ±¬∑ObjDS(HS).\\nProof. We construct a matching Min the following way:\\ninitially M=U=‚àÖ. For each edge {a, b} ‚ààE(HS) pick\\nanyu‚ààf‚àí1\\nS(a) and v‚ààf‚àí1\\nS(b) with u, v /‚ààU, further\\nadd{u, v}toM, and add u, vtoU. Since ‚àÜ( HS)‚â§Œ±\\nand|f‚àí1\\nS(c)|=Œ±for all c‚ààV(HS), this algorithm will\\nalways find a matching Mwith|M|=|E(HS)|.\\nNow consider any u, v‚ààVand let PS=a0, . . . , a ‚Ñì\\nbe a shortest path between fS(u) and fS(v) in HS.\\nFor each edge {ai, ai+1}of the path PSwe have a\\ncorresponding matching edge {vi\\ns, vi+1\\nt}inM. W.l.o.g.\\nassume fS(vi\\ns) =aiandfS(vi+1\\nt) =ai+1. We denote\\nbyx‚àíya shortest path between xandyinG. We\\ncan find a walk from utovof the following form:\\nP=u‚àív0\\ns, v1\\nt‚àív1\\ns, v2\\nt, . . . , v‚Ñì‚àí1\\ns, v‚Ñì\\nt‚àív. The ‚Äú ‚àí‚Äù parts\\nofPare paths along edges of the infrastructure graph,\\nbetween two nodes that are merged into the same super-\\nnode, which means these parts have length at most 2 Œ±.\\nThe ‚Äú,‚Äù parts of Pare always edges from M. This\\nmeans Phas length at most ( ‚Ñì+ 1)(2 Œ±) +‚Ñì. This is at\\nmost 2 Œ±for‚Ñì= 0 and at most 5 Œ±‚Ñìfor‚Ñì‚â•1.\\nLetH=G+M. From the above, it follows that\\nObjD(H) =X\\nu,v‚ààVdistH(u, v)Du,v‚â§\\nX\\nu,v‚ààV(2Œ±+ 5Œ±distHS(fS(u), fS(v)))Du,v\\n= 2Œ±+ 5Œ±ObjDS(HS)‚â§7Œ±ObjDS(HS)\\nWhere the last inequality holds, because Obj‚àó(‚àó)‚â•1\\nfor any demand and any graph.\\nAn important property of demand-aware networks\\nis that their cost can be lower bounded by a metric\\nrelated to the demand matrix, namely conditional en-\\ntropy, with a logarithmic factor in the maximum degree.\\nLemma 4.4. (DAN lowerbound [4]) For any\\ndemand Dand any demand-aware network G\\nwith maximum degree at most ‚àÜit holds that\\nfCE(D)/log2(‚àÜ + 1) ‚àí1‚â§ObjD(G), where fCE(D)\\nis the conditional entropy of the demand matrix,\\nthat is fCE(D) =P\\nv‚ààVdv¬∑P\\nu‚ààVlog2dv\\nDu,vwith\\ndv=P\\nu‚ààVDu,v.We are now ready to prove the main statement of\\nthis section.\\nTheorem 4.1. Given a graph Gwith a demand graph\\nDof average degree at most1\\nŒ±, then for Œ±= 12 the\\nSpiderDAN computes a matching Mfor which\\nObjD(G+M)‚â§c¬∑min\\nmatching M‚Ä≤\\nonV(G)ObjD(G+M‚Ä≤)\\nfor some non-negative constant cthat depends only on\\nŒ±and‚àÜ(G).\\nProof. LetSbe the super-graph constructed from G\\nandfSthe corresponding node to super-node mapping.\\nThe super-demand DShas an average degree at most 1.\\nGiven DSas input, the algorithm by Avin et al. [4]\\ncomputes a demand-aware network HSwith maximum\\ndegree at most 12, whose expected path length under\\nDSis optimal up to a constant factor. More specifically,\\nObjDS(HS) + 1‚â§c1¬∑fCE(DS), for some non-negative\\nconstant c1that depends only on Œ±. Furthermore, one\\ncan see that ObjDS(HS)‚â§2¬∑c1¬∑fCE(DS), because\\nObj‚àó(‚àó)‚â•1 for any demand and any graph. This in\\ncombination with Lemma 4.4 means it is a constant\\nfactor approximation for an Œ±-degree demand-aware\\nnetwork for DS.\\nUsing Lemma 4.3 on HSwe obtain a matching M\\noverV(G) such that ObjD(G+M)‚â§7Œ±¬∑ObjDS(HS)‚â§\\n14¬∑Œ±c1¬∑fCE(DS). From the well-known decomposition\\n(or grouping) property of entropy [4, 9], which intu-\\nitively means that merging two probability values into\\none by adding them, does not increase entropy, it fol-\\nlows that f CE(DS))‚â§fCE(D) and consequently we have\\nObjD(G+M)‚â§14¬∑Œ±c1¬∑fCE(D).\\nFrom Lemma 4.4 and Lemma 4.2 we know that the\\ncost of an optimal matching Moptis bounded from below\\nby f CE(D)/log2(Œ±(1 + ‚àÜ( G)) + 1) ‚àí1. By rearranging\\nterms and setting c2= 1/log2(Œ±(1 + ‚àÜ( G)) + 1) we\\nobtain c2¬∑fCE(D)‚â§ObjD(G+Mopt) + 1. Using again\\nthat Obj‚àó(‚àó)‚â•1 for any demand and any graph, we\\nobtain ObjD(G+Mopt)‚â•1\\n2c2fCE(D)), which proves\\nthat the matching Mcomputed by SpiderDAN is a\\nconstant factor approximation (with factor c= 28¬∑Œ±¬∑\\nc1/c2).\\nWe remark that Theorem 4.1 would be able to\\nachieve Œ±= 5 if we replace the algorithm from Avin\\net al. [4] by the recent algorithm from Figiel et al. [17];\\nthe proof is analogous. However, we point out that\\nthe result of this recent paper is not needed to ensure\\nconstant approximation of our algorithm.\\nRunning time. The running time of our algorithm is\\nO(n2¬∑logn). Grouping nodes into super-nodes using\\na depth-first search algorithm depends on the numberof edges of the infrastructure graph which is at most\\nO(n2).\\nDemand-graph requires a running time of O(n2), as\\nit requires going over the whole demand matrix. Then,\\nrunning the DAN algorithm [4] has a running time\\nofO(n2¬∑logn). Turning edges of DAN to matchings\\nalso takes O(n), as turning Œ±edges of a super node to\\nmatchings takes a constant time. Hence, in total, the\\nrunning time of the algorithm is O(n2¬∑logn)¬ß.\\n5 Heuristics\\nWe will experimentally compare SpiderDAN against sev-\\neral natural heuristics that we discuss below. Note that\\nmost of these heuristics do not provide any theoretical\\nguarantees on the solution quality.\\nGreedy. This algorithm matches pairs of nodes one by\\none. A pair of nodes is valid if it does not increase\\nthe degree of nodes to higher than 1. The Greedy\\nalgorithm starts by sorting all pairs of nodes based\\non their demand in descending order. Starting with\\nthe edge of highest demand, it considers the next valid\\nedge in the sorted list as the next matching edge. The\\nrunning time of this algorithm is O(n2¬∑logn), which\\ncomes from the sorting of the list. Among all heuristics,\\nthis is the easiest algorithm to implement, given that it\\ndoes not rely on other algorithms.\\nMatching on demand. This heuristic builds on top\\nof the well-known maximum weighted matching [18, 33].\\nTheMatching on demands algorithm uses the demand\\ngraph that we introduced before, in which each edge is\\nweighted by its demand, except the infrastructure edges.\\nThe algorithm then considers the maximum matching\\non the demand graph as its output. The running time\\nof this algorithm is O(n3), given the currently best\\nmaximum weighted matching algorithm [13].\\nSuperChord. Based on the idea of creating super-\\ngraphs, we aim to benefit from the well-known\\nChord [46] protocol. To this end, we create a super-\\ngraph by combining x=W(n¬∑ln 2)\\nln 2consecutive nodes,\\nwhere Wrepresents Lambert Wfunction, and lg the\\nnatural logarithm function¬∂. We then build the Chord\\non then\\nxsuper-nodes of the super-graph. The result-\\ning graph has degree log(n\\nx). We set x=W(n¬∑ln 2)\\nln 2as it\\nensures x= log(n\\nx). This allows us to view the log(n\\nx)\\noutgoing edges of a super-node as matching edges ini-\\ntiated from the nodes within the super-node. Observe\\n¬ßThe running time of the algorithm can be improved to\\nO(n¬∑logn), if the sparsity of demand matrix is known beforehand\\nand the demand matrix is given in a sparse representation.\\n¬∂If the number of super-nodes is not a power of two, we\\nconsider the super-graph with closest and smaller power of two as\\nthe number of super-nodes.that the super-graph can be created in linear time in n,\\nand adding xedges on each ofn\\nxsuper-nodes only takes\\nO(n) time. Thus, with a running time of Œò( n), the\\nalgorithm is relatively fast‚Äñ. We note that the Chord\\nalgorithm ensures a log ndiameter (considering degree\\nlogn) [46]. As the size of super-nodes is O(logn), we\\ncan ensure that any two nodes can reach each other via\\na shortest path of length O(logn), which in turn implies\\nthat SuperChord is an O(logn) approximation.\\n6 Experimental Evaluation\\nIn this section, we evaluate the weighted average short-\\nest path length and the running time of our approxima-\\ntion algorithm, the heuristics.\\nQ1. How fast are our algorithms in practice?\\nQ2. How do our algorithms perform on real-world de-\\nmands?\\nQ3. Under which demand parameters do our algorithms\\nperform better?\\nQ4. What is the effect of the underlying infrastructure\\ngraph on the performance of algorithms?\\nWe believe answering the above questions would help\\ndevelopers in selecting the right algorithm for their use\\ncases.\\n6.1 Demand matrices. To evaluate our results, we\\nconsider both real-world instances and synthetically\\ngenerated ones. By doing so, we first show the benefits\\nof each algorithm in the wild, and then suggest the best\\nalgorithmic choices for possible use cases in the future.\\nZipf distribution demand. The Zipf distribution [6]\\nhas shown to be an effective estimator for traffic fre-\\nquency distribution in data center networks [3, 49]. The\\nZipf distribution depends on a parameter Œ∂ >0, which\\nindicates the skewness of the distribution. With lower\\nvalues of Œ∂, the distribution is more skewed. We use\\na range Œ∂‚àà[2,10] for Zipf values in our evaluations.\\nConcretely, for given Œ∂andnelements, the probabil-\\nity mass function for an x‚àà[1, n] is determined by\\nf(x) =\\x00\\nxŒ∂¬∑\\x00Pn\\ni=11\\niŒ∂\\x01\\x01‚àí1. When running our experi-\\nments for various values of Œ∂, we normalize the sum to\\nensure the total demand remains the same.\\nSparse demand. In order to test our algorithms on\\nan even wider range of possibilities, we generate random\\ndemand matrices with controlled sparsity. For a sparsity\\nparameter Œ≥, we ensure that each cell of a matrix has\\nhigh demand (determined by the user, we considered\\n‚ÄñWe remark that computing the cost of this algorithm depends\\non the size of demand graph.100) with probability 1 ‚àíŒ≥. We consider the range\\nŒ≥‚àà[0.1,0.9] for sparsity values.\\nReal-world demands. Our code has been tested\\non a range of real-world data center traces [3] that\\nhas been the base of comparison for many previous\\nworks [5, 27, 41]. In particular, we focused on the\\nMeta (formerly known as Facebook) dataset [43]. This\\ndataset contains communication between racks and\\nservers within three data center clusters (Database,\\nWeb Services, and Hadoop, sorted by their number of\\nnodes) which we call AtoCrespectively. We focus\\non the communications between racks. We summarize\\neach dataset into a list, in which the frequency of\\ncommunication between each rack pair is listed.\\nFurthermore, we consider a set of 66 instances from\\nSuiteSparse matrix collection (formerly known as the\\nUniversity of Florida Sparse Matrix Collection) [12],\\ncovering various applications. We have chosen symmet-\\nric matrices with up to 10 ,000 rows, that also have pos-\\nitive values.\\n6.2 Infrastructure graphs. In our evaluations, we\\nhave used symmetric infrastructure graphs. We believe\\nthat the following infrastructure graphs can give us the\\ninsight that we need to incorporate our algorithms in\\nreal-world applications.\\nRing. A ring is the simplest symmetric infrastructure\\nthat ensures the connectivity of the graph. Hence, this\\nstructure echoes the effect of the added matching the\\nmost. The ring graph has been a backbone of fundamen-\\ntal advancements in the design of reconfigurable net-\\nworks [38, 46]. We believe the ring is a good candidate\\nto be the default infrastructure to run our experiments\\non: it has a high diameter, symmetric, and simple con-\\nnected graph, and hence can show the effect of various\\nalgorithms in cost reduction more clearly. We mention\\nexplicitly when using other infrastructure graphs in our\\nexperiments.\\n2D and 3D Torus. A torus is a natural extension of a\\nring, which ensures more connectivity in the infrastruc-\\nture graph. A torus is a grid that preserves symmetry\\nby connecting border nodes to each other. The grid-like\\nstructures have been the basis of previous studies on the\\neffect of adding a matching, for example when discussing\\nKleinberg‚Äôs model for small world networks [14, 32]. In\\nour evaluations, we considered both 2D torus structures\\n(where each node has 4 neighbors) and 3D torus struc-\\ntures (where each node has 8 neighboring nodes).\\n6.3 Results. Our code is written in python 3 .10,\\nbenefiting from networkx [25] and gurobipy [23] li-\\nbraries. Our visualizations use Matplotlib [29]. The\\ncode was executed on a machine with Intel¬ÆXeon¬Æ\\n102103\\nNumber of nodes104\\n103\\n102\\n101\\n100101Runnig time (s)\\nGreedy\\nMatching on demandsSuperChord\\nSpiderDANFigure 3: The running time of all of our algorithms is\\ndisplayed. The number of vertices are powers of two\\n(up to 4096 vertices). We have considered randomly\\ngenerated demand matrices with sparsity value 0 .9. We\\ncapped off the running times at 10 seconds, and showed\\nit in log-log format for better visibility.\\nCPU E5-1620 CPU with a clock frequency of 3.60GHz,\\nand 64GB RAM.\\nWe now focus on answering questions proposed at\\nthe beginning of the section, showing how our algo-\\nrithms behave given the above-mentioned demand ma-\\ntrices and infrastructure graphs. Before that, we want\\nto point out that we also tested an MIP-formulation (de-\\ntailed in Appendix B) solved with Gurobi optimizer [23].\\nWe do not include it in our plots as it hit our time\\nlimit of 1 hour per instance already on instances with\\n‚âà20 vertices. Notably, even the LP-relaxation could\\nnot solve the instances with ‚âà200 vertices within the\\ntime limit.‚àó‚àó\\nIn the SpiderDAN , when we transform edges of the\\nsuper-graph to matching, i.e., picking nodes inside cor-\\nresponding super-nodes to connect to each other, we se-\\nlect the pair of nodes that have highest demand between\\nthem. Furthermore, we noticed in our experiments that\\nfor algorithms that use super nodes, sometimes these\\nsupernodes do not cover all nodes in the infrastructure\\ngraph (i.e., the size of the infrastructure graph is not\\ndivisible by the size of a supernode). In such a case, we\\nrunMatching on demand on those remaining nodes to\\ncomplete the matching.\\nA1. Running time of algorithms. All our pro-\\nvided algorithms except Matching on demands are suf-\\nficiently fast on the graph sizes that we expect in prac-\\ntice (with up to a couple of thousands of nodes) with\\n‚àó‚àóWe tried the LP-relaxation for lower bounds. However, we\\ndo not discuss these bounds as we only can solve a few smaller\\ninstances with the LP and the LP-bound was on average a factor\\nof‚âà2 away from the MIP-solution (when we have both).FB-A FB-B FB-C\\nFacebook dataset102103104Average weighted distanceGreedy\\nMatching on demandsSuperChord\\nSuperDAN(a) Data sets from Facebook.\\nSuiteSparse datasets104\\n103\\n102\\n101\\n100101102103Average Weighted Distance\\nGreedy Matching on Demand SuperChord SpiderDAN (b) Data sets from SuiteSparse matrix collection.\\nFigure 4: Left: the results on the three datasets from Facebook. Right: Results for 66 instances from SuiteSparse\\nmatrix collection. The instances are sorted by the quality of SpiderDAN .\\nrunning times measured in seconds, see Figure 3 for a\\nplot. There are, however, notable differences between\\nthe algorithms. SuperChord is by far the fastest al-\\ngorithm as it essentially ignores the demands (which\\ncan be O(n2) many). Next are Greedy andSpiderDAN\\nthat show very similar running times. Matching on\\ndemands is clearly the slowest algorithm with the com-\\nputation of a maximum weight perfect matching being\\nthe overall most costly operation.\\nA2. Results on real-world data sets. Here we focus\\non the Facebook and SparseSuite datasets. As shown\\nin Figure 4a, the results are mixed on the Facebook\\ndataset. Matching on demands and Greedy perform\\nvery similar. SpiderDAN performs very well on clusters\\nA and C, but is a bit worse on cluster B. Previous\\nwork [43] has shown that cluster B has a higher average\\ndemand degree, which is the reason behind the slightly\\nhigher cost across different algorithms. Moreover, it\\naligns with our theoretical findings (see Theorem 4.1),\\nthat SpiderDAN performs better on low average demand\\ndegree.\\nGiven that SuperChord performs poorly on clusters\\nB and C, we suspect the demands to be very skewed in\\nthese instances. In contrast, SuperChord gives the best\\nresults on cluster A.\\nAs shown in Figure 4b, on sparse instances, our\\nalgorithms (except the demand-agnostic SuperChord )\\nperform more or less the same, but in most of instances\\nwhere there is a noticeable difference, SpiderDAN seems\\nto perform better.\\nA3. Effect of demand parameters. Here, we discuss\\nthe effects of the parameters of three synthetic demands,\\nnamely the sparsity value ( Œ≥), and the zeta value of the\\nZipf distribution ( Œ∂). We go through the details of the\\nresults for each parameter.‚Ä¢Sparsity of the random distribution. As\\nexpected, with more sparse demand our algorithms\\nprovide solutions of lower cost, see Figure 5a. A\\nsparser demand can cause a higher fraction of the\\ndemand being directly covered by the edges in\\nthe matching, that is, the demand pairs will have\\na distance of one, and hence a lower cost. On\\nthe other hand, the demand-agnostic SuperChord\\nperforms very well when faced with less sparse\\ndemand, especially in this case when the non-zero\\ndemands are equal. The random demands are quite\\nuniform, which benefits SuperChord .\\n‚Ä¢Zeta of the Zipf distribution. Similar to be-\\nfore, based on our results shown in Figure 5b, we\\ncan see that as the zeta of Zipf distribution grows,\\ni.e. data becomes more uniform, the algorithms\\nperform better. However, in contrast to the previ-\\nous case, here, the values of non-zero demands can\\nbe different, therefore SuperChord does not have\\nthe edge that it had beforehand, and SpiderDAN\\nperforms better than it and other algorithms.\\nA4. Effect of infrastructure graphs. Given the\\nselected set of infrastructures, we can observe how\\nan increase in the average degree of infrastructure\\ngraphs affects the cost of our algorithms. In particular,\\nwe believe our algorithms echo the inherent improved\\naverage distances in the infrastructure, as we saw a\\nrapid improvement going from a ring (essentially a 1D\\ntorus) to a 2D torus. However, the improvement is much\\nless when going from 2D to 3D torus, as can be seen in\\nFigure 5c. Moreover, it can be seen that the ranking of\\nthe algorithms by solution quality stays the same for 1D,\\n2D, and 3D torus. Hence, only considering the ring in\\nour other experiments highlights the differences betweenGreedy Matching on demands SuperChord SpiderDAN\\n0.1 0.3 0.5 0.7 0.9\\nSparsity ( ) Value \\n103104105Average Weighted Distance\\n(a) Varying sparsity.\\n1.75 2 4 8\\nZeta ( ) Value\\n101102103104Average Weighted Distance\\n (b) Varying zeta values.\\nRing 2D T orus 3D T orus\\nInfrastructures50100150200Average weighted distance\\n (c) Various infrastructures.\\nFigure 5: Effect of various parameters on the approximation ratio of algorithms (the cost of the respective\\nalgorithm divided by the cost of ring). Figure 5c considers on 4096 node (as it is even and a power of six, so we\\ncan have both 2D and 3D Torus with this size), and we used sparse instances with Œ≥= 0.9. The two other figures\\nare based on 4096 and 1024 nodes, due to the time limit that we set for each instance.\\nthe algorithms.\\n6.4 Summary & Outlook We first start by recap-\\nping the answers to the proposed questions:\\nA1. With SuperChord being the by far fastest algo-\\nrithm, SpiderDAN and Greedy are on shared sec-\\nond place and are still fast enough on the large\\nreal-world instances.\\nA2. Our results indicate that our algorithms using\\nsuper-graphs, in particular SpiderDAN , can be a\\ngood option to reduce the cost in real-world in-\\nstances, in conjunction with other greedy algo-\\nrithms.\\nA3. We observed that our algorithms can exploit the\\nunderlying demand structure, to provide close to\\noptimal outcomes. In particular, part of our algo-\\nrithms show promising results with high sparsity\\nand high zeta values.\\nA4. We observed that our algorithm can utilize the\\nunderlying infrastructure graph to enhance their\\noutcome, echoing the reduction in the average\\ndistance of the infrastructure graphs.\\nIn summary, we can recommend SuperChord for uni-\\nform demands: it provides the best and fastest solutions\\nin this case. However, for skewed demands especially\\nin real-world datasets, the heuristic can perform quite\\npoorly.\\nFor each heuristic there is a real-world dataset\\nwhere SpiderDAN provides solutions of better quality\\nthan the heuristic. Overall SpiderDAN provides com-\\nparable results to the heuristics; there is no instancewhere it is outperformed significantly. Given that it\\ncomes with some guarantees on the solution quality, we\\nrecommend it when solving real-world instances.\\n7 Conclusion\\nIn this paper, we tackled the problem of minimizing\\nthe demand-aware average shortest path, via matching\\naddition. Our goal is to augment the given physical\\nnetwork based on communication frequencies. We pro-\\nvided insights into its computational complexity and\\nexact and efficient algorithms. We started exploring\\nthe computational complexity of MWASP , showing NP-\\nhardness even in restricted cases and providing the\\nconstant-factor approximation algorithm SpiderDAN for\\nhighly skewed sparse demand matrices. To argue about\\ngeneral demand matrices we performed an extensive em-\\npirical evaluation. We thereby compared the SpiderDAN\\nalgorithm together with various heuristics to the exact\\nsolution (provided by a mixed integer program) on a\\nseries of real-world and synthetic datasets.\\nOur paper opens interesting directions for future\\nwork. Observe that our SpiderDAN is designed for low\\naverage degrees in the demand graph. The lower bound,\\non the other hand, is based on conditional entropy and\\nis constant for low-entropy demand matrices. An open\\nquestion is whether it is possible to find a constant ap-\\nproximation for certain low entropy demand matrices\\nthat are not covered by our method. One particularly\\nengaging example is the ring demand graph where the\\ninfrastructure graph is a different ring. As a contribu-\\ntion to the research community, and to ensure repro-\\nducibility, we open-source our code and experimental\\nartifacts at the following URL:\\nhttps://github.com/inet-tub/SuperDANReferences\\n[1] Florian Adriaens and Aristides Gionis. Diame-\\nter minimization by shortcutting with degree con-\\nstraints. In ICDM , 2022.\\n[2] Noga Alon, Andr¬¥ as Gy¬¥ arf¬¥ as, and Mikl¬¥ os Ruszink¬¥ o.\\nDecreasing the diameter of bounded degree graphs.\\nJ. Graph Theory , 2000.\\n[3] Chen Avin, Manya Ghobadi, Chen Griner, and\\nStefan Schmid. On the complexity of traffic traces\\nand implications. In ACM SIGMETRICS , 2020.\\n[4] Chen Avin, Kaushik Mondal, and Stefan Schmid.\\nDemand-aware network designs of bounded degree.\\nDistributed Computing , 2020.\\n[5] Marcin Bienkowski, David Fuchssteiner, and Stefan\\nSchmid. Optimizing reconfigurable optical data-\\ncenters: The power of randomization. In SC. ACM,\\n2023.\\n[6] Lee Breslau, Pei Cao, Li Fan, Graham Phillips,\\nand Scott Shenker. Web caching and zipf-like\\ndistributions: Evidence and implications. In IEEE\\nINFOCOM , 1999.\\n[7] F. R. K. Chung and M. R. Garey. Diameter bounds\\nfor altered graphs. J. Graph Theory , 1984.\\n[8] Fan RK Chung. Diameters of graphs: Old prob-\\nlems and new results. Utilitas Mathematica Pub.\\nIncorporated, 1987.\\n[9] Thomas M. Cover and Joy A. Thomas. Elements\\nof information theory (2. ed.) . Wiley, 2006.\\n[10] Wenkai Dai, Klaus-Tycho Foerster, David\\nFuchssteiner, and Stefan Schmid. Load-\\noptimization in reconfigurable data-center\\nnetworks: Algorithms and complexity of flow\\nrouting. ACM Trans. Model. Perform. Eval.\\nComput. Syst. , 2023.\\n[11] Wenkai Dai, Mike Dinitz, Klaus-Tycho Foerster,\\nLong Luo, and Stefan Schmid. Approximation\\nalgorithms for minimizing congestion in demand-\\naware networks. In IEEE INFOCOM , 2024.\\n[12] Timothy A. Davis and Yifan Hu. The university of\\nflorida sparse matrix collection. ACM Trans. Math.\\nSoftw. , 2011.\\n[13] Ran Duan and Seth Pettie. Linear-time approxi-\\nmation for maximum weight matching. J. ACM ,\\n2014.[14] David A. Easley and Jon M. Kleinberg. Networks,\\nCrowds, and Markets - Reasoning About a Highly\\nConnected World . Cambridge University Press,\\n2010.\\n[15] Nathan Farrington, George Porter, Sivasankar\\nRadhakrishnan, Hamid Hajabdolali Bazzaz,\\nVikram Subramanya, Yeshaiahu Fainman, George\\nPapen, and Amin Vahdat. Helios: a hybrid\\nelectrical/optical switch architecture for modular\\ndata centers. In ACM SIGCOMM , 2010.\\n[16] Nathan Farrington, George Porter, Sivasankar\\nRadhakrishnan, Hamid Hajabdolali Bazzaz,\\nVikram Subramanya, Yeshaiahu Fainman, George\\nPapen, and Amin Vahdat. Helios: a hybrid\\nelectrical/optical switch architecture for modular\\ndata centers. ACM SIGCOMM CCR , 2011.\\n[17] Aleksander Figiel, Janne H Korhonen, Neil Olver,\\nand Stefan Schmid. Demand-aware network design\\nwith steiner nodes and a connection to virtual net-\\nwork embedding. arXiv preprint arXiv:2308.10579 ,\\n2023.\\n[18] Zvi Galil. Efficient algorithms for finding maximum\\nmatching in graphs. ACM Comput. Surv. , 1986.\\n[19] Michael R. Garey and David S. Johnson. Comput-\\ners and Intractability: A Guide to the Theory of\\nNP-Completeness . Freeman, 1979.\\n[20] Kiran Garimella, Gianmarco De Francisci Morales,\\nAristides Gionis, and Michael Mathioudakis. Re-\\nducing controversy by connecting opposing views.\\nInACM WSDM , 2017.\\n[21] Andrew Gozzard, Max Ward, and Amitava Datta.\\nConverting a network into a small-world network:\\nFast algorithms for minimizing average path length\\nthrough link addition. Information Sciences , 2018.\\n[22] Elena Grigorescu. Decreasing the diameter of\\ncycles. J. Graph Theory , 2003.\\n[23] Gurobi Optimization, LLC. Gurobi Optimizer Ref-\\nerence Manual, 2023. URL https://www.gurobi.\\ncom.\\n[24] Shahrzad Haddadan, Cristina Menghini, Matteo\\nRiondato, and Eli Upfal. Repbublik: Reducing\\npolarized bubble radius with link insertions. In\\nWSDM , 2021.\\n[25] Aric A. Hagberg, Daniel A. Schult, and Pieter J.\\nSwart. Exploring network structure, dynamics, and\\nfunction using networkx. In SciPy , 2008.[26] Matthew Nance Hall, Klaus-Tycho Foerster, Stefan\\nSchmid, and Ramakrishnan Durairajan. A survey\\nof reconfigurable optical networks. In OSN, 2021.\\n[27] Kathrin Hanauer, Monika Henzinger, Stefan\\nSchmid, and Jonathan Trummer. Fast and heavy\\ndisjoint weighted matchings for demand-aware dat-\\nacenter topologies. In IEEE INFOCOM , 2022.\\n[28] Kathrin Hanauer, Monika Henzinger, Lara Ost,\\nand Stefan Schmid. Dynamic demand-aware link\\nscheduling for reconfigurable datacenters. In IEEE\\nINFOCOM , 2023.\\n[29] John D. Hunter. Matplotlib: A 2d graphics\\nenvironment. Comput. Sci. Eng. , 2007.\\n[30] Ruben Interian, Jorge R. Moreno, and Celso C.\\nRibeiro. Polarization reduction by minimum-\\ncardinality edge additions: Complexity and integer\\nprogramming approaches. Int. Trans. Oper. Res. ,\\n2021.\\n[31] Jon Kleinberg. The small-world phenomenon: an\\nalgorithmic perspective. In STOC , 2000.\\n[32] Jon M Kleinberg. Navigation in a small world.\\nNature , 2000.\\n[33] Tomohiro Koana, Viatcheslav Korenwein, Andr¬¥ e\\nNichterlein, Rolf Niedermeier, and Philipp\\nZschoche. Data reduction for maximum matching\\non real-world graphs: Theory and experiments.\\nACM J. Exp. Algorithmics , 2021.\\n[34] Janardhan Kulkarni, Stefan Schmid, and Pawel\\nSchmidt. Scheduling opportunistic links in two-\\ntiered reconfigurable datacenters. In ACM SPAA ,\\n2021.\\n[35] F Thomson Leighton. Arrays and trees. Introduc-\\ntion to Parallel Algorithms and Architectures , 1992.\\n[36] He Liu, Matthew K. Mukerjee, Conglong Li, Nico-\\nlas Feltman, George Papen, Stefan Savage, Srini-\\nvasan Seshan, Geoffrey M. Voelker, David G. An-\\ndersen, Michael Kaminsky, George Porter, and\\nAlex C. Snoeren. Scheduling techniques for hybrid\\ncircuit/packet networks. In CoNEXT , 2015.\\n[37] Adam Meyerson and Brian Tagiku. Minimizing\\naverage shortest path distances via shortcut edge\\naddition. In RANDOM , 2009.\\n[38] Vahab S. Mirrokni, Mikkel Thorup, and Morteza\\nZadimoghaddam. Consistent hashing with\\nbounded loads. In ACM-SIAM SODA , 2018.[39] Manos Papagelis, Francesco Bonchi, and Aristides\\nGionis. Suggesting ghost edges for a smaller world.\\nInACM CIKM , 2011.\\n[40] Nikos Parotsidis, Evaggelia Pitoura, and Panayi-\\notis Tsaparas. Selecting shortcuts for a smaller\\nworld. In SIAM SDM , 2015.\\n[41] Arash Pourdamghani, Chen Avin, Robert Sama,\\nand Stefan Schmid. Seedtree: A dynamically\\noptimal and local self-adjusting tree. In IEEE\\nINFOCOM , 2023.\\n[42] Leon Poutievski, Omid Mashayekhi, Joon Ong, Ar-\\njun Singh, Mukarram Tariq, Rui Wang, Jianan\\nZhang, Virginia Beauregard, Patrick Conner, Steve\\nGribble, et al. Jupiter evolving: transform-\\ning google‚Äôs datacenter network via optical circuit\\nswitches and software-defined networking. In ACM\\nSIGCOMM , 2022.\\n[43] Arjun Roy, Hongyi Zeng, Jasmeet Bagga, George\\nPorter, and Alex C Snoeren. Inside the social net-\\nwork‚Äôs (datacenter) network. In ACM SIGCOMM\\nCCR , 2015.\\n[44] Neta Rozen-Schiff, Klaus-Tycho Foerster, Stefan\\nSchmid, and David Hay. Chopin: Combining\\nDistributed and Centralized Schedulers for Self-\\nAdjusting Datacenter Networks. In OPODIS , 2023.\\n[45] A. A. Schoone, H. L. Bodlaender, and\\nJ. Van Leeuwen. Diameter increase caused\\nby edge deletion. J. Graph Theory , 1987.\\n[46] Ion Stoica, Robert Tappan Morris, David R.\\nKarger, M. Frans Kaashoek, and Hari Balakrish-\\nnan. Chord: A scalable peer-to-peer lookup ser-\\nvice for internet applications. In ACM SIGCOMM ,\\n2001.\\n[47] Guohui Wang, David G. Andersen, Michael\\nKaminsky, Konstantina Papagiannaki, T.S. Eu-\\ngene Ng, Michael Kozuch, and Michael Ryan. c-\\nthrough: part-time optics in data centers. In ACM\\nSIGCOMM , 2010.\\n[48] Duncan J Watts and Steven H Strogatz. Collective\\ndynamics of ‚Äòsmall-world‚Äônetworks. Nature , 1998.\\n[49] George Kingsley Zipf. Relative frequency as a\\ndeterminant of phonetic change. Harvard studies\\nin classical philology , 1929.A Omitted Proofs\\nIn this section, we overview the proofs omitted in the\\nmain body of the paper.\\nA.1 Proof of lemma 4.1\\nProof. We prove by induction on the number of super-\\nnodes created in the super-node creation algorithm.\\nWe prove a stronger statement than above: we prove\\nthat our algorithm additionally keeps the tree connected\\nafter removing already picked nodes.\\nWhen we have zero super-nodes, and we consider\\nthe deepest node in DFS, and all the nodes in the sub-\\ntree of its grand-parent of distance Œ±. We know that we\\ncan go from any of the nodes in the subtree to any other\\nnode by traversing at most 2 ¬∑Œ±edges (by simply going\\nto the grand-parent from the source node and then to\\nthe destination node). Furthermore, we have at least\\nŒ±nodes in this subtree. Hence, a super-node can be\\ncreated in this sub-tree, and the distance between any\\ntwo nodes is at most 2 ¬∑Œ±. Furthermore, by iteratively\\ngrouping the leaves into the super-node, we can ensure\\nthat the tree remains connected.\\nNow let us assume that we have selected ksuper-\\nnodes, and consider nodes that are being selected in\\nthe super-node k+ 1. Given that after ksuper-node\\ncreation the tree remains connected, we can simply use\\nsimilar arguments to the zero case, to show that the\\ndistance between nodes in the selected super-node is at\\nmost 2 ¬∑Œ±and the tree remains connected after grouping\\nnodes into k+ 1 super-node.\\nA.2 Proof of lemma 4.2\\nProof. Given H=G+MletHSbe the graph obtained\\nby merging nodes of Hinto super-nodes in the same way\\nthat Swas obtained from Gusing the node mapping\\nfS. The graph HShas maximum degree at most\\nŒ±(1 + ‚àÜ( G)).\\nConsider any u, v‚ààV(G) with uÃ∏=vand let\\nP=v0, . . . , v ‚Ñìbe a shortest path between uandvin\\nH, where v0=u, v‚Ñì=v. Let PS=fS(v0), . . . , f S(v‚Ñì),\\nwhich is a walk in HS. Therefore dist H(u, v)‚â•\\ndistHS(fS(u), fS(v)). We now have that\\nObjD(H) =X\\nu,v‚ààVdistH(u, v)¬∑Du,v‚â§\\nX\\nu,v‚ààVdistHS(fS(u), fS(v))¬∑Du,v= ObjDS(HS)Table 1: A summary of variables used in the MIP.\\nInput variables Description\\nDu,v Demand between nodes uandv.\\ndegu Degree of node u.\\nMIP variables Description\\nau,v Indicating whether the edge ( u, v) is\\nadded to the matching or not\\ndist(u, v) Distance between nodes uandv.\\nyu,v\\nw Binary variable indicating if shortest\\npath between uandvgoes through w\\nor not.\\nAlgorithm 1 Mixed Integer Program to Compute\\nOptimal Solution\\n1:MinimizeP\\nu,v‚ààVdist(u, v)¬∑Du,v\\n2:distu,v= 1 ‚àÄ(u, v)‚ààE\\n3:foru, v‚ààV&uÃ∏=v& (u, v)/‚ààEdo\\n4: yu,v\\nw‚àà {0,1}\\n5: au,v‚àà {0,1}\\n6: au,v=av,u\\n7: dist(u, v)‚â•1\\n8: dist(u, v)‚â§au,v+ (1‚àíau,v)¬∑M\\n9: dist(u, v)‚â§dist(u, w) + dist( w, v) ‚àÄw‚ààV\\n10: dist(u, v)‚â•dist(u, w) + dist( w, v) + (yu,v\\nw‚àí1)¬∑\\nM ‚àÄw‚ààV\\n11:P\\nw‚ààV\\\\{u,v}yu,v\\nw+au,v= 1\\n12:foru‚ààVdo\\n13:P\\n‚àÄv‚ààV&vÃ∏=u‚ààV& (u,v)/‚ààEau,v= 1\\n14: disu,u= 0\\nB Mixed Integer Program\\nIn this section, we detail our Mixed Integer Program\\n(MIP), Program 1, as an exact solution to the problem.\\nA summary of variables used in our MIP is in Table 1.\\nOur goal for this MIP is to minimize distance times\\nthe demand for each pair of nodes (Line 1). In doing\\nso, we respect the edges of the infrastructure graph by\\nsetting the distance between their two endpoints equal\\nto one (Line 2) and the distance of a node to itself is zero\\n(Line 14). Our MIP uses a symmetric binary variable\\nau,vto decide whether it wants to add a matching edge\\nbetween nodes uandv. It therefore goes over all pairs\\nof nodes uÃ∏=vthat do not already have an edge in the\\ninfrastructure graph in Line 3.\\nIn order to have a matching, each node should have\\none active edge (Line 13). Lines 7 and 8 ensure that an\\nactive edge has a distance of one. We then force shortest\\npath distances between all other nodes in Lines 9 to 11.',\n",
       " 'Probe-Me-Not: Protecting Pre-trained Encoders\\nfrom Malicious Probing\\nRuyi Ding, Tong Zhou, Lili Su, Aidong Adam Ding, Xiaolin Xu, Yunsi Fei\\nNortheastern University, Boston, MA 02115, USA\\n{ding.ruy, zhou.tong1, l.su, a.ding, x.xu, y.fei }@northeastern.edu\\nAbstract ‚ÄîAdapting pre-trained deep learning models to\\ncustomized tasks has become a popular choice for developers to\\ncope with limited computational resources and data volume. More\\nspecifically, probing‚Äìtraining a downstream head on a pre-trained\\nencoder‚Äìhas been widely adopted in transfer learning, which\\nhelps to prevent overfitting and catastrophic forgetting. However,\\nsuch generalizability of pre-trained encoders raises concerns\\nabout the potential misuse of probing for harmful intentions,\\nsuch as discriminatory speculation and warfare applications.\\nIn this work, we introduce EncoderLock, a novel applicability\\nauthorization method designed to protect pre-trained encoders\\nfrom malicious probing, i.e., yielding poor performance on\\nspecified prohibited domains while maintaining their utility in\\nauthorized ones. Achieving this balance is challenging because of\\nthe opposite optimization objectives and the variety of downstream\\nheads that adversaries can utilize adaptively. To address these\\nchallenges, EncoderLock employs two techniques: domain-aware\\nweight selection and updating to restrict applications on prohibited\\ndomains/tasks, and self-challenging training scheme that iteratively\\nstrengthens resistance against any potential downstream classifiers\\nthat adversaries may apply. Moreover, recognizing the potential\\nlack of data from prohibited domains in practical scenarios,\\nwe introduce three EncoderLock variants with different levels\\nof data accessibility: supervised (prohibited domain data with\\nlabels), unsupervised (prohibited domain data without labels), and\\nzero-shot (no data or labels available). Extensive experiments\\nacross fifteen domains and three model architectures demonstrate\\nEncoderLock‚Äôs effectiveness over baseline methods using non-\\ntransferable learning. Additionally, we verify EncoderLock‚Äôs\\neffectiveness and practicality with a real-world pre-trained\\nVision Transformer (ViT) encoder from Facebook. These results\\nunderscore the valuable contributions EncoderLock brings to the\\ndevelopment of responsible AI.\\nI. I NTRODUCTION\\nAs the complexity of learning tasks increases, leveraging\\npre-trained models becomes a popular strategy for developers\\nto train their customized models efficiently. Among various\\ntransfer learning methods, model probing has emerged as one\\nof the most common and lightweight strategies to utilize pre-\\nlearned knowledge effectively [ 4], [7], [81]. It involves freezing\\nthe encoder parts of pre-trained models while fine-tuning only\\nthe downstream heads. The encoders often include early layers\\nof pre-trained models with more complex structures, which is\\nresponsible for extracting useful information from raw data\\nto latent representations, on which downstream heads perform\\nspecific tasks such as classification and generation [32], [47].\\nProbing offers several advantages, including resource effi-\\nciency, because of its low requirements on data and computa-\\ntional resources, and semantic consistency, as it helps avoid\\ncatastrophic forgetting‚Äìthe performance reduction due to theencoder‚Äôs loss of pre-learned knowledge after extensive fine-\\ntuning [ 12], [13], [39]. Furthermore, probing allows the pre-\\ntrained encoder to be used as a black-box, either as local private\\nmodels [ 21], [29], [51] or cloud services through APIs [ 14],\\n[58], [63], ensuring better intellectual property protection [ 74].\\nNowadays, many companies, such as Clarifai [ 14] and Ope-\\nnAI [ 58], offer commercial encoder APIs, allowing users to\\ninput data and obtain latent feature vectors, which can then be\\nused for various downstream real-world applications.\\nHowever, the general availability of the pre-trained encoder\\nfor probing also raises concerns about malicious probing ,\\ni.e., users can probe the encoder for unethical or harmful\\ntasks [ 10]. Examples include building classification heads for\\ndiscriminatory speculation [ 40], [75] or autonomous weapons\\nin warfare applications [ 55]. To address these concerns, model\\nowners have set strict policies regarding the utilization of pre-\\ntrained encoders. For instance, OpenAI1explicitly prohibits\\nusers from employing their encoder services for ‚Äúany illegal,\\nharmful, or abusive activity‚Äù. However, relying solely on\\npolicies, without concrete technological barriers, is insufficient\\nto prevent model misuse. Considering malicious probing not\\nonly poses ethical risks but also represents a serious form\\nof infringement on the intellectual property of model owners,\\ndesign-time countermeasures are urgently needed for protecting\\nthe encoders with applicability authorization [79].\\nProactively preventing pre-trained encoders from malicious\\nprobing presents three challenges. Challenge 1: Integrity of\\nPre-trained Encoder. The protection strategy should maintain\\nthe encoder‚Äôs functionality on authorized domains (those\\nfor which the encoder is designed), while restricting misuse\\nonprohibited domains (those not allowed due to malicious\\nintent). Furthermore, it is advisable to have a small impact on\\nadmissible domains (those are gray-listed and not explicitly\\nconsidered during encoder design). Challenge 2: Robustness to\\nMalicious Probing. Malicious users can customize downstream\\nheads with various configurations (e.g., hyper-parameters and\\nclassifier architectures). The protection method must be robust\\nagainst these diverse setups. Challenge 3: Accessibility to\\nProhibited Domains. Effective protection requires pre-defined\\nprohibited domains, while a lack of samples from these domains\\ncan significantly impact its performance. A few studies design\\nprotection against direct applicability on prohibited tasks‚Äì\\nmalicious users can do inference but no further fine-tuning [ 78],\\n[79]. They introduced a training strategy considering solely a\\ngiven prohibited dataset with clear labels and an authorized\\ndataset, called Non-Transferable Learning (NTL) . Unfortunately,\\nNTL doesn‚Äôt apply to pre-trained encoders‚Äìas malicious users\\n1https://openai.com/policies/terms-of-usearXiv:2411.12508v1  [cs.CR]  19 Nov 2024EmbeddingsAuthorized \\nDomain\\nProhibited \\nDomainAcc. 95%\\nAcc. 80%\\nAuthorized \\nDomain\\nProhibited \\nDomainProbingAcc. 94%\\nAcc. 10%Pre-trained Encoder\\nProbing\\nSupervised EncoderLock\\nData + Label from \\nprohibited domain\\nUnsupervised EncoderLock\\nOnly Data from \\nprohibited domainZero -shot EncoderLock\\nNO data but descriptions of \\nprohibited  domain\\nEmbeddings\\nLocked Encoder\\nChanged weightsUnprotected Encoder\\n`\\nFig. 1. Applicability Authorization with EncoderLock : Fixed pre-trained\\nencoders accept user inputs and return representations. Users can utilize them\\nfor various customized tasks by probing with downstream heads. EncoderLock\\naims to prevent malicious probing to pre-defined prohibited domains, which\\nmay have different levels of data accessibility, marked by different colors.\\ncan further probe encoders with downstream heads using\\nprohibited data. Therefore, we propose EncoderLock , a new\\napplicability authorization strategy for pre-trained encoders.\\nEncoderLock is based on our new three-level threat model\\nfor model applicability authorization for pre-trained encoders,\\nfollowing a paradigm akin to that used in representation\\nlearning, as illustrated in Fig. 1: a) Level 1 Label-enriched :\\nThe provider has a labeled dataset of the prohibited domain, b)\\nLevel 2 Label-free : The provider only has an unlabeled dataset,\\nc)Level 3 Theme-only : The provider has no data but knows\\nthe theme they wish to exclude the encoder from processing.\\nThese levels represent real-world model providers with different\\ndata accessibility. Throughout the paper, we will use this color\\ncoding to represent these data accessibility levels.\\nEncoderLock proposes the following solutions to address\\nall three challenges against malicious probing. First, we\\npropose domain-aware weight selection and updating , which\\nidentifies critical weights to the target domain and adjusts\\nthem, successfully restricting the model‚Äôs transferability to the\\ntarget domain while minimizing its effect on other authorized\\ndomains (addressing Challenge 1). To ensure the robustness\\nof EncoderLock against customized malicious downstream\\nheads (addressing Challenge 2), we introduce a minimax\\noptimization‚Äì self-challenging training , which refines the en-\\ncoder‚Äôs feature space iteratively by continuously adjusting\\nauxiliary downstream heads. Together, these strategies constitute\\nsupervised EncoderLock , which effectively addresses the Level\\n1 scenario. To address Challenge 3, we extend two EncoderLock\\nvariants for stricter accessibility to the prohibited domain. For\\nLevel 2 where only an unlabeled target dataset is available,\\nwe introduce unsupervised EncoderLock , including a novel\\nregularization term based on contrastive loss in the feature\\nspace, which deliberately obfuscates features in the target\\ndataset. For Level 3, we propose zero-shot EncoderLock , which\\nleverages an AI agent and a text-to-image generative model\\nto build a reliable pathway from semantic description to an\\nunlabeled synthetic dataset. To ensure the synthetic dataset\\nis representative of the target domain and comprehensive, wepropose a prompt refining method utilizing the AI agent.\\nOur Contributions: We propose EncoderLock , a novel\\nand proactive protection on the pre-trained encoder against\\nmalicious probing. The contributions of this work include:\\n1)EncoderLock provides a robust applicability authorization\\nframework to owners of pre-trained encoders. It maintains\\nthe encoder‚Äôs performance on authorized domains with the\\ndomain-aware weight selection algorithm and offers robust\\ndefense against diverse customized probing through a self-\\nchallenging training scheme.\\n2)We propose a three-level threat model following the practical\\ndata availability of representation learning. Correspondingly,\\nwe present three variants of EncoderLock with novel\\ntechniques to address different levels of target domain data\\naccessibility, tackling realistic comprehensive scenarios.\\n3)We conduct extensive experiments to evaluate EncoderLock\\nacross twelve domains and three encoder architectures,\\nincluding a large, real-world Vision Transformer [ 8]. Our\\nresults demonstrate the effectiveness of all three Encoder-\\nLock variants. Specifically, we assess EncoderLock in a real\\napplicability authorization scenario, preventing a pre-trained\\nencoder from being misused for military purposes while\\nkeeping its generalizability to civilian ones.\\nII. B ACKGROUND\\nA. Pre-trained Encoders and Model Probing\\nPre-trained models are widely used in computer vision [ 54],\\n[59], [86], representation learning [ 5], [70], [85], [90], and\\nnatural language processing [ 31], [62], [77], which embed pre-\\nlearned knowledge as the model initialization to reduce the\\ncomplexity in training new tasks. Taking transfer learning in\\nvision tasks as an example, there are three common strategies:\\nFull Fine-tuning : Full fine-tuning leverages the entire pre-\\ntrained model as the training initialization and fine-tunes it\\nwith the target dataset. It often has good performance but has\\nthe risk of stability and catastrophic forgetting [12], [73].\\nPrompting : Rather than finetune the model parameters, prompt-\\ning redirects the pre-trained model via modification on the\\ninputs (i.e., visual prompt). Prompting is efficient but perfor-\\nmance experiences a larger degradation [ 3], [23], [43], [83].\\nModel Probing : Probing freezes the early layers of the pre-\\ntrained model (e.g., deep convolutional layers or self-attention\\nlayers [ 17], [30]) as the fixed pre-trained encoder and fine-tunes\\nthe downstream classifier. It has a small training cost and high\\nstability of the training process [4], [26], [66], [82].\\nIn this work, we focus on model probing as it is more\\nefficient and stable than fully fine-tuning and has better\\nperformance than prompting. Probing also supports pre-trained\\nencoders from different training schemes, which can be\\ncategorized into three types: supervised ,unsupervised , and self-\\nsupervised . For supervised learning, the model (i.e., encoder and\\ndownstream head) is trained directly using labeled training data\\nand a loss function (e.g., cross-entropy loss) [ 18]. Unsupervised\\nlearning aims to learn from unlabeled data, using methods\\nsuch as Gaussian Mixtures Model (GMM) [ 67], Variational\\nAutoencoder (V AE) [ 45], and Generative Adversarial Network\\n(GAN) [ 27]. Self-supervised learning aims to train an encoder to\\npredict one part of data given another part of the input [ 11], [33],[41]. It leverages the inherent data characteristics and shows\\nincreasing robustness and generalizability of the encoder [ 35].\\nSpecifically, given the input (e.g., an image), one will use\\ndata augmentation operations (e.g., cropping, color jitter, and\\nadding random noise) to build augmented images. The training\\nobjective is to make the encoder generate similar embeddings\\nfor augmented images from the same input, denoted as positive\\npairs; while ensuring the discrepancy of embeddings from\\ndifferent images, denoted as negative pairs. The training of\\na self-supervised encoder utilizes contrastive loss [ 11], [33],\\nwhich increases the similarity between positive pairs but\\ndecreases those of negative pairs. Our design of EncoderLock\\nconsiders various data accessibility of prohibited domains,\\nwhich aligns with the training process of pre-trained encoders‚Äì\\nwith or without labeled datasets.\\nB. Applicability Authorization\\nRecently, applicability authorization, a new IP protection\\nscheme, has been proposed to address the rising concerns of IP\\ninfringement on DNN models [ 78], [79], [91], [20]. Traditional\\nmodel IP protection aims to protect the rights of owners of\\nDNN models with two typical defense strategies: ownership\\nverification and usage authorization. Ownership verification is\\ndesigned to trace the illegal behavior of IP infringement using\\nmethods such as embedding watermarks during the training\\nprocedure or recording fingerprints of the model owner [ 42],\\n[84], [88]. In contrast, usage authorization aims to restrict\\nuser access to the model, ensuring that only verified, trusted\\nusers can access with assigned authorization keys [ 2], [9].\\nInstead of protecting the model parameters or hyper-parameters\\ndirectly like traditional methods, applicability authorization\\nfocuses on the unauthorized transfer of the pre-trained models\\n[91]. Specifically, it aims to prevent malicious transfer learning\\nthrough which an attacker can abuse the pre-trained model for\\nprohibited data or tasks, i.e., non-transfer-learning. In this work,\\nwe further propose EncoderLock to address the challenges of\\napplicability authorization of pre-trained encoders to safeguard\\nthem from unauthorized probing.\\nC. Non-Transferable Learning (NTL)\\nWang et al. [ 79] introduced NTL for applicability authoriza-\\ntion of an entire model without any fine-tuning. In particular,\\nNTL leverages a negative regularization term on the model‚Äôs\\ntarget domain performance:\\nLNTL =LS+RT (1)\\nwhere LSis the Kullback‚ÄìLeibler (KL) divergence/loss on\\nthe source dataset, aiming to retain the model‚Äôs performance\\non the source domain. Model non-transferability comes from\\nthe regularization term, defined as RT=‚àímin(Œ≤, Œ±¬∑LT¬∑\\nLdis)[79], where LTis the KL loss on the target dataset, Ldis\\nmeasures the feature space distance between the source and\\ntarget domains (using Maximum Mean Discrepancy), and Œ±\\nandŒ≤are scaling factors. Another prior work [ 78] proposes\\nan additional CUTI-domain for regularization on private style\\nfeatures with the RTas‚àíLT. In addition, previous works also\\nproposed ‚Äòsource-only‚Äô NTL for cases when there is no target\\ndata available. As the term ‚Äòsource-only‚Äô indicates, this strategy\\nleverages generative models (i.e., GAN) to create a synthetic\\ndataset, which serves as the boundary from the source domain\\nto prohibit the model‚Äôs transferability to all other domains.D. Limitations of Prior Works\\nPrior works focus on the case when the attacker uses\\nthe trained model directly but cannot fine-tune it [ 78], [79].\\nHowever, with the increasing popularity and low cost of probing\\nthe pre-trained (fixed) encoder, the applicability authorization\\n(model non-transferability) can be bypassed in a few probing\\nepochs. Moreover, previous methods predominantly add a\\nregularization term solely based on the model outputs. Although\\n[79] introduces a feature space distance as a regularization\\nbetween the source and target domains, they cannot ensure\\nrestriction as the class discrepancy might still be large on the\\nfeature space. Furthermore, previous methods only consider\\nthe case of supervised NTL, i.e., the defender has access\\nto one labeled target dataset and one labeled source dataset.\\nOur Challenge 3 is closer to the practical scenario where the\\ndefender lacks knowledge about the prohibited target. Pre-\\ntrained encoders and model probing bring new challenges\\nin data availability for applicability authorization. Our work\\nEncoderLock aims to address them accordingly.\\nIII. T HREAT MODEL\\nIn this work, we tackle applicability authorization for pre-\\ntrained encoders, aiming to prevent malicious users from\\nprobing the encoder for harmful tasks (i.e., unethical, illegal, or\\nabusive activities). In this paper, we focus on vision encoders\\nand image classification as the downstream task.\\nA. Malicious Users\\nThe attackers are users with malicious intent to breach the\\nusage policy of fixed pre-trained encoders with probing [28].\\nObjective. Their objective is to exploit pre-trained encoders for\\ntasks that are not allowed, specifically, accurately classifying\\nsamples from prohibited domains. Other forms of DNN IP\\ninfringement of the pre-trained encoder, e.g., model stealing\\nattacks, are out of the scope of this paper.\\nCapabilities. Capabilities of the malicious users include:\\n‚Ä¢They can probe a pre-trained encoder using inputs from\\nprohibited target domains and utilize the representations to\\ntrain a local downstream classifier for inference. Although\\nthey query the encoder (as a service or local private model), it\\nis a black-box with both structure and parameters unknown.\\n‚Ä¢Users can build their own downstream classifiers, customiz-\\ning the classifier‚Äôs hyper-parameters and fine-tuning the\\nparameters with any learning rates and optimizers.\\n‚Ä¢Following the common setting of probing, we assume that\\nthe attacker has a small amount of data from the prohibited\\ndomain for fine-tuning (e.g., 10% from the target domain).\\nB. Model Owner\\nModel owners aim to safeguard the pre-trained encoder\\nagainst malicious probing proactively. Following the common\\ndefinition of transfer learning, the dataset on which the encoder\\nis trained is defined as the source domain (authorized\\ndomain) , and the dataset that the (malicious) downstream\\nclassifier is trained for is the target domain (prohibited\\ndomain) . Moreover, we define data domains other than these\\ntwo as admissible domains , indicating that the usage of theUnsupervised EncoderLock\\nSection IV. D\\nùê∑ùëÜ\\nùê∑ùëáùêøùëíùëôùëêùëúùëõùë°Update critical weights\\nrotate\\nclip\\njitterrotate\\nclip\\njitterAugmentationsThe start of Round ùíì \\nfor EncoderLock\\nSupervised EncoderLock\\nSection IV. C\\nùê∑ùëÜ\\nùê∑ùëáùêøùëíùëôUpdate critical weights\\nùêøùëá\\nùê∂ùúÉùëáùëü\\nùê∂ùúÉùëÜ\\nùê∂ùúÉùëáùëü+1\\nSelf-challengingCopy \\nZero -shot EncoderLock\\nSection IV. E\\nùêøùëíùëôùëêùëúùëõùë°Update critical weights\\nrotate\\nclip\\njitterAugmentationsTheme\\n AI agent\\n Text-to-image ùê∑ùëá‚Ä≤Refine prompts\\nùê∑ùëÜ\\nùê∑ùëá‚Ä≤\\nEncoder ùëìùúô Critical weights set  ùëÅr‚àí1 \\nùêøùëÜ\\nùêøùëáGradient flow\\nlayer lweight i\\n|‚àáùêøùëÜùëô,ùëñ\\n‚àáùêøùëáùëô,ùëñ|‚ãØ\\n‚ãÆ ‚ã± ‚ãÆ\\n‚ãØWeight importance score\\n Update critical \\nweights set to ùëÅr \\nSelect top \\nN weightsùê∑ùëÜ \\n ùêµùëÜ\\n ùê∑ùëá \\n ùêµùëá\\n \\nSynthetic \\nDatasetDomain -aware Weight Selection\\nSection IV. BFig. 2. Overview of the proposed EncoderLock framework and paper organization . The procedure in Round rincludes: 1. domain-aware critical weight\\nselection algorithm : take data batches BSandBTfrom the authorized source dataset DSand the prohibited target dataset DT, respectively, and calculate the\\nweight importance with gradients of loss LSandLTand choose critical weights to update for the round rasNr, note here specific losses depend on different\\nlevels of accessibility of the target domain; 2. EncoderLock weight update algorithm (with three variants for the three levels of target domain dataset), utilizing\\nthe supervised EncoderLock loss Lel, unsupervised contrastive loss Lcont\\neland the generated synthetic dataset D‚Ä≤\\nT, respectively.\\nencoder on these domains is allowed but their performance is\\nnot guaranteed like the source domain.\\nProtection Objective. The major goal includes: restricting\\nthe pre-trained encoder from being probed for the prohibited\\ndomain; preserving its performance on the authorized domain.\\nCapabilities. The capabilities of the model owner include:\\n‚Ä¢The model owner has full control of the encoder to adjust the\\narchitecture, hyper-parameters, and parameters, and manage\\ntraining strategies for the encoder before deploying it.\\n‚Ä¢The owner has no access to the user‚Äôs dataset after deploy-\\nment to detect if the probing samples belong to the prohibited\\ndomain, and has no knowledge about the probing process or\\ndownstream heads.\\n‚Ä¢The owner may have different levels of accessibility to the\\nprohibited domain, from high to low: a) Level 1 : The owner\\nhas a labeled target dataset; b) Level 2 : The owner obtains\\nthe target dataset, but it is unlabeled; c) Level 3 : The owner\\nonly has an abstract concept about the prohibited target\\ndomain (i.e., a text description), which is called a ‚Äòtheme‚Äô. We\\npropose three variants of EncoderLock to address different\\nlevels of accessibility, respectively.\\nIV. P ROPOSED FRAMEWORK : ENCODER LOCK\\nIn this work, we propose a new applicability authorization\\nstrategy for a pre-trained encoder against malicious probing,\\nwhich we call EncoderLock . Fig. 2 depicts an overview of\\nthe framework, which consists of two major steps - domain-\\naware weight selection algorithm and specific weight updatingalgorithms catering to the different levels of accessibility of the\\ntarget domain. By managing the weights, EncoderLock restricts\\nthe encoder from being probed on the prohibited target domain\\nto extract useful information, while ensuring that the encoder\\ncorrectly responds to authorized (source) inputs. Existing\\nliterature mostly focuses on protecting pre-trained models from\\nbeing transferred [ 78], [79], while our EncoderLock targets\\npre-trained encoders, with several unprecedented challenges\\noutlined in Section IV-A.\\nA. Design Objectives and Challenges for EncoderLock\\nIn addition to the traditional design objective of controlling\\nthe transferability of pre-trained models to prohibited target\\ndomains [ 79], EncoderLock faces three additional challenges\\nthat need to be effectively addressed.\\nChallenge 1. Preservation of Integrity: The integrity of\\nthe encoder lies in maintaining the pre-learned knowledge\\nabout the authorized domains. One question is raised: how\\ncan EncoderLock make minimal modifications to the encoder\\nto restrict it on the prohibited domain while preserving the\\nintegrity on the source domain?\\nChallenge 2. Robustness to malicious probing: When\\nmalicious users adjust the downstream heads for the prohibited\\ndomain with any learning rate and optimizer, how to success-\\nfully ‚Äòlock‚Äô the encoder against malicious probing?\\nChallenge 3. Different target domain data accessibility: In\\nreality, as the defender (model owner) may have various levels\\nof knowledge about the target domain, how should EncoderLockOutput Neuron IndexInput Neuron IndexGradient ValueMNIST USPSFig. 3. Visualization of weight importance in a pre-trained model‚Äî The\\nX-Y plane represents the weight matrix of a selected dense layer in a model\\ntrained on MNIST and probed for USPS. The color and height indicate each\\nweight‚Äôs importance to the output (the higher and darker, the more important).\\nbe designed for practical scenarios including unlabeled datasets\\nor even no samples from prohibited domains?\\nB. Domain-aware Weight Selection\\nTo address Challenge 1 , we propose a domain-aware weight\\nselection strategy to selectively update weights that are critical\\nonly for the target domain, thereby minimizing EncoderLock‚Äôs\\nnegative impact on the integrity of the pre-trained encoder. Our\\nstrategy is motivated by two observations of DNN models: 1)\\nWeight importance varies across different domains . Different\\nsets of critical weights in the same model may respond to\\ndifferent domains, which can be measured by the weight‚Äôs\\ngradient magnitude. This notion has been exploited in achieving\\ndomain-specific pruning [ 50], effective fault injections on DNN\\nparameters [ 52], [65], [89], and watermarking embedding [ 84].\\nFig. 3 demonstrates one example of different weight importance\\nfor different domains. The model is a multi-layer perceptron\\nnetwork trained on MNIST (source domain), and the distribution\\nof the weight gradients is shown in red color. When this\\nmodel runs inference for another dataset USPS (target domain),\\nthe weights gradient profile is shown in blue color, distinctly\\ndifferent from that on MNIST. 2) Over-parametrization ‚ÄîDNNs\\noften have more weights than required, with a large portion\\nbeing insignificant. This characteristic is widely utilized in\\nmodel compression for efficiency [ 22], [53], [92], where only\\ncritical weights are retained while others are pruned away.\\nOur Domain-aware Weight Selection (DWS) algorithm is\\ndescribed in Function 1. The search process runs iteratively.\\nIn the rthround, it uses datasets from the source and target\\ndomains, DSandDT, to search for and update the critical\\nweight set Nr. It is important to note that the composition of\\nDTdepends on the level of data accessibility. For supervised\\nEncoderLock (Level-1), inputs from the target dataset have\\nlabels, and the loss LTis calculated using cross-entropy, similar\\ntoLS. In the unsupervised and zero-shot EncoderLock (Level-\\n2 and Level-3), the target dataset consists of unlabeled or\\nsynthetic images, and we propose using a contrastive loss\\nfor these unlabeled inputs, presented in Eq. (7). The weight\\nimportance score is defined as the magnitude ratio of gradients\\nfor the ithweight in layer lbetween the target and sourceFunction 1 Domain-aware Weight Selection\\nInput: Source domain DS, Target domain DT, Pre-trained\\nencoder parameters œï, Number of new critical weights N,\\nSet of critical weights for the previous round Nr‚àí1\\nOutput: Set of critical weights Nr\\n1:function DWS(DS,DT,œï,Nr‚àí1,N)\\n2: Sample a training batch BTfromDT\\n3: Sample a training batch BSfromDS\\n/* Compute gradients for both batches */\\n4:‚àáLT‚ÜêComputeGradients( œï,BT)\\n5:‚àáLS‚ÜêComputeGradients( œï,BS)\\n/* Compute scores and select critical weights */\\n6: Compute score for ithweight in lthlayer:\\x0c\\x0c\\x0c‚àáLl,i\\nT\\n‚àáLl,i\\nS\\x0c\\x0c\\x0c\\n7: Select top Nweights: argmaxN\\x0c\\x0c\\x0c‚àáLl,i\\nT\\n‚àáLl,i\\nS\\x0c\\x0c\\x0c\\n8:Nr‚Üê N r‚àí1‚à™ {selected weights }\\n9: return Nr\\n10:end function\\ndomains |‚àáLl,i\\nT/‚àáLl,i\\nS|. This score is used to identify weights\\nthat are critical to target domains but less crucial to the source.\\nThe search process is iterated across Rrounds, with\\nNweights selected in each round, resulting in a total of\\nN√óRweights to update. The values of NandRare two\\nhyperparameters that control the number of altered weights\\nand will be discussed further in Section VI-A . Such design\\nallows us to process the datasets in batches and implement the\\nself-challenging training scheme (see Section IV-C2).\\nIn Section IV-C toIV-E , we further discuss more details\\nabout how to update the selected weights and how to achieve\\nrobustness against downstream fine-tuning ( Challenge 2 ). The\\nmethod varies across different levels of accessibility to the\\ntarget domain, as shown in three branches of Fig. 2.\\nC. Supervised EncoderLock\\nLevel 1 EncoderLock is supervised, with a labeled target\\ndomain dataset. Specifically, given the source domain DSand\\ntarget domain DT, with (xS, yS)‚àà DSand(xT, yT)‚àà DT\\nas the corresponding datasets, let fœïdenote the pre-trained\\nencoder, and CŒ∏SandCŒ∏Tdenote the auxiliary downstream\\ntask classifiers for the source and target domains, respectively.\\nOur objective is to find an optimal encoder œï‚àóthat minimizes\\nLSbut maximizes LT, which are expressed as:\\nLS=L(CŒ∏S(fœï(xS)), yS) (2)\\nLT=L(CŒ∏T(fœï(xT)), yT) (3)\\nwhere Lis the classification loss function (i.e., cross-entropy\\nloss) and is used to compute gradient in Algorithm 1 for weight\\nselection. To restrict the impact on the encoder‚Äôs generalizability,\\nwe require ‚à•œï‚àó‚àíœï‚à•0‚â§M(:=N√óR), where ‚à•¬∑‚à•0is‚Ñì0norm\\nandMsignifies the weight change budget.\\nThe fundamental supervised EncoderLock consists of three\\nsteps: 1) Domain-aware weight selection, 2) Non-transferability\\nupdating, and 3) Self-challenging downstream model training.\\nWe run these three steps iteratively for Rrounds or until\\nthe accuracy of the auxiliary downstream classifier reaches\\nthe early stopping criterion. For the three different levels of\\ntarget domain data accessibility, the weight search and updateAlgorithm 1 Self-challenging Training Scheme\\nInput: Pre-trained encoder with œï, Source domain DS, Target\\ntraining dataset Dtrain\\nT, Target validation dataset Dvalid\\nT,\\nNumber of critical weights N, Number of rounds R,\\nDesired target accuracy Œ±goal.\\nOutput: Encoder with supervised EncoderLock œï‚àó\\n1:forr= 1 toRdo\\n/* Initialize critical weights set for the first round*/\\n2: ifr== 1 then\\n3: Initialize set Nr‚àí1‚Üê ‚àÖ\\n4: end if\\n/* Begin Domain-aware Weight Selection */\\n5:Nr= DWS( DS,DT,œï,Nr‚àí1,N)\\n/* Minimax optimization for enhancing robustness */\\n6: œï‚àó‚Üêoptimize weights in Nrto minimize Lel(5)\\n7: Initialize an auxiliary downstream CT(¬∑;Œ∏T)\\n8: Fine-tune CTusingDtrain\\nT with encoder œï‚àó\\n9: Compute accuracy Œ±TofCTonDvalid\\nT\\n/* Stop Criterion */\\n10: ifŒ±T< Œ±goal or‚à•œï‚àó‚àíœï‚à•0> N√óRthen\\n11: return œï‚àó\\n12: end if\\n13:end for\\nalgorithms are similar, but with different loss functions. But\\nthe supervised EncoderLock, with its loss design for the output\\nspace, requires an additional self-challenging training step to\\nensure its robustness. We next discuss the other two design\\nsteps for supervised EncoderLock in detail.\\n1)Weight Updating for Non-transferability: With critical\\nweights selected to update, we design a loss function in the\\nform of Equation (1), focusing on the regularization term RTto\\nmitigate the malicious probing for the target domain. Previous\\nregularization terms [ 78], [79] only consider the target domain,\\nwhich leads to unstable performance especially when LSand\\nLTare at different orders of magnitude. In particular, when\\nLSis very small (i.e., near zero), the introduction of RTwill\\ncause a strong impact on LS. Therefore, we propose a new\\nlog-ratio regularization term considering both LSandLT:\\nLel=LS+RT,where RT= log(1 + Œ±LS\\nLT) (4)\\nSuch logarithmic regularization term gently penalizes the loss\\nratio between the source and target, with Œ±moderating the\\nbalance between preserving the source domain accuracy and\\nenforcing the target domain non-transferability. Consequently,\\nthe optimization objective for the encoder is defined as:\\nœï‚àó= arg min\\nœïLel(œï, Œ∏S, Œ∏T)s.t.‚à•œï‚àó‚àíœï‚à•0‚â§M‚àÄŒ∏S, Œ∏T(5)\\n2)Self-challenging Training Scheme: To update the encoder\\nweights in supervised EncoderLock following Equation (5), we\\nconsider the auxiliary downstream classifier to compute LT.\\nHowever, malicious users have full control of the downstream\\nclassifier, including adjusting the architecture and choosing the\\noptimization method, and can fine-tune the model parameters\\nbased on the extracted features. Consequently, the performance\\nof the pre-trained encoder and the classifier on the target\\ndomain can be improved. Relying solely on a fixed-weight\\ntarget classifier could lead to vulnerability, where supervised\\nEncoderLock may only be non-transferable for given auxiliary\\nclassifiers but not for others the malicious user opts for.\\nFeature Space\\nSource Domain Samples Target Domain Samples\\nClass: Dogs Class: Birds \\nClass: Anti -aircraft Class: Combat vehicles \\nFig. 4. Design motivation of unsupervised EncoderLock\\nTo improve robustness against any potential malicious prob-\\ning for supervised EncoderLock, we propose a self-challenging\\ntraining scheme with a minimax problem formulation as:\\nœï‚àó= arg min\\nœïmax\\nŒ∏TLel(œï, Œ∏S, Œ∏T)s.t.‚à•œï‚àó‚àíœï‚à•0‚â§M (6)\\nSpecifically, during the iterations of updating critical weights\\nin the encoder part, we also adjust the target downstream\\nmodels iteratively. It is noted that the training objective of the\\ntarget downstream models will be adversarial to EncoderLock‚Äôs\\napplicability objective‚Äîthe fine-tuning aims to extract useful\\nfeatures in the embeddings to enhance the target domain\\nperformance. The retrained downstream model adjusts itself fre-\\nquently to create a challenging target downstream classifier that\\nwill increase Lel(decreasing LT), prompting the supervised\\nEncoderLock to adjust more critical weights on the encoder part.\\nAlgorithm 1 outlines this self-challenging training process. To\\nensure the randomness of the target downstream classifiers,\\nevery iteration we retrain it from scratch (with a random\\ninitialization). The iterative training proceeds until the target\\ndownstream classifier‚Äôs accuracy drops below a predefined\\nthreshold or reaches the maximum number of altered weights\\nM. The self-challenging training scheme ensures a gradual and\\nsmooth reduction in the encoder‚Äôs transferability, forcing the\\nencoder part to extract features that are less useful for the target\\ndomain, thereby leading to more robust performance even when\\nthe attacker probes the downstream model adaptively.\\nD. Unsupervised EncoderLock\\nIn this section, we address Level-2 accessibility of the\\ntarget domain via unsupervised EncoderLock . This scenario\\nis practically relevant when the goal is to prevent transferring\\nto arbitrary sets of images while getting their labels is either\\ninfeasible or expensive. Our method leverages the technique\\nfrom self-supervised representation learning [ 11], [33], which\\nbuilds a highly distinguishable feature space without labeling.\\nThe design idea for unsupervised EncoderLock is as follows:\\nfor the latent embeddings of samples from the source domain,\\nwe aim to ensure their high discrepancy between classes; while\\nfor those from the prohibited target domain, our objective is to\\nobfuscate the latent clusters boundary so that the embeddings\\nwould not contain much information about the class. As shown\\nin Fig. 4, an expected encoder will automatically cluster the\\nsamples from the source domain but blur the class boundaries\\nof the target domain. Due to such direct manipulation towards\\nthe encoder‚Äôs feature space, the unsupervised EncoderLock\\nis always robust to different downstream heads and doesn‚Äôt\\nrequire further self-challenging training.\\nTowards this goal, we introduce a self-supervised regular-\\nization term RTto be used in Equation (4). Specifically, givenAI agent : Let‚Äôs start by testing a few initial \\nprompts for the stable diffusion model. These \\nprompts will focus on ‚Äúmilitary vehicles‚ÄùTheme: military vehicles \\n>> RESPONSE: Similarity matrix between prompts: \\n(antique cannon , futuristic tank , 0.3), ‚Ä¶\\nAI agent : The prompts Antique Cannon and \\nFuturistic  Tank  has a high similarity in the \\nlatent space. Let‚Äôs refine them to Artillery \\nSystem  and Combat Vehicles .Antique Cannon\\nStealth Bomber\\nFuturistic Tank\\n‚Ä¶\\nDrone Carrier\\nNuclear SubmarineInitial  Prompts\\n Text -to-Image Model\\nArtillery System  \\nStealth Bomber\\nCombat Vehicles \\n‚Ä¶\\nDrone Carrier\\nNuclear SubmarineRefined  Prompts\\nEmbedding similarity>> RESPONSE: Similarity matrix between prompts:         \\n(antique cannon , futuristic tank , 0.02), ‚Ä¶\\nAI agent : All generated images with refined \\nprompts have low similarity. This dataset can \\nrepresent the theme ‚Äúmilitary vehicle‚Äù\\nVision encoder\\nVision encoderEmbedding similarity\\nText -to-Image Model\\nSynthetic Dataset using refined prompts for zero -shot EncoderLockFig. 5. Building synthetic datasets for zero-shot EncoderLock\\na batch of samples from the target domain, we leverage data\\naugmentation, including random crop, color jitter, or Gaussian\\nblur [ 71], [56], to create a set of positive pairs and a set\\nof negative pairs. Any pair with a sample and an augmented\\nsample from the same original image is defined as positive, and\\nwe denote their feature space as (zi,Àúzi), where ziis defined as\\nthe normalized embedding of the sample xiusing the encoder\\nf. Any pair with augmented samples from different original\\nimages is defined as negative, denoted as (zi,Àúzj)iÃ∏=j. We\\ndefine the contrastive loss function Lcontas:\\nLcont:=‚àí1\\nNBNBX\\ni=1log(sim(zi,Àúzi)PNB\\nj=1sim(zi,Àúzj)) (7)\\nwhere NBis the batch size, and sim(¬∑,¬∑)computes the cosine\\nsimilarity between the normalized embeddings. We select pairs\\nfromSto compute Lcont\\nS, and from Tto compute Lcont\\nT. They\\nare used to compute gradients in Algorithm1.\\nThe presented loss function aims to increase the similarity\\nbetween any positive pairs but reduce what between negative\\npairs, effectively pushing the encoder to learn representations\\nthat clearly distinguish similar samples from dissimilar ones\\nwithin feature space. We follow the regularization framework\\nin Eq. (4) and penalize ratios between contrastive losses:\\nRcont\\nT= log(1 + Œ±Lcont\\nS\\nLcont\\nT)\\nFor the unsupervised EncoderLock, self-challenging training\\nis not necessary because this loss function directly penalizes\\nthe discrepancy of the feature space for the target dataset.\\nTherefore, as shown in Fig. 2, the procedure of unsupervised\\nEncoderLock in one round includes: 1) Domain-aware weight\\nselection with Lcont; 2) Update Encoder‚Äôs Non-transferability\\nwithRcont\\nT, without retraining the challenging classifier.\\nE. Zero-shot EncoderLock\\nIn this section, we address Level-3 accessibility of the\\ntarget domain for EncoderLock, where the model owner even\\nhas no target samples. This represents the most practical and\\nrelevant scenario, as the definition of harmful content is often\\nvague in real-world applications. For instance, in most cases,\\na DNN product‚Äôs user guidelines regulate prohibited contentTABLE I. DATASETS USED IN EVALUATION OF ENCODER LOCK\\nDataset Abbr. Type Feat. Supervised Unsupervised Zero-shot\\nMNIST[48] MT digits\\nDatasets that are used in\\nthe baselines[ 79], [78]. All\\nsamples are resized into\\n(32,32,3) and the label\\nspace is 10.‚àö ‚àö-\\nUSPS [38] UP digits‚àö ‚àö-\\nSVHN[57] SN digits‚àö ‚àö-\\nMNIST-M[25] MM digits‚àö ‚àö-\\nSynthetic Digits[24] SD digits‚àö ‚àö-\\nCIFAR-10[46] CF image‚àö- -\\nSTL-10[15] ST image‚àö- -\\nEMNIST[16] EM char. 47-class characters‚àö- -\\nCIFAR-100[46] CF100 image 100-class images‚àö- -\\nImageNette[36] - image High resolution images\\nwith the shape of\\n(224,224,3).‚àö ‚àö ‚àö\\nImageWoof[36] - image‚àö ‚àö ‚àö\\nMilitary Vehicle[6]2- image‚àö ‚àö ‚àö\\nusing text descriptions of unethical or sensitive material. How\\nto turn such vague scope description into representative and\\ncomprehensive target domain dataset is a challenge. We define\\nthe basic knowledge about the target domain as a theme , which\\ncan be in the form of a text description, keywords, or reference\\nfigures. Using the target theme, zero-shot EncoderLock aims\\nto generate a synthetic dataset for applicability authorization\\nwithout relying on real-world samples or labels.\\nFig. 5 presents the framework of zero-shot EncoderLock,\\nillustrating the process of generating a synthetic dataset for\\n‚Äúmilitary vehicles ‚Äù. Section V-D will showcase the full results.\\nFirst, we employ a large language model (e.g., GPT-4 [ 1]) as\\nan AI agent to generate text inputs, known as prompts, for the\\ngiven theme. These prompts are then fed into pre-trained text-\\nto-image models (e.g., CLIP [ 64] and Stable Diffusion [ 68]) to\\ngenerate the synthetic dataset. To ensure the synthetic dataset\\ncomprehensively covers the target domain, we introduce a\\nprompt refining framework. Using a pre-trained vision encoder,\\nwe extract latent features from the synthetic images and\\ncompute pairwise similarity scores between the initial prompts.\\nThis similarity matrix serves as feedback to the AI agent,\\nenabling it to analyze the scores, identify similar prompts,\\nand refine them accordingly. For example, as shown in Fig. 5,\\nAntique Cannon andFuturistic Tank exhibit high similarity due\\nto their shared barrel feature. Consequently, the AI agent revises\\nthese prompts to be Artillery System andCombat Vehicles . The\\nrefinement process continues until all prompt pairs demonstrate\\nlow similarity or the similarity stops decreasing. Finally, we\\nemploy the synthetic dataset generated in the last round for\\nunsupervised EncoderLock training, resulting in an encoder\\nwith restricted transferability to the target ‚Äútheme.‚Äù\\nV. E XPERIMENTS\\nA. Experiment Setup\\nBaselines: As the first of its kind work addressing malicious\\nprobing of pre-trained encoders, there is no prior work for direct\\ncomparison with our EncoderLock. The closest related work is\\nthe SOTA non-transferable learning, including NTL [79] and\\nCUTI [78]. For such baseline work, we adopt the pre-trained\\nmodel, freeze the encoder part, and train a new downstream\\nhead on the prohibited target domain to evaluate the baseline\\nmodel‚Äôs resistance against malicious probing.\\nDatasets: Table I lists the twelve datasets for our evaluation.\\nIn addition to the five digits datasets used in the previous\\nwork [ 79], [78], we also assess datasets with larger label spaces,\\ni.e., EMNIST (47 classes) and CIFAR-100 (100 classes). By\\nutilizing text-to-image generators, zero-shot EncoderLock is\\nevaluated with more complex datasets. We test ImageNette [ 36]\\nas the source dataset and the military vehicle dataset [ 6] asTABLE II. SUPERVISED ENCODER LOCK PERFORMANCE :THE ENCODER TRANSFERABILITY ‚ÄîPRE AND POST -ENCODER LOCK ACCURACY ARE\\nREPORTED ,DESIGNATED AS ‚ÄòBEFORE (%)‚áíAFTER (%)‚Äô. B OLD VALUES SHOW ACCURACIES ON THE SOURCE DOMAIN\\nSource \\\\Target MT UP SN MM SD ‚àÜW Drop S Drop T\\nVGG-11 : 133M Parameters\\nMT 99.53‚áí99.32 96.35‚áí8.47 43.74‚áí18.98 68.24‚áí18.05 69.65‚áí13.67 0.63‚Ä∞ 0.21%‚Üì78.70%‚Üì\\nUP 97.70‚áí11.35 97.91‚áí94.94 58.23‚áí16.86 65.24‚áí15.43 87.10‚áí16.93 0.43‚Ä∞ 2.97%‚Üì76.16%‚Üì\\nSN 95.30‚áí19.72 92.68‚áí15.89 94.04‚áí90.51 71.51‚áí32.17 96.96‚áí15.66 2.50‚Ä∞ 3.53%‚Üì76.62%‚Üì\\nMM 98.85‚áí12.71 94.67‚áí17.99 53.80‚áí23.80 94.24‚áí92.71 85.89‚áí28.20 0.99‚Ä∞ 1.53%‚Üì 75.28%‚Üì\\nSD 97.13‚áí20.19 93.57‚áí18.68 90.40‚áí41.42 71.53‚áí40.91 99.83‚áí98.89 1.78‚Ä∞ 0.94%‚Üì64.13%‚Üì\\nResNet-18 : 11.4M Parameters\\nMT 99.48‚áí99.12 93.77‚áí13.16 41.47‚áí19.67 70.02‚áí20.77 72.48‚áí16.69 0.31‚Ä∞ 0.28%‚Üì71.73%‚Üì\\nUP 95.10‚áí13.89 96.11‚áí95.69 33.72‚áí11.36 55.79‚áí9.04 61.76‚áí16.53 0.29‚Ä∞ 0.44%‚Üì76.98%‚Üì\\nSN 94.65‚áí9.53 88.04‚áí15.65 91.06‚áí90.12 66.52‚áí11.36 95.08‚áí10.45 0.41‚Ä∞ 1.03%‚Üì86.02%‚Üì\\nMM 98.82‚áí24.05 92.33‚áí12.81 48.18‚áí7.01 91.49‚áí90.39 78.03‚áí18.53 0.13‚Ä∞ 1.20%‚Üì80.87%‚Üì\\nSD 96.76‚áí10.39 91.68‚áí8.47 86.74‚áí30.70 68.56‚áí9.95 99.42‚áí97.77 0.18‚Ä∞ 1.65%‚Üì82.53%‚Üì\\nthe prohibited target3following a practical scenario. Further,\\nwe evaluate the influence of EncoderLock on three admissible\\ndomains‚Äìordinary vehicles4, weapons5, and animals6.\\nModels: Our method is assessed using three prevalent DNN\\narchitectures: VGG-11 [ 72], ResNet-18 [ 34], and Vision Trans-\\nformer (ViT) [ 8]. We leverage the supervised pre-trained models\\nfor VGG-11 and ResNet-187and fine-tune them on various\\nauthorized domains (source). The early convolutional layers\\n(residual blocks) of VGG-11 and ResNet-18 are considered en-\\ncoders, while the output dense layer(s) are used as downstream\\nheads for probing. The ViT utilizes a vision encoder structure\\nthat is trained with self-supervised learning.\\nHyperparameters: Hyperparameters for supervised and unsu-\\npervised EncoderLock can be found in Appendix A.\\nMetric: The metric to quantify the encoder‚Äôs resistance to\\nmalicious probing is the relative accuracy drop in both the\\ntarget and the source domains, defined asacco‚àíaccm\\nacco, where\\naccois the probing accuracy of the original encoder, and accm\\nis the one when modified with protection methods. A higher\\naccuracy drop indicates a strong restriction on given domains.\\nWe expect the accuracy drop in the source domain to be low\\nfor preserving the model integrity, while the accuracy drop\\nin the target domain to be high for robustness to malicious\\nprobing. In Section V-E, we further introduce the Performance\\nProtection Index (PPI) to evaluate the restriction on prohibited\\ndomains and the influence on authorized or admissible domains\\nsimultaneously for comparison.\\nPlatform: Our implementation uses PyTorch 1.5.0 on Ubuntu\\n18.04.6 with NVIDIA TITAN RTX.\\nTraining cost: With this experimental setup, training Encoder-\\nLock requires between 0.1 and 6 GPU hours, depending on the\\nencoder architecture and dataset size. Detailed training costs\\nfor various configurations are provided in Appendix C.\\nB. Evaluating the Supervised EncoderLock\\nTable II demonstrates the performance of supervised En-\\ncoderLock across the datasets of digits for VGG-11 and ResNet-\\n18. We compare the accuracy after probing the encoder with\\ncorresponding default downstream classification heads, before\\nand after applying supervised EncoderLock, report the relative\\naccuracy drop on the source domain (columns Drop S) and\\n3https://www.kaggle.com/datasets/amanrajbose/millitary-vechiles\\n4https://www.kaggle.com/datasets/marquis03/vehicle-classification\\n5https://huggingface.co/datasets/Kaludi/data-csgo-weapon-classification\\n6https://www.kaggle.com/datasets/alessiocorrado99/animals10/code\\n7https://pytorch.org/vision/stable/models.html\\nFig. 6. Accuracy drop across distinct source and target domains‚Äî It\\nassesses the transferability of VGG-11 encoder with CF ( Left) and MT ( Right )\\nas the source. Each data point illustrates the simultaneous impact on accuracies\\non the source and target domain with supervised EncoderLock.\\nthe average accuracy drop on the target domains ( Drop T),\\nand also present the average percentage of weight change\\n(column ‚àÜW). The experimental results of VGG-11 reveal\\nthat EncoderLock exhibits a steep reduction (up to 78.70%)\\nin performance on the target domains while ensuring minimal\\ndegradation on the source domain (highlighted in bold, up\\nto3.53%). Moreover, the accuracy degradation on ResNet-18\\nshows even a better restriction on the prohibited domain, from\\n71.73% to86.02%. In contrast, the degradation of ResNet-\\n18 encoder with EncoderLock on the authorized domain is\\nminimized from 0.28% to1.65%. Moreover, with less than\\n0.08% of the weights changed on average, the supervised\\nEncoderLock preserves a higher generalizability of the pre-\\ntrained encoder to the admissible domains and avoids the\\ncatastrophic forgetting of the encoder‚Äôs pre-learned knowledge.\\nIt will be further discussed in Section V-E as a comparison\\nbetween different EncoderLock and baseline methods.\\nIn addition to the accuracy drop, we find that the complexity\\nof the datasets (domains) affects the EncoderLock‚Äôs perfor-\\nmance. For instance, on VGG-11, we observe that restricting\\ntransferring from a complex domain (e.g., SN, MM, and SD,\\ncomprising RGB-colored digit images) to a simple domain (e.g.,\\nMT and UP, including grayscale images) is more challenging.\\nSpecifically, the supervised EncoderLock requires more weights\\nto be changed, and yields a smaller target accuracy drop and\\na larger source accuracy drop, when the source domain is\\nSN which is a more realistic, 3-channeled digits dataset (the\\nStreet View House Number) A similar phenomenon is also\\nobserved in ReseNet-18 supervised EncoderLock, with the\\nhighest average weight modification at 0.041% . Moreover,\\nthe similarity between the source and target domains alsoTABLE III. UNSUPERVISED ENCODER LOCK PERFORMANCE ON ENCODER (VGG-11) TRANSFERABILITY\\nSource \\\\Target MT UP SN MM SD ‚àÜW Drop S Drop T\\nMT 99.53‚áí99.22 96.35‚áí16.84 43.74‚áí19.61 68.24‚áí12.26 69.65‚áí17.90 0.12‚Ä∞ 0.31%‚Üì73.51%‚Üì\\nUP 97.70‚áí45.02 97.91‚áí96.44 58.23‚áí9.59 65.24‚áí13.88 87.10‚áí12.66 0.22‚Ä∞ 1.50%‚Üì75.41%‚Üì\\nSN 95.30‚áí20.74 92.68‚áí17.09 94.04‚áí94.33 71.51‚áí50.85 96.96‚áí94.01 0.21‚Ä∞ 0.31%‚Üë47.93%‚Üì\\nMM 98.85‚áí40.26 94.67‚áí30.74 53.80‚áí33.97 94.24‚áí93.30 85.89‚áí55.79 0.15‚Ä∞ 0.99%‚Üì 49.68%‚Üì\\nSD 97.13‚áí76.68 93.57‚áí86.75 90.40‚áí75.31 71.53‚áí27.23 99.83‚áí99.44 0.20‚Ä∞ 0.39%‚Üì26.74%‚Üì\\n0 Rounds 10 Rounds 30 Rounds 50 Rounds\\nSource: MT Target: MM\\nFig. 7. Unsupervised EncoderLock -Latent Space Change via Rounds\\naffects the supervised EncoderLock‚Äôs performance, e.g., the\\nnon-transferability of SD ‚ÜíSN is the worst as they are similar.\\nUnlike prior research [ 79], [78] that only examines the\\napplicability authorization between domains that share the same\\nlabel space (e.g., 0 ‚àº9 digits), during malicious probing, one\\ncould aspire to transfer the queried features from the encoder\\nto domains with distinctly different label spaces. Thus, we also\\nevaluate the performance of supervised EncoderLock on VGG-\\n11 under various circumstances: a) transition between distinct\\ntask types (CF to MT); b) variation in the size of the label\\nspace for similar tasks (CF to CF100); c) changes in both the\\ntask type and the label space size (CF to EM). The results are\\npresented in Fig. 6. Overall, supervised EncoderLock achieves\\ngood performance across all these transfer tasks, manifesting\\na much higher drop in the target domain than the source.\\nSimilar to experiments on digit datasets, the more distinct the\\ntarget dataset is from the source dataset, the better performance.\\nFor instance, supervised EncoderLock performs well when\\ntransferring across different tasks, i.e., transitioning from digits\\nto images (CF) and vice versa; while it results in a lower\\nperformance when transferring from CF to CF100 as these\\ntwo share a large number of similar features. Notably, when\\nMT is the source domain and CF100 is the target domain,\\nEncoderLock can significantly reduce the accuracy on CF100\\nto1.19%, closely resembling random guessing across the 100\\nclasses, i.e., the encoder fails to extract features. While for\\ntransferring between MT and EM (both digits), the accuracy\\nonly drops to 32.04%. We posit that a pre-trained encoder\\ntends to capture more intricate features when applied on similar\\ndomains, while there will be more distinct features when the\\nsource and target domains diverge significantly. We characterize\\nthe similarity between datasets using MMD in Appendix B.\\nC. Evaluating the Unsupervised EncoderLock\\nThe unsupervised EncoderLock addresses the challenge\\nwhen the model owner has no access to the true labels of sam-\\nples from the target domain. To demonstrate the effectivenessof the proposed contrastive loss in Eq. (7), we conduct the\\nsame evaluation on digits datasets with results on VGG-11\\nshown in Table III and ResNet-18 in Appendix E Table IX.\\nThe unsupervised EncoderLock also successfully restricts the\\nencoder‚Äôs transferability to the target domain and maintains the\\nencoder‚Äôs integrity on the source domain, with a small number\\nof weight changes.\\nCompared with supervised EncoderLock, the unsupervised\\nEncoderLock demonstrates better performance in preserving\\naccuracy within the source domain, but worse performance in\\nreducing the target accuracy. This is due to the proposed Rcont\\nT,\\nutilizing contrastive loss, necessitates a more discriminative\\nlatent space for the source domain by reducing Lcont\\nS. To further\\nillustrate, in Fig. 7 we visualize the change of the latent space\\nfrom both the source (MT) and target (MM) domains using\\nt-SNE [ 76]. With more rounds of unsupervised EncoderLock\\n(indicating more training epochs and more weight updates), the\\nsource domain remains class-discriminative, while the target\\ndomain becomes less discriminative. However, some small\\nclusters of classes can still be observed, which leads to a\\nhigher accuracy when we fine-tune a downstream classifier on\\nsuch a latent space on the target domain.\\nD. Evaluating the Zero-shot EncoderLock\\nFor zero-shot EncoderLock, we utilize the GPT-4 API as\\nthe AI agent to interpret the semantic meaning of the prohibited\\ntheme8with a pre-trained stable diffusion model for text-to-\\nimage generation9to create synthetic datasets. To show the\\neffectiveness of the AI agent and prompts refining (Fig. 5), we\\ntest the zero-shot EncoderLock under three scenarios: 1) The\\nprompts are created manually (without AI agents); 2) An AI\\nagent generates prompts randomly ; 3) The AI agent generates\\nprompts and refines them. We utilize 10 prompts to the\\ngenerative model for 1,000 fake images as the prohibited dataset.\\nPrompts and synthetic images can be found in Appendix K.\\nWe evaluate one-shot EncoderLock‚Äôs performance on the\\nsource domain (ImageNette [ 36]) and the target domain (a real\\nmilitary vehicle dataset [ 6]) with a pre-trained encoder using\\nResNet-18. Furthermore, we select three admissible domains to\\nevaluate EncoderLock‚Äôs impact on other domains, neither source\\nnor target. Considering the semantics of ‚Äòmilitary vehicles‚Äô, we\\nutilize two admissible datasets with closer semantic meaning\\nand one unrelated. 1). Ordinary vehicles : A 10-class normal\\nvehicle classification dataset related to the prohibited domain\\nin ‚Äòvehicle‚Äô, i.e., sedan, SUV , and bus. 2). Weapons : An 11-\\nclass game-based weapon classification dataset, related to the\\nprohibited domain in ‚Äòmilitary‚Äô, i.e., AK-47, Famas, and UPS.\\n3).Animals : A 10-class animal classification dataset with no\\n8https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4\\n9https://huggingface.co/CompVis/stable-diffusion-v1-4Fig. 8. Comparison among different EncoderLock ‚Äìon authorized, prohibited, and admissible domains. We randomly select a percentage of domain-aware\\nweights as the X-axis, indicating the optimization process of domain-aware weights. The authorized and prohibited domains are run 5times with error bars plotted.\\nobvious semantic meaning with the prohibited domain, i.e.,\\nchicken, cat, and butterfly.\\nFig. 8 shows the performance of different EncoderLock\\nvariants on authorized, prohibited, and three admissible domains.\\nWe present the testing accuracy degradation on each domain\\nversus the percentage of critical weights modified. The original\\npre-trained encoder has an average accuracy 96.59% on the\\nauthorized domain and 60.55% on the prohibited domain. Super-\\nvised EncoderLock shows the strongest performance restriction\\non the prohibited domain ( 11.48%) and meanwhile preserving a\\nhigh accuracy on the authorized domain ( 94.17%). The primary\\nreason behind the efficacy of the supervised EncoderLock is its\\nwell-defined prohibited domain. This clarity allows for targeted\\nmodifications of critical weights. As a result, EncoderLock can\\nenhance specific aspects of the encoder‚Äôs performance while\\nmaintaining its overall generalization capability. Unsupervised\\nEncoderLock demonstrates the second highest performance\\nwith a label-free prohibited domain by restricting the target\\nperformance to 18.34% and keeping the accuracy on the source\\nat93.43%. The introduction of contrastive loss penalizes more\\ngeneral features than the supervised loss, causing a larger\\ndegradation on the authorized and admissible domains. Zero-\\nshot EncoderLock works on the most challenging accessibility\\nto the prohibited domain. Without any data from it, using\\nprompts from military fans (manual prompts in Appendix K)\\nshows similar to the one-time initial prompt from the AI\\nagent. The prompts and generated synthetic images are not\\ngeneral enough to describe the entire prohibited domain, thereby\\nleading to the smallest restriction on the military vehicle\\ndataset. (Manual: 41.81%, AI initial: 38.61%) When we refine\\nthe AI prompts for more general features of the prohibited\\ndomain with the presented refinement algorithm, there is\\na noticeable increase in the EncoderLock restriction ability\\n(23.69%). However, such prompts cause the largest accuracy\\ndegradation on the authorized domain ( 92.86%), as its wide\\nrestriction on the encoder‚Äôs features. In addition to the relevance\\nof prompts, the synthetic dataset‚Äôs quality also affects the\\nstrength of EncoderLock. Specifically, a high-quality synthetic\\ndataset, generated after large number of inference iterations\\nand bearing small noise, enhances the defense capability of\\nEncoderLock. More detailed results are shown in Appendix F.\\nOther than the performance of authorized and prohibited\\ndomains, our evaluation on the admissible domains indicates\\nthe impact of EncoderLock on the generalizability to unknown\\ndata. In particular, all five EncoderLock‚Äôs restrictions on the\\nprohibited ‚Äòmilitary vehicles‚Äô show higher penalties on the\\n‚Äòweapon‚Äô dataset. On the other hand, the impact on the encoders‚Äô\\nperformance on ordinary vehicles is small, similar to what fromthe animal dataset. We will further present a deep analysis\\nof this phenomenon in Section V-F, where we show the\\nencoder‚Äôs attention shifting from the attack module of the\\nmilitary vehicles, i.e., the barrel. As a result, it has a lower\\neffect on distinguishing between various types of vehicles but\\nhas a significant impact on different types of firearms.\\nIn Fig. 8, we also present the probing performance of\\nEncoderLock when the portion of critical weights to change\\nvaries. In particular, the pre-trained encoder‚Äôs performance\\nkeeps dropping on the prohibited domain but its performance\\ndegradation on the authorized and admissible domain shows\\na ‚Äòdegradation peak‚Äô at the point of changing half of critical\\nweights. This phenomenon is likely due to the integrity of\\nEncoderLock‚Äôs critical weights updating process affected by\\nthe random sampling. A complete updating process with either\\nsupervised loss (5)or unsupervised loss (7)ensures the encoder\\ncannot perform well on the target domain but still effective\\non the source domain, as shown the good performance when\\nchanging 100% critical weights. However, only updating half\\nof them will break the encoder‚Äôs integrity on the source domain\\nand the admissible domain as it breaks the network connectivity\\nbetween critical weights, leading to unstable EncoderLock. Such\\nphenomenon also shows in the standard deviation on the curves\\nin Fig. 8, where when only changing half of critical weights,\\nthe EncoderLock performance has the largest variance.\\nE. Comparison with Prior Methods\\nWe compare EncoderLock with SOTA baselines: NTL [ 79]\\nand CUTI [ 78] in protection against malicious probing. Specif-\\nically, we probe the encoder on the target dataset with fine-\\ntuned downstream heads. Moreover, we propose a new stable\\nmetric‚Äì Protection Performance Index (PPI) ‚Äìto measure the\\nperformance in restricting on the target domain ( DT) relative\\nto what in retention on the base domain ( DB), defined as:\\nPPI(DT,DB) =accT\\no/accT\\nm\\naccBo/accBm(8)\\nPPI measures the ratio of change in performance before and\\nafter applying protection for a prohibited target domain with an\\nauthorized or admissible base domain. A higher PPI indicates\\nbetter non-transferability and resistance to malicious probing.\\nIn Fig. 9, we compare the PPI of supervised and unsuper-\\nvised EncoderLock with the baselines in five pairs of authorized\\nand prohibited domains when probing the downstream heads\\nby epochs. Our proposed supervised EncoderLock shows the\\nbest non-transferability performance, while the unsupervised\\nEncoderLock also demonstrates good performance overall. In\\naddition, we also present the PPI between admissible domainsFig. 9. Comparison on Authorized PPI ‚Äìbetween EncoderLock with NTL [ 79] and CUTI [ 78] on different pairs of authorized domains and prohibited domains.\\nThe higher the better. Probing for multiple epochs can‚Äôt increase the performance on prohibited domains but keeps its performance on authorized domains.\\nFig. 10. Comparison on Admissible PPI ‚Äìbetween EncoderLock with NTL [ 79] and CUTI [ 78] on different pairs of admissible domains and prohibited domains.\\nThe higher the better. EncoderLock shows the minimal impact on admissible domains while restricting the encoder‚Äôs performance on prohibited domains.\\nand prohibited domains in Fig. 10. The PPI of supervised\\nand unsupervised EncoderLock is still better than the baselines.\\nHowever, the advantage is not obvious due to the prohibited and\\nadmissible domains are highly similar as they are all digital\\ndatasets. Therefore, critical weights on prohibited domains\\noften overlap with those in admissible ones, causing a higher\\naccuracy drop as the weights are not optimized during the\\nupdating process of the EncoderLock. A good example can\\nbe found in Fig. 8, where we analyze the admissible domain\\nwith distinct semantic meaning (i.e., animals versus military\\nvehicle), EncoderLock can preserve the encoder generalizability\\nto semantic unrelated domains. Additional numerical results\\ncan be found in Appendix H Table XI to XVIII.\\nWhen we fine-tune the downstream classifier on the encoder\\ntrained using NTL and CUTI, the encoder performance on the\\nsource domain may decrease considerably, or its performance on\\nthe target domain may still be high. This indicates that previous\\nmethods are not suitable for the scenario of malicious probing.\\nTo explain this phenomenon, we visualize the source (MM)\\nand target (UP) domains on the latent space with different\\ntrained encoders in Fig. 11, where the colors represent the\\ntrue labels of the testing dataset. The self-challenging scheme\\nemployed in supervised EncoderLock effectively preserves\\na higher discrepancy within the source domain, while very\\nlittle class-related information is discernible within the target\\ndomain. The unsupervised EncoderLock, despite lacking class\\ninformation, still successfully reduces class distinguishability\\non the target domain. In contrast, NTL [ 79] penalizes the\\nMaximum Mean Discrepancy between the source and target\\nlatent spaces, showcasing the distribution difference between\\nthese domains, but samples from the same class still cluster\\ntogether in the target domain. Therefore, its transferability\\ncan be resumed via fine-tuning. CUTI [ 78] only considers\\ntarget performance during training and shows the worst non-\\ntransferability‚Äìits target domain is clearly clustered by classes.\\nSupervised \\nEncoderLockUnsupervised \\nEncoderLockNTL CUTISource: MM Target: UPFig. 11. Latent Feature Visualization ‚ÄìFeature Space of Different Methods\\nF . Interpretation of EncoderLock\\nTo further understand the changes in the encoders generated\\nby different variants of EncoderLock, we use the encoders\\ntrained from Section V-D and visualize their decision-making\\nprocess with Gradient-weighted Class Activation Mapping\\n(GradCAM) [ 69], as illustrated in Fig. 12. Specifically, the\\nGradCAM attribution is computed for the last convolutional\\nlayer in the encoder (ResNet-18) and is upsampled to act as\\na mask added to the original input (the cool-warm heatmap\\nin Fig. 12). The highlighted red part indicates the feature that\\nthe encoder focuses on to make its prediction. We observe\\nthat the original pre-trained encoder focuses on the English\\nSpringer correctly, and it performs well on the military dataset\\n(tanks) as its focus is moved to the main gun barrel of the tank.\\nHowever, the supervised EncoderLock, which has less effect\\non the source domain data, switches the encoder‚Äôs focus to the\\ntank track, leading the fine-tuned downstream classifier to make\\na wrong prediction as ‚ÄòArmored combat support vehicles‚Äô. The\\nunsupervised EncoderLock, aiming to blur the entire feature\\nspace‚Äôs class-discrepancy, generates a more vague interpretationPre-trained Encoder Supervised Unsupervised Zero -shot (Manual)  Zero -shot (Initial) Zero -shot (Refined) Original ImageAuthorized \\n(English springer)Prohibited\\n (Tanks)Fig. 12. Interpretation of Different EncoderLock using GradCAM [69] ‚Äìthe red parts highlight the focus of encoder to make decisions.\\nof the decision process‚Äîthe focus is mainly on the vehicle but\\nnot on specific features of tank. In Appendix J, we also visualize\\nthe GradCAM results on three different admissible domains.\\nThe findings indicate that the attack module of military weapons\\nalso experiences a loss of focus in the model, resulting in a\\nsignificant impact of EncoderLock on the weapon dataset.\\nWe also visualize the GradCAM of zero-shot EncoderLock\\nwith different types of prompts. The manual prompts show less\\neffectiveness in the model‚Äôs non-transferability, still focusing\\non some useful features such as the tank roof and gun barrel.\\nThe AI-agent-generated prompts (with or without refinement)\\nhave a strong effect on the source domain, especially the green\\npart (bash in the source figure), considering ‚Äògreen‚Äô is likely\\nrelated to the camouflage. With the proposed prompt refinement\\nprocess, the synthetic dataset obfuscates most of the useful\\nfeatures of the military theme, leading to a very vague focus on\\nthe target domain. The interpretation of EncoderLock further\\nreinforces its effectiveness: the supervised EncoderLock has\\nthe highest performance as it has the ground truth labels, and\\ntherefore able to move away the encoder‚Äôs focus on specific\\nfeatures; while the unsupervised and zero-shot EncoderLock\\ndirectly cause the latent feature space to be less informative,\\nleading to all input features having similar importance.\\nG. Real-world Case Study\\nPrevious evaluations mainly focus on small encoders\\nextracted from supervised-trained DNN models with basic\\narchitectures (e.g., VGG-11 and ResNet-18). To demonstrate\\nthat EncoderLock is practical in protecting real-world encoders,\\nwe apply it to a public encoder based on Vision Transformer\\n(ViT) released by Facebook10[8]. This encoder is a transformer\\npre-trained on a large collection of images (ImageNet-1k [ 19]\\nwith a resolution of 224√ó224) in a self-supervised fashion.\\nThe input images are presented to the model as a sequence\\nof fixed-size patches (the patch size is 16√ó16). ViT learns an\\ninner representation of images that can extract features useful\\nfor downstream tasks with probing heads (downstream models).\\nAs reported, the training process requires approximately 2.6\\ndays with a computational power of 16 GPUs. The pre-trained\\nencoder represents a valid IP, and our proposed EncoderLock\\nfor applicability authorization aims to protect the IP.\\nWe first evaluate the performance of the supervised Encoder-\\nLock, considering ImageNette [ 36] and CIFAR-100 [ 64] as the\\n10https://huggingface.co/facebook/dino-vits16\\nsupervised unsupervised zero -shot\\nsupervised supervisedsupervisedFig. 13. Evaluation on a Pre-trained ViT :Top row - source (target) accuracy\\nversus number of changed weights for different levels of EncoderLock; Bottom\\nrow: performance of supervised EncoderLock on different datasets.\\nsource domains and the military dataset and Imagewoof dataset\\nas the target domains. The ImageWoof dataset is a selected\\nsubset of ImageNet with different types of dogs11. Note that to\\nfit the input resolution of ViT, the CIFAR-100 inputs are resized\\nto224√ó224. We also evaluate the ViT with EncoderLock on\\nsome simple datasets, as shown in Appendix D. In Fig. 13,\\nwe visualize the accuracy after fine-tuning the downstream\\nclassifier on the source and target domains, respectively, versus\\nthe number of weight changes. We observe that for the pre-\\ntrained ViT, the supervised EncoderLock still works effectively\\nin restricting the target domain performance and preserving the\\nsource domain performance. In addition, the largest accuracy\\ndrop often happens in the early epochs (early sets of domain-\\nspecific weights). This demonstrates the effectiveness of the\\nproposed domain-aware weight selection algorithm when the\\nmost important (target-domain sensitive) weights are selected.\\nSimilarly, we apply unsupervised and zero-shot Encoder-\\nLock on the same ViT encoder between the ImageNette (i.e.,\\nauthorized) and military (i.e., prohibited) datasets, and the\\nresults are shown in Fig. 13. All three variations of EncoderLock\\neffectively limit the encoder‚Äôs performance on the prohib-\\nited domain. Notably, supervised ( 21.56%) and unsupervised\\n(18.15%) EncoderLock provide stronger protection than the\\nzero-shot version ( 29.26%), due to their higher accessibility\\nduring training. And supervised EncoderLock outperforms in\\nmaintaining the accuracy in the authorized domain, as the\\nmore accurate penalization on the labeled prohibited domain.\\nIn conclusion, all three variants of EncoderLock demonstrate\\neffectiveness for safeguarding real-world pre-trained encoders.\\n11https://github.com/fastai/imagenetteWeights ( 10‚àí3)Weights ( 10‚àí3)Weights ( 10‚àí3)Fig. 14. Ablation studies‚Äì a) regularizer weights for supervised EncoderLock\\nŒ±; b) regularizer weights for unsupervised EncoderLock ;c) changed weights\\nnumber per round(N); The probing accuracy on the source and target domains\\nare reported on the lefty-axis and the changed weight percentage is on the\\nright y-axis. Original accuracy on source and target are 99.53% and94.91% .\\nVI. D ISCUSSIONS\\nA. Ablation Studies of EncoderLock\\n1)EncoderLock on Probing with Various Architectures:\\nAs shown in Challenge 2, the attacker takes full control of\\nthe downstream classifier and can optimize it for malicious\\nprobing. In this section, we further evaluate the protection\\nperformance of EncoderLock with various downstream head\\nstructures. Taking supervised EncoderLock on VGG-11 as an\\nexample, we delve into diverse depths and widths for the linear\\nprobing heads, including varying numbers of layers and hidden\\ndimensions (no hidden layer when there is only one layer in\\nthe classifier). The detailed results of these experiments are\\npresented in Table IV, which shows that one can leverage the\\nfeatures from our pre-trained encoders to achieve impressive\\nperformance ( >99%) on the source task but only achieve up to\\n17.89% on the target task, across a comprehensive collection of\\nconfigurations of the downstream classifiers. This demonstrates\\nthe effectiveness of the self-challenging training scheme.\\n2)EncoderLock on Probing with More Prohibited Data: We\\nconsider a real-world attacker who can be adaptive in attacking\\nthe encoders. In our initial malicious probing scenario, we\\nassume an attacker uses a small amount of data ( 10%) to\\nattempt redirecting the pre-trained encoder. However, once the\\nattacker becomes aware of the defense mechanism, s/he can\\nprobe the downstream head with a larger amount of prohibited\\ndata. We assess the resistance of EncoderLock against such\\npossible attacks in Appendix G. Thanks to the self-challenging\\ntraining in supervised EncoderLock and the manipulation of the\\nlatent feature space in unsupervised EncoderLock, our approach\\ndemonstrates strong resistance to malicious probing, even when\\nthe attacker gains access to the entire prohibited dataset.\\n3)EncoderLock with Various Hyperparameters: we employ\\nthe transfer scenario from MT to UP as an example.\\nRegularization Term ( Œ±):The regularization term in Eqn. (4)\\nplays an important role in balancing the model integrity on\\nthe source domain and preventing malicious probing on the\\ntarget domain. We vary the weight Œ±and Fig. 14 (a) (b)\\nshow the effect of the regularization term for supervised and\\nunsupervised EncoderLock, respectively. It is worth noting that\\nthe regularization term ensures that all Œ±values sustain the\\noriginal performance on the source domain, due to the log-scale\\nterm. Comparing different Œ±, we choose Œ±= 103for supervised\\nEncoderLock and Œ±= 1 for unsupervised EncoderLock.\\nNumber of Critical Weights to Update per Round ( N):\\nFig. 14 (c) inspects the repercussions of varying the number\\nof critical weights ( N) selected in each round, in the range of\\n1to200. When Nis too small, the training of EncoderLock isslow and can not converge even with the maximum number of\\nrounds ( R= 100 ). A larger Nwill cause more weight change\\nfor EncoderLock, reducing the encoder‚Äôs generalizability. Our\\nselection of Ncan be found in Table V.\\nB. Security Analysis of EncoderLock\\nThe security of an encoder protected with EncoderLock\\ncan be assessed by setting an accuracy threshold ( accth) on\\nthe prohibited domain, with the accuracy drop of it on the\\nauthorized domain below a certain constraint ( œµ). For example,\\naccthcan be defined by the accuracy of a train-from-scratch\\nmodel, and the œµis set at 2%, as outlined in Appendix I. In\\nthis case, the encoder is deemed secure because an attacker\\nlacks incentive for malicious probing, with its performance no\\nbetter than direct training.\\nC. Future Works\\nIn this work, we have demonstrated the effectiveness of\\nEncoderLock for different levels of domain data accessibility.\\nHowever, EncoderLock still requires the EaaS provider to\\nclearly specify the prohibited domain, meaning that the provider\\nshould know what the encoder is allowed to do and what\\nshould be prohibited. This causes inconvenience in automatic\\ndetection and restriction on any unknown ‚Äòtheme‚Äô of harmful\\ntasks. One potential avenue for future work is to incorporate\\nEncoderLock with toxicity content detection utilizing large\\nlanguage models [ 60], [80] to automatically identify and restrict\\nprohibited domains without relying on explicit specifications\\nfrom the model owner. This approach would further enhance\\nthe flexibility and adaptability of EncoderLock. Furthermore,\\nwhile we have showcased the effectiveness of EncoderLock\\nin image classification tasks, pre-trained encoders are useful\\nfor many other applications, such as image generation and\\nsemantic segmentation. Extending EncoderLock to these dif-\\nferent tasks may unlock additional potential in controlling the\\nencoder‚Äôs transferability. The main challenge will be how to\\ncombine different forms of loss terms (i.e., generation loss or\\nsegmentation loss) with the loss for a pre-trained encoder.\\nVII. C ONCLUSIONS\\nIn this work, we address a new security issue arising during\\nthe probing process of pre-trained encoders: restricting the\\napplicability to harmful prohibited domains. We recognize a\\nrealistic challenge about the data accessibility to prohibited\\ndomains and propose supervised, unsupervised, and zero-shot\\nEncoderLock for different levels of knowledge of prohibited\\ndomains. We propose novel and effective domain-aware weight\\nselection and self-challenging training to maintain the encoder‚Äôs\\nintegrity while protecting it against malicious probing. Our\\nevaluation has validated the efficacy of EncoderLock in resisting\\nmalicious probing across various domains and encoders.\\nREFERENCES\\n[1] J. Achiam et al. , ‚ÄúGpt-4 technical report,‚Äù arXiv preprint\\narXiv:2303.08774 , 2023.\\n[2] M. Alam et al. , ‚ÄúDeep-lock: Secure authorization for deep neural\\nnetworks,‚Äù arXiv preprint arXiv:2008.05966 , 2020.\\n[3] H. Bahng et al. , ‚ÄúExploring visual prompts for adapting large-scale\\nmodels,‚Äù arXiv preprint arXiv:2203.17274 , 2022.TABLE IV . PERFORMANCE OF THE SUPERVISED ENCODER LOCK ON VARIOUS CLASSIFIER CONFIGURATIONS ‚ÄîSOURCE (MT) TO TARGET (UP).\\n# Layers 1 2 3 4\\nHidden dim. / 256 512 1024 2048 4096 256 512 1024 2048 4096 256 512 1024 2048 4096\\nsize (M) 0.25 6.42 12.85 25.7 51.4 102.8 6.49 13.11 26.75 55.59 119.59 6.56 13.38 27.8 59.8 136.37\\nAccel\\nS(%) 99.12 99.09 99.21 99.20 99.29 99.20 99.32 99.29 99.24 99.27 99.36 99.29 99.29 99.26 99.19 99.30\\nAccel\\nT(%) 17.89 17.89 17.89 13.15 17.89 17.89 9.87 17.89 17.89 17.89 17.89 13.15 17.89 8.47 17.89 17.89\\n[4] Y . Belinkov, ‚ÄúProbing classifiers: Promises, shortcomings, and advances,‚Äù\\nComputational Linguistics , 2022.\\n[5] Y . Bengio et al. , ‚ÄúRepresentation learning: A review and new perspec-\\ntives,‚Äù IEEE transactions on pattern analysis and machine intelligence ,\\nvol. 35, no. 8, pp. 1798‚Äì1828, 2013.\\n[6] A. Bose, ‚ÄúMilitary vehicles dataset,‚Äù https://www.kaggle.com/datasets/\\namanrajbose/millitary-vechiles, n.d., accessed: 2023-04-23.\\n[7] R. Cao, ‚ÄúPutting representations to use,‚Äù Synthese , p. 151, 2022.\\n[8] M. Caron et al. , ‚ÄúEmerging properties in self-supervised vision trans-\\nformers,‚Äù in ICCV , 2021, pp. 9650‚Äì9660.\\n[9] A. Chakraborty, A. Mondai, and A. Srivastava, ‚ÄúHardware-assisted\\nintellectual property protection of deep learning models,‚Äù in DAC , 2020.\\n[10] B. Cheatham, K. Javanmardian, and H. Samandari, ‚ÄúConfronting the\\nrisks of artificial intelligence,‚Äù McKinsey Quarterly , pp. 1‚Äì9, 2019.\\n[11] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, ‚ÄúA simple framework\\nfor contrastive learning of visual representations,‚Äù in ICML , 2020.\\n[12] X. Chen, S. Wang et al. , ‚ÄúCatastrophic forgetting meets negative transfer:\\nBatch spectral shrinkage for safe transfer learning,‚Äù NeurIPS , 2019.\\n[13] A. Chronopoulou, C. Baziotis, and A. Potamianos, ‚ÄúAn embarrassingly\\nsimple approach for transfer learning from pretrained language models,‚Äù\\narXiv preprint arXiv:1902.10547 , 2019.\\n[14] Clarifai, ‚Äúgeneral-image-embedding3,‚Äù 2020. [Online]. Available:\\nhttps://clarifai.com/clarifai/main/models/general-image-embedding\\n[15] A. Coates, A. Ng, and H. Lee, ‚ÄúAn analysis of single-layer networks in\\nunsupervised feature learning,‚Äù in AISTATS , 2011, pp. 215‚Äì223.\\n[16] G. Cohen, S. Afshar et al. , ‚ÄúEmnist: Extending mnist to handwritten\\nletters,‚Äù in IJCNN . IEEE, 2017, pp. 2921‚Äì2926.\\n[17] J.-B. Cordonnier, A. Loukas, and M. Jaggi, ‚ÄúOn the relationship between\\nself-attention and convolutional layers,‚Äù arXiv:1911.03584 , 2019.\\n[18] P. Cunningham, M. Cord, and S. J. Delany, ‚ÄúSupervised learning,‚Äù in\\nMachine learning techniques for multimedia . Springer, 2008, pp. 21‚Äì49.\\n[19] J. Deng, W. Dong et al. , ‚ÄúImagenet: A large-scale hierarchical image\\ndatabase,‚Äù in CVPR . IEEE, 2009, pp. 248‚Äì255.\\n[20] R. Ding, L. Su, A. A. Ding, and Y . Fei, ‚ÄúNon-transferable pruning,‚Äù\\ninEuropean Conference on Computer Vision . Springer, 2025, pp.\\n375‚Äì393.\\n[21] Y . Dong, W.-j. Lu et al. , ‚ÄúPuma: Secure inference of llama-7b in five\\nminutes,‚Äù arXiv preprint arXiv:2307.12533 , 2023.\\n[22] J. Frankle and M. Carbin, ‚ÄúThe lottery ticket hypothesis: Finding sparse,\\ntrainable neural networks,‚Äù arXiv preprint arXiv:1803.03635 , 2018.\\n[23] Y . Gan, Y . Bai et al. , ‚ÄúDecorate the newcomers: Visual domain prompt\\nfor continual test time adaptation,‚Äù in AAAI , 2023.\\n[24] Y . Ganin and V . Lempitsky, ‚ÄúUnsupervised domain adaptation by\\nbackpropagation,‚Äù in ICML . PMLR, 2015, pp. 1180‚Äì1189.\\n[25] Y . Ganin, E. Ustinova et al. , ‚ÄúDomain-adversarial training of neural\\nnetworks,‚Äù The journal of machine learning research , 2016.\\n[26] M. Gao, Q. Wang et al. , ‚ÄúTuning pre-trained model via moment probing,‚Äù\\ninICCV , 2023, pp. 11 803‚Äì11 813.\\n[27] I. Goodfellow, J. Pouget-Abadie et al. , ‚ÄúGenerative adversarial networks,‚Äù\\nCommunications of the ACM , vol. 63, no. 11, pp. 139‚Äì144, 2020.\\n[28] J. Gu, J. Kuen et al. , ‚ÄúSelf-supervised relationship probing,‚Äù NeurIPS ,\\nvol. 33, pp. 1841‚Äì1853, 2020.\\n[29] K. Gupta, N. Jawalkar et al. , ‚ÄúSigma: secure gpt inference with function\\nsecret sharing,‚Äù Cryptology ePrint Archive , 2023.\\n[30] K. Han, Y . Wang et al. , ‚ÄúA survey on vision transformer,‚Äù IEEE\\ntransactions on pattern analysis and machine intelligence , vol. 45,\\nno. 1, pp. 87‚Äì110, 2022.\\n[31] X. Han, Z. Zhang et al. , ‚ÄúPre-trained models: Past, present and future,‚Äù\\nAI Open , vol. 2, pp. 225‚Äì250, 2021.[32] K. He, X. Chen et al. , ‚ÄúMasked autoencoders are scalable vision learners,‚Äù\\ninPCVPR , 2022, pp. 16 000‚Äì16 009.\\n[33] K. He, H. Fan et al. , ‚ÄúMomentum contrast for unsupervised visual\\nrepresentation learning,‚Äù in CVPR , 2020, pp. 9729‚Äì9738.\\n[34] K. He, X. Zhang et al. , ‚ÄúDeep residual learning for image recognition,‚Äù\\ninCVPR , 2016, pp. 770‚Äì778.\\n[35] D. Hendrycks et al. , ‚ÄúUsing self-supervised learning can improve model\\nrobustness and uncertainty,‚Äù NeurIPS , vol. 32, 2019.\\n[36] J. Howard, ‚Äúimagenette.‚Äù [Online]. Available: https://github.com/fastai/\\nimagenette/\\n[37] J. Howard and S. Gugger, ‚ÄúFastai: A layered api for deep learning,‚Äù\\nInformation , vol. 11, no. 2, p. 108, 2020.\\n[38] J. J. Hull, ‚ÄúA database for handwritten text recognition research,‚Äù IEEE\\nTransactions on pattern analysis and machine intelligence , vol. 16, no. 5,\\npp. 550‚Äì554, 1994.\\n[39] M. Iman, H. R. Arabnia, and K. Rasheed, ‚ÄúA review of deep transfer\\nlearning and recent advancements,‚Äù Technologies , 2023.\\n[40] International Women‚Äôs Day, ‚ÄúGender and ai: Addressing bias in artifi-\\ncial intelligence,‚Äù https://www.internationalwomensday.com/Missions/\\n14458/Gender-and-AI-Addressing-bias-in-artificial-intelligence, 2024,\\naccessed: 2024-06-22.\\n[41] A. Jaiswal, A. R. Babu, M. Z. Zadeh, D. Banerjee, and F. Makedon,\\n‚ÄúA survey on contrastive self-supervised learning,‚Äù Technologies , vol. 9,\\nno. 1, p. 2, 2020.\\n[42] H. Jia, C. A. Choquette-Choo, V . Chandrasekaran, and N. Papernot,\\n‚ÄúEntangled watermarks as a defense against model extraction,‚Äù in USENIX\\nSecurity 21 , 2021, pp. 1937‚Äì1954.\\n[43] M. Jia, L. Tang et al. , ‚ÄúVisual prompt tuning,‚Äù in ECCV . Springer,\\n2022, pp. 709‚Äì727.\\n[44] D. P. Kingma and J. Ba, ‚ÄúAdam: A method for stochastic optimization,‚Äù\\nICLR , 2015.\\n[45] D. P. Kingma and M. Welling, ‚ÄúAuto-encoding variational bayes,‚Äù arXiv\\npreprint arXiv:1312.6114 , 2013.\\n[46] A. Krizhevsky, G. Hinton et al. , ‚ÄúLearning multiple layers of features\\nfrom tiny images,‚Äù 2009.\\n[47] P. H. Le-Khac, G. Healy, and A. F. Smeaton, ‚ÄúContrastive representation\\nlearning: A framework and review,‚Äù Ieee Access , vol. 8, pp. 193 907‚Äì\\n193 934, 2020.\\n[48] Y . LeCun, L. Bottou et al. , ‚ÄúGradient-based learning applied to document\\nrecognition,‚Äù Proceedings of the IEEE , vol. 86, no. 11, 1998.\\n[49] Y . Li, Z. Zhang et al. , ‚ÄúModeldiff: Testing-based dnn similarity\\ncomparison for model reuse detection,‚Äù in ISSTA , 2021, pp. 139‚Äì151.\\n[50] B. Liu et al. , ‚ÄúTranstailor: Pruning the pre-trained model for improved\\ntransfer learning,‚Äù in AAAI , vol. 35, no. 10, 2021.\\n[51] R. Liu et al. , ‚ÄúSecdeep: Secure and performant on-device deep learning\\ninference framework for mobile and iot devices,‚Äù in IoTDI , 2021.\\n[52] Y . Liu, L. Wei, B. Luo, and Q. Xu, ‚ÄúFault injection attack on deep\\nneural network,‚Äù in ICCAD . IEEE, 2017, pp. 131‚Äì138.\\n[53] Z. Liu, M. Sun et al. , ‚ÄúRethinking the value of network pruning,‚Äù arXiv\\npreprint arXiv:1810.05270 , 2018.\\n[54] P. Marcelino, ‚ÄúTransfer learning from pre-trained models,‚Äù Towards data\\nscience , vol. 10, no. 330, p. 23, 2018.\\n[55] B. Marr, ‚ÄúIs artificial intelligence dangerous? 6 ai risks everyone should\\nknow about,‚Äù Forbes. Retrieved May , vol. 13, p. 2022, 2018.\\n[56] A. Miko≈Çajczyk et al. , ‚ÄúData augmentation for improving deep learning\\nin image classification problem,‚Äù in IIPhDW . IEEE, 2018, pp. 117‚Äì122.\\n[57] Y . Netzer, T. Wang et al. , ‚ÄúReading digits in natural images with\\nunsupervised feature learning,‚Äù in NIPS workshop on deep learning\\nand unsupervised feature learning , vol. 2011, no. 2. Granada, 2011.[58] OpenAI, ‚ÄúOpenai‚Äôs embeddings api,‚Äù 2020, accessed: 4 October 2024.\\n[Online]. Available: https://platform.openai.com/docs/guides/embeddings\\n[59] S. Parisi et al. , ‚ÄúThe unsurprising effectiveness of pre-trained vision\\nmodels for control,‚Äù in ICML . PMLR, 2022, pp. 17 359‚Äì17 371.\\n[60] M. Phute, A. Helbling et al. , ‚ÄúLlm self defense: By self examination,\\nllms know they are being tricked,‚Äù in ICLR , 2023.\\n[61] L. Prechelt, ‚ÄúEarly stopping-but when?‚Äù in Neural Networks: Tricks of\\nthe trade . Springer, 2002, pp. 55‚Äì69.\\n[62] X. Qiu, T. Sun et al. , ‚ÄúPre-trained models for natural language processing:\\nA survey,‚Äù Science China technological sciences , 2020.\\n[63] W. Qu, J. Jia, and N. Z. Gong, ‚ÄúReaas: Enabling adversarially robust\\ndownstream classifiers via robust encoder as a service,‚Äù in NDSS , 2023.\\n[64] A. Radford, J. W. Kim et al. , ‚ÄúLearning transferable visual models from\\nnatural language supervision,‚Äù in ICML . PMLR, 2021, pp. 8748‚Äì8763.\\n[65] A. S. Rakin, Z. He, and D. Fan, ‚ÄúBit-flip attack: Crushing neural network\\nwith progressive bit search,‚Äù in ICCV , 2019, pp. 1211‚Äì1220.\\n[66] A. Ravichander, Y . Belinkov, and E. Hovy, ‚ÄúProbing the probing\\nparadigm: Does probing accuracy entail task relevance?‚Äù arXiv preprint\\narXiv:2005.00719 , 2020.\\n[67] D. A. Reynolds et al. , ‚ÄúGaussian mixture models.‚Äù Encyclopedia of\\nbiometrics , vol. 741, no. 659-663, 2009.\\n[68] R. Rombach, A. Blattmann et al. , ‚ÄúHigh-resolution image synthesis with\\nlatent diffusion models,‚Äù 2021.\\n[69] R. R. Selvaraju, M. Cogswell et al. , ‚ÄúGrad-cam: Visual explanations\\nfrom deep networks via gradient-based localization,‚Äù in ICCV , 2017.\\n[70] Y . Shen, C. Guo et al. , ‚ÄúFinancial feature embedding with knowledge\\nrepresentation learning for financial statement fraud detection,‚Äù Procedia\\nComputer Science , vol. 187, pp. 420‚Äì425, 2021.\\n[71] C. Shorten and T. M. Khoshgoftaar, ‚ÄúA survey on image data augmen-\\ntation for deep learning,‚Äù Journal of big data , 2019.\\n[72] K. Simonyan and A. Zisserman, ‚ÄúVery deep convolutional networks for\\nlarge-scale image recognition,‚Äù in ICLR , 2019.\\n[73] N. Tajbakhsh, J. Y . Shin et al. , ‚ÄúConvolutional neural networks for\\nmedical image analysis: Full training or fine tuning?‚Äù IEEE transactions\\non medical imaging , vol. 35, no. 5, pp. 1299‚Äì1312, 2016.\\n[74] Y . Tan, G. Long et al. , ‚ÄúFederated learning from pre-trained models: A\\ncontrastive learning approach,‚Äù NeurIPS , vol. 35, 2022.\\n[75] The White House Office of Science and Technology Policy, ‚ÄúAi bill of\\nrights: Algorithmic discrimination protections,‚Äù https://www.whitehouse.\\ngov/ostp/ai-bill-of-rights/algorithmic-discrimination-protections-2/,\\n2024, accessed: 2024-06-22.\\n[76] L. Van der Maaten and G. Hinton, ‚ÄúVisualizing data using t-sne.‚Äù Journal\\nof machine learning research , vol. 9, no. 11, 2008.\\n[77] H. Wang, J. Li et al. , ‚ÄúPre-trained language models and their applications,‚Äù\\nEngineering , 2022.\\n[78] L. Wang, M. Wang, D. Zhang, and H. Fu, ‚ÄúModel barrier: A compact un-\\ntransferable isolation domain for model intellectual property protection,‚Äù\\ninCVPR , 2023, pp. 20 475‚Äì20 484.\\n[79] L. Wang et al. , ‚ÄúNon-transferable learning: A new approach for model\\nownership verification and applicability authorization,‚Äù in ICLR , 2022.\\n[80] Y .-S. Wang and Y . Chang, ‚ÄúToxicity detection with generative prompt-\\nbased inference,‚Äù arXiv preprint arXiv:2205.12390 , 2022.\\n[81] J. C. White, T. Pimentel, N. Saphra, and R. Cotterell, ‚ÄúA non-linear\\nstructural probe,‚Äù arXiv preprint arXiv:2105.10185 , 2021.\\n[82] Z.-F. Wu, C. Mao et al. , ‚ÄúStructured model probing: Empowering efficient\\ntransfer learning by structured regularization,‚Äù in CVPR , 2024.\\n[83] L. Yang, Y . Wang et al. , ‚ÄúFine-grained visual prompting,‚Äù NeurIPS ,\\nvol. 36, 2024.\\n[84] P. Yang, Y . Lao, and P. Li, ‚ÄúRobust watermarking for deep neural\\nnetworks via bi-level optimization,‚Äù in ICCV , 2021, pp. 14 841‚Äì14 850.\\n[85] H.-C. Yi et al. , ‚ÄúGraph representation learning in bioinformatics: trends,\\nmethods and applications,‚Äù Briefings in Bioinformatics , 2022.\\n[86] L. Yuan, D. Chen et al. , ‚ÄúFlorence: A new foundation model for computer\\nvision,‚Äù arXiv preprint arXiv:2111.11432 , 2021.\\n[87] M. D. Zeiler, ‚ÄúAdadelta: an adaptive learning rate method,‚Äù arXiv preprint\\narXiv:1212.5701 , 2012.[88] J. Zhang, Z. Gu et al. , ‚ÄúProtecting intellectual property of deep neural\\nnetworks with watermarking,‚Äù in ASIACCS , 2018, pp. 159‚Äì172.\\n[89] P. Zhao, S. Wang, C. Gongye, Y . Wang, Y . Fei, and X. Lin, ‚ÄúFault\\nsneaking attack: A stealthy framework for misleading deep neural\\nnetworks,‚Äù in DAC , 2019, pp. 1‚Äì6.\\n[90] G. Zhong, L.-N. Wang, X. Ling, and J. Dong, ‚ÄúAn overview on data\\nrepresentation learning: From traditional feature learning to recent deep\\nlearning,‚Äù The Journal of Finance and Data Science , 2016.\\n[91] T. Zhou, S. Ren, and X. Xu, ‚ÄúArchlock: Locking dnn transferability at\\nthe architecture level with a zero-cost binary predictor,‚Äù in The Twelfth\\nInternational Conference on Learning Representations , 2024.\\n[92] M. Zhu and S. Gupta, ‚ÄúTo prune, or not to prune: exploring the efficacy\\nof pruning for model compression,‚Äù arXiv preprint arXiv:1710.01878 ,\\n2017.\\nAPPENDIX A\\nHYPERPARAMETER CONFIGURATION OF ENCODER LOCK\\nTABLE V . H YPERPARAMETERS USED IN THE EXPERIMENT\\nHyperparameters N R Œ± LR\\nSupervised EncoderLock 100 100 1,000 0.01\\nUnsupervised EncoderLock 200 100 10 0.01\\nIn this section, we provide the hyper-parameters config-\\nuration in this work. During training EncoderLock, we use\\n1,000samples from the training dataset. During fine-tuning of\\nthe downstream classifiers, we use 10% of the training data\\nand evaluate with the entire testing data (usually 20% of the\\ndata). The fine-tuning process uses the Adam optimizer [ 44]\\nwith adaptive learning rate scheduling [ 87] and an early\\nstopping criterion [ 61] (patience= 10). For the default supervised\\nEncoderLock configuration, we use N= 100 ,R= 100 ,\\nandŒ±= 103. For the default unsupervised EncoderLock\\nconfiguration, we use N= 200 ,R= 100 , and Œ±= 101.\\nHyperparameters are evaluated in Section VI-A.\\nAPPENDIX B\\nDATASET SIMILARITY\\nTABLE VI. DATASETS ‚ÄôFEATURE SPACE COSINE SIMILARITY\\nMT UP MM SN SD EM CF10 STL10 CF100\\nMT 0.999 0.707 0.577 0.452 0.706 0.942 0.404 0.303 0.448\\nUP 0.712 0.999 0.551 0.652 0.892 0.786 0.395 0.276 0.425\\nMM 0.579 0.570 0.993 0.570 0.606 0.590 0.505 0.549 0.5151\\nSN 0.467 0.650 0.570 0.998 0.777 0.521 0.522 0.455 0.548\\nSD 0.712 0.891 0.587 0.768 0.999 0.791 0.452 0.345 0.483\\nEM 0.938 0.780 0.553 0.514 0.794 0.999 0.404 0.296 0.432\\nCF10 0.408 0.398 0.499 0.519 0.454 0.405 0.996 0.831 0.972\\nSTL10 0.302 0.280 0.566 0.459 0.347 0.301 0.836 0.993 0.788\\nCF100 0.450 0.428 0.507 0.557 0.470 0.438 0.971 0.801 0.995\\nEvaluating the transferability of our EncoderLock inherently\\ninvolves understanding the similarities between datasets, as this\\nnot only influences encoder transfer learning performance but\\nalso reflects the intrinsic characteristics of the data domains.\\nQuantifying domain similarity is challenging, yet crucial for\\na comprehensive evaluation. To address this, we adopt an\\napproach inspired by previous work [ 49], utilizing cosine\\nsimilarity as a metric to compare the features of input samples\\nacross different domains. We employ a widely-used feature\\nextractor, a PyTorch pre-trained VGG-16 model, to extract\\nlatent features from pairs of data domains and calculate their\\ncosine similarity. The results, presented in Table VI, corroborate\\nthe observations made in Section V-B regarding the high\\nsimilarity between the MT domain and the UP and EM\\ndomains‚Äîhighlighted in bold within the table. This supports the\\nnotion that similarities in feature space significantly impact the\\ntransferability of both targeted and source-only EncoderLock.APPENDIX C\\nTRAINING COST OF ENCODER LOCK\\nTABLE VII. T RAINING TIME COMPLEXITY OF ENCODER LOCK\\nArchitecture Level Source Target SIZE DWS (s) DWU (s) SC (s)\\nResNet-18 sup. MT UP 32 0.771¬±0.147 5.85¬±0.021 2.33¬±1.59\\nResNet-18 unsup./zero-shot MT UP 32 1.03¬±0.086 45.8¬±0.126 -\\nVGG-11 sup. MT UP 32 0.821¬±0.275 8.66¬±0.039 8.75¬±9.11\\nVGG-11 unsup./zero-shot MT UP 32 1.34¬±0.498 45.1¬±0.122 -\\nResNet-18 sup. ImageNette Military 224 1.45¬±0.217 59.2¬±2.23 53.1¬±10.2\\nResNet-18 unsup./zero-shot ImageNette Military 224 1.82¬±0.286 455.2¬±1.03 -\\nIn Table VII, we present the training time costs of su-\\npervised, unsupervised, and zero-shot EncoderLock on our\\nplatform equipped with an RTX TITAN GPU. Note the zero-\\nshot EncoderLock has similar computational complexity as\\nthe unsupervised version, because they employ the same\\ntraining strategy. We choose to examine two representative\\nsource-target domain pairs: a smaller digits pair (MT and\\nUP) with an input size of 32√ó32√ó3, and a more complex\\nimage pair (ImageNette and Military) with an input size of\\n224√ó224√ó3. Additionally, we assess the training costs\\nassociated with two different encoder architectures, ResNet-18\\nand VGG-11. We break down the training complexity into\\nthe three key steps described in EncoderLock: domain-aware\\nweight search (DWS), domain-aware weight updating (DWU),\\nand self-challenging training (SC). The results demonstrate\\nthat the primary computational burden lies in the DWU step,\\nand for supervised EncoderLock, the self-challenging phase is\\nalso costly, where the classifier must be retrained to enhance\\nrobustness against adversarial attacks. Furthermore, we find that\\nDWS and DWU in unsupervised and zero-shot EncoderLock\\nincur higher computational costs compared to the supervised\\nversion. This is attributed to the contrastive learning-based loss\\nfunction (7). When comparing different input sizes, higher-\\nresolution images lead to substantially longer training time\\nbecause of the larger feature maps to compute during the\\nforward pass of the encoder, especially in the DWU process.\\nLarge models also require more training time. It is worth noting\\nthat such training cost is a one-time expense, and there is no\\nperformance impact on the protected encoder during inference.\\nAPPENDIX D\\nNUMERICAL RESULTS ‚Äì SUPERVISED ENCODER LOCK (VIT)\\nFollowing a similar setting in Section V-G, we consider that\\nthe model provider aims to prevent the pre-trained ViT encoder\\non ImageNette from being transferred to a specified simple\\nunauthorized domain. In particular, we evaluate EncoderLock\\non four target datasets: CF, ST, MT, and CF100. To fit the input\\nsize of ViT, we resize the target input to 224√ó224√ó3and\\nmonitor the accuracy degradation on both the target datasets and\\nImageNet [ 37], the source dataset. During training, we follow\\nthe ViT fine-tuning instructions and use the last hidden state as\\nthe extracted feature space and connect to a single-dense-layer\\noutput classifier. From results shown in Table VIII, we find\\nthat EncoderLock reaches the non-transferability design goal ‚Äî\\nreducing the ViT performance on the target domain by 65.8%\\nbut keeping the accuracy on the source domain with a small\\naccuracy drop of 2.15%. Our experimental results demonstrate\\nthat EncoderLock is effective when applied to a large encoder\\npre-trained on a large amount of samples.TABLE VIII. ENCODER LOCK ON VIT‚ÄìSOURCE TASK\\n(IMAGE NETTE [36])\\nTarget Accorg\\nSAccorg\\nTAccel\\nS Accel\\nT Drop S Drop T\\nCF\\n87.2674.99 86.83 36.70 0.49%‚Üì51.9%‚Üì\\nST 73.50 86.80 10.53 0.53%‚Üì85.7%‚Üì\\nMT 92.06 85.20 47.56 2.36%‚Üì48.3%‚Üì\\nCF100 53.31 82.62 12.07 5.32%‚Üì77.4%‚Üì\\nAPPENDIX E\\nUNSUPERVISED ENCODER LOCK ON RESNET-18\\nIn this section, we present the additional results using\\nResNet-18 for the unsupervised EncoderLock in Table IX.\\nThe unsupervised EncoderLock also shows promising result in\\napplicability authorization.\\nAPPENDIX F\\nIMPACT OF THE SYNTHETIC DATASET QUALITY ON\\nZERO-SHOT ENCODER LOCK\\nIn addition to prompt relevance, the generation quality of the\\nsynthetic dataset significantly affects the performance of zero-\\nshot EncoderLock. We measure the quality with two metrics:\\nthe noise level of the synthetic images and the generation quality\\nof the diffusion model. To ensure a fair comparison, we use\\nthe same set of prompts (refined prompts from the prohibited\\ndomain) for zero-shot EncoderLock and select an encoder\\nwith the same degradation level on the authorized domain as\\nreported in Section V-D (greater than 92%). We report the\\nEncoderLock performance in the format of ( accS\\nm, accT\\nm). Note\\nthat the defense goal of EncoderLock is a higher accS\\nmand\\nlower accT\\nm.\\ntarget acc 23.69 %target acc 32.02 %target acc 37.13 %target acc 46.30 %\\nprotection performancenoise level\\nFig. 15. Zero-shot EncoderLock performance with different noise levels\\nFig. 15 shows an example image with different levels of\\nGaussian noise. Introducing random noise into the images\\nreduces EncoderLock‚Äôs ability to restrict the prohibited domain.\\nWe assess the impact of different noise levels, at œÉ= 1,\\nœÉ= 5, and œÉ= 10 , respectively. The zero-shot EncoderLock\\nperformance degrades to ( 91.90%,32.02%), (91.85%,37.13%),\\nand ( 92.20%,46.30%), where the performance on the noise-\\nfree synthetic dataset is ( 92.86%,23.69%). High level of noise\\nsignificantly reduces the synthetic quality, leading to poorer\\nperformance of the zero-shot EncoderLock.\\ntarget acc 46.25% target acc 37.56% target acc 25.60% target acc 23.69%\\nProtection performancegeneration inference iterations\\nFig. 16. Zero-shot EncoderLock performance with different diffusion qualitiesTABLE IX. UNSUPERVISED ENCODER LOCK ‚ÄôS PERFORMANCE ON ENCODER (RESNET-18) TRANSFERABILITY\\nSource \\\\Target MT UP SN MM SD ‚àÜW Drop S Drop T\\nMT 99.48‚áí98.87 93.77‚áí10.26 41.47‚áí11.27 70.02‚áí32.04 72.48‚áí16.17 3.27‚Ä∞ 0.61%‚Üì73.45%‚Üì\\nUP 95.10‚áí37.66 96.11‚áí94.95 33.72‚áí18.15 55.79‚áí18.67 61.76‚áí15.18 2.91‚Ä∞ 1.21%‚Üì62.13%‚Üì\\nSN 94.65‚áí22.62 88.04‚áí6.98 91.06‚áí89.55 66.52‚áí13.92 95.08‚áí88.9 1.41‚Ä∞ 1.66%‚Üì63.43%‚Üì\\nMM 98.82‚áí17.94 92.33‚áí17.14 48.18‚áí18.18 91.49‚áí90.97 78.03‚áí43.83 1.19‚Ä∞ 0.57%‚Üì67.34%‚Üì\\nSD 96.76‚áí50.06 91.68‚áí21.72 86.74‚áí30.86 68.56‚áí17.68 99.42‚áí97.75 2.58‚Ä∞ 1.67%‚Üì65.80%‚Üì\\nWe regenerate the synthetic datasets with varying number\\nof inference iterations in the stable diffusion model. The\\ngeneration quality improves as we increase the iteration count\\nfrom 5,10,20, to50(the value used in the original setting), as\\nshown in Fig. 16. Consequently, the protection performance im-\\nproves from ( 92.71%,46.25%) to (92.25%,37.56%), (92.31%,\\n25.60%) and ( 92.86%,23.69%). A higher-quality synthetic\\ndataset provides clearer potential features of the prohibited\\ndomain, thus enhancing the restriction performance. However,\\nincreasing the number of inference iterations in the generator\\nleads to higher computational costs. Running 5 inference\\niterations achieves a speed of 8.2items/s, whereas 50 inference\\niterations reduce the speed to 0.8items/s.\\nAPPENDIX G\\nENCODER LOCK PERFORMANCE ON VARIOUS PROBING DATA\\nSupervised EncoderLock\\nUnsupervised EncoderLock\\nFig. 17. EncoderLock performance for various volumes of probing data.\\nWe evaluate the scenario where an attacker probes the\\nencoder using varying amounts of probing data, ranging from\\n10% to the entire dataset, in both supervised and unsupervised\\nsettings. The results are illustrated in Fig. 17. Notably, for\\nthe prohibited target domain, the accuracy remains low even\\nwhen the attacker utilizes the full prohibited dataset, while the\\nprotection slightly degrades with more data. For the authorized\\nsource domain, the accuracy remains consistently high even\\nwith a small amount of probing data. These results demonstrate\\nthe robustness of both supervised and unsupervised versions\\nof EncoderLock against malicious probing attempts.\\nAPPENDIX H\\nCOMPARE WITH PREVIOUS WORK\\nIn this section, we present more comparison results with\\nbaselines similar to Table X. From Table XI to Table XVIII,\\nwe compare the baseline methods and the proposed supervised\\nEncoderLock and unsupervised EncoderLock between different\\npairs of digit datasets. The observation is similar to our\\nconclusion in Section V-E. Specifically, under the condition of\\nfine-tuning the downstream model fine-tuning, the proposed\\nmethods outperform baselines.TABLE X. C OMPARISON THE EFFECTIVENESS OF ENCODER LOCK ON\\nTHE TARGET DOMAIN AND ITS PERFORMANCE ON OTHER DOMAINS WITH\\nBASELINES [79], [78]. I N THIS TABLE ,BOLD TEXT INDICATES THE BEST\\nPERFORMANCE ,UNDERLINED DENOTES THE SECOND -BEST PERFORMANCE .\\nTHE GRAY ROW DENOTES ORIGINAL TRANSFER ACCURACY .\\nMethods \\\\DomainSource Target Other Domains\\nMM UP MT SN SD\\nOriginal Accuracy 94.2% 94.7% 98.9% 53.8% 85.9%\\nNTL [79] 81.6% 77.3% 97.9% 30.4% 62.1%\\nCUTI [78] 66.6% 90.6% 96.0% 52.3% 82.0%\\nSupervised EncoderLock 93.5% 17.8% 98.8% 39.3% 69.1%\\nUnsupervised EncoderLock 93.3% 30.7% 98.9% 49.0% 78.4%\\nTABLE XI. C OMPARISON OF ENCODER LOCK WITH BASELINES\\nMethods \\\\DomainSource Target Other Domains\\nUP MM MT SN SD\\nOriginal Accuracy 97.9% 65.2% 97.7% 58.2% 87.1%\\nNTL [79] 90.2% 12.8% 93.4% 19.3% 33.8%\\nCUTI [78] 93.9% 33.7% 93.6% 31.1% 78.9%\\nSupervised EncoderLock 96.9% 15.6% 81.2% 31.2% 60.9%\\nUnsupervised EncoderLock 96.7% 9.59% 60.5% 22.6% 31.9%\\nTABLE XII. C OMPARISON OF ENCODER LOCK WITH BASELINES\\nMethods \\\\DomainSource Target Other Domains\\nMT UP MM SN SD\\nOriginal Accuracy 99.5% 96.4% 68.2% 43.7% 69.6%\\nNTL [79] 97.4% 78.1% 37.4% 19.8% 38.6%\\nCUTI [78] 97.9% 93.6% 66.5% 48.4% 86.5%\\nSupervised EncoderLock 99.3% 8.47% 25.3% 19.6% 14.5%\\nUnsupervised EncoderLock 99.1% 16.8% 49.3% 25.4% 43.1%\\nTABLE XIII. C OMPARISON OF ENCODER LOCK WITH BASELINES\\nMethods \\\\DomainSource Target Other Domains\\nSN UP MT MM SD\\nOriginal Accuracy 94.0% 92.7% 95.3% 71.5% 97.0%\\nNTL [79] 78.0% 88.4% 96.3% 72.3% 92.6%\\nCUTI [78] 62.1% 91.3% 95.5% 67.0% 82.2%\\nSupervised EncoderLock 87.3% 19.7% 12.0% 51.9% 84.8%\\nUnsupervised EncoderLock 94.7% 17.1% 94.2% 70.3% 96.9%\\nTABLE XIV . C OMPARISON OF ENCODER LOCK WITH BASELINES\\nMethods \\\\DomainSource Target Other Domains\\nSD MT UP SN MM\\nOriginal Accuracy 99.8% 97.1% 93.6% 53.8% 71.5%\\nNTL [79] 95.9% 90.6% 89.6% 57.0% 62.0%\\nCUTI [78] 87.1% 96.0% 88.7% 61.9% 64.3%\\nSupervised EncoderLock 99.6% 11.9% 90.4% 88.9% 50.6%\\nUnsupervised EncoderLock 99.5% 76.6% 90.9% 87.9% 62.3%\\nTABLE XV . C OMPARISON OF ENCODER LOCK WITH BASELINES\\nMethods \\\\DomainSource Target Other Domains\\nUP SD MT SN MM\\nOriginal Accuracy 97.9% 87.1% 97.7% 58.2% 65.2%\\nNTL [79] 89.2% 28.2% 89.7% 18.7% 17.1%\\nCUTI [78] 92.8% 77.7% 95.0% 35.1% 37.3%\\nSupervised EncoderLock 96.8% 16.9% 96.8% 45.7% 57.5%\\nUnsupervised EncoderLock 96.7% 12.7% 88.0% 19.7% 42.3%TABLE XVI. C OMPARISON OF ENCODER LOCK WITH BASELINES\\nMethods \\\\DomainSource Target Other Domains\\nMT MM UP SN SD\\nOriginal Accuracy 99.5% 68.2% 96.4% 43.7% 69.6%\\nNTL [79] 97.6% 33.7% 79.9% 19.8% 33.4%\\nCUTI [78] 98.1% 56.1% 93.2% 34.4% 84.9%\\nSupervised EncoderLock 99.3% 23.2% 69.3% 19.6% 31.4%\\nUnsupervised EncoderLock 99.2% 12.3% 87.1% 26.1% 43.6%\\nTABLE XVII. C OMPARISON OF ENCODER LOCK WITH BASELINES\\nMethods \\\\DomainSource Target Other Domains\\nSN MM MT UP SD\\nOriginal Accuracy 94.0% 71.5% 95.3% 92.7% 97.0%\\nNTL [79] 77.8% 72.6% 96.8% 89.8% 92.2%\\nCUTI [78] 65.4% 61.5% 95.5% 89.7% 87.9%\\nSupervised EncoderLock 91.8% 14.6% 17.3% 93.1% 91.8%\\nUnsupervised EncoderLock 94.6% 50.8% 93.3% 93.8% 97.1%\\nTABLE XVIII. C OMPARISON OF ENCODER LOCK WITH BASELINES\\nMethods \\\\DomainSource Target Other Domains\\nSD MM UP SN MT\\nOriginal Accuracy 99.8% 71.5% 93.6% 53.8% 97.1%\\nNTL [79] 89.8% 56.1% 90.5% 51.4% 96.1%\\nCUTI [78] 90.7% 66.1% 91.5% 67.1% 96.8%\\nSupervised EncoderLock 99.8% 40.9% 94.2% 90.2% 97.2%\\nUnsupervised EncoderLock 99.5% 27.2% 88.7% 99.6% 88.8%\\nAPPENDIX I\\nSECURITY ANALYSIS ‚ÄìTRAIN -FROM -SCRATCH ACCURACY\\nMT√†UPUP√†MTMM√†UPNumber of Training/Probing EpochsNumber of Training/Probing EpochsNumber of Training/Probing EpochsAccuracy (%)\\nFig. 18. EncoderLock‚Äôs Performance Versus Train-from-scratch\\nHere we apply the security assessment definition in Sec-\\ntionVI-B on three example pairs of domains: MT to UP ,UP to\\nMT, and MM to UP for the supervised EncoderLock on VGG-\\n11. Their accuracy drops on the authorized domain are 0.07%,\\n0.25%, and 0.17%, respectively, all below the accuracy drop\\nconstraint ( œµ=2%). Fig. 18 shows the probing performance\\nof the EncoderLock-protected and unprotected encoders on\\nthe prohibited domain, compared to the accuracy of the ‚Äútrain-\\nfrom-scratch‚Äù model. It demonstrates that starting from the\\nEncoderLock-protected encoder allows the model to achieve\\nlower accuracy and faster convergence on the prohibited domain,\\nthan a model trained from scratch. Therefore, the protected\\nencoder can be considered SECURE , as an attacker would\\nhave no motivation to perform malicious probing. By contrast,\\nthe original encoder is NOT SECURE , as the accuracy (red) is\\nalways higher than train-from-scratch accuracy (gray) as shown\\nin Fig. 18.\\nAPPENDIX J\\nENCODER LOCK GRADCAM ONADMISSIBLE DOMAINS\\nIn this section, we present the additional results to visualize\\nthe admissible domains with GradCAM in Fig. 19.\\nAPPENDIX K\\nGENERATED PROMPTS &IMAGES\\nThe (refined) prompts for generating synthetic datasets in\\nzero-shot EncoderLock. The theme is military vehicles .\\nPre-trained Encoder Supervised Unsupervised Zero -shot (Manual)  Zero -shot (Initial) Zero -shot (Refined) Original ImageAdmissible 1\\n (Sedan)Admissible 2\\n (Weapon)Admissible 3\\n (hen)Fig. 19. Interpretation of Different EncoderLock using GradCAM [ 69]‚Äì\\nthe red parts highlight the focus of encoder to make decisions.\\nManual Prompts. See Figure 20\\n‚Ä¢Armored Personnel Carrier\\n‚Ä¢Anti-tank Combat Vehicle\\n‚Ä¢Tactical Missile Vehicle\\n‚Ä¢Forward Command Vehicle\\n‚Ä¢Communication Support Vehicle‚Ä¢Artillery Tractor\\n‚Ä¢Logistic Support Transport Vehicle\\n‚Ä¢Tank\\n‚Ä¢Self-propelled Artillery\\n‚Ä¢Multi-functional Infantry Vehicle\\nInitial Prompts: The synthetic images shown in Figure 21\\n‚Ä¢futuristic tank, stealth design\\n‚Ä¢antique cannon, ceremonial use\\n‚Ä¢amphibious assault vehicle, coastal\\noperations\\n‚Ä¢drone carrier truck, mobile base\\n‚Ä¢armored medical evacuation vehi-\\ncle, red cross\\n‚Ä¢cyberpunk hoverbike, scout unit‚Ä¢nuclear-powered submarine, deep-\\nsea exploration\\n‚Ä¢stealth bomber, night operation\\n‚Ä¢battlefield command and control\\ncenter, high-tech\\n‚Ä¢anti-aircraft missile system, mobile\\ndefense\\nRefined Prompts: The synthetic image shown in Figure 22\\n‚Ä¢Armored Ground Vehicle, Modern\\nCombat\\n‚Ä¢Artillery System, Classic Aesthet-\\nics\\n‚Ä¢Amphibious Assault Transport\\n‚Ä¢Drone Carrier, Tactical\\n‚Ä¢Field Support Unit, Healthcare‚Ä¢Reconnaissance Craft, Urban\\nAerial\\n‚Ä¢Deep Sea Explorer, Nuclear Propul-\\nsion\\n‚Ä¢Stealth Surveillance Plane\\n‚Ä¢Command Center, High-Tech\\n‚Ä¢Missile Defense Network, Mobile\\nSelf-propelled Artillery\\n Artillery Tractor\\n Logistic Support Transport Vehicle\\n Armored Personnel \\nCarrier\\nForward Command Vehicle\\nAnti -tank Combat Vehicle\\n Communication Support Vehicle Tank\\n Multi -functional Infantry Vehicle\\n Tactical Missile Vehicle\\nFig. 20. Manual Prompts and Generated Synthetic Dataset\\nAntique Cannon\\n Battlefield Command and Control Center\\n Stealth Bomber\\n Futuristic Tank\\n Cyberpunk Hoverbike\\nArmored Medical Evacuation Vehicle\\n Anti-aircraft Missile System Amphibious Assault Vehicle\\n Drone Carrier Truck\\n Nuclear-powered Submarine\\nFig. 21. AI agent Initial Prompts and Generated Synthetic Dataset\\nAmphibious Assault Transport \\n Reconnaissance Craft\\n Deep Sea Explorer\\n Command Center\\n Armored Ground Vehicle\\nDrone Carrier\\n Field Support Unit Missile Defense Network\\n Stealth Surveillance Plane \\n Artillery System\\nFig. 22. AI agent Refined Prompts and Generated Synthetic Dataset',\n",
       " '   \\n \\n  1 \\n Optimal Transcoding Preset Selection for Live Video \\nStreaming  \\nZahra Nabizadeh Shahr -Babak1, Maedeh Jamali1, Nader Karimi2, Shadrokh Samavi3, Shahram  Shirani1 \\n1Electrical and Computer Engineering, McMaster University, Hamilton, Canada  \\n2Electrical and Computer Engineering, Isfahan University of Technology, Isfahan, Iran  \\n3Computer Science, Seattle University, Seattle, USA  \\n \\n \\nAbstract  \\nIn today‚Äôs digital landscape, video content dominates Internet traffic, underscoring the need for efficient video \\nprocessing to support seamless live streaming experiences on platforms like YouTube Live, Twitch, and Facebook \\nLive. This paper introduces a c omprehensive framework designed to optimize video transcoding parameters, \\nexplicitly focusing on preset and bitrate selection to minimize distortion while respecting bitrate and transcoding \\ntime constraints. The framework comprises three main steps: featur e extraction, prediction, and optimization. It \\nleverages extracted features to predict transcoding time and rate -distortion, employing both supervised and \\nunsupervised methods. Using integer linear programming, it identifies the optimal sequence of presets  and bitrates \\nfor video segments, ensuring real -time application feasibility under set constraints. The results demonstrate the \\nframework‚Äôs effectiveness in enhancing video quality for live streaming, maintaining high standards of video \\ndelivery while mana ging computational resources efficiently. This optimization approach meets the evolving \\ndemands of video delivery by offering a solution for real -time transcoding optimization. Evaluation using the User \\nGenerated Content dataset showed an average PSNR impr ovement of 1.5 dB over the default Twitch configuration, \\nhighlighting significant PSNR  gains. Additionally, subsequent experiments demonstrated a BD -rate reduction of \\n‚àí49.60%, reinforcing the  framework‚Äôs superior performance over Twitch‚Äôs default configura tion. \\nIndex Terms : Optimization, Video Encoding, Time -Quality Tradeoff, Rate -Distortion Curve, Preset, Real -Time \\nApplication  \\n \\nI. Introduction  \\nIn the current online environment, video content plays a dominant role, comprising a sizable portion  of overall \\nInternet traffic. This trend is expected to persist, leading to an even greater emphasis on video  as a primary source \\nof information consumption [1]. Live video streaming has become a ubiquitous tool  for communication and \\nentertainment in today‚Äôs digital world. Platforms like YouTube Live, Twitch, and Facebook Live facilitate real -\\ntime broadcasting of events, conferences, games, and more, enabling content  \\ncreators to reach a global audience instantaneously. However, ensuring a seamless and uninterrupted viewing \\nexperience for diverse viewers requires efficient video processing. In this context, fast transcoding emerges as a \\ncritical technology for successfu l live streaming [2].     \\n \\n  2 \\n Video transcoding involves converting video files from one format or resolution to another to ensure compatibility \\nwith various devices and network conditions. The process of transcoding is computationally intensive, requiring \\noptimization of encoding para meters to achieve the desired balance between compression efficiency, visual quality, \\nand processing speed [3]. At the core of this optimization process is the delicate balance between three critical \\nmetrics: bitrate, distortion, and transcoding time [4]. Various articles have proposed different methods to address \\nthese challenges or to enhance these metrics, each contributing to the field in unique ways.  \\nYang et al in [5] propose a novel framework for constructing efficient bitrate ladders in adaptive video streaming. \\nThis method directly predicts the optimal transcoding resolution at each preset bitrate using  a temporal attentive \\ngated recurrent network, thus eliminating the need for pre -encoding. By capturing  spatial -temporal features from \\nvideo clips and treating bitrate ladder estimation as a multi -task classification  problem, this approach significantly \\nreduces computational overhead while maintaining high video quality.  \\nWang et al. [6] propose an innovative approach to optimize video transcoding by incorporating perceptual  quality \\nof the input video. By leveraging a machine learning -based metric to detect low -quality user generated content, \\ntheir quality guided transcoding framework adjusts transcoding parameters to reduce  bitrate without sacrificing \\nperceptual quality [6].  \\nThe paper [7] introduces a method to enhance immersive video streaming by optimizing the configurations  of the \\nTest Model for Immersive Video (TMIV) codec. The authors propose two Neural Network -based  algorithms: a \\nConvolutional Neural Network (CNN) and a Deep Reinforcement Learning (DRL) algorithm.  The CNN algorithm \\naddresses the configuration optimization as a regression problem, while the DRL  algorithm approaches it as a \\ndecision -making problem. The algorithms are designed to maximize video  quality while  minimizing decoding time \\nand bandwidth consumption.  \\nThe authors of [8] present a novel approach to enhancing the per -shot video coding framework by incorporating  a \\ncomplexity dimension into the optimization process. This method aims to optimize encoding parameters  by \\nconsidering the Rate -Distortion -Complexity (R -D-C) trade -offs. The authors propose a hyperbolic R -D-C model \\nand apply convex hull analysis to identify  the most efficient encoding parameters. By preprocessing  video \\nsequences into shots and encoding each shot with various parameters, the method filters out suboptimal  versions \\nusing convex hull techniques. This approach achieves significant bitrate savings, demonstrating BD _rate [9] gains \\nand offers flexible complexity management suitable for both professional and user -generated  content.   \\nThe paper [10] introduces ViSOR, an encoding scheme that uses Video Super -Resolution (VSR) to optimize  bitrate \\nand reduce energy consumption in online streaming. By leveraging the ultrafast x265 preset, ViSOR  achieves low \\nlatency but compensates for potential quality loss with client -side VSR. This method reduces bitrate and saves \\nencoding energy while maintaining high perceptual quality.  \\nIn [11], the proposed method addresses the energy consumption of video encoding processes in HTTP adaptive \\nstreaming. By evaluating different x265 presets for 500 video sequences, the authors optimize preset selection to \\nbalance encoding time, energy consumption, and compression efficiency.  \\nFor large -scale video transcoding systems, it is common practice to apply fixed settings (such as bitrates or \\nConstant Rate Factor (CRF)) across all videos, which fails to account for the intrinsic variability of the content. \\nDifferent research efforts  have shown that better performance can be achieved by optimizing transcoding  \\nparameters according to the Rate -Distortion (R -D) curves of individual videos, tailoring the process to the  specific \\ncharacteristics of each video [12], [13].     \\n \\n  3 \\n In this work, a sequence of video segments is considered,  and the goal is to optimize  the quality of the sequence  \\nwhile meeting  the limitation of time and bitrate. To address this, in our framework, the transcoding time and  video \\nqualities  are predicted for the input video. For the sequence of videos, various sets of bitrates  and configuration \\nsettings are explored to achieve an optimized solution for real -time streaming. Based on the  extracted features, \\ndifferent configurations can be selected for each video, enabling resource sharing across  the sequence. To achieve \\nthis without decoding, based on [2], a model is used to predict the transcoding time,  and a model is proposed to \\npredict the R -D curve of the video in different presets. Finally, by  leveraging  this information and using Integer \\nLinear Programming (ILP) [14], the optimized set of segments is selected  for each sequence.  \\nThe structure of the paper is as follows: First, the optimization problem is defined in the problem statement  section, \\nwhere the formulation of each part is explained. In the proposed method section, the different  components of our \\nsolution are described. The next section presents the experimental results, their analysis  and the result  of \\noptimization processe s. Finally, the conclusion of the w ork is provided in the last section.  \\n \\nII. Problem Statement  \\nIn the domain of video transcoding, the optimization of encoding parameters plays a pivotal role in  achieving desired \\ncompression efficiency, visual quality, and processing speed. At the core of this optimization  lies the interplay \\nbetween three key metrics: bitrate, distortion, and transcoding time. Bitrate, the amount  of data required to represent \\na unit of video content, is influenced by factors such as the complexity of  the content, the desired visual quality, \\nand the chosen encoding parameters. Distortion, a measure of the  quality loss introduced during compression, is \\ndependent on both the bitrate and the effectiveness of the  compression algorithm  in preserving visual fidelity. \\nTranscoding time, representing the duration required  to encode a video sequence, is affected by factors including \\nthe computational complexity of the encoding  process, the  selected encoding settings, and the hardware resources \\navailable for transcoding. The plots  (a) and (c) in Figure 1 show the transcoding time for two different video \\nsegments, categorized as  houto  and lyric types, at five different presets. As seen from the plots, the transcoding time \\nfor the same video  depends on the selected preset. This dependency on the preset is also evident in the R -D behavior \\nof the  video segments. The plots (b) and (d) in Figure 1 verify this.     \\n \\n  4 \\n  \\nFig. 1. The transcoding time and R -D curve of two different video types, (a, b) houto video, (c, d) Lyric video in \\ndifferent presets.  \\n \\nWe formulate the transcoding optimization problem using the following equations. The first equation imposes a \\nconstraint on the total bitrate (R) across a sequence of L content segments encoded using corresponding presets (P). \\nThis constraint ensures that the total data rate of all encoded content remains within specified thresholds. The second \\nequation introduces a constraint on the total transcoding time (T), limiting the overall processing duration for \\ntranscoding the content sequences. Restricting the p rocessing time maintains eÔ¨Äicient utilization of computational \\nresources while meeting real -time encoding requirements. Lastly, the third equation formulates an objective \\nfunction aimed at minimizing the total distortion (D) incurred during encoding. By se eking to minimize quality loss \\nacross all encoded content, this objective ensures that the visual fidelity of the output videos is preserved while \\nadhering to the bitrate and transcoding time constraints. Together, these equations provide a comprehensive \\nframework for optimizing video transcoding parameters, enabling practitioners to balance compression efficiency, \\nvisual quality, and computational resources in video transcoding applications. The following sections will describe \\neach of these equations in d etail.  \\n   \\n \\n  5 \\n that the combined data rate of all encoded content remains within acceptable limits, which is important for the \\nefficient use of network bandwidth or storage resourc es. \\nC. Total Distortion Minimization Objective  \\nEquation (3) represents the objective of minimizing the total distortion incurred during encoding across the L \\nsegments of the sequence. Distortion refers to the quality loss introduced during the encoding process, typically \\nmeasured using metrics such as Peak Signal -to-Noise Ratio (PSNR) or Structural Similarity Index (SSI) [15]. The \\nobjective is to find the optimal operating point that minimizes the total distortion across all L content segments, \\nwhere each content segment is encoded using a specific pres et p and a specific bitrate r. Minimizing distortion is \\nessential for preserving the visual quality of the encoded content, ensuring that the encoded videos maintain high \\nperceptual fidelity.  \\n   \\n \\n  6 \\n  \\nSolving the optimization problem in Equation (4) determines the optimal  bitrate and encoding preset for each video \\nsegment to achieve the best overall visual quality for the entire sequence. The challenge in solving Equation (4) is \\nthat transcoding time and distortion for a video segment depends on different parameters such a s the target bitrate, \\nthe selected preset and the content of the video segment. In video streaming applications, these dependencies cannot \\nbe established without encoding the video segment at different rates and various presets as this process takes a \\ncons iderable time. Therefore, building on our prior work  [2] on transcoding time prediction for different presets, \\nwe propose a learning -based  transcoding optimization framework. Our main contributions are:  \\n‚Ä¢ Proposing a learning based transcoding rate -distortion predictions using supervised and unsupervised learning \\napproaches.  \\n‚Ä¢ Mapping our problem to an ILP involves using the learned transcoding rate -distortion function and transcoding \\ntime prediction to optimally assign bitrates and presets to the video segments of a video stream.  \\nIII. PROPOSED METHOD  \\nIn live video streaming, achieving optimal trade -offs between transcoding efficiency, processing time, and output \\nquality is crucial for delivering high -quality video content while efficiently utilizing computational resources. One \\nof the challenges in thi s problem is the fact that the transcoding time and distortion of a video segment is preset \\ndependent. Moreover, the R -D behavior of the video segments differ from each other depending on the context type. \\nOne solution to characterize these dependencies is  to encode a video segment at different rates and with different \\npresets and then solve the optimization of Equation (4). However, this ‚Äôexhaustive‚Äô approach is not optimal for live \\nstreaming applications. To address this challenge, we introduce a framewor k for optimizing video transcoding \\nparameters, particularly preset selection. In this framework, we consider constraints on total bitrate and transcoding \\ntime, ensuring that resource consumption remains within acceptable bounds and live streaming is guaran teed. \\nAdditionally, we aim to minimize distortion,  preserving the visual quality of the encoded content. The block \\ndiagram of our proposed framework is  presented in Figure 2. This framework consists  of three steps, extracting \\nfeatures, predicting rate distortion,  and transcoding time and optimization process. In the following, these steps will \\nbe explained with more  details.  \\n   \\n \\n  7 \\n Fig. 2. The block diagram of the proposed framework.  \\nA. Feature Extraction  \\nOur proposed framework includes two prediction blocks, making the extracted features a crucial subblock. In this \\nframework, features are extracted from the ingested video in the transcoder and used by two predictors to estimate \\ntranscoding time and R -D beh avior. Ideal features are those that effectively predict transcoding time and R -D \\nbehavior while being quick to extract. The live streaming nature of our application limits us from using features \\nthat are complex to compute. In our study on predicting tran scoding time, we extract various features categorized \\ninto frame -based, motion -based, and visual -based types, as employed in [2], for each video segment. For frame -\\nbased features, the number, and types of macroblocks, including their sizes, are crucial for describing the video‚Äôs \\ncomplexity. Additionally, features that describe  object characteristics within the video are important for the \\ncompression process. Motion -based features, which capture the movement within the video, also play a significant \\nrole in the prediction process. Also, visual features further impact compression efficiency. We also include ‚Äôwidth‚Äô \\nand ‚Äôheight‚Äô in our feature vector. These features are embedded within the header and metadata of the incoming \\nencoded stream, so extracting them i ncurs no extra processing time, making them ideal for our application. The \\ncomplete list of features is shown in Table I, with features used for transcoding time prediction marked with  a check \\nmark  the ‚ÄùTranscoding Time‚Äù column.  \\nAlongside time, another key characteristic of video compression is R -D behavior. Predicting the R -D behavior of a \\nvideo segment requires a dataset containing the R -D class of each segment and the features that describe it.  Using \\nthese extracted features, a classifier assigns an R -D category to each video  segment. Effective video classification \\nby quality depends on integrating features that precisely capture the  segment‚Äôs complexity and content.  \\nFor R -D prediction, various features related to frames and motion are extracted. Some of these features  overlap with \\nthose used in transcoding time prediction, while others are specifically tailored for predicting  PSNR. For example, \\none set of features for R -D prediction includes Quantization Parameters (QP) of  different frames, which influence \\nhow visual data is compressed and represented. This set includes the  average QP_Y for P -frames, B -frames, and I -\\nframes. These parameters are essential for balancing visual  quality and compression efficiency, connecting to the \\nvisual representation analyzed in transcoding time  feature set. Frame -based features used in this prediction are \\nnormalized to the number of frames in each  video segment, providing a standardized view that facilitates \\ncomparison across videos of different lengths.  Motion Vect or Mean (MV_Mean) feature, which focuses  on motion \\nvectors, are key indicators of movement  within the video. The inclusion of MV_Mean features highlights the \\nimportance of motion analysis in both  the R -D and transcoding time prediction.  \\n   \\n \\n  8 \\n Since both sets of features for regression and classification are derived from video headers or metadata,  they are \\nwell-suited for real -time applications. Table I lists all the features extracted from video segments, with features used \\nfor R -D prediction marked with ‚Äúcheck mark‚Äù in  the ‚ÄùR -D‚Äù column.  \\nB. Transcoding Time Prediction using Regression Models  \\nTranscoding time prediction involves estimating the time required to encode a video sequence based on features \\nextracted from the ingest video. Regression models are trained using labeled data consisting of video features and \\ncorresponding transcoding time s. During training, the model learns the relationship between the features and the \\ntranscoding time. Once trained, the regression model can predict transcoding time for new video sequences based \\non their features. The choice of regression model and the fea tures used can significantly impact the accuracy of \\ntranscoding time prediction.  \\nSince regression models are supervised learning models, the initial step is to generate a dataset. This involves \\nutilizing the extracted features along with preset -based recorded transcoding times, a process which will be \\nelaborated upon in Dataset Generat ion section. Subsequently, the regressors are trained using this dataset.  For this \\npurpose, the features are utilized as input, while the transcoding time serves as the output. As  outlined in [2], a \\ndedicated regressor is trained for each preset. This stud y employs the same regressors as  [2]. By leveraging the \\nselected regressors, transcoding times are predicted for each video sequence. Through  the training of the regressors, \\nthe T ij value is predicted.  \\nC. Rate -Distortion Prediction  \\nRate-distortion prediction entails finding the most suitable encoding parameters, such as bitrate, for each  video \\nsequence to strike a balance between compression efficiency and visual quality. This task is challenging  owing to \\nthe diverse nature of videos. To tackle this problem, there are two main components, generating  labels for videos \\nand predicting distortion and bitrate. In generating labels for videos, the preparation of  R-D curves and clustering \\nare essential steps. These processes are inspired by the proposed method in [16].  \\nIn this work, different presets are considered but in [16] only the ‚Äùveryfast‚Äù preset which is the default of  Twitch \\nconfiguration. These steps will be explained in the following .  \\n1) Rate -Distortion Label Generation: In the realm of video transmission over the Internet, a common  strategy has \\nbeen to allocate uniform bitrates to all segments. However, this approach fails to consider the  inherent diversity \\namong video segments. In Figure 3, the R -D curves of four different videos show the  diversity of rate distortion \\namong different videos and different segments of one video. Some segments exhibit  intricate details and extensive \\nmotion, while others are simpler in nature. Consequently, assigning identical  bitrates to all segments becomes \\ninefficient. This disparity in complexity necessitates the allocation of higher  bitrates to segments with greater \\nintricacy to maintain acceptable quality, adhering to the principles of R-D theory. At its core, R -D theory aims to \\nrepresent a source using the fewest possible bits while preserving a  desired level of reproduction quality [3]. This \\ndelicate balance between source fidelity and coding rate forms  the essence of the rate -distortion trade -off, a crucial \\nconsideration in the design of any lossy compression  system. The R -D curve offers invaluable insights into a video‚Äôs \\nbehavior and facilitates the allocation of a  bitrate.  \\nFor optimal  transcoding, rate and distortion values are necessary. However, due to the diversity of videos, predicting \\nprecise rate and distortion values for a video segment poses a challenge. Thus, taking a learning approach can offer \\na solution. In this approach, t he R-D curve of a large video training set is generated and clustered. To construct an \\nR-D model for transcoding, a diverse selection of videos spanning various content genres, such as animations, \\nlectures, news, and TV clips, was transcoded across a range  of bitrates and presets. As Figure 1 (b,d) illustrates, the    \\n \\n  9 \\n R-D curves for two different video types exhibit different behaviors at varying presets. Each curve in each plot \\ncorresponds to a preset, highlighting how a video segment encoded with a preset exhibits diverse PSNR values at \\nidentical bitrates. This visual ization underscores the inadequacy of uniform bitrate allocation for all presets during \\ntransmission.  \\n \\nFig. 3. R -D curve of segments of two video types, (a) game video, (b) livemusic video, (c) houto video, \\n(d) hdr video.  \\n \\nDue to the diverse types and content of videos, a clustering method is employed to group similar video sequences \\nbased on their R -D curves. Clustering algorithms, such as K -means clustering or hierarchical clustering, partition \\nthe video sequences into clu sters such that sequences within the same cluster are more similar than those in other \\nclusters. Each cluster represents a set of video sequences with similar characteristics.   \\n \\n   \\n \\n  10 \\n TABLE I Extracted Features and their usage in framework   \\n \\n   \\n \\n  11 \\n  \\n \\nFig. 4. The centroid of the six clusters for ultrafast preset for the video segments in dataset.  \\n \\nDetermining the optimal  number of clusters is a crucial aspect of the K -Means algorithm. For this purpose, the \\nexperimental results show that the small values result in significant gaps between the centroids, thereby restricting \\nour ability to select an optimal bitrate tailored to specific use cases. To address this limitation, we chose to increase \\nthe number of clusters, enabling more precise bitrate selection aligned with user requirements. It is important to \\nconsider that excessively increasing the number of clusters can dimin ish the gaps between centroids, making it \\nchallenging to accurately distinguish between different videos. Experiments with varying numbers of clusters \\nindicate that six clusters achieve an effective balance between precision  and comprehensiveness [16].  \\nFigure 4 highlights  the centroids of these clusters for the ultrafast preset, with  each centroid representing  the R -D \\nbehavior for the videos within its respective cluster. The cluster centroid   \\n2) Rate -Distortion Curve Classification: By utilizing the extracted features and the video cluster number, a \\nclassification model was trained. In this study, the Support Vector Machine (SVM) is chosen to determine the R -D \\nclass of each video from six categ ories of the R -D. \\nD. Rate -Distortion -Transcoding time Optimization  \\nIn the final phase of our framework, we employ ILP to solve the optimization problem, given the linearity of the \\nobjective function and the constraints defined by Equations (1)  to (4). ILP is a special case of linear programming \\n(LP) where the solution space is restricted to integer values. Our primary objective is to minimize distortion, as \\n   \\n \\n  12 \\n outlined in Equation (4), by choosing the optimal encoding preset and bitrate for each video segment. We aim to \\nkeep both the transcoding time and bitrate within their respective thresholds, as defined in Equations (1 -2). \\nIn our problem, each video segment can be transcoded at a bitrate and with a preset. We refer to this pair as the \\ndefinition for that segment. In a sequence with L segments, we must select one definition for each segment to \\nachieve minimum distortion while  meeting the bitrate and transcoding time constraints. This optimization problem \\ncan be formulated as an ILP model. The relevant parameters ‚Äîvariables,  objective, and constraints ‚Äîare specified \\nas follows:  \\nThe ILP is formulated based on these parameters, analogous to Equations (1 -4). The objective function aims to \\nminimize the total distortion across all segments by summing the distortions Dij weighted by the decision variables \\nxij . \\nIn ILP models, selecting an appropriate solver is crucial, especially for applications like real -time streaming, where \\nthe search space is expensive . For our problem, efficiency is paramount. We employ the CBC (Coin -or Branch and \\nCut) solver [17], [18], an open -source mixed -integer programming solver from the Computational Infrastructure \\nfor Operations Research (COIN -OR) project. CBC is specifically d esigned to efficiently tackle large -scale integer \\nprogramming problems using advanced techniques:  \\n‚Ä¢ Branch and Bound: CBC employs the branch and bound algorithm to systematically explore the  solution space. \\nThis method divides the problem into smaller subproblems (branching) and solves these  subproblems while \\ndiscarding those that do not yield better solutions (bounding).  \\n‚Ä¢ Cutting Planes: CBC generates and incorporates cutting planes, which are additional constraints that  help to tighten \\nthe feasible region, thus improving the efficiency of the solver.  \\n‚Ä¢ Heuristics: CBC uses various heuristics to find good feasible solutions quickly, helping to speed up the  \\nconvergence to the optimal solution.  \\n‚Ä¢ Presolving: Before the main solving process, CBC the performs presolving steps to simplify the problem  by \\nremoving redundant constraints and fixing variable bounds, reducing the overall problem size and  complexity.  \\n   \\n \\n  13 \\n various parts  of the solution space simultaneously, further enhancing its performance.  By leveraging these \\ntechniques, the CBC solver efficiently identifies the optimal set of encoding presets and  bitrates that minimize rate -\\ndistortion while adhering to the transcoding time and bitrate constraints in  real-time applications.   \\nIV. Experimental Results  \\nIn this section, we discuss how our proposed framework was implemented and the results. Firstly, we provide details \\nabout the platforms we used. Following that, we explain the specifics of the dataset we employed. After that the \\nresult  of previous optimization works and the results obtained from our framework are reported. Additionally, we \\nprovide an analysis of the results in each part.  \\nA. Implementation details  \\nThe Python programming language is used for implementing the framework. FFmpeg and its extensions are used \\nto extract features and perform transcoding in different presets. The system configuration used for our transcoding \\ntime experiments is CPU: AMD Ryze n 9 5900X @3.70 GHz. For clustering and classification method, the Intel(R) \\nCore(TM) i7 -3770 CPU @3.40GHz system is used.  \\nB. Dataset Generation  \\nAs regression, clustering, and classification techniques are learning -based methods, having a dataset for both \\ntraining and testing is crucial. The features utilized for predicting transcoding time and R -D were discussed in \\nSection III -A. To extract these features, the videos are segmented into two -second chunks to ensure consistent \\nfeature comparison.  \\nIn this study, we utilize the video dataset from [4], comprising eleven distinct video types such as lectures, games, \\nsports, news, and live music, totaling 165 videos. The incoming stream bitrate to the transcoder is set to \\napproximately 8000 Kbps and a s et of target bitrates, {200,400,600,800,1000,2000,3000,4000,5000,6000} Kbps is \\nused for transcoding with H.264 codec. Each video is segmented into two -second chunks, and the features \\nmentioned in the Feature Extraction section are extracted. For generating  transcoding time data, all videos undergo \\ntranscoding to different target bitrates with varying presets, and the transcoding time  is recorded. The selected \\npresets in this work are {veryslow, slow, fast, veryfast, ultrafast }. For  the R -D aspect, the R -D curve of each video, \\ntranscoded to various target bitrates with different presets, is stored.  This portion of the dataset is used for the \\nclustering model, where the centroids of the clusters serve as  target labels for the samples in the classification mode l. \\nThe number of two -second chunks are 877 and the chunks in our dataset are 43850 (877 √ó 10 (number  of bitrates) \\n√ó 5 (number of presets) . With this dataset, the three models ‚Äîregression, clustering, and  classification ‚Äîare trained. \\nSince the dataset does not specifically segregate training and testing data, 5 -fold Cross  validation is utilized for \\nlearning processes.   \\n \\nC. Previous Optimization Works  \\nIn the context of optimizing the real -time video streaming process, various configuration parameters such  as bitrate, \\npreset, and rate -distortion play a crucial role. Several research articles have been published in  this field, exploring \\nthe impact of these factors on streaming performance. There are no specific articles  available for a direct comparison \\nwith our results. However, in this section, we report the results from  several optimization studies to provide a \\nreference range in this field.     \\n \\n  14 \\n In [5], the objective is to select the optimal resolution for each bitrate preset. This study utilizes the Inter -4K dataset \\n[19], which includes videos at various resolutions. A deep learning model is employed to extract  features, and a Bi -\\ndirectional GRU is used to predict the class of bitrate and resolution. The classification  achieves a maximum \\naccuracy of 0.86, and the BD -rate is 1.21% compared to the optimal convex hull.  Additionally, [5] investigates the \\nrelationship between bitrate and resolution.  \\nIn [6], the transcoding process is optimized by leveraging perceptual features to reduce bitrate without  sacrificing \\nquality. This method achieves an average bitrate reduction of up to 5%. The dataset in [6] is the  same as our dataset.   \\nIn [7], the trade -off between video quality, decoding time, and bandwidth consumption is addressed in  the TMIV \\ncodec. The article proposes a configuration that optimizes  perceived video quality, minimizes  decoding time, and \\nreduces bandwidth consumption by transmitting an appropriate number of source views  to users. In this work, an \\nexhaustive search is used to find the optimal result, and all the results are compared with the default configuration. \\nThe utility metric, which compares the quality of videos based on their decoding time, is increased by 6%.  \\nIn [8], the per -shot encoding framework is extended by introducing complexity as an adjustable parameter, enabling \\nencoding configurations to adapt according to resource availability and content requirements. In this approach, \\ncomplexity is controlled thro ugh preset configurations, while bitrate management is achieved using Quantization \\nParameter or CRF. The proposed Rate -Distortion -Complexity (R -D-C) optimization model is evaluated on the BVI -\\n1004K dataset, demonstrating up to a -19.17% BD -rate improvement  compared to a conventional per -shot encoding \\nmethod constrained to specific fixed presets.  \\nThis exploration demonstrates that in the optimization field of video streaming, various perspectives, parameters, \\nand applications can be considered. Consequently, most studies in this field compare optimization results against a \\ndefault or baseline. In t his work, we use two baselines to evaluate and compare our results.  \\nD. Transcoding Time Prediction  \\nUsing the generated dataset are trained for each preset using 5 -fold cross validation, and the best regressor is selected \\namong them [2]. For all presets, the Light  Gradient Boosting Machine (LightGBM) regressor is selected as the best \\nmodel. LightGBM is an efficient and scalable implementation of gradient boosting that uses histogram -based \\ndecision tree learning, leaf -wise growth, and techniques like Exclusive Featu re Bundling (EFB) and Gradient -based \\nOne-Side Sampling (GOSS) to enhance performance. It is designed to handle large datasets and high -dimensional \\ndata with high accuracy and speed. LightGBM is widely used for  various predictive modeling tasks including \\nclassification and regression [20]. The results of transcoding time  prediction using LightGBM are reported in Table \\nII. In this tabel, four evaluation metrics, Mean Squared  Error (MSE), Mean Absolute Percentage Error (MAPE), \\nMean Absolute Error (MAE) and R -squared (R2)  are reported. The first three metrics demonstrate the accuracy of \\nthe models in predicting transcoding  R2 indicates how well the data points fit the model.  The mean percentage error  \\nfor all five presets is around 2%. For a clearer understanding, the predicted transcoding time values and  the actual \\nvalues for a video segment coded with different bitrates and presets are illustrated in Figure 5.  In this figure, each \\nrow shows the result of one preset.  To reduce the number of extracted features for real -time applications, Recursive \\nFeature Elimination with  the Cross  Validation (RFECV) method the same as in [2] is implemented. During testing, \\nthe selected features  are extracted solely from the videos, and the transcoding time for each preset is predicted. \\nFrom the  evaluation metrics presented in Table II, the MAPE of the regressor, which better illustrates the differences  \\nfor each preset before and after feature selection, is reported in Table III.   \\n \\n    \\n \\n  15 \\n TABLE II:  The results of transcoding time prediction for different presets  \\nE. Rate -Distortion Prediction  \\nIn this section, we will explain the parameters and methods selected during the two stages of R -D prediction, R -D \\nLabel generation and classification.  \\n1) Rate -Distortion Label generation: After extracting the R -D curves for all video segments, we apply K -means \\nclustering with six clusters to categorize the R -D behavior of the videos. After clustering, the centroid of each cluster \\nrepresents the R -D behav ior for that cluster and the videos within it. The cluster number is then assigned as the R -\\nD class for each video. Details of the clustering and curve fitting process are explained in [16]. While [16] uses only \\none preset, this paper applies the method to  videos encoded with various presets. Figure 6 illustrates the clustering \\nresults for these presets. Each plot in this figure displays the range of quality changes and the clustering for each \\npreset. As shown in Figure 6, videos encoded with the ultrafast preset exhibit lower PSNR values compared to those \\nencoded with the very slow preset. The highest PSNR value for the ultrafast preset is approximately 60 dB, which \\nis lower than that achieved by the other presets. For each cluster, a centroid is depicted, like the one shown in Figure \\n4. This  centroid represents the R -D curves within the corresponding cluster. To identify the most appropriate curve  \\nfor each cluster, we employ curve fitting techniques akin to those described in [16]. Figure 7 presents the centroids \\nof the six clusters for different presets after curve fitting. Each plot shows the centroid of one cluster number for \\ndifferent presets. As illustrated in the figure, the ultrafast preset is notably distinct from the other presets, reflecting \\na sign ificant difference in video quality.    \\nTABLE III: Transcoding time of a video segment with different presets after feature selection  \\n \\n   \\n \\n  16 \\n  \\nFig. 5. The results of the predicted values and the actual values of one video for different bitrates and presets after \\nfeature selection.  \\n   \\n \\n  17 \\n  \\nFig. 6. R -D curve clustering of different presets using KMeans clustering, (a) veryslow preset, (b) slow preset, \\n(c)fast preset, (d) veryfast preset, (e) ultrafast preset  \\n2) Rate -Distortion Class Prediction: The SVM classification model is trained to predict the class (cluster) of each \\nvideo at each preset. We consider several kernels such as linear, RBF, and polynomial (2nd order) and learn the \\nSVM using these kernels. In order to have more accurate results, 5 -fold cross validation is used during training. \\nBased on achieved results in Table IV, the polynomial kernel 2nd order is selected as the final kernel in classification \\nmethod. The achieved accuracy in polynomial class ification is 0.73. By identifying the class of each video, its \\ncentroid R -D curve determines its rate and distortion characteristics.  \\nF. Optimization Result  \\nAfter predicting all the required information, including transcoding time and R -D curve, we proceed to select the \\noptimized preset and bitrate that minimize distortion. In our experiments, the value for L is considered six, and the \\nnumber of definitions fo r each segment (M) is fifty (10 bitrates √ó 5 presets). According to the block diagram in \\nFigure 2, the prediction of the transcoding time and R -D could be done in parallel.  \\n   \\n \\n  18 \\n Fig. 7. Combination of cluster centroid fitted curves for different presets in each cluster.  \\nTABLE IV: SVM classification using different kernels  \\nTo set T th, the maximum processing time for predicting transcoding time and R -D are considered. The processing \\ntime for transcoding time prediction is 0.02 seconds, and for R -D prediction, it is 0.0002 seconds. Therefore, the \\nprediction time is 0.02 seconds. Since t he video segments are two seconds, considering the processing time for \\nprediction (0.02 seconds) and optimization (0.02 seconds), the thresholds for time can be defined as 11.76 (6 √ó (2 - \\n0.04)) seconds. In our experiments, it is set to 11 seconds. To defi ne the threshold for bitrate, the bandwidth of \\nvideo streaming platforms is explored. Bandwidth  used by platforms like Twitch1 for video streaming, is between \\n3000 kbps and 6000 kbps, the bitrate is 5000 kbps. Based on this, the threshold is defined as 30000 (6 √ó 5000). By \\nconsidering these values, the ILP algorithm is applied to six randomly selected chunks from a total of 877 chunks. \\nThis process is repeated 877 times, and for each run, the sum of PSNR, transcoding time, and bitrate for the selected \\noptio ns for each segment is recorded. For these selected chunks, the sum of PSNR, transcoding time, and bitrate is \\nalso recorded for the scenario where all segments are encoded with a 5000 kbps bitrate using the fast (baseline1) \\nand veryfast (baseline2) presets . The fast preset is the middle preset  in our selection set, while veryfast is the default \\npreset for platforms like Twitch 2.  \\nIn Table V, the average PSNR for two scenarios ‚Äîselecting options with ILP and the default options  are reported. \\nThe ILP output shows a significant improvement in PSNR, increasing it by 9.45 dB compared to  the veryfast  preset \\n(approximately 1.5 dB per segment). Similarly, when compared to the fast preset, the  ILP output results in an \\nincrease of 5.92 dB in PSNR (approximately 1 dB per segment).  \\nG. Bitrate and Transcoding Time Limitation  \\n   \\n \\n  19 \\n In our work, there are two constraints, bitrate and transcoding time, that we simulate to reflect different  streaming \\nscenarios. By changing each of these parameters, the selected operating point for each video  segment varies. In this \\nsection, we consider the effect of these two parameters.  To evaluate the compression efficiency between our \\nproposed method and the baseline2 method, we computed  the BD -rate, a widely used metric for comparing the rate -\\ndistortion performance of video codecs. The BD -rate quanti fies the average percentage difference in bitrate required \\nby the two codecs to achieve the same  quality (measured by PSNR). For this purpose, by considering the same \\nbitrate threshold (R th) for the ILP and the baseline2, the PSNR of the encoded segments with these two \\nconfigurations was recorded.  \\nThe considered values for R th are 6 √ó {200, 400, 600, 800, 1000, 2000, 3000, 4000, 5000, 6000} Kbps. The T th is \\nfixed in this experiment (11 seconds). Given these settings, each experiment was run 877 times, and the mean \\naverage PSNR was calculated for both methods. The R -D curves of the two methods are shown in Figure 8. The \\ncalculated BD -rate for this experiment i s -49.60%.   \\nTABLE V: The results of the ILP and other two baselines  \\nTo demonstrate the effect of time constraints, the bitrate constraint was fixed while the time constraint was varied. \\nBy altering the time limitation, the distribution of selected presets across diverse types  of videos was recorded. In \\nthese experiments, the transcoding time limitation was set to [11, 8, 5, 3] seconds. This approach helps to identify \\nwhich types of videos are more complex. For example, Figures 9 and 10 illustrate the distribution of selected p resets \\nfor two types of videos across various t ime constraints. These figures enable inference on the influence of video \\ncontent and time constraints on preset selection. It is evident how changes in transcoding time constraints affect the \\npreset distribution for these two types of videos, highlighting  the importance of selecting the optimal operating \\npoint, especially in real -time applications.  \\n \\nFig. 8. The R -D curve of ILP and baseline2 method.  \\nFurthermore, the results suggest that sports videos are more complex than music videos, evidenced by the lack of \\nresponse when the time limitation is set to 3 seconds. This complexity is corroborated by the average extracted \\nmotion vectors, where the mean magnitude for sports videos is 711,971.4, compared to 547,081.9 for music videos.  \\n   \\n \\n  20 \\n  \\nFig. 9. Distribution of Presets for Music Videos.  \\n \\nFig. 9. Distribution of Presets for Music Videos.  \\n   \\n \\n  21 \\n  \\nFig. 10. Distribution of Presets for Sport Videos.  \\n \\nV. Conclusion  \\nIn this work, we propose a comprehensive framework to optimize video transcoding parameters, with a focus on \\nselecting presets and bitrates to minimize distortion while adhering to constraints on bitrate and transcoding time. \\nBy utilizing extracted feature s for both transcoding time and R -D prediction, our approach employs linear \\nprogramming to determine the most efficient sequence of presets and bitrates for video segments in real -time \\napplications. Although predicting R -D is a challenging problem, our app roach achieves an accuracy of 73%. Our \\nexperimental results demonstrated significant improvements in PSNR compared to both baseline configurations. \\nThe ILP algorithm, when applied across multiple video segments, consistently outperformed both baselines, \\nachieving substantial gains in video quality. Specifically, the ILP output increased the PSNR by 9.45 dB over \\nbaseline2 and by 5.92 dB over baseline1, which corresponds to approximately 1.5 dB and 1 dB improvements per \\nsegment, respectively. These results al so highlight the impact of transcoding time limitations on the diversity of \\npreset selection and underscore the importance of making informed preset  choices.  \\nThese findings highlight the effectiveness of our proposed method in enhancing video quality for live  streaming \\nplatforms. By optimizing encoding parameters, our framework ensures high -quality video delivery  while efficiently \\nmanaging computational resources. This work contributes to the ongoing efforts to improve  live video streaming, \\nproviding a robust solution for real -time video transcoding optimization.   \\n \\n \\n \\n \\n   \\n \\n  22 \\n References  \\n[1] G. Lu, W. Ouyang, D. Xu, X. Zhang, C. Cai, and Z. Gao, ‚ÄúDvc: An end -to-end deep video compression \\nframework,‚Äù in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. \\n11 006 ‚Äì11 015.  \\n[2] Z. N. Shahre -Babak, N. Karimi, K. Rapaka, T. Amara, S. Samavi, and S. Shirani, ‚ÄúHigh -quality live video \\nstreaming via transcoding time prediction and preset selection,‚Äù arXiv preprint arXiv:2312.05348, 2023.  \\n[3] A. Ortega and K. Ramchandran, ‚ÄúRate -distortion methods for image and video compression,‚Äù IEEE Signal \\nprocessing magazine, vol. 15, no. 6, pp. 23 ‚Äì50, 1998.  \\n[4] Y. Wang, S. Inguva, and B. Adsumilli, ‚ÄúYoutube ugc dataset for video compression research.‚Äù IEEE, 2019, pp. \\n1‚Äì5. \\n[5] J. Yang, M. Guo, S. Zhao, J. Li, and L. Zhang, ‚ÄúOptimal transcoding resolution prediction for efficient per -title \\nbitrate ladder estimation,‚Äù arXiv preprint arXiv:2401.04405, 2024.  \\n[6] Y. Wang, H. Talebi, F. Yang, J. G. Yim, N. Birkbeck, B. Adsumilli, and P. Milanfar, ‚ÄúVideo transcoding \\noptimization based on input perceptual quality,‚Äù in Applications of Digital Image Processing XLIII, vol. 11510. \\nSPIE, 2020, pp. 201 ‚Äì211.  \\n[7] C. -F. Hsu, T. -H. Hung, and C. -H. Hsu, ‚ÄúOptimizing immersive video coding configurations using deep learning: \\na case study on tmiv,‚Äù ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), \\nvol. 18, no. 1, pp. 1 ‚Äì25, 2022.  \\n[8] H. Zhong, J. Xu, C. Zhu, D. Feng, and L. Song, ‚ÄúComplexity -oriented per -shot video coding optimization,‚Äù in \\n2022 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 2022, pp. 1 ‚Äì6. \\n[9] N. Barman, M. G. Martini, and Y. Reznik, ‚ÄúBj√∏ntegaard delta (bd): A tutorial overview of the metric, evolution, \\nchallenges,  and recommendations,‚Äù arXiv preprint arXiv:2401.04039, 2024.  \\n[10] V. V. Menon, P. T. Rajendran, A. Premkumar, B. Bross, and D. Marpe, ‚ÄúVideo super -resolution for optimized \\nbitrate  and green online streaming,‚Äù arXiv preprint arXiv:2402.03513, 2024.  \\n[11] H. Amirpour, V. V. Menon, S. Afzal, R. Prodan, and C. Timmerer, ‚ÄúOptimizing video streaming for \\nsustainability and  quality: The role of preset selection in per -title encoding,‚Äù in 2023 IEEE International Conference \\non Multimedia and  Expo (ICME). IEEE, 2023, pp. 1679 ‚Äì1684.  \\n[12] J. De Cock, Z. Li, M. Manohara, and A. Aaron, ‚ÄúComplexity -based consistent -quality encoding in the cloud,‚Äù \\nin 2016  IEEE International Conference on Image Processing (ICIP). IEEE, 2016, pp. 1484 ‚Äì1488.  \\n[13] C. Chen, Y. -C. Lin, S. Benting, and A. Kokaram, ‚ÄúOptimized transcoding for large scale adaptive streaming \\nusing playback  statistics,‚Äù in 2018 25th IEEE International Conference on Image Processing (ICIP). IEEE, 2018, \\npp. 3269 ‚Äì3273.  \\n[14] R. E. Bellman and S. E. Dreyfus, Applied dynamic programming. Princeton university press, 2015, vol. 2050.   \\n[15] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, ‚ÄúImage quality assessment: from error visibility to \\nstructural similarity,‚Äù IEEE transactions on image processing, vol. 13, no. 4, pp. 600 ‚Äì612, 2004.  \\n[16] M. Jamali, N. Karimi, S. Samavi, and S. Shirani, ‚ÄúA parametric rate -distortion model for video transcoding,‚Äù \\narXiv preprint arXiv:2404.09029, 2024.     \\n \\n  23 \\n [17] J. Forrest and R. Lougee -Heimer, ‚ÄúCbc user guide,‚Äù in Emerging theory, methods, and applications. INFORMS, \\n2005, pp. 257 ‚Äì277.  \\n[18] M. J. Saltzman, ‚ÄúCoin -or: an open -source library for optimization,‚Äù Programming languages and systems in \\ncomputational economics and finance, pp. 3 ‚Äì32, 2002.  \\n[19] A. Stergiou and R. Poppe, ‚ÄúAdapool: Exponential adaptive pooling for information -retaining downsampling,‚Äù \\nIEEETransactions on Image Processing, vol. 32, pp. 251 ‚Äì266, 2022.  \\n[20] G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, and T. -Y. Liu, ‚ÄúLightgbm: A highly efficient \\ngradient boosting decision tree,‚Äù Advances in neural information processing systems, vol. 30, 2017.  \\n \\n \\n \\n ',\n",
       " '1\\nA Graph Neural Architecture Search Approach for\\nIdentifying Bots in Social Media\\nGeorgios Tzoumanekas, Michail Chatzianastasis, Loukas Ilias, George Kiokes, John Psarras, Dimitris Askounis\\nAbstract ‚ÄîSocial media platforms, including X, Facebook, and\\nInstagram, host millions of daily users, giving rise to bots-\\nautomated programs disseminating misinformation and ideolo-\\ngies with tangible real-world consequences. While bot detection in\\nplatform X has been the area of many deep learning models with\\nadequate results, most approaches neglect the graph structure\\nof social media relationships and often rely on hand-engineered\\narchitectures. Our work introduces the implementation of a\\nNeural Architecture Search (NAS) technique, namely Deep and\\nFlexible Graph Neural Architecture Search (DFG-NAS), tailored\\nto Relational Graph Convolutional Neural Networks (RGCNs) in\\nthe task of bot detection in platform X. Our model constructs\\na graph that incorporates both the user relationships and their\\nmetadata. Then, DFG-NAS is adapted to automatically search for\\nthe optimal configuration of Propagation and Transformation\\nfunctions in the RGCNs. Our experiments are conducted on\\nthe TwiBot-20 dataset, constructing a graph with 229,580 nodes\\nand 227,979 edges. We study the five architectures with the\\nhighest performance during the search and achieve an accuracy\\nof 85.7%, surpassing state-of-the-art models. Our approach not\\nonly addresses the bot detection challenge but also advocates for\\nthe broader implementation of NAS models in neural network\\ndesign automation.\\nIndex Terms ‚ÄîBot detection, Graph Neural Networks, Neural\\nArchitecture Search, Propagation, Transformation, Social Media\\nPlatform X\\nI. I NTRODUCTION\\nSocial media are online community platforms and apps that\\nlet users create, share, and interact with each other‚Äôs content.\\nSocial media content can be text, photos, videos, GIFs, audio,\\netc. Social media can be used for various reasons, from\\nusers who share interests communicating to getting informed\\nabout current worldwide events. Social media can also be\\nused for detecting early signs of stress and depression [1]‚Äì\\n[3]. The existence of social media in our day-to-day lives\\nis more prevalent than ever. As of 2023, there are roughly\\n4.9 billion social media users, a percentage that is more than\\n60% of the entire population and more than 100 social media\\nplatforms. X, previously known as Twitter, stands out as one\\nof the most widely recognized social media platforms. Twitter\\nwas launched in 2006. It revolves around the concept of\\nG. Tzoumanekas, L. Ilias, J. Psarras, and D. Askounis are with the Decision\\nSupport Systems Laboratory, School of Electrical and Computer Engineering,\\nNational Technical University of Athens, 15780 Athens, Greece (e-mail:\\nlilias@epu.ntua.gr; askous@epu.ntua.gr; john@epu.ntua.gr).\\nM. Chatzianastasis is with DaSciM, LIX, Ecole Polytechnique, In-\\nstitut Polytechnique de Paris, Palaiseau, 91120, France (email: mixal-\\nisx97@gmail.com).\\nG. Kiokes is with the Laboratory of Electrical Machines and Installations,\\nDivision of Electrical, Electronics and Informatics, School of Engineering,\\nMerchant Marine Academy of Aspropyrgos, 19300 Aspropyrgos, Greece\\n(email: gkiokes@iccs.gr).‚Äúfollowing‚Äù other users. A user can follow accounts they are\\ninterested in and see their tweets in their timeline (‚Äúfollowing‚Äù)\\nand conversely can have ‚Äúfollowers‚Äù that see their tweets.\\nNowadays, it has been established as a powerful tool for real-\\ntime news updates, public discourse, and social movements,\\nand continues to evolve and enhance its user experience. In\\n2023, Twitter was renamed to X by then-CEO Elon Musk. The\\nextensive presence of social media in the modern landscape\\nhas led to the emergence of accounts that automate interactions\\non social media platforms, often mimicking human behavior,\\nthe so-called bots. These bots can be coded to perform a\\nvariety of tasks, such as automatically publishing content,\\nliking, sharing, following, or commenting on posts. Some can\\neven be programmed to engage in conversations to promote\\nspecific agendas. Their behavior differs depending on their\\nintent and purpose, but they might share features, such as very\\nhigh or very low activity levels and more structured and char-\\nacteristic language patterns [4]. Uyheng et al. [5] examined the\\norigin and traits of trolling messages, finding that they often\\noriginate from automated bots and are distinguished by their\\nuse of abusive language, reduced cognitive complexity, and\\nspecific targeting of individuals or entities. Their study also\\nnoted a tendency for bots to target right-leaning sources of\\ninformation, while trolls tended to engage with less polarized\\ncontent, spreading misinformation across diverse audiences.\\nBots are very efficient in spreading misinformation, particu-\\nlarly when programmed with optimized values for factors like\\nwalking speed, network distribution, and strategy [6]. Fake\\nnews and bots have had significant tangible consequences\\nin several cases. Users tend to believe conspiracy theories\\nand misinformation, and correction attempts can sometimes\\nbackfire [7]. Users might also share fake news for altruistic\\nor self-promotional purposes, yet those with greater social\\nmedia literacy are better equipped to identify and refrain from\\nspreading fake news [8]. Therefore, there‚Äôs a need for measures\\nto promote truthful reporting in media and detect any cases of\\nmisinformation dissemination.\\nThe need to detect bot accounts to shut them down is\\nquite immediate, assessing the hazards of their uncontrollable\\npresence on social media. Several studies to identify bots\\nfrom real users have been conducted that provide satisfactory\\nresults. There have been several approaches, including super-\\nvised learning [9], unsupervised learning [10], reinforcement\\nlearning [11], and GNN-based architectures [12]. However,\\nall these traditional neural architectures often rely on fixed\\nparameters that are manually designed. Constructing efficient\\nneural network architectures requires extensive feature engi-\\nneering and can be a quite challenging and time-consumingarXiv:2411.16285v1  [cs.LG]  25 Nov 20242\\nprocedure. Also, fixed architectures often mitigate the models‚Äô\\nadaptability on other datasets and tasks. Motivated by these\\nlimitations, we examine the implementation of Neural Archi-\\ntecture Search (NAS) to automate the process of discovering\\noptimal architectures. NAS explores a search space of possible\\narchitectures and identifies the configurations that enhance the\\nmodel‚Äôs performance.\\nA Neural Architecture Search method that has been pro-\\nposed to solve the performance issues of fixed architectures is\\nDeep and Flexible Graph Neural Architecture Search (DFG-\\nNAS) [13]. It employs an evolutionary algorithm to explore a\\nvast space of permutations of Propagation and Transformation\\noperations, to find the one with the best accuracy in the valida-\\ntion set. Addressing the limitations of previous bot detection\\nmodels due to their fixed architectures we employ DFG-NAS\\non a GNN-based approach for bot detection. This approach\\nleverages the user‚Äôs semantical and property information and\\nconstructs a heterogeneous graph out of the follower-following\\nrelationships between users. Then, we adapt the DFG-NAS\\napproach to handle Relational Graph Convolutional Neural\\nNetworks (RGCNs). The model automatically searches for\\nthe permutation of Propagation (P) and Transformation (T)\\nfunctions, the two main processes of the message-passing\\nprotocol, with the highest validation accuracy. The model is\\nalso amplified with the use of the Gate operation on the P\\nconnections and the use of the skip-connection operation on\\nthe T connections.\\nTo the authors‚Äô knowledge, DFG-NAS has not been em-\\nployed before in the task of bot detection. All our experiments\\nwere performed on the Twibot-20 dataset [14]. The following\\nsums up the contributions of our work:\\n‚Ä¢We implement DFG-NAS, tailored to RGCNs, to auto-\\nmatically determine the most effective permutation of the\\nmessage-passing operations.\\n‚Ä¢We perform experiments to demonstrate the benefits of\\narchitecture search in bot detection and compare our\\nmethod to state-of-the-art models.\\n‚Ä¢We perform a thorough ablation study on the necessity of\\nthe user metadata in our graph, the Gate operation, and\\nthe skip-connection operation in NAS.\\nII. R ESEARCH OBJECTIVE\\nIt is evident to any social media user that bots continue to\\ndominate the digital landscape despite extensive efforts in bot\\ndetection and platform initiatives to stop their activities. As\\ntechnology advances, bots are programmed to mimic human\\nmannerisms more effectively, making them more resilient\\nagainst detection mechanisms. Beyond the irritation they pose\\nto everyday users, some bots can have tangible and detrimental\\neffects on human society. In 2016 fake news stories spread\\nwidely during the U.S. presidential election campaign aiming\\nto influence the public vote [15]. Throughout the COVID-19\\npandemic [16], bots spread misinformation about the virus and\\nthe vaccines on social media, leading to mob panic, confusion,\\nand even resistance to public health measures. Fake news is\\noften framed in a manner that fosters negativity in social\\ndiscussions and hinders individuals‚Äô ability to consider diverseperspectives, contributing to the formation of ‚Äôecho chambers‚Äô\\non social media platforms [17]. Bots also exacerbate cyberbul-\\nlying by mass-targeting users, leading to serious psychologi-\\ncal consequences. Social media platforms face challenges in\\neffectively moderating such content. Cyberbullying detection\\nmethods often rely on unclear definitions and are prone to\\nbiases in data annotation [18]. Their evolving nature raises\\nconcerns about the efficiency of current preventive measures,\\nhighlighting the need for innovation to prevent the dangers\\nposed by this digital phenomenon.\\nThe motivation for this research was constructing a model\\ncharacterized by adaptability across future datasets, ensuring\\nresilience in the face of evolving technology through time.\\nMany contemporary models rely on fixed architectures, often\\nstruggling to demonstrate their efficiency on novel datasets.\\nAlthough Neural Architecture Search (NAS) has shown sig-\\nnificant advantages in various test cases, its application to bot\\ndetection remains relatively underexplored, with limited but\\npromising results noted in studies such as [19]. Considering\\nthe dynamic nature of the social media landscape and the con-\\ntinuous evolution of bots, more flexible architectures specif-\\nically designed for bot identification could offer a practical\\nsolution to mitigate their real-world consequences.\\nThis research aims to showcase the efficiency advantages\\nof architecture search and perhaps pave the way for more im-\\nplementations of NAS models in bot detection in the ongoing\\nbattle against automated malicious activities.\\nIII. R ELATED WORK\\nA. Bot and fake news detection models\\nThe task of bot identification has attracted numerous studies\\nand many state-of-the-art models propose fascinating method-\\nologies. We could mainly divide these models into supervised\\nlearning approaches, unsupervised learning approaches, and\\nGNN-based approaches. In this section, we present some\\nbaseline models proposed for bot detection and discuss how\\nthey fall into the above categories.\\nLee et al. [9] applied various machine learning algorithms,\\nincluding SVMs, Naive Bayes, and decision trees, to build\\nand evaluate a supervised bot detection model. The features\\nused in their analysis included account-based features (e.g.,\\nthe number of followers, friends, tweets), temporal features\\n(e.g., time of account creation, tweet frequency), and content-\\nbased features (e.g., usage of URLs, hashtags). Kuduganta\\net al. [20] suggested a deep learning model that uses the\\nuser‚Äôs tweets and some metadata features. This architecture\\nincludes a tokenizer, GloVE embedding layer, LSTM, and\\nDense layers. Wei et al. [21] used only users‚Äô tweets with\\nno context of prior knowledge on user profiles, friendship\\nnetworks, or behaviour. They proposed a recurrent neural\\nnetwork (RNN) model that used word embeddings to encode\\ntweets, a three-layer Bidirectional LSTM (BiLSTM), and a\\nsoftmax layer at the binary output. Cai et al. [22] proposed\\ntheir model (BeDM) that involved deep neural networks in\\nbot detection. They employed convolutional neural networks\\n(CNNs) and LSTM, using only the tweet semantics, such\\nas the frequency and the type of publications. Botometer3\\n[23] is a web-based program developed by Davis et al. at\\nIndiana University. It leverages more than 1,000 features to\\nclassify Twitter accounts as bots and humans, such as friends,\\nthe structure of the social network, user meta-data, temporal\\nactivity, and sentiment analysis. Botometer distinguishes the\\naccounts by an overall bot score (ranging from 0 to 5), along\\nwith several other scores. The greater the score, the greater\\nthe probability that this account is linked to a bot. Yang et\\nal. [24] presented a thorough introduction of the latest version\\nof Botometer for new users and demonstrated a case study.\\nAlarfaj et al. [25] utilized features based on content attained\\nfrom the Twitter API and employed state-of-the-art classifiers,\\nlike MLPs, random forest, and naive Bayes. Features included\\nmessages, special characters, sentiment analysis, etc. Alothali\\net al. [26] introduced their framework, called Bot-MGAT,\\nwhich stands for bot multi-view graph attention network. The\\nscientists pointed out that other approaches couldn‚Äôt adjust to\\ndifferent datasets since there wasn‚Äôt enough recently updated\\nlabeled data, which made sense given the constantly shifting\\nbehavior of the bots. They presented a methodology that\\nmakes use of transfer learning (TL) to leverage the multi-view\\ngraph attention mechanism. The framework also benefited\\nfrom semi-supervised learning, using labeled and unlabeled\\ndata. The authors used the TwiBot-20 [14] due to its graph\\nstructure, extracting 18 features for the training. Feng et al.\\n[27] suggested SATAR. In particular, SATAR leverages the\\nuser‚Äôs semantics, property, and neighborhood information. It\\nadjusts by fine-tuning parameters and pre-training on a huge\\nnumber of self-supervised users. The authors proposed two\\nalternative models: SATAR FCandSATAR FT. Ilias et al.\\n[28] proposed two methods for bot detection using deep learn-\\ning techniques. Their first approach extracts a substantial 71\\nfeatures per user to utilize for account classification to bots and\\ngenuine users. They also employed various feature selection\\ntechniques to discard redundant and irrelevant features. Their\\nsecond methodology proposes a deep learning architecture\\nfor tweet-level classification. This architecture incorporates\\nan attention mechanism atop the Bidirectional Long Short-\\nTerm Memory (BiLSTM) layer. During the learning phase,\\nthe attention mechanism helps the model better focus on\\nrelevant information. Ilias et al. [29] focused solely on user\\ndescriptions and sequences of actions performed by Twitter\\naccounts. Their approach includes both unimodal (text or\\nimage) and multimodal (both text and image) methods. They\\ndesigned digital DNA sequences per user based on tweet type\\nand content, converted these sequences into 3D images, and\\nfine-tuned pre-trained vision models like AlexNet, ResNet,\\nand VGG16. For bot detection through user descriptions, they\\nfine-tuned TwHIN-BERT, a transformer model. In multimodal\\napproaches, they use VGG16 for visual representation and\\nTwHIN-BERT for textual representation, proposing three fu-\\nsion methods: concatenation, gated multimodal unit (GMU),\\nand cross-attention. They conducted their experiments on\\nthe Cresci‚Äô17 dataset. Wei et al. [30] proposed their model\\nBOTLE. Their model utilizes a recurrent neural network\\n(RNN) with Bidirectional Gated Recurrent Units (BiLGRU)\\nconnecting two hidden layers of opposite directions leading\\nto the same output. Notably, BOTLE does not rely on hand-crafted features or pre-existing information regarding account\\nprofiles. Linguistic embeddings, including word, character,\\npart-of-speech, and named-entity embeddings, are employed to\\nencode tweet content, eliminating the need for labor-intensive\\nfeature engineering. Bazmi et al. [31] introduced the Multi-\\nView Co-Attention Network (MVCAN), which aims to capture\\nthe latent topic-specific credibility of both users and news\\nsources. This model represents news articles, users, and news\\nsources in a manner that encodes topical viewpoints, socio-\\ncognitive biases, and partisan biases as vectors. These features\\nare encoded using a variant of the Multi-Head Co-Attention\\n(MHCA) mechanism. Shevtsov et al. [32] introduced their\\nmodel BotArtist, constructed on a semi-automatic machine\\nlearning pipeline, that requires minimal features for training,\\ntaking into consideration the loads of data needed by previ-\\nous approaches and the recent monetization of Twitter API\\nrequests. Sujith et al. [33] proposed a supervised learning\\napproach that used multiple models to detect bots. Their\\nclassification of accounts relied on features like user metadata,\\ntweet content, and posting history, among others. In addition\\nto identifying bot accounts, the authors assigned a level of\\nsignificance or influence to them, prioritizing the removal of\\nthe most influential or harmful bot accounts. Liu et al. [34]\\nproposed BotMoE, which leverages three perspectives of user\\ninformation (metadata, text, and graph representations) and\\nincorporates a community-aware Mixture-of-Experts (MoE)\\nlayer to assign users to different communities. The user\\nrepresentations are fused with an extractor fusion layer and\\nsupervised learning is employed to train the BotMoE frame-\\nwork to perform community-aware bot detection. Saxena et al.\\n[35] proposed two frameworks for recognizing accounts that\\ndisseminate false information on Twitter. Initially, they em-\\nployed profile-based data, including the verified status, profile\\nphoto, and account lifetime and activity. Then, they combined\\ntweet-propagation patterns and assigned a credibility score to\\neach user, signifying their authenticity. Dimitriadis et al. [36]\\nproposed CALEB that is based on the Conditional Generative\\nAdversarial Network (CGAN) and its extension, Auxiliary\\nClassifier GAN (AC-GAN). By developing realistic artificial\\nbot varieties, they were able to replicate the evolution of bots.\\nAs a result, they enhanced already-existing datasets and were\\nable to identify bots before they emerged.\\nYang et al. [37] used a combination of unsupervised and\\nsupervised learning methods for bot detection. Specifically, the\\nauthors utilized minimal features derived from user metadata,\\ntemporal patterns, network structure, sentiment analysis, and\\nlinguistic cues that they fed into a machine learning pipeline,\\nthat reduced dimensionality and included classification algo-\\nrithms. Cresci et al. [10] introduced the Social Fingerprinting\\ntechnique for bot detection, a Digital DNA technique that mod-\\nels social network users‚Äô behaviors. Each user is represented\\nas a sequence of characters depending on the type and content\\nof the tweets they publish, simulating a DNA sequence. The\\nauthors try to find similarities in the sequences defining the\\nlength of the Longest Common Substring (LCS) between two\\nsequences. For a set of real users, the length of LCS was\\nfound to be particularly small, leading to the conclusion that\\nlonger sequences than the average LCS were bots. Based on4\\nthis idea, the authors developed two techniques, one based\\non supervised learning and another on unsupervised learning\\nto find similarities in the behaviour of accounts. Quezada et\\nal. [38] developed a real-time bot infection detection model\\nthat analyzes Domain Name System (DNS) traffic events.\\nThey extracted 13 attributes from DNS logs to create unique\\nfingerprints for servers. Using Isolation Forest, an algorithm\\nfor unsupervised learning, they identified anomalies in the\\nfingerprints to classify hosts as infected or not. The model\\nalso utilized Domain Generation Algorithms (DGA) to search\\nfor queries to anomalous domains. Finally, a Random Forest,\\na supervised learning algorithm, was employed to create a\\nmodel for detecting future bot infections on hosts. Miller et\\nal. [39] approached bot identification as an anomaly detection\\nproblem. They extracted 107 features from user‚Äôs tweets\\nand property information and adapted two stream cluster-\\ning algorithms, StreamKM++ and DenStream, to facilitate\\nspam detection and identified bot users as abnormal outliers.\\nChavoshi et al. [40] developed DeBot, a bot detection system\\nfor social media, using warped correlation to identify likely\\nbot accounts based on their high synchronicity, a characteristic\\nunlikely in human users. DeBot doesn‚Äôt require labeled data\\nand operates on activity correlation. Moreover, through the\\nutilization of a lag-sensitive hashing technique, it can promptly\\ncluster accounts for real-time classification. Minnich et al. [41]\\nproposed their real-time unsupervised model BotWalk. Using\\nmetadata, content, temporal, and network features they employ\\nanomaly detection, comparing each user to a seed bank of\\nlabeled accounts iteratively. Mannocci et al. [42] proposed\\nMulBot, an unsupervised bot detection system that utilizes\\nmultivariate time series (MTS) analysis. They employed an\\nLSTM autoencoder to map the MTS data into a latent space\\nand then conducted clustering on this encoded data to find\\ndense clusters of users exhibiting similar behavior, assuming\\nthis was a common trait of bot accounts. MulBot also show-\\ncases effectiveness in identifying and distinguishing various\\nbotnets. Wu et al. [43] employed unsupervised machine learn-\\ning techniques, specifically K-Means and Agglomerative clus-\\ntering, for Twitter bot detection. They used account activity,\\npopularity, and verification status, among other features for the\\nclustering. Koggalahewa et al. [44] introduced an unsupervised\\nmethod for bot identification based on a user‚Äôs peer approval\\nin the social network. They based peer acceptance between\\ntwo users on their shared interests over a multitude of issues.\\nLopes et al. [45] introduced their botnet identification model,\\ndesigned to detect networks of compromised devices under\\nmaster control. Their approach relies on analyzing network\\nflow behavior through a contemporary method known as the\\nEnergy-based Flow Classifier (EFC). EFC employs inverse\\nstatistics to enhance anomaly detection.\\nAlhosseini et al. [46] introduced the use of graph convo-\\nlutional neural networks (GCNN) in bot identification. They\\nnoted that besides the users‚Äô features, the construction of a\\nsocial network would enhance a model‚Äôs ability to distinguish\\nthe bots from the genuine users. Feng et al. [47] introduced\\nthe aspect of diversity in relationships and influence dynamics\\namong users in the Twittersphere for bot detection. They\\nproposed a bot detection framework that leverages a networkwith users as nodes and the different relations as edges.\\nThen they aggregated messages across users and operated\\nheterogeneity-aware Twitter bot detection. They conducted\\ntheir experiments using the Twi-Bot20 dataset. Feng et al. [12]\\nproposed their model for bot detection BotRGCN, which is\\nshort for Bot detection with Relational Graph Convolutional\\nNetworks. BotRGCN builds a heterogeneous graph out of\\nthe following relationships and uses information, such as the\\nuser‚Äôs description, tweets, numerical and categorical property\\nset, and neighborhood information. The experiments were\\nconducted on the Twi-Bot20 dataset [14], but BotRGCN\\ncould exploit other types of relations if supported by the\\ndataset. Ku Àásen et al. [48] examined the structural dynamics of\\nconversations between humans and bots on Twitter following\\nemotionally charged riot events. They introduced ‚Äùemotion-\\nexchange motifs‚Äù to identify recurring patterns in emotional\\nmessage exchanges. Their findings revealed that human con-\\nversations exhibited various motifs with reciprocal edges and\\nself-loops, indicating interactive dialogue. In contrast, bots\\ntypically disseminated identical messages to multiple users or\\ndid not anticipate replies. Moreover, bots frequently initiated\\nconversations and often conveyed fear-inducing messages. Bui\\net al. [49] introduced a graph-based method for bot detection.\\nThey detailed their data collection process and identified spe-\\ncific behaviors indicative of an account being associated with a\\nbot. These behaviors can include engagement with other users,\\nnonsensical usernames and profile information, repetitive con-\\ntent posting, and retweeting activity. These observations are\\nutilized to label the accounts accordingly. Dehghan et al.\\n[50] suggested that the local social network formed around\\neach account can aid in identifying the bots. To prove their\\nhypothesis, they compared two classes of embedding algo-\\nrithms, the former of which focused on proximity data and the\\nlatter that focused on nodes‚Äô neighborhoods. They discovered\\nthat the structural embeddings presented higher information\\nunderlining the valuable information that is embedded within\\neach node‚Äôs local network. Pham et al. [51] introduced their\\napproach Bot2Vec, which eliminated the need for user profile\\nfeatures. To improve the model‚Äôs generalization on many social\\nmedia platforms, they used only local neighborhood relations\\nand the community structure of the graph that represented the\\nusers and employed an random walk strategy in the com-\\nmunities. Noekhah et al. [52] proposed their model ‚ÄùMulti-\\niterative Graph-based opinion Spam Detection‚Äù (MGSD) that\\naims to identify various types of spam entities. It analyzes\\nall kinds of relationships between them and utilizes domain-\\nindependent features, allowing for generalization across types\\nof opinionated documents. Trained on both existing and novel\\nfeatures, MGSD assigns a spam score to each entity. Ye et\\nal. [53] proposed HOFA, a graph-based framework for bot\\ndetection, featuring two key modules: Homophily-Oriented\\nGraph Augmentation (Homo-Aug) and Frequency Adaptive\\nAttention (FaAt). The Homo-Aug employs an MLP to extract\\nuser representations and generate a k-NN graph. Meanwhile,\\nthe FaAt module acts as a low-pass filter for homophilic edges\\nand a high-pass filter for heterophilic edges. This function\\nprevents excessive smoothing of user features by the neigh-\\nborhood. El-Mawass et al. [54] explored using the output of5\\nexisting supervised classification systems to detect spammers.\\nThey incorporated the classifiers‚Äô outputs as prior beliefs\\nwithin a probabilistic graphical model framework. Proposing\\na bipartite users-content interaction graph, they facilitated the\\nspread of beliefs to similar accounts. Constructing a Markov\\nRandom Field on a graph of similar users, they employed\\nLoopy Belief Propagation to derive the predictions. Their\\nfindings demonstrated a notable enhancement in recall while\\nmaintaining precision.\\nB. Neural Architecture Search approaches\\nGraph neural architecture search is proposed as the solution\\nto performance limitations due to a fixed architecture. Parame-\\nter tuning in neural networks can be a challenging task. Many\\nNAS methods have been suggested that include variations in\\nthe search space, the optimization method, and the architecture\\nevaluation. We will divide these methods based on their opti-\\nmization method, which will include reinforcement learning,\\nevolutionary algorithms and gradient-based methods.\\nZhou et al. [55] proposed the automated graph neural\\nnetworks (Auto-GNN) framework. Auto-GNN searches for\\nthe best GNN architecture possible in a predetermined search\\nspace, divided into six classes of actions: hidden dimension,\\nattention function, attention head, aggregate function, com-\\nbine function, and activation function. For efficiency rea-\\nsons, the authors designed a conservative explorer to pre-\\nserve the optimal neural architecture discovered during the\\nsearch. The authors also implemented constrained parameter\\nsharing, adapted to the heterogeneous GNN architecture. Two\\nexperimental methods were presented: inductive, in which the\\ngraph structure and node features on the testing and validation\\nsets are unknown during training, and transductive, which\\ninvolves the availability of unlabeled data for testing and\\nvalidation during training. Gao et al. [56] proposed GraphNAS\\nto implement an automatic search of the best graph neu-\\nral architecture based on reinforcement learning. The search\\nspace covers sampling functions, aggregation functions, and\\ngated functions. GraphNas also uses more efficient parameter-\\nsharing techniques than other contiguous models for CNNs\\nand RNNs. After training 1000 different architectures, the\\nfive best ones were used for the testing, which surpassed\\nhuman-invented ones or those produced by random searches.\\nZhao et al. [57] proposed the SNAG framework (Simplified\\nNeural Architecture Search for Graph neural networks). The\\nsuggested framework had two key components: Node aggre-\\ngators, which focused on neighborhood features, and Layer\\naggregators, which focused on the range of the neighborhood\\nused. The search space algorithm was a variant of Reinforce-\\nment Learning that adopted the weight-sharing mechanism\\n(SNAGWS). Nunes et al. [58] presented one NAS methods\\nfor optimizing GNNs based on reinforcement learning and\\none based on evolutionary algorithms. The authors defined\\ntwo cases of search spaces: Macro, where the architectures\\ngenerated have the same structure, and Micro, where the\\narchitectures are not rigidly structured but combine several\\nconvolutional schemas. They concluded that EA and RL found\\nvery similar architectures to those found by a random search,a significantly simpler technique. However, they pointed out\\nthat whilst the other approaches generated large structures in\\nas much as 80% of the situations, EA created the majority of\\nGPU-fitting designs. Li et al. [59] proposed Meta-GNAS that\\nuses meta-reinforcement learning from past tasks to apply that\\nknowledge to new tasks. Additionally, they speed up the search\\nby using a predictive model to evaluate the potential graph\\nneural architectures instead of training them from scratch.\\nPeng et al. [60] implemented a NAS approach to human\\naction recognition from skeleton movements. The search space\\nwas enlarged with diverse spatial-temporal graph modules\\nwhile constructing higher-order connections between nodes\\nusing Chebyshev polynomial approximation. The search al-\\ngorithm used is an evolutionary adaptation with a high sam-\\npling efficiency, denoted CEIM (Cross-Entropy method with\\nImportanceMixing). Jiang et al. [61] adapted the method of\\nneural architecture search to the conception of GNNs for\\npredicting molecular properties. The authors designed neural\\nnetworks for message-passing (MPNNs) between nodes. To\\nfind an optimal MPNN from the user-defined search space,\\nthey used regularized evolution (RE) from the DeepHyper\\npackage. Zhang et al. [13] proposed DFG-NAS, an innovative\\nmethod that allows for automatic search of very deep and\\nadaptable GNN architectures. DFG-NAS focuses on exploring\\nmacro-architectures, specifically the implementation details\\nof atomic propagation (P) and transformation (T) operations\\nwithin the GNN. P is linked to the graph structure, whereas\\nT concentrates on the non-linear transformations within the\\nneural network. In addition, they adopted gating and skip-\\nconnection mechanisms for deeper GNN pipelines. They\\nused an evolutionary algorithm to find the optimal archi-\\ntecture, which supported four cases of mutation. Peng et\\nal. [62] introduced Fast-ENAS as a computationally efficient\\nalternative to Evolutionary Neural Architecture Search. This\\nmethod utilizes a training-free performance metric that is\\ncomputed with a single forward pass. The authors enhance\\nthe search process by incorporating a GCN-based contrastive\\npredictor, aiming to improve the accuracy of the estimated\\nperformance of a candidate architecture, bringing it closer\\nto its actual performance. Shang et al. [63] introduced AG-\\nENAS, which brings two key innovations to the Evolutionary\\nNeural Architecture Search process. Firstly, it employs an\\nadaptive parameter adjustment mechanism based on popula-\\ntion diversity and fitness, enhancing the adaptation of ge-\\nnetic operators‚Äô associated parameters. Secondly, the model\\nintroduces a mutation operator guided by the gene potential\\ncontribution. It improves offspring quality by assigning weight\\nto more valuable genes through a distribution index matrix.\\nThe concept of aging is integrated into environmental selection\\nto mitigate premature convergence. Lopes et al. [64] presented\\nGEA (Guided Evolutionary Architecture), which tackles the\\nproblem of other NAS models getting trapped in suboptimal\\nsolutions during the search process. GEA overcomes this\\nchallenge by generating and evaluating multiple architectures\\nusing a zero-proxy estimator and selecting only one with the\\nbest-performing one for the next generation. This approach\\nexpands the search space without increasing complexity, as\\nnew architectures are derived from previous ones through6\\nmutations.\\nZhao et al. [65] proposed their framework SANE. The\\nsearch space has similarities with the search space from\\nthe SNAG framework, with Node and Layer aggregators.\\nHowever, the authors presented a novel differentiable search\\nalgorithm. Cai et al. [22] introduced a GNAS approach\\nfeaturing a uniquely designed search space and a gradient-\\nbased search approach. The authors developed a three-level\\nGraph Neural Architecture Paradigm (GAP) that includes two\\ntypes of fine-grained atomic operations (neighbor aggrega-\\ntion and feature filtering) that are derived from message-\\npassing, to build the search space. Li et al. [66] introduced\\nan innovative dynamic one-shot search space designed for\\nmulti-branch neural architectures within GNNs. The dynamic\\nnature of the search space offers a larger capacity than a\\nlarger predefined search space. The architectures with lower\\nimportance weights are removed periodically from the pop-\\nulation, while the candidate operations are unique to every\\nedge of the computational graph. The authors performed both\\nsupervised and unsupervised techniques for the training part.\\nZhao et al. [67] proposed a gradient-based architecture search\\nmethod for predicting a system‚Äôs remaining useful life. Their\\napproach models the search space as a directed acyclic graph\\n(DAG), where nodes represent latent representations and edges\\nrepresent transformation operations. By employing candidate\\noperations like ReLU and tanh, along with the softmax func-\\ntion, they make the search space continuous and the objective\\nfunction differentiable, facilitating gradient-based optimization\\nmethods to find the optimal architecture.\\nC. Related work review findings\\nFrom the aforementioned research works, it is clear that\\nthere have been many approaches to the task of bot detection.\\nPrevious studies include supervised, unsupervised, and graph\\nneural network (GNN) based methods. While they have shown\\npromising results, the relentless evolution of bot accounts\\ntoward simulating human-like patterns poses a significant\\nchallenge to their effectiveness. These models are constrained\\nby fixed architectures, limiting their adaptability to newer\\ndatasets.\\nLittle work has been done in employing Neural Archi-\\ntecture Search methods in GNN-based methodologies for\\nbot detection. Our work shifts the focus on overcoming the\\nperformance limitations due to fixed architectures, by utilizing\\nDFG-NAS to search for the best configuration of Propagation\\nand Transformation functions in the message passing protocol\\nof our RGCNs. Instead of extensive feature engineering our\\nmodel searches for the permutation with the highest accuracy\\nand aims for better adaptability in newer datasets that will\\ndepict future bots‚Äô behavior. Moreover, DFG-NAS presents\\nhigh advantages, as it is suitable for GNN-based methods and\\novercomes over-smoothing and model degradation issues with\\nthe gate and skip-connection operations.\\nIV. D ATASET\\nThe TwiBot-20 Dataset [14] is a publicly available dataset,\\nconstructed with a breadth-first search (BFS) methodology.TABLE I\\nTWIBOT-20 D ATASET ATTRIBUTES\\nAttribute Description\\nID ID from Twitter to identify the user\\nprofile profile information from Twitter API\\ntweet 200 recent tweets of the user\\nneighbor 20 random followers and followings of the user\\ndomain domain of the user (politics, business, entertainment, sports)\\nlabel label of the user (‚Äô1‚Äô: bot, ‚Äô0‚Äô:human)\\nThe dataset includes information about each user‚Äôs profile\\ninformation obtained from the Twitter API, recent tweets, and\\ndomains of the user‚Äôs interest. It also contains information\\nabout the user‚Äôs neighborhood, which helps us construct a\\nheterogeneous graph from the following relationships. Table 1\\npresents all the attributes of the TwiBot-20 Dataset and a short\\ndescription of them. The information from the user profiles\\nis further mentioned in the preprocessing part of the model.\\nThe graph that is constructed consists of 229,580 nodes and\\n227,979 edges. The objective of the bot detection system is\\nto distinguish between bots and genuine users by analyzing\\ninformation from user descriptions, tweets, numerical and\\ncategorical properties, as well as neighborhood information.\\nV. M ETHODOLOGY\\nIn this part, we present a complete analysis of our methodol-\\nogy. First, we describe the preprocessing of the user metadata\\nused in our model. Next, we introduce the use of Relational\\nGraph Convolutional Neural Networks and the two functions\\nin Message Passing. Last, we explain the use of DFG-NAS\\n[13] in searching for the best permutation of Propagation\\nand Transformation functions. In Figure 1, we depict the\\narchitecture of the model on a higher level, while Figure 2\\npresents the connections between the different layers of an\\nexample configuration of P and T functions.\\nA. Data Preprocessing\\nWe follow the preprocessing suggested by Feng et al. for\\nBotRGCN [12]. Each user‚Äôs representation includes metadata\\nthat are preprocessed as follows:\\n‚Ä¢Overall : User‚Äôs description, tweets, numerical and cate-\\ngorical properties are encoded and concatenated to finally\\nrepresent the user‚Äôs metadata:\\nr= [rb;rt;rnum\\np;rcat\\np]‚ààRD√ó1(1)\\nwhere Dis the user embedding dimension. Each feature‚Äôs\\nprocession and representation are explained below. Later\\nwe will prove that the model‚Äôs performance is attributed\\nto all these features and not only to the heterogeneous\\ngraph.\\n‚Ä¢User description : The user descriptions are encoded with\\npre-trained RoBERTa:\\n¬Øb=RoBERTa ({bi}L\\ni=1),¬Øb‚ààRDs√ó1(2)\\nwhere ¬Øbdenotes the user description representation and\\nDsis the dimension of the RoBERTa embedding. The\\nvectors for the user‚Äôs description are derived:\\nrb=œï(WB¬∑¬Øb+bB), rb‚ààRD/4√ó1(3)7\\nTABLE II\\nUSERNUMERICAL PROPERTIES\\nFeature Name Description\\n#followers number of followers\\n#followings number of followings\\n#favorites number of likes\\n#statuses number of statuses\\nactive days number of active days\\nscreen name length screen name character count\\nwhere WBandbBrepresent trainable parameters, œï\\ndenotes the activation function, and Dis the dimension\\nof the embedding.\\n‚Ä¢User tweets : The user tweets are also encoded us-\\ning RoBERTa. The ultimate representation of the user‚Äôs\\ntweets, denoted as rt, is computed as the average of the\\nrepresentations of all individual tweets.\\n‚Ä¢User numerical properties : The user‚Äôs numerical prop-\\nerties are adopted straight from the Twitter API with no\\nfeature engineering and presented in Table 2. For this\\ninformation z-score normalization is conducted to get the\\nrepresentation rnum\\np from a fully connected layer.\\n‚Ä¢User categorical properties : The user‚Äôs categorical prop-\\nerties are also encoded with MLPs and GNNs, without\\nfeature engineering, just as the numerical properties. They\\nare adopted straight from the Twitter API and presented\\nin Table 3. After one-hot encoding, they are concatenated\\nand transformed through a fully connected layer and\\nleaky-relu to get their representation rcat\\np.\\nTABLE III\\nUSERCATEGORICAL PROPERTIES\\nFeature Name Description\\nprotected protected or not\\ngeo enabled geo-location enabled or not\\nverified verified or not\\ncontributors enabled enable contributors or not\\nistranslator is translator or not\\nistranslation enabled translation or not\\nprofile background tile the background tile\\nprofile user background image background image or not\\nhasextended profile extended profile or not\\ndefault profile the default profile\\ndefault profile image the default profile image\\nB. Relational Graph Convolutional Neural Networks\\nOur method builds a heterogeneous graph out of the fol-\\nlowing relationships. Users are considered nodes and the\\n‚Äôfollowing‚Äô and ‚Äôfollowers‚Äô relations are represented as edges\\nconnecting the nodes. The user‚Äôs ‚Äôfollowers‚Äô are therefore\\nrepresented differently than the user‚Äôs ‚Äôfollowing‚Äô. The het-\\nerogeneous graph that is constructed can represent better the\\nrelations between users and more relations between the users\\ncould be integrated into the graph if supported by the dataset.\\nThe users also contain the concatenated metadata that we\\ndescribed below.\\nTo combine the users‚Äô representations with the relationships\\nbetween users we make use of RGCNs. The message-passing\\nprocess in RGCNs comprises two fundamental operations:propagation (P) of the representations of the user‚Äôs neighbors\\nand transformation (T) on these representations. Below we\\ndescribe the process behind the two functions:\\n‚Ä¢Propagation (P) : Propagation includes message aggre-\\ngation from neighbour nodes without explicit node fea-\\nture transformation. The mathematical expression for the\\npropagation step is as follows:\\nh(l+1)\\ni =X\\nr‚ààRX\\nj‚ààNr\\ni1\\nci,rW(l)\\nrh(l)\\nj (4)\\nwhere h(l+1)\\ni is the new node feature after propagation, R\\nis the set of relations, Nr\\niare the neighbors of the node\\nwith relation r, ci,ris a normalization constant that can\\nbe learned or chosen in advance (for example ci,r=Nr\\ni)\\nandW(l)\\nris the learnable weight matrix for relation r.\\n‚Ä¢Transformation (T) : Transformation occurs on each\\nnode based on the relations. The mathematical expression\\nfor the transformation step is as follows:\\nh(l+1)\\ni =Wrooth(l)\\ni+X\\nr‚ààR(Wrh(l)\\ni) (5)\\nwhere h(l+1)\\ni is the new node feature after transformation,\\nWroot is the learnable weight matrix for the root node,\\nWris the learnable weight matrix for relation r and R is\\nthe set of relations.\\nWe segregate these two types of functions since com-\\nbinations of them will construct the search space for the\\narchitecture search.\\nC. Graph Neural Architecture Search\\nThe use of Graph Neural Networks offers undeniable advan-\\ntages in the task of bot detection. However, maximizing their\\nperformance may require extensive feature engineering. This is\\nwhy we employ Graph Neural Architecture Search, using the\\nmodel DFG-NAS [13]. Thus, we search for the permutation of\\nPropagation and Transformation steps that achieves the highest\\naccuracy. Most G-NAS methods have a fixed pipeline length\\nsince the performance decreases with too many P operations\\nas the layers become deeper, which is referred to as the over-\\nsmoothing issue. Propagation and transformation operations\\nregulate the effect of smoothing. Moreover, with unlimited\\npipeline length DFG-NAS searches for more flexible pipelines\\nof P and T operations, using an evolutionary algorithm. It also\\nmakes use of gating and skip-connection mechanisms in the\\nP and T operations, respectively.\\nThe search space includes P-T combinations and the number\\nof P-T operations. The output of node vin the l-th layer\\nis represented by o(l)\\nvin a single P or T operation within a\\nsingle GNN layer of the model. The layer indices of all P\\nand T operations are included in two sets, LPandLT. The\\nconnections of P and T are depicted in Figure 1(b) and also\\ndescribed below:\\nPropagation connections : An imminent problem in GNNs\\nis over-smoothing or under-smoothing, a problem that arises\\nwith too many or too few propagation operations. To achieve8\\nUser profile\\nDescription\\nT weets\\nNumerical\\nproperty\\nCategorical\\npropertyMLPT (\\n...DFG-NAS Architecture\\nReal User\\nBotRGCN (P) RGCN (T)\\nT (\\nT ( ) T ( )T ( ))\\n)T ( )\\nFig. 1. Model used for Bot detection. User metadata is fed to the architecture proposed by NAS. The P step includes message aggregation from neighbour\\nnodes. The T step includes the transformation process on each node based on neighbour relations. In the final part, an MLP decides whether the account\\nbelongs to a real user or a bot.\\nFig. 2. Example of connections between the layers of NAS architecture. New\\nT steps congregate information from all previous T steps. P steps propagate\\ntheir embeddings and sum them up for the next T step.\\nsuitable smoothness for different nodes, the P operations are\\namplified with a gating mechanism. If the next operation is\\nalso P, the result of the l-th P operation is the propagated\\nnode embedding of o(l‚àí1). On the other hand, if T is the next\\noperation, a node-adaptive combination weight is allocated\\nfor the node embeddings propagated by all of the previous\\nP operations. Formulatively:\\nz(l)\\nv=P(o(l‚àí1)\\nv) (6)\\no(l)\\nv=\\uf8f1\\n\\uf8f2\\n\\uf8f3z(l)\\nv, followed by PX\\ni‚ààLP,i‚â§lsoftmax (ai)z(i)\\nv, followed by T (7)\\nwhere ai=œÉ(s¬∑oi\\nv)represents the weight for the i-th layer\\noutput of node v. Here, sis the learnable vector shared among\\nthe entirety of nodes, and œÉdenotes the Sigmoid function. To\\nensure proper scaling, the Softmax function is employed to\\nnormalize the sum of gating scores, making it equal to 1.\\nTransformation connections : An imminent issue with\\nGNNs is the model degradation issue, caused by a hyperbolic\\namount of transformation operations and may result in a\\nreduction of the model‚Äôs accuracy. To mitigate this issue,\\nskip-connection mechanisms are used in T operations. EachT operation‚Äôs input is the total of all the T operations‚Äô outputs\\nup to the last layer and the output from the layer before it.\\nThe input and output of the l-th T operation can be formulated\\nas:\\nz(l)\\nv=o(l‚àí1)\\nv+X\\ni‚ààLT,i<m (l)o(i)\\nv (8)\\no(l)\\nv=œÉ(z(l)\\nvw(l)) (9)\\nwhere m(l)represents the index of the last T operation\\nbefore the l-th layer, and W(l)denotes the trainable parameter\\nin the l-th T operation.\\nEvolutionary algorithms are a class of optimization algo-\\nrithms inspired by biological evolution that aim to achieve\\nthe best accuracy in offspring through mutations. In our case,\\neach GNN architecture is represented as a sequence of P and\\nT operations. Each pipeline can be considered a chromosome\\nand the mutations that occur simulate nature‚Äôs mutations.\\nThese mutations can happen at any random position in the\\nsequence. In our instance, four different cases of mutation can\\nbe enforced:\\n‚Ä¢+P: append a propagation operation\\n‚Ä¢+T: append a transformation operation\\n‚Ä¢P‚ÜíT: replace a propagation operation with a transfor-\\nmation one\\n‚Ä¢T‚ÜíP: replace a transformation operation with a propa-\\ngation one\\nInitially, k distinct GNN designs are generated at random\\nand evaluated on the validation set. These architectures rep-\\nresent the initial population set Q. Subsequently, m (m <k)\\nmembers of the population are randomly sampled, and parent\\nA is determined by selecting the member with the highest\\nvalidation accuracy. By enforcing a random mutation of the\\nfour presented on A, a child architecture B is produced. B\\nis then evaluated and added to the population, and the oldest\\nperson is eliminated. After T generations of this procedure, the\\narchitecture with the best performance is eventually returned.9\\nDFG-NAS returns a sequence of P and T steps. As illus-\\ntrated in Figure 1(a), each step consists of an RGCN that\\nconducts one of the two main functions as we described\\nincorporating both the user metadata and the user relations.\\nAfter the RGCNs layers an MLP is employed to finally\\ndistinguish bots from genuine users.\\nVI. E XPERIMENTS\\nA. Baselines\\nWe compare our proposed apporach to the state-of-the-art\\nmodels that are referenced in the paper of BotRGCN [12].\\nThese experiments are all ran on the same dataset as the one\\nwe used for a fair comparison. We are using the published\\nresults for the comparison. More specifically, we compare our\\nmodel to these state-of-the-art models:\\n‚Ä¢Lee et al. [9] employed different supervised algorithms\\nwith several user features.\\n‚Ä¢Yang et al. [37] used a combination of supervised and\\nunsupervised learning with minimal user features.\\n‚Ä¢Kudugunta et al. [20] used both the tweets and the\\naccount metadata.\\n‚Ä¢Wei et al. [21] employed an RNN model utilizing only\\nthe user‚Äôs tweets.\\n‚Ä¢Miller et al. [39] extracted 107 features and employed\\nstream clustering algorithms.\\n‚Ä¢Cresci et al. [10] identified bots by computing the longest\\ncommon substring between encoded sequences of users.\\n‚Ä¢Botometer [23] is a web-based program that leverages\\nmore than 1,000 user features.\\n‚Ä¢Alhosseini et al. [46] introduced graph convolutional\\nneural networks in bot detection.\\n‚Ä¢SATAR [27] leverages the user‚Äôs semantics, property, and\\nneighborhood information\\n‚Ä¢BotRGCN et al. [12] used the user‚Äôs description, tweets,\\nnumerical and categorical properties, and neighborhood\\ninformation.\\n‚Ä¢Ilias et al. [29] designed two cross-attention layers based\\non the digital DNA sequence.\\nB. Experiment Settings\\nThe experiment was run on Google Colab using Nvidia‚Äôs\\nT4 GPUs. The population set k for the architectural search is\\n15, and the maximum generation time T is 80. The training\\nbudget of each GNN architecture is 70 epochs. These numbers\\nalthough limited due to our resources, provide a great example\\nof the efficiency of our model. More complex architectures\\nthat we tested do not necessarily provide better results. Also,\\nthe number of epochs is sufficient to get a good idea of each\\narchitecture‚Äôs accuracy. Adam optimizer is used for training,\\nand its learning rate is set to 0.04. The criterion is Cross\\nEntropy Loss and the regularization factor is 2e-4. Dropout\\nis applied to all feature vectors at a rate of 0.5, and dropout\\namong GNN layers is set to 0.8.\\nAfter running the NAS method we process the results and\\nexamine the five architectures with the best accuracy in the\\nvalidation set. Each architecture is now trained with 100\\nFig. 3. Permutations of Propagation (P) and Transformation (T) functions\\nof the top-5 performing architectures from DFG-NAS. Their validation\\naccuracies in the architecture search (from up to down) are: 87.01%, 86.99%,\\n86.95%, 86.89%, 86.82%\\nepochs on the TwiBot-20 dataset [14]. The train set is 70% of\\nthe dataset, the validation set is 20% and the test set is 10%.\\nAdam optimizer with a learning rate of 1e-3 is also used for\\ntraining. Then each architecture is tested on the test set. We\\nwill present the findings of these experiments below.\\nC. Evaluation Metrics\\nWe assess our model‚Äôs performance using its Accuracy, F1-\\nscore, Precision, Recall, Specificity, and MCC. These metrics\\nare computed by labeling the bots as the positive class and\\nthe genuine users as the negative class. To compare the\\nperformance of our model to the other baseline models we\\nwill only use the metrics Accuracy, F1-score, and MCC.\\nVII. R ESULTS\\nEach architecture during the search is saved with its P-T\\nconfiguration, accuracy in the validation set, and accuracy in\\nthe test set. In Figure 2, the five architectures with the highest\\nvalidation accuracy that are chosen from the NAS method are\\ndepicted.\\nThese architectures are trained and tested from scratch in\\nTwiBot-20 dataset. We present all the metrics attained by all\\nthe architectures in Table 4.\\nAll selections achieve good metrics and present advantages\\nin bot detection over state-of-the-art methods. These results\\nunderscore the significant advantages that emerge from em-\\nploying architecture search techniques regarding the field of\\nbot recognition. Moreover, they establish the efficiency of\\nutilizing user features and relationships between users in bot\\ndetection.\\nUpon closer examination of the results, the third architecture\\nachieves the best evaluation metrics. The fifth architecture has\\nthe highest precision. However, all the architectures present\\nhigh metrics of accuracy, F1-score, and MCC and whichever\\narchitecture we choose could compete with state-of-the-art\\nmodels. From now on we will refer to the third architecture\\nas our model, since it provides the highest accuracy.\\nIn Table 5 we present the performance of the baseline\\nmethods on the TwiBot-20 dataset compared to ours. We\\nsee that our model benefits from the search for the fittest\\narchitecture that we performed beforehand, as it achieves a\\nhigher accuracy, F1-score, and MCC than other state-of-the-\\nart methods.10\\nTABLE IV\\nPERFORMANCE OF THE ARCHITECTURES FROM ARCHITECTURE SEARCH .VALUES ARE REPORTED AS MEAN ¬±STANDARD DEVIATION . FIVE RUNS\\nOF RESULTS ARE AVERAGED . THE BEST OUTCOMES FOR EACH EVALUATION METRIC ARE IN BOLD .\\nModel Accuracy F1-score Precision Recall Specificity MCC\\n1st Architecture 0.852¬±0.005 0.865¬±0.008 0.851¬±0.015 0.880¬±0.031 0.818¬±0.027 0.702¬±0.010\\n2nd Architecture 0.855¬±0.004 0.869¬±0.005 0.853¬±0.007 0.886¬±0.012 0.819¬±0.012 0.709¬±0.009\\n3rd Architecture 0.857¬±0.004 0.871¬±0.003 0.849¬±0.008 0.895¬±0.007 0.812¬±0.013 0.712¬±0.007\\n4th Architecture 0.852¬±0.006 0.864¬±0.008 0.856¬±0.009 0.873¬±0.026 0.828¬±0.018 0.702¬±0.013\\n5th Architecture 0.852¬±0.007 0.864¬±0.008 0.858¬±0.003 0.872¬±0.019 0.829¬±0.007 0.703¬±0.014\\nTABLE V\\nPERFORMANCE OF MODELS ON THE TWIBOT-20 DATASET .VALUES ARE REPORTED AS MEAN ¬±STANDARD DEVIATION . FIVE RUNS OF RESULTS ARE\\nAVERAGED . THE BEST OUTCOMES FOR EACH EVALUATION METRIC ARE IN BOLD .\\nModel Accuracy F1-score MCC\\n[9] 0.7456 0.7823 0.4879\\n[37] 0.8191 0.8546 0.6643\\n[20] 0.8174 0.7517 0.6710\\n[21] 0.7126 0.7533 0.4193\\n[39] 0.4801 0.6266 -0.1372\\n[10] 0.4793 0.1072 0.0839\\n[23] 0.5584 0.4892 0.1558\\n[46] 0.6813 0.7318 0.3543\\n[27] 0.8412 0.8642 0.6863\\n[12] 0.8462 0.8707 0.7021\\n[29] 0.7466 0.7630 ‚Äì\\nours 0.8568 ¬±0.004 0.8712 ¬±0.003 0.7116 ¬±0.007\\nTABLE VI\\nTRAINING MODEL WITH LESS FEATURES . VALUES ARE REPORTED AS MEAN ¬±STANDARD DEVIATION . FIVE RUNS OF RESULTS ARE AVERAGED . THE\\nBEST OUTCOMES FOR EACH EVALUATION METRIC ARE IN BOLD .\\nModel Accuracy F1-score Precision Recall Specificity MCC\\nOurs 0.857¬±0.004 0.871¬±0.003 0.849¬±0.008 0.895¬±0.007 0.812¬±0.013 0.712¬±0.007\\nw/o description 0.859¬±0.004 0.875¬±0.004 0.845¬±0.002 0.906¬±0.008 0.804¬±0.004 0.718¬±0.008\\nw/o tweets 0.833¬±0.007 0.858¬±0.007 0.796¬±0.004 0.93¬±0.013 0.719¬±0.005 0.671¬±0.016\\nw/o numerical 0.859¬±0.003 0.872¬±0.005 0.856¬±0.012 0.889¬±0.023 0.823¬±0.021 0.716¬±0.007\\nw/o categorical 0.792¬±0.003 0.814¬±0.001 0.791¬±0.010 0.840¬±0.014 0.738¬±0.021 0.582¬±0.005\\ndes + tweets 0.759¬±0.007 0.773¬±0.009 0.789¬±0.022 0.758¬±0.034 0.761¬±0.045 0.519¬±0.014\\ncat + num 0.817¬±0.001 0.855¬±0.001 0.749¬±0.001 0.996¬±0.001 0.607¬±0.002 0.668¬±0.002\\nVIII. A BLATION STUDY\\nTo demonstrate our model‚Äôs effectiveness and integrity we\\nwill perform an ablation study on the basic ideas: the user‚Äôs\\nfeatures used for the training, the Gate operation, and the skip-\\nconnection operation.\\nTo prove that using multi-modal information is vital to our\\nmodel performance we will train the architecture that produces\\nthe best results with reduced features. We will reduce one\\nfeature at a time and use combinations of the features for the\\ntraining. We present the results in Table 6.\\nWe see that training with reduced features may achieve\\nhigher metrics in some cases. Notably, training without de-\\nscriptions has a higher F1-score than the original model but has\\na lower precision. Also, training without tweets has a higher\\nrecall value. Training without numerical properties has a\\nhigher precision and specificity but a lower MCC than training\\nwithout description. Training with only the categorical and\\nnumerical properties has the highest recall. Therefore, training\\nwith combinations of features does not achieve as high metrics\\nas training with all the features in each case, meaning that all\\nfeatures contribute to the model‚Äôs performance. These remarks\\nare important to consider for future research in ensuring the\\ndataset‚Äôs quality, but training the model with all the featuresprovided makes it more adaptable to other datasets. For further\\nunderstanding we will train the model using only one feature\\nat a time, to investigate their importance separately. We present\\nthe results in Table 7.\\nObviously, the model trained with all the features has\\nthe best performance. From the results, we deduce that the\\ncategorical property is the feature that contributes the most\\nto the model‚Äôs sufficient accuracy. This ablation study proves\\nthat all features are advantageous for training our model to\\nperform well in the task of bot detection. However, they do\\nnot contribute equally, and more studies to enhance the quality\\nof the datasets could benefit future studies of bot detection.\\nNext, we compare the architecture that results from the\\narchitecture search with a Gate operation and without a Gate\\noperation. The findings of this ablation study are depicted\\nin Table 8. We see that the architecture without the gate\\nhas a reduced accuracy by 0.5% compared to the model‚Äôs\\nand a reduced F1-score by 0.46%. The gating mechanism\\ndynamically consolidates information from all propagation\\nsteps, effectively regulating the smoothness of various nodes.\\nWithout it, the T operations take as input only the last output\\nof the P steps. This is the reason the model underperforms\\nwithout the Gate operation in the P functions, as it may suffer11\\nTABLE VII\\nTRAINING MODEL WITH ONLY ONE FEATURE . VALUES ARE REPORTED AS MEAN ¬±STANDARD DEVIATION . FIVE RUNS OF RESULTS ARE AVERAGED .\\nTHE BEST OUTCOMES FOR EACH EVALUATION METRIC ARE IN BOLD .\\nModel Accuracy F1-score Precision Recall Specificity MCC\\nOurs 0.857¬±0.004 0.871¬±0.003 0.849¬±0.008 0.895¬±0.007 0.812¬±0.013 0.712¬±0.007\\nonly description 0.699¬±0.007 0.74¬±0.008 0.695¬±0.015 0.793¬±0.033 0.589¬±0.046 0.392¬±0.014\\nonly tweets 0.585¬±0.011 0.643¬±0.017 0.602¬±0.008 0.691¬±0.037 0.461¬±0.033 0.157¬±0.022\\nonly numerical 0.679¬±0.02 0.758¬±0.013 0.641¬±0.018 0.929¬±0.034 0.385¬±0.063 0.383¬±0.039\\nonly categorical 0.817¬±0.001 0.853¬±0.001 0.747¬±0.001 1.000¬±0.001 0.6¬±0.001 0.667¬±0.001\\nTABLE VIII\\nABLATION STUDY ON GATE OPERATION . VALUES ARE REPORTED AS MEAN ¬±STANDARD DEVIATION . FIVE RUNS OF RESULTS ARE AVERAGED . THE\\nBEST OUTCOMES FOR EACH EVALUATION METRIC ARE IN BOLD .\\nModel Accuracy F1-score Precision Recall Specificity MCC\\nWith Gate 0.857¬±0.004 0.871¬±0.003 0.849¬±0.008 0.895¬±0.007 0.812¬±0.013 0.712¬±0.007\\nWithout Gate 0.853¬±0.003 0.867¬±0.004 0.845¬±0.010 0.891¬±0.016 0.808¬±0.018 0.704¬±0.007\\nTABLE IX\\nABLATION STUDY ON SKIP -CONNECTION OPERATION . VALUES ARE REPORTED AS MEAN ¬±STANDARD DEVIATION . FIVE RUNS OF RESULTS ARE\\nAVERAGED . THE BEST OUTCOMES FOR EACH EVALUATION METRIC ARE IN BOLD .\\nModel Accuracy F1-score Precision Recall Specificity MCC\\nWith skip 0.857¬±0.004 0.871¬±0.003 0.849¬±0.008 0.895¬±0.007 0.812¬±0.013 0.712¬±0.007\\nWithout skip 0.849¬±0.009 0.860¬±0.01 0.857¬±0.010 0.863¬±0.026 0.831¬±0.017 0.695¬±0.018\\nfrom over-smoothing. The architectures that are examined dur-\\ning this search have more T steps and shallower propagation\\nprocesses, failing to obtain information from nodes during\\nmessage passing as successfully as the original model. This\\nablation study proves the importance of the Gate operation in\\nthe P functions during our architecture search.\\nFinally, we compare the architecture that results from the ar-\\nchitecture search with a skip-connection operation and without\\na skip-connection operation. The findings of this ablation study\\nare depicted in Table 9. We see that the architecture without\\nthe gate has a reduced accuracy by 0.93% compared to the\\nmodel‚Äôs and a reduced F1-score by 1.2%. Without the skip-\\nconnection operation, the input of the T steps is only the output\\nof the last step. This may lead to the degradation of the model\\nas the transformation functions can increase. The processing of\\nthe messages from nodes is not as effective and the accuracy\\ndeclines. This ablation study proves the importance of the skip-\\nconnection operation in the T functions during our architecture\\nsearch.\\nIX. D ISCUSSION\\nA. Implications\\nThe proliferation of social media bots has prompted con-\\ncerns regarding user safety and their broader societal impact.\\nBot detection, a focal point of contemporary studies, is not\\nonly explored through the lens of machine learning but also\\ndelves into the realms of social science. Various methodologies\\nhave been employed, encompassing supervised or unsuper-\\nvised learning or a hybrid of both. A relatively recent and\\ninnovative approach involves Graph Neural Network (GNN)-\\nbased architectures, integrating diverse user features and inter-\\nactions to construct a comprehensive graph representation. In\\nour work, we formulate a heterogeneous graph that captures\\nthe following relationships between users, incorporating nodeswith information on user profiles, tweets, and interests. This\\nnovel contribution enhances existing bot detection research by\\ndemonstrating the efficacy of integrating and analyzing user\\nrelationships.\\nAs technology advances, the adaptive nature of bots poses\\nan ongoing challenge for detection models, rendering many\\nstate-of-the-art architectures ineffective against newer datasets.\\nThe pressing need for adaptable models underscores the im-\\nportance of overcoming the limitations associated with fixed\\narchitectures. Neural Architecture Search (NAS) models prove\\nto be a promising solution, demonstrating their potential to\\nenhance model efficiency in real-world tasks by automati-\\ncally searching through various architectures. Historically, the\\nadoption of NAS techniques for bot detection is limited, so\\nwe propose the implementation of an adapted DFG-NAS. By\\nintegrating DFG-NAS and tailoring it to Relational Graph\\nConvolutional Networks, we explore optimal permutations of\\nPropagation and Transformation steps in the message-passing\\nprotocol of the RGCN layers. Our investigation showcases\\nsuperior performances of the top architectures compared to\\nstate-of-the-art models. Our work is one of the starting points\\nin implementing architecture search models on bot detection.\\nOur research findings encourage further exploration into how\\nNAS models can automatically construct more effective ar-\\nchitectures, resulting in a future restraint of the existence of\\nbots.\\nB. Applicability of our Approach to Different Types of Social\\nInteraction\\nIn this section, we examine the applicability of our intro-\\nduced approach to other types of social interaction besides\\nsocial media.\\n‚Ä¢Online Gaming: Bots impersonate human players to\\nmanipulate game outcomes. Bots are capable of play-12\\ning without breaks. Therefore, they are able to gather\\nresources, items, and so on very quickly which help them\\ngo to the next stage of gaming [68]. Thus, people end\\nplaying with bots; so, it is impossible to win them. This\\nfact entails serious issues, i.e., unfair gaming. Therefore,\\nthe early detection of bots in gaming is crucial, in order\\nto ensure fair play in competitive and multiplayer games.\\nOur method could be adapted by using response times,\\nmovement patterns, and time-series data as input features.\\n‚Ä¢Customer Reviews and Rating Platforms: Bots are\\noften used for creating fake reviews and inflating rating in\\nreview platforms, including Amazon and Yelp. The main\\naim of bots is to promote specific products, restaurants,\\nand so on. Our approach could be easily adapted to this\\ncase, since textual, timing, and user behaviour features\\nwill be used.\\n‚Ä¢Digital Voting and Polling Systems: Bots are used\\nto alter the results of Internet Polling [69]. Therefore,\\nearly recognition of bots in voting is crucial, so as to\\nensure reliable outcomes. Our method can be adapted by\\nintegrating features, such as IP addresses, voting patterns,\\nand timing.\\n‚Ä¢Email and Messaging Systems: Bots are responsible for\\nspam and phishing. Early detection of bots is crucial for\\nenhancing security. Features, including email headers, IP\\naddresses, etc., must be incorporated in our study.\\nC. Limitations\\nOur study comes with some limitations. Firstly, we con-\\nducted our experiments only on one dataset, which does not\\nensure generalizabilty of our proposed approach. Therefore, in\\nthe future, we aim to test our method on TwiBot-22 dataset\\n[70]. Secondly, our method is based on the collection of\\nlabelled data. Obtaining labelled data is a difficult task. For\\nthis reason, unsupervised and self-supervised learning algo-\\nrithms have been developed for addressing the issue of labels‚Äô\\nscarcity. Applying unsupervised and self-supervised learning\\nin conjunction with our approach is one of our future plans.\\nThirdly, we did not tune the hyperparameters due to limited\\naccess to GPU resources. Hyperparameter tuning ensures that\\noptimal performance is obtained. Finally, we represented each\\nuser as a concatenation of features. Concatenation does not\\ncapture the inherent correlation of the different modalities.\\nIn the future, we aim to use multimodal fusion methods for\\nconstructing each user‚Äôs representation [71]‚Äì[73].\\nX. C ONCLUSIONS AND FUTURE WORK\\nAs social media continues to play a pivotal role in shaping\\npublic opinion and discourse, the development of effective\\nand adaptive bot detection methods becomes increasingly\\ncrucial for maintaining the integrity and trustworthiness of\\nonline information. In this study, we introduced a novel model\\nfor identifying bots, integrating GNNs and NAS algorithms,\\ndemonstrating significant performance gains. The integration\\nof Graph Neural Architecture Search empowered us to dy-\\nnamically determine optimal combinations of propagation\\nand transformation operations in the graph neural networkarchitecture. This adaptive architecture effectively addresses\\nthe constraints imposed by fixed structures, introducing a\\nlevel of flexibility essential for improving the performance\\non the bot detection task. From the experiment results we\\nconclude that the five architectures with the highest validation\\naccuracy, during the architecture search, are quite efficient in\\nour task and compete with other models. Meanwhile, the one\\nwith the highest accuracy achieves a test accuracy of 85,68%,\\nsurpassing other state-of-the-art models for bot detection. The\\noutcomes of the experiment present promising prospects for\\nintegrating more Neural Architecture Search (NAS) methods\\ninto the domain of bot detection in various social media\\nplatforms.\\nThe exploration of dynamic graph adaptations stands as a\\ncrucial avenue for future research in the task of bot identi-\\nfication in social media platform X. The dynamic nature of\\nsocial networks, characterized by the continuous incorporation\\nof new users, necessitates the development of mechanisms to\\nseamlessly integrate these additions into the evolving graph\\nstructure. Investigating methods for real-time graph updates\\nand exploring how the model adapts to the inclusion of new\\nusers will enhance the system‚Äôs agility in capturing emerging\\nbot behaviors within the dynamic social landscape. Further-\\nmore, the prospect of transferring our model to other social\\nmedia platforms emerges as a key future avenue. Extending the\\napplicability of our approach beyond X involves understanding\\nthe unique dynamics and characteristics of different platforms.\\nFuture work should focus on developing a transferable frame-\\nwork capable of recognizing bot-like behaviors across diverse\\nsocial networks. By addressing the nuances and variations\\nin user interactions and content features, we can contribute\\nto the development of a versatile bot detection system with\\nbroader applications in the ever-expanding realm of social\\nmedia platforms.\\nREFERENCES\\n[1] Loukas Ilias and Dimitris Askounis. Multitask learning for recognizing\\nstress and depression in social media. Online Social Networks and\\nMedia , 37-38:100270, 2023.\\n[2] Loukas Ilias, Spiros Mouzakitis, and Dimitris Askounis. Calibration\\nof transformer-based models for identifying stress and depression in\\nsocial media. IEEE Transactions on Computational Social Systems ,\\n11(2):1979‚Äì1990, 2024.\\n[3] Marios Kerasiotis, Loukas Ilias, and Dimitris Askounis. Depression\\ndetection in social media posts using transformer-based models and\\nauxiliary features. Social Network Analysis and Mining , 14:196, 2024.\\n[4] Izzat Alsmadi and Michael J. O‚ÄôBrien. How many bots in russian troll\\ntweets? Information Processing & Management , 57(6):102303, 2020.\\n[5] Joshua Uyheng, J.D. Moffitt, and Kathleen M. Carley. The language\\nand targets of online trolling: A psycholinguistic approach for social\\ncybersecurity. Information Processing & Management , 59(5):103012,\\n2022.\\n[6] Yaozeng Zhang, Jing Ma, and Fanshu Fang. How social bots can\\ninfluence public opinion more effectively: Right connection strategy.\\nPhysica A: Statistical Mechanics and its Applications , 633:129386,\\n2024.\\n[7] Yi Xu, Deru Zhou, and Wei Wang. Being my own gatekeeper, how i\\ntell the fake and the real ‚Äì fake news perception between typologies and\\nsources. Information Processing & Management , 60(2):103228, 2023.\\n[8] Yantian Mi and Oberiri Destiny Apuke. How does social media\\nknowledge help in combating fake news? testing a structural equation\\nmodel. Thinking Skills and Creativity , page 101492, 2024.13\\n[9] Kyumin Lee, Brian Eoff, and James Caverlee. Seven months with the\\ndevils: A long-term study of content polluters on twitter. Proceedings of\\nthe International AAAI Conference on Web and Social Media , 5(1):185‚Äì\\n192, Aug. 2021.\\n[10] Stefano Cresci, Roberto Di Pietro, Marinella Petrocchi, Angelo Spog-\\nnardi, and Maurizio Tesconi. Social fingerprinting: detection of spambot\\ngroups through dna-inspired behavioral modeling. IEEE Transactions on\\nDependable and Secure Computing , page 1‚Äì1, 2017.\\n[11] Mohammad Alauthman, Nauman Aslam, Mouhammd Al-kasassbeh,\\nSuleman Khan, Ahmad Al-Qerem, and Kim-Kwang Raymond Choo.\\nAn efficient reinforcement learning-based botnet detection approach.\\nJournal of Network and Computer Applications , 150:102479, 2020.\\n[12] Shangbin Feng, Herun Wan, Ningnan Wang, and Minnan Luo. Botrgcn:\\nTwitter bot detection with relational graph convolutional networks.\\nInProceedings of the 2021 IEEE/ACM International Conference on\\nAdvances in Social Networks Analysis and Mining , ASONAM ‚Äô21, page\\n236‚Äì239. ACM, 11 2021.\\n[13] Wentao Zhang, Zheyu Lin, Yu Shen, Yang Li, Zhi Yang, and Bin\\nCui. Deep and flexible graph neural architecture search. In Kamalika\\nChaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and\\nSivan Sabato, editors, Proceedings of the 39th International Conference\\non Machine Learning , volume 162 of Proceedings of Machine Learning\\nResearch , pages 26362‚Äì26374. PMLR, 17‚Äì23 Jul 2022.\\n[14] Shangbin Feng, Herun Wan, Ningnan Wang, Jundong Li, and Minnan\\nLuo. Twibot-20: A comprehensive twitter bot detection benchmark. In\\nProceedings of the 30th ACM International Conference on Information\\n& Knowledge Management , CIKM ‚Äô21, page 4485‚Äì4494, New York,\\nNY , USA, 2021. Association for Computing Machinery.\\n[15] Alessandro Bessi and Emilio Ferrara. Social bots distort the 2016 u.s.\\npresidential election online discussion. First Monday , 21, 11 2016.\\n[16] Emilio Ferrara. What types of covid-19 conspiracies are populated by\\ntwitter bots? First Monday , 5 2020.\\n[17] Christian Scheibenzuber, Laurentiu-Marian Neagu, Stefan Ruseti,\\nBenedikt Artmann, Carolin Bartsch, Montgomery Kubik, Mihai Dascalu,\\nStefan Trausan-Matu, and Nicolae Nistor. Dialog in the echo chamber:\\nFake news framing predicts emotion, argumentation and dialogic social\\nknowledge building in subsequent online discussions. Computers in\\nHuman Behavior , 140:107587, 2023.\\n[18] Tanjim Mahmud, Michal Ptaszynski, Juuso Eronen, and Fumito Masui.\\nCyberbullying detection for low-resource languages and dialects: Re-\\nview of the state of the art. Information Processing & Management ,\\n60(5):103454, 2023.\\n[19] Yingguang Yang, Renyu Yang, Yangyang Li, Kai Cui, Zhiqin Yang,\\nYue Wang, Jie Xu, and Haiyong Xie. Rosgas: Adaptive social bot\\ndetection with reinforced self-supervised gnn architecture search. ACM\\nTransactions on the Web , 17(3):1‚Äì31, May 2023.\\n[20] Sneha Kudugunta and Emilio Ferrara. Deep neural networks for bot\\ndetection. Information Sciences , 467:312‚Äì322, October 2018.\\n[21] Feng Wei and Uyen Trang Nguyen. Twitter bot detection using bidirec-\\ntional long short-term memory neural networks and word embeddings.\\nIn2019 First IEEE International Conference on Trust, Privacy and\\nSecurity in Intelligent Systems and Applications (TPS-ISA) , pages 101‚Äì\\n109, 2019.\\n[22] Shaofei Cai, Liang Li, Jincan Deng, Beichen Zhang, Zheng-Jun Zha,\\nLi Su, and Qingming Huang. Rethinking graph neural architecture\\nsearch from message-passing. In 2021 IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages 6653‚Äì6662,\\n2021.\\n[23] Clayton Allen Davis, Onur Varol, Emilio Ferrara, Alessandro Flammini,\\nand Filippo Menczer. Botornot: A system to evaluate social bots. In\\nProceedings of the 25th International Conference Companion on World\\nWide Web , WWW ‚Äô16 Companion, page 273‚Äì274, Republic and Canton\\nof Geneva, CHE, 2016. International World Wide Web Conferences\\nSteering Committee.\\n[24] Kai-Cheng Yang, Emilio Ferrara, and Filippo Menczer. Botometer 101:\\nSocial bot practicum for computational social scientists. Journal of\\nComputational Social Science , 5(2):1511‚Äì1528, 2022.\\n[25] Fawaz Khaled Alarfaj, Hassaan Ahmad, Hikmat Ullah Khan, Abdul-\\nlah Mohammaed Alomair, Naif Almusallam, and Muzamil Ahmed.\\nTwitter bot detection using diverse content features and applying ma-\\nchine learning algorithms. Sustainability , 15(8), 2023.\\n[26] Eiman Alothali, Motamen Salih, Kadhim Hayawi, and Hany Alashwal.\\nBot-mgat: A transfer learning model based on a multi-view graph\\nattention network to detect social bots. Applied Sciences , 12(16), 2022.\\n[27] Shangbin Feng, Herun Wan, Ningnan Wang, Jundong Li, and Minnan\\nLuo. Satar: A self-supervised approach to twitter account representation\\nlearning and its application in bot detection. In Proceedings of the30th ACM International Conference on Information; Knowledge Man-\\nagement , CIKM ‚Äô21, page 3808‚Äì3817. ACM, 10 2021.\\n[28] Loukas Ilias and Ioanna Roussaki. Detecting malicious activity in twitter\\nusing deep learning techniques. Applied Soft Computing , 107:107360,\\n2021.\\n[29] Loukas Ilias, Ioannis Michail Kazelidis, and Dimitris Askounis. Mul-\\ntimodal detection of bots on x (twitter) using transformers. IEEE\\nTransactions on Information Forensics and Security , 19:7320‚Äì7334,\\n2024.\\n[30] Feng Wei and Uyen Trang Nguyen. Twitter bot detection using\\nneural networks and linguistic embeddings. IEEE Open Journal of the\\nComputer Society , 4:218‚Äì230, 2023.\\n[31] Parisa Bazmi, Masoud Asadpour, and Azadeh Shakery. Multi-view co-\\nattention network for fake news detection by modeling topic-specific\\nuser and news source credibility. Information Processing & Manage-\\nment , 60(1):103146, 2023.\\n[32] Alexander Shevtsov, Despoina Antonakaki, Ioannis Lamprou, Polyvios\\nPratikakis, and Sotiris Ioannidis. Botartist: Twitter bot detection ma-\\nchine learning model based on twitter suspension. arXiv preprint\\narXiv:2306.00037 , 2023.\\n[33] K Sujith, Shreya Chowdhury, Arsh Goyal, Anand Vardhan Hegde,\\nand Ramamoorthy Srinath. Twitter bot detection and ranking using\\nsupervised machine learning models. In 2022 International Conference\\non Data Science, Agents & Artificial Intelligence (ICDSAAI) , volume 01,\\npages 1‚Äì6, 2022.\\n[34] Yuhan Liu, Zhaoxuan Tan, Heng Wang, Shangbin Feng, Qinghua Zheng,\\nand Minnan Luo. Botmoe: Twitter bot detection with community-\\naware mixtures of modal-specific experts. In Proceedings of the 46th\\nInternational ACM SIGIR Conference on Research and Development in\\nInformation Retrieval , SIGIR ‚Äô23, page 485‚Äì495, New York, NY , USA,\\n2023. Association for Computing Machinery.\\n[35] Naman Saxena, Adwitiya Sinha, Tanishk Bansal, and Ankita Wadhwa. A\\nstatistical approach for reducing misinformation propagation on twitter\\nsocial media. Information Processing & Management , 60(4):103360,\\n2023.\\n[36] Ilias Dimitriadis, George Dialektakis, and Athena Vakali. Caleb: A\\nconditional adversarial learning framework to enhance bot detection.\\nData & Knowledge Engineering , 149:102245, 2024.\\n[37] Kai-Cheng Yang, Onur Varol, Pik-Mai Hui, and Filippo Menczer.\\nScalable and generalizable social bot detection through data selec-\\ntion. Proceedings of the AAAI Conference on Artificial Intelligence ,\\n34(01):1096‚Äì1103, April 2020.\\n[38] Vicente Quezada, Fabian Astudillo-Salinas, Luis Tello-Oquendo, and\\nPaul Bernal. Real-time bot infection detection system using dns\\nfingerprinting and machine-learning. Computer Networks , 228:109725,\\n2023.\\n[39] Zachary Miller, Brian Dickinson, William Deitrick, Wei Hu, and\\nAlex Hai Wang. Twitter spammer detection using data stream clustering.\\nInformation Sciences , 260:64‚Äì73, 2014.\\n[40] Nikan Chavoshi, Hossein Hamooni, and Abdullah Mueen. Debot:\\nTwitter bot detection via warped correlation. In 2016 IEEE 16th\\nInternational Conference on Data Mining (ICDM) , pages 817‚Äì822,\\n2016.\\n[41] Amanda Minnich, Nikan Chavoshi, Danai Koutra, and Abdullah Mueen.\\nBotwalk: Efficient adaptive exploration of twitter bot networks. In 2017\\nIEEE/ACM International Conference on Advances in Social Networks\\nAnalysis and Mining (ASONAM) , pages 467‚Äì474, 2017.\\n[42] L. Mannocci, S. Cresci, A. Monreale, A. Vakali, and M. Tesconi.\\nMulbot: Unsupervised bot detection based on multivariate time series.\\nIn2022 IEEE International Conference on Big Data (Big Data) , pages\\n1485‚Äì1494, Los Alamitos, CA, USA, dec 2022. IEEE Computer Society.\\n[43] Jeremy Wu, Eric Teng, and Ziyue Cao. Twitter bot detection through\\nunsupervised machine learning. In 2022 IEEE International Conference\\non Big Data (Big Data) , pages 5833‚Äì5839, 2022.\\n[44] Darshika Koggalahewa, Yue Xu, and Ernest Foo. An unsupervised\\nmethod for social network spammer detection based on user information\\ninterests. Journal of Big Data , 9(1):Article number: 7, January 2022.\\n[45] Daniele A. G. Lopes, Marcelo A. Marotta, Marcelo Ladeira, and Jo Àúao\\nJ. C. Gondim. Botnet detection based on network flow analysis using\\ninverse statistics. In 2022 17th Iberian Conference on Information\\nSystems and Technologies (CISTI) , pages 1‚Äì6, 2022.\\n[46] Seyed Ali Alhosseini, Raad Bin Tareaf, Pejman Najafi, and Christoph\\nMeinel. Detect me if you can: Spam bot detection using inductive\\nrepresentation learning. In Companion Proceedings of The 2019 World\\nWide Web Conference , WWW ‚Äô19, page 148‚Äì153, New York, NY , USA,\\n2019. Association for Computing Machinery.14\\n[47] Shangbin Feng, Zhaoxuan Tan, Rui Li, and Minnan Luo. Heterogeneity-\\naware twitter bot detection with relational graph transformers. In AAAI\\nConference on Artificial Intelligence , 2021.\\n[48] Ema Ku Àásen and Mark Strembeck. You talkin‚Äô to me? exploring\\nhuman/bot communication patterns during riot events. Information\\nProcessing & Management , 57(1):102126, 2020.\\n[49] Thi Bui and Katerina Potika. Twitter bot detection using social network\\nanalysis. In 2022 Fourth International Conference on Transdisciplinary\\nAI (TransAI) , pages 87‚Äì88, 2022.\\n[50] Ali Dehghan, Krzysztof Siuta, Andrzej Skorupka, Anuja Dubey, An-\\nthony Betlen, Derek Miller, Wei Xu, Bogdan Kami ¬¥nski, and Pawe≈Ç\\nPra≈Çat. Detecting bots in social-networks using node and structural\\nembeddings. Journal of Big Data , 10(1):119, 2023.\\n[51] Phu Pham, Loan T.T. Nguyen, Bay V o, and Unil Yun. Bot2vec: A\\ngeneral approach of intra-community oriented representation learning for\\nbot detection in different types of social networks. Information Systems ,\\n103:101771, 2022.\\n[52] Shirin Noekhah, Naomie binti Salim, and Nor Hawaniah Zakaria.\\nOpinion spam detection: Using multi-iterative graph-based model. In-\\nformation Processing & Management , 57(1):102140, 2020.\\n[53] Sen Ye, Zhaoxuan Tan, Zhenyu Lei, Ruijie He, Hongrui Wang, Qinghua\\nZheng, and Minnan Luo. Hofa: Twitter bot detection with homophily-\\noriented augmentation and frequency adaptive attention. arXiv preprint\\narXiv:2306.12870 , 2023.\\n[54] Nour El-Mawass, Paul Honeine, and Laurent Vercouter. Similcatch:\\nEnhanced social spammers detection on twitter using markov random\\nfields. Information Processing & Management , 57(6):102317, 2020.\\n[55] Kaixiong Zhou, Xiao Huang, Qingquan Song, Rui Chen, and Xia Hu.\\nAuto-gnn: Neural architecture search of graph neural networks. Frontiers\\nin Big Data , 5, 2022.\\n[56] Yang Gao, Hong Yang, Peng Zhang, Chuan Zhou, and Yue Hu. Graph\\nneural architecture search. In Proceedings of the Twenty-Ninth Interna-\\ntional Joint Conference on Artificial Intelligence , IJCAI‚Äô20, 2021.\\n[57] Huan Zhao, Lanning Wei, and Quanming Yao. Simplifying architecture\\nsearch for graph neural network. ArXiv , abs/2008.11652, 2020.\\n[58] Matheus Nunes and Gisele L. Pappa. Intelligent Systems: 9th Brazilian\\nConference, BRACIS 2020, Rio Grande, Brazil, October 20‚Äì23, 2020,\\nProceedings, Part I . Springer International Publishing, 2020.\\n[59] YuFei Li, Jia Wu, and TianJin Deng. Meta-gnas: Meta-reinforcement\\nlearning for graph neural architecture search. Engineering Applications\\nof Artificial Intelligence , 123:106300, 2023.\\n[60] Wei Peng, Xiaopeng Hong, Haoyu Chen, and Guoying Zhao. Learning\\ngraph convolutional network for skeleton-based human action recog-\\nnition by neural searching. Proceedings of the AAAI Conference on\\nArtificial Intelligence , 34(03):2669‚Äì2676, 4 2020.\\n[61] S. Jiang and P. Balaprakash. Graph neural network architecture search\\nfor molecular property prediction. In 2020 IEEE International Confer-\\nence on Big Data (Big Data) , pages 1346‚Äì1353, Los Alamitos, CA,\\nUSA, dec 2020. IEEE Computer Society.\\n[62] Yameng Peng, Andy Song, Vic Ciesielski, Haytham Fayek, and Xiaojun\\nChang. Fast evolutionary neural architecture search by contrastive\\npredictor with linear regions. In Proceedings of the Genetic and\\nEvolutionary Computation Conference , GECCO ‚Äô23, page 1257‚Äì1266,\\nNew York, NY , USA, 2023. Association for Computing Machinery.\\n[63] Ronghua Shang, Songling Zhu, Hangcheng Liu, Teng Ma, Weitong\\nZhang, Jie Feng, Licheng Jiao, and Rustam Stolkin. Evolutionary\\narchitecture search via adaptive parameter control and gene potential\\ncontribution. Swarm and Evolutionary Computation , 82:101354, 2023.\\n[64] Vasco Lopes, Miguel Santos, Bruno Degardin, and Lu ¬¥ƒ±s A. Alexandre.\\nGuided evolutionary neural architecture search with efficient perfor-\\nmance estimation. Neurocomputing , page 127509, 2024.\\n[65] Huan ZHAO, Quanming YAO, and Weiwei TU. Search to aggregate\\nneighborhood for graph neural network. In 2021 IEEE 37th International\\nConference on Data Engineering (ICDE) , pages 552‚Äì563, 2021.\\n[66] Yanxi Li, Zean Wen, Yunhe Wang, and Chang Xu. One-shot graph\\nneural architecture search with dynamic search space. Proceedings of\\nthe AAAI Conference on Artificial Intelligence , 35(10):8510‚Äì8517, May\\n2021.\\n[67] Jiakun Zhao, Ruifeng Zhang, Zheng Zhou, Si Chen, Ju Jin, and Qingfang\\nLiu. A neural architecture search method based on gradient descent for\\nremaining useful life estimation. Neurocomputing , 438:184‚Äì194, 2021.\\n[68] Ah Reum Kang, Jiyoung Woo, Juyong Park, and Huy Kang Kim.\\nOnline game bot detection based on party-play log analysis. Computers\\n& Mathematics with Applications , 65(9):1384‚Äì1395, 2013. Advanced\\nInformation Security.[69] Shahriar Mohammadi and Hossein Abbasimehr. A high level security\\nmechanism for internet polls. In 2010 2nd International Conference on\\nSignal Processing Systems , volume 3, pages V3‚Äì101‚ÄìV3‚Äì105, 2010.\\n[70] Shangbin Feng, Zhaoxuan Tan, Herun Wan, Ningnan Wang, Zilong\\nChen, Binchi Zhang, Qinghua Zheng, Wenqian Zhang, Zhenyu Lei,\\nShujie Yang, Xinshun Feng, Qingyue Zhang, Hongrui Wang, Yuhan\\nLiu, Yuyang Bai, Heng Wang, Zijian Cai, Yanbo Wang, Lijing Zheng,\\nZihan Ma, Jundong Li, and Minnan Luo. Twibot-22: towards graph-\\nbased twitter bot detection. In Proceedings of the 36th International\\nConference on Neural Information Processing Systems , NIPS ‚Äô22, Red\\nHook, NY , USA, 2024. Curran Associates Inc.\\n[71] Loukas Ilias, Dimitris Askounis, and John Psarras. A multimodal\\napproach for dementia detection from spontaneous speech with tensor\\nfusion layer. In 2022 IEEE-EMBS International Conference on Biomed-\\nical and Health Informatics (BHI) , pages 1‚Äì5, 2022.\\n[72] Loukas Ilias and Dimitris Askounis. Context-aware attention layers\\ncoupled with optimal transport domain adaptation and multimodal\\nfusion methods for recognizing dementia from spontaneous speech.\\nKnowledge-Based Systems , 277:110834, 2023.\\n[73] Michail Chatzianastasis, Loukas Ilias, Dimitris Askounis, and Michalis\\nVazirgiannis. Neural architecture search with multimodal fusion methods\\nfor diagnosing dementia. In ICASSP 2023 - 2023 IEEE International\\nConference on Acoustics, Speech and Signal Processing (ICASSP) , pages\\n1‚Äì5, 2023.',\n",
       " 'Characterizing the Fragmentation of the Social\\nMedia Ecosystem\\nEdoardo Di Martino1*, Alessandro Galeazzi2, Michele Starnini3, 4,\\nWalter Quattrociocchi5, Matteo Cinelli5\\n1*Department of Social Sciences and Economics, Sapienza University of\\nRome, P.le Aldo Moro, 5, 00185, Rome, Italy.\\n2Department of Mathematics, University of Padova, Via Trieste, 63,\\n35121 , Padova, Italy.\\n3Department of Engineering, Universitat Pompeu Fabra, 08018,\\nBarcelona, Spain.\\n4CENTAI,10138, Torino, Italy.\\n5Department of Computer Science, Sapienza University of Rome, Viale\\nRegina Elena, 295, 00161, Rome, Italy.\\n*Corresponding author(s). E-mail(s): edoardo.dimartino@uniroma1.it;\\nContributing authors: alessandro.galeazzi@unipd.it;\\nmichele.starnini@upf.edu; walter.quattrociocchi@uniroma1.it;\\nmatteo.cinelli@uniroma1.it;\\nAbstract\\nThe entertainment-driven dynamics of social media platforms encourage users\\nto engage with like-minded individuals and consume content aligned with their\\nbeliefs. These dynamics may amplify polarization by reinforcing shared perspec-\\ntives and reducing exposure to diverse viewpoints. Simultaneously, users migrate\\nfrom one platform to another, either forced by moderation policies, such as\\nde-platforming, or spontaneously seeking environments more aligned with their\\npreferences. These migrations foster the specialization and differentiation of the\\nsocial media ecosystem, with platforms increasingly organized around specific\\nuser communities and shared content preferences. This shift marks an evolution\\nfrom echo chambers enclosed within platforms to ‚Äúecho platforms‚Äù, i.e., entire\\nplatforms functioning as ideologically homogeneous niches. This study introduces\\nan operational framework to systematically analyze these dynamics, by exam-\\nining three key dimensions: platform centrality (central vs. peripheral), news\\nconsumption (reliable vs questionable), and user base composition (uniform vs\\n1arXiv:2411.16826v1  [cs.CY]  25 Nov 2024diverse). To this aim, we leverage a dataset of 126M URLs posted by nearly 6M\\nusers on nine social media platforms, namely Facebook, Reddit, Twitter (now\\nX), YouTube, BitChute, Gab, Parler, Scored, and Voat. We find a clear sep-\\naration between mainstream and alt-tech platforms, with the second category\\nbeing characterized by a peripheral role in the social media ecosystem, a greater\\nprevalence of unreliable content, and a heightened ideological uniformity. These\\nfindings outline the main dimensions defining the fragmentation and polarization\\nof the social media ecosystem.\\nIntroduction\\nSocial media are now a major source of news and opinions for many, reshaping how\\npeople access information and engage in public discussions. This shift comes as trust in\\ntraditional media continues to decline [1, 2]. This transformation is driven by platforms\\nprioritizing entertainment and user engagement over informational accuracy [3‚Äì6].\\nSuch dynamics, combined with individual preferences, encourage users to join like-\\nminded peers and consume content aligning with their pre-existing beliefs, reinforcing\\nshared perspectives and potentially limiting exposure to diverse viewpoints [5, 7, 8].\\nThese behaviors have been closely linked to heightened exposure to hate speech [9‚Äì12],\\npolitical polarization [13‚Äì15], and the formation of echo chambers [5, 16]. Such clusters\\namplify ideological homogeneity, further reduce exposure to opposing viewpoints, and\\nshape opinions through repeated interactions [17, 18]. Over time, this clustering may\\ndrive groups toward more extreme positions [19, 20], influencing user behavior across\\nplatforms [5, 21‚Äì23]. Beyond digital interactions, this polarization affects offline social\\ncohesion and public discourse, highlighting its broader societal consequences.\\nTo address these challenges, platforms have increasingly implemented moderation\\npolicies aimed at managing tight-knit, ‚Äúproblematic‚Äù communities [24]. However, mod-\\neration policies, such as de-platforming, often push users toward minimally regulated\\nplatforms [16, 25‚Äì28]. At the same time, users may spontaneously leave platforms\\nwhere they find limited alignment with their preferences, seeking alternatives. For\\nexample, many users migrated from Twitter to Mastodon [29] or BlueSky [30, 31]\\nfollowing changes in platform management.\\nWhen users leave mainstream platforms, they often migrate to less popular digital\\nspaces, commonly referred to as ‚Äúalt-tech‚Äù or ‚Äúfringe‚Äù platforms [26, 32, 33]. Examples\\ninclude Gab, Parler, BitChute, and Rumble, which have emerged as counterparts to\\nmainstream services like Twitter, Reddit, YouTube, and Facebook. These platforms\\nfrequently market themselves as champions of free speech, appealing to users who\\nperceive mainstream platforms as ideologically biased or overly regulated. Such migra-\\ntions, whether driven by dissatisfaction with moderation policies or forced exclusion\\nthrough bans, contribute to the fragmentation of the digital public sphere [34, 35].\\nBoth spontaneous and forced migrations foster differentiation and specialization\\nwithin the social media ecosystem, with platforms increasingly organized around spe-\\ncific user communities and shared content preferences [36‚Äì39]. These shifts represent\\na departure from the internet‚Äôs original vision as a globally interconnected network\\n2where information flows freely across borders [40, 41]. Instead, entire platforms are\\nnow defined by ideologically homogeneous communities and aligned content, creating\\nniches where opposing perspectives are rarely encountered [37]. This scenario‚Äîwhere\\nan entire platform is characterized by a like-minded user base with limited exposure\\nto diverse opinions‚Äîlays the foundation for the concept of an ‚Äúecho platform‚Äù, an\\nextension of the echo chamber phenomenon to the level of entire digital ecosystems.\\nIn this paper, we introduce an operational framework to characterize the fragmen-\\ntation of the social media ecosystem, providing a methodological basis for identifying\\nthe role of different platforms. Using a dataset comprising over 126 million URLs col-\\nlected from nine platforms‚Äîfour mainstream (Facebook, Twitter, Reddit, YouTube)\\nand five alt-tech or fringe platforms (BitChute, Gab, Parler, Scored, Voat)‚Äîand nearly\\nsix million unique users, we define three axes to characterize platform roles: (i) central-\\nity (central vs. peripheral), (ii) news consumption (reliable vs. questionable content),\\nand (iii) user base composition (uniform vs. diverse).\\nOur analysis reveals distinct patterns of fragmentation and information consump-\\ntion. Interactions between mainstream and alt-tech platforms are limited, reinforcing\\nthe fragmentation of the social media ecosystem. Alt-tech platforms exhibit signifi-\\ncantly higher levels of ideological homogeneity, with user communities sharing content\\nalmost exclusively aligned with their dominant narratives. Interestingly, user behavior\\non Reddit more closely resembles alt-tech platforms than mainstream ones. Fur-\\nthermore, alt-tech platforms disproportionately amplify questionable content while\\nshowing a notable absence of reliable news sources compared to their mainstream\\ncounterparts.\\nResults\\nWe characterize the social media ecosystem along three key dimensions, providing\\ndetailed insights into the structural and functional distinctions between platforms.\\n‚Ä¢Centrality: Central vs Peripheral Role. Platforms can be classified as either\\ncentral or peripheral based on their position within the broader information ecosys-\\ntem. Some platforms could serve as central nodes in the network connecting different\\nplatforms, due to their high connectivity. Others may be detached from the core,\\noperating as niches where users are less likely to point to other platforms‚Äô content.\\n‚Ä¢News Consumption: Reliable vs. Questionable Sources. The reliability of\\nnews circulating on platforms may vary significantly. Some platforms primarily fea-\\nture content from reliable, well-established news sources that align with journalistic\\nstandards. Others display a higher prevalence of questionable content, including\\nconspiracy theories and unverified narratives.\\n‚Ä¢Political Leaning of Users: Uniform vs Diverse. User base composition can\\ndiffer substantially across platforms. Some platforms host a heterogeneous (broad)\\nuser base, encompassing individuals with diverse political stances and interests.\\nOthers are characterized by a homogeneous (narrow) user base where like-minded\\nindividuals share similar ideas and narratives.\\n3Table 1 : Dataset Statistics and Evaluation Metrics by Platform. For each platform,\\nwe report the following metrics: number of unique users ( N), number of URL links\\n(nu), number of unique domains linked ( nd), PageRank centrality ( PR), fraction of\\nquestionable sources shared ( q), and the variance of the users‚Äô political leaning distri-\\nbution ( œÉ2). The figures for the number of unique users, URLs, and unique domains\\nare provided after the data preprocessing procedures.\\nPlatform N nu ndPR q œÉ2\\nFacebook 300k 10.7M 74k 0.09 0.15 0.16\\nReddit 59k 320k 9k 0.14 0.03 0.04\\nTwitter 5M 103M 114k 0.16 0.16 0.16\\nYouTube 12k 512k 12k 0.11 0.14 0.19\\nBitChute 17k 1.9M 45k 0.14 0.17 0.21\\nGab 15k 453k 7k 0.08 0.65 0.06\\nParler 300k 8.2M 70k 0.11 0.51 0.07\\nScored 45k 883k 12k 0.06 0.77 0.04\\nVoat 27k 215k 10k 0.12 0.36 0.11\\nThe systematic characterization of these three dimensions provides a robust frame-\\nwork for understanding the role of different platforms in shaping the digital public\\nsphere. Moreover, it underscores areas for targeted policy and research interventions\\nto mitigate polarization and misinformation.\\nThe characterization is based on a comprehensive dataset of URLs collected from\\nposts (or video descriptions for YouTube and BitChute) across nine social media\\nplatforms. As shown in Table 1, this dataset includes four ‚Äúmainstream‚Äù platforms\\n(Facebook, Twitter, Reddit, and YouTube) and five ‚Äúalt-tech‚Äù platforms (BitChute,\\nGab, Parler, Scored, and Voat). These URLs, linking to internal and external domains,\\nserve as a proxy for a macro-level analysis of platform behavior (see the Materials and\\nMethods section for further details).\\nPlatform Centrality\\nTo analyze the roles of different platforms within the information ecosystem, we model\\nit as a weighted, directed graph. In this network, nodes represent platforms, and the\\nweight of a directed edge between two nodes corresponds to the number of URL links\\npointing from one platform to another. In other words, an edge from platform ito\\nplatform jexists if an account of platform iposted a URL linking directly to platform\\nj.\\nAs the nodes of the network correspond to the platforms we take into account,\\na full representation of the weighted adjacency matrix (see SI) can provide insights\\ninto how platforms redirect users and share content within the ecosystem. However,\\n4to account for variations in the size of the datasets collected from each platform, we\\ndisplay the observed edge weights rescaled by those generated using a null model that\\npreserves the total number of outgoing and incoming URL links for each platform (see\\nsection Materials and Methods for details).\\n0.98\\n0.79\\n0.580.861.11\\n1.22\\n0.580.762.34\\n0.33\\n2.474.680.19\\n1.33\\n33.681.352.520.39\\n1.330.220.96\\n0.54\\n4.677.59\\n0.971.581.37\\n0.38\\n5.20.04\\n0.43\\n12.697.455.920.32\\n1.76\\n0.070.040.53\\n0.521.63.811.88\\n1.77\\n4.62.211.17\\n1.091.550.430.19\\n1.3\\n0.290.860.1\\n0.12\\n7.351.030.52\\n2.37\\n1.836.190.46\\n0.24\\n19.5NA\\nNANA\\nNANANANA\\nScoredVoatParlerGabBitChuteYouTubeTwitterRedditFacebook\\nFacebookRedditTwitterYouTubeBitChuteGabParlerVoat\\nScored\\nToFrom\\nFig. 1 :Centrality. Rescaled adjacency matrix showing the ratio between the\\nobserved and expected number of URLs pointing from one platform to another. Green\\n(red) cells indicate values greater (smaller) than one. A value of 0 indicate no observed\\nURLs.\\nFig. 1 displays the ratio of observed edge weights to those expected under the null\\nmodel. Ratios greater than one indicate that a platform links to another more fre-\\nquently than expected under random distribution, while ratios less than one indicate\\nless frequent linking. Fig. 1 reveals distinct patterns of inter-platform linking: Twitter\\nconsistently receives more URLs than expected from all other platforms, positioning it\\nas a central node in the information ecosystem. Surprisingly, Facebook receives fewer\\nlinks than expected from other mainstream platforms, except YouTube, likely due to\\nthe prevalence of self-promotional links in video descriptions. Alt-tech platforms gener-\\nally receive fewer links than expected by mainstream ones, and are instead more tightly\\nconnected among themselves. Nevertheless, the strongest link weight with respect to\\nthe null model is observed from Gab to Reddit, suggesting a potential content-sharing\\ndynamic or migration pathway between these two platforms. BitChute is linked much\\n5more than expected by all fringe platforms and slightly less by mainstream ones, while\\nthe opposite is true for YouTube. Finally, Scored receives no incoming links from six of\\nthe nine platforms, likely due to its limited popularity and low exposure, underscoring\\nits peripheral role in the broader ecosystem.\\nTo better quantify the relative importance of each platform within the ecosystem,\\nwe computed their PageRank centrality from the rescaled adjacency matrix shown in\\nFig. 1. The results presented in Table 1 reveal that Twitter, Reddit, and BitChute\\nexhibit the highest scores, reflecting their prominent roles in the network. Voat, Par-\\nler, and YouTube display lower centrality scores but still maintain relatively strong\\nconnection patterns. Facebook, Gab, and Scored rank as the least central platforms\\nin the ecosystem.\\nNews Consumption\\nNext, we investigate whether the type of news and content shared differs signifi-\\ncantly across the nine platforms by analyzing the news domains‚Äô political bias. To this\\naim, we use Media Bias/Fact Check (commonly referred to as MBFC, which can be\\naccessed by https://mediabiasfactcheck.com), a widely referenced news rating agency\\nthat provides political bias and reliability labels for news outlets and other information\\nproducers [5, 42, 43]. We extract the domains from the URLs shared across the nine\\nplatforms and match them with the political bias and reliability information obtained\\nfrom MBFC. Domains not classified by MBFC are labeled as ‚Äúunreported‚Äù, though this\\ndesignation does not necessarily imply an absence of political bias. Domains catego-\\nrized as ‚Äúextreme-left‚Äù are excluded from the analysis due to their negligible presence\\nin the dataset (see Materials and Methods for details).\\nFig. 2 illustrates users‚Äô news consumption across platforms, showing the propor-\\ntion of URLs directing to domains with differing political leanings, as categorized by\\nMBFC. A clear pattern emerges: mainstream platforms such as Facebook, Reddit, and\\nTwitter tend to skew toward left-leaning content, while alt-tech platforms‚Äîincluding\\nGab, Parler, Voat, and Scored‚Äîpredominantly link to right-leaning content. Voat\\nstands out among alt-tech platforms by directing a notable portion of its traffic to\\nleft- or left-center-leaning domains. Interestingly, 79.7% of these links were shared\\nacross eleven of the 2,423 distinct subverses analyzed, many of which (e.g., v/Anon-\\nAll, v/GreatAwakening) align with alt-right or conspiracy groups (a detailed list of\\nsubverses is available in the SI). This pattern suggests that these links may have\\nbeen shared in an ironic or derogatory manner. YouTube and BitChute heavily link\\nto ‚Äúunreported‚Äù domains, reflecting their role as platforms that host a broad range of\\nless conventional or unvetted content.\\nTo further illustrate these patterns, Table 2 reports each platform‚Äôs ten most fre-\\nquently linked domains. While Facebook and Twitter often feature sources having a\\nleft-center bias, such as CNN, The New York Times, or The Washington Post, Reddit\\ndisplays a more homogenous profile as no right-biased sources appear among the most\\nlinked domains. In contrast, platforms such as Gab, Parler, Scored, and Voat direct a\\nsubstantial proportion of their traffic to sources that are both right-biased and have low\\nreliability. Scored, for instance, channels 55% of its external traffic to ‚ÄúPatriots.Win‚Äù,\\nan online forum created following the ban of the ‚Äúr/The Donald‚Äù subreddit in June\\n60.01 0.07 0.01 0.03 0.1 0.01 0.770.1 0.05 0.09 0.22 0.13 0.06 0.36\\n0.03 0.34 0.02 0.04 0.37 0.05 0.16\\n0.03 0.22 0.01 0.05 0.37 0.05 0.270.14 0 0.16 0.48 0.04 0.06 0.12\\n0.02 0.65 0 0.03 0.17 0.02 0.10.06 0.07 0.13 0.41 0.12 0.06 0.14\\n0.06 0.14 0.04 0.15 0.25 0.07 0.290.03 0.02 0.06 0.12 0.12 0.06 0.59\\nScoredVoatParlerGabBitChuteYouTubeTwitterRedditFacebook\\nleft left_center center right_center rightextreme_right unreported\\nToFromFig. 2 :News Consumption . Fraction of URLs directing to domains with differing\\npolitical leanings, as categorized by MBFC. Domains without a political bias identified\\nby MBFC are categorized as ‚Äúunreported‚Äù. News consumption on mainstream plat-\\nforms is skewed toward left-leaning content, while alt-tech platforms predominantly\\nlink to right-leaning content. YouTube and BitChute prominently share ‚Äúunreported‚Äù\\ndomains.\\n2015 [33]. This forum is known for using unreliable sources and a lack of transparency\\nand moderation [44, 45]. Although both YouTube and BitChute frequently link to\\n‚Äúunreported‚Äù domains, BitChute‚Äôs links often direct users to alternative news web-\\nsites, including extremist domains like ‚ÄúGlavasic.com‚Äù, a platform created by Serbian\\nphilosopher Dragan GlavaÀá si¬¥ c to promote his white supremacist manifesto.\\nNext, we analyze the similarity between the news diets across the platforms in\\nthe information ecosystem. To this aim, we identify the top 20 most-linked domains\\nfor each platform and compute the weighted cosine similarity between the domain\\nvectors for all pairs of platforms. This method quantitatively measures how closely\\nplatforms align in their shared content. The results, illustrated in Fig. 3, reveal two\\ndistinct cliques characterized by high similarity scores. The first clique consists of the\\nmainstream platforms Facebook, Twitter, and Reddit, all exhibiting cosine similarity\\nscores above 0 .7. The second clique includes the alt-tech platforms Gab, Voat, and\\nParler, with similarity scores ranging from 0 .74 to 0 .88. Conversely, the similarity\\nbetween mainstream and alt-tech platforms does not exceed 0 .5, with Facebook and\\nTwitter showing slightly higher similarity to specific fringe platforms. This observation\\nmay indicate that the news consumption of certain echo chambers on Facebook and\\nTwitter partially overlaps with those on fringe platforms. In contrast, Reddit appears\\nto be the mainstream platform most dissimilar to alt-tech platforms, exhibiting the\\n7Table 2 : Most shared domains by platform. Name and percentage of links toward the\\nten domains most linked by each platform. Each domain is color coded to represent\\nits political bias as follows: left, left-center, center, right-center, right, extreme-right,\\nunreported . An asterisk following a domain‚Äôs name indicates that it is flagged as a\\nquestionable source.\\nFacebook Reddit Twitter YouTube BitChute\\nDomain % links Domain % links Domain % links Domain % links Domain % links\\nCNN 2.35 The Hill 6.00 The New York Times 6.90 Fox News* 8.09 Infowars* 2.12\\nFox News* 2.30 CNN 5.82 The Washington Post 6.56 MSNBC 1.97 Renegade Tribune* 1.82\\nThe Hill 1.79 The Washington Post 4.73 CNN 5.72 Tv9Hindi 1.86 Rebel News* 1.36\\nThe Washington Post 1.72 The New York Times 3.95 Fox News* 3.22 The Young Turks 1.77 London Real 1.13\\nThe New York Times 1.65 Politico 2.79 Politico 2.34 CNBC 1.55 Zero Hedge* 0.81\\nNBC News 1.57 Reuters 2.37 The Hill 2.30 PBS NewsHour 1.47 Brave 0.74\\nYahoo News 1.50 Associated Press 2.18 NBC News 2.27 Fox Business 1.47 Banned News* 0.74\\nBreitbart* 1.46 The Guardian 2.10 Breitbart* 2.23 NBC News 1.10 Glavasic.com* 0.69\\nDaily Wire 1.22 NBC News 2.07 Raw Story 2.19 France 24 0.91 NewsWars* 0.48\\nMSN 1.04 Business Insider 1.93 The Gateway Pundit* 1.92 CBS News 0.67 fuckthejews* 0.39\\nGab Parler Scored Voat\\nDomain % links Domain % links Domain % links Domain % links\\nThe Gateway Pundit* 13.71 The Gateway Pundit* 8.35 Patriots* 57.68 The Gateway Pundit* 5.47\\nBreitbart* 12.07 Fox News* 7.02 Breitbart* 3.36 Breitbart* 4.62 Left\\nFox News* 4.09 Breitbart* 5.82 MAGA* 2.62 Zero Hedge* 4.05 Left Center\\nZero Hedge* 2.47 The Epoch Times* 4.45 The Gateway Pundit* 2.31 DailyMail* 2.34 Center\\nThe Epoch Times* 2.43 Western Journal* 1.68 Fox News* 1.64 Fox News* 2.12 Right Center\\nInfoWars* 2.29 New York Post 1.63 Zero Hedge* 0.99 RT News* 1.61 Right\\nGNews* 1.97 Town Hall* 1.18 New York Post 0.92 The Hill 1.50 Extreme Right\\nNew York Post 1.70 The Blaze* 1.07 Washington Examiner* 0.72 New York Post 1.44 Unreported\\nThe Hill 1.31 Zero Hedge* 1.07 DailyMail* 0.70 CNN 1.17\\nDailyCaller 1.19 The Federalist* 1.07 DailyCaller 0.56 Washington Examiner 1.03\\nlowest similarity in news consumption. This aligns with the tendency of Reddit users to\\npredominantly share left-leaning content, while extreme-right sources are absent and\\nright-center or right-leaning content is relatively uncommon. YouTube shows moderate\\nsimilarity to other mainstream platforms, particularly Twitter, and to Parler for what\\nconcerns the alt-tech platforms, while BitChute and Scored are dissimilar to all other\\nplatforms. This difference may be attributed to the extreme nature of the content\\nfrequently shared on BitChute and the limited popularity and reach of Scored, which\\nresults in unique news diets that diverge significantly from both mainstream and other\\nalt-tech platforms. Refer to the SI for the cosine similarity matrix.\\nFig. 3 also highlights the fraction of questionable domains (as defined by MBFC)\\nshared by each platform. Mainstream platforms share a relatively small fraction of\\nunreliable sources, while alt-tech platforms display significantly higher proportions of\\nsuch content, reinforcing the distinct content dynamics within the information ecosys-\\ntem. These results are reported in more detail in Table 1, showing the fraction of\\nquestionable sources shared by users on each platform. Alt-tech platforms Gab, Par-\\nler, and especially Scored exhibit the highest proportions. Voat shows a lower fraction,\\nwhile BitChute has a value of 0 .17, similar to the one observed in mainstream plat-\\nforms such as Facebook, Twitter, and YouTube. Reddit stands out for its extremely\\nlow fraction of questionable sources shared (0 .03). These findings underscore the dis-\\ntinction between alt-tech platforms, where questionable sources appear to circulate\\nmore freely, and mainstream platforms, where such content constitutes only a small\\nproportion of shared material. The BitChute results, while differing from the general\\ntrend observed among alt-tech platforms, align with the platform‚Äôs propensity to host\\na significant volume of ‚Äúunreported‚Äù domains.\\n8RedditTwitterYouTube\\nBitChuteGabParler\\nVoatScored\\nFacebook\\nNews Consumption\\nQuestionable Reliable UnreportedFig. 3 : Cosine similarity network based on the platforms‚Äô 20 most linked domains. The\\nsize of the different nodes is proportional to the volume of links shared in the platform,\\nwhile the colors of the pies indicate the fraction of questionable or reliable content\\nshared. We observe two cliques with high similarity: one made up of mainstream\\nplatforms (Facebook, Twitter, Reddit) that share a majority of reliable news sources,\\nand one made up of alt-tech ones (Gab, Parler, Voat) sharing a higher fraction of\\nquestionable sources. Scored, BitChute, and, to an extent, YouTube remain fairly\\nseparated from the rest of the platforms.\\nUser base\\nTo assess the diversity of the user bases across platforms, we evaluate the distribution\\nof users‚Äô political leaning. We infer the political leaning of active users (i.e., users\\nwho shared 10 or more URLs toward domains with an associated political bias) based\\non their posting activity. We assign a numerical score to news media, ranging from\\n‚àí1 (extreme-left) to +1 (extreme-right), excluding unreported sources. The political\\nleaning of each user is then defined as the average of the leanings of the news domains\\nthey post (see section Materials and Methods for further details).\\nFig. 4 illustrates the political leaning distribution of users in the nine platforms. We\\nnote that the user bases of Facebook and Twitter are composed of two distinct groups\\nspanning the whole ideological spectrum, resulting in a bimodal distribution of polit-\\nical leanings. This result suggests the presence of segregated communities endorsing\\nopposite narratives. The same pattern is not observed on some alt-tech platforms (Gab,\\n9Parler, and Scored), where users are concentrated within a narrower leaning interval,\\nforming a homogeneous, right-leaning user base. Reddit exhibits similar behavior to\\nalt-tech platforms but with a homogeneous user base skewed significantly leftward, in\\nline with previous findings [46, 47]. Voat and BitChute stand out among the alt-tech\\nplatforms, displaying a relatively heterogeneous user base compared to their counter-\\nparts. For Voat, this heterogeneity likely stems from the significant portion of traffic\\nredirected toward left-biased sources by alt-right groups, as previously observed. For\\nBitChute, the high proportion of unreported sources on the platform likely contributes\\nto its heterogeneity. Specifically, since we only include users who shared at least 10\\nURLs with an associated political bias in our computation, this criterion captures only\\na small, seemingly diverse subset of the user base.\\n0.000.050.100.150.20\\n‚àí1.0‚àí0.50.00.51.0Relative FrequencyFacebook\\n0.00.10.20.3\\n‚àí1.0‚àí0.50.00.51.0Relative FrequencyYouTube\\n0.00.10.20.3\\n‚àí1.0‚àí0.50.00.51.0Reddit\\n0.000.050.100.150.20\\n‚àí1.0‚àí0.50.00.51.0Parler\\n0.000.040.080.12\\n‚àí1.0‚àí0.50.00.51.0Voat\\n0.00.10.20.3\\n‚àí1.0‚àí0.50.00.51.0Relative FrequencyTwitter\\n0.000.050.100.150.20\\n‚àí1.0‚àí0.50.00.51.0BitChute\\n0.000.050.100.150.20\\n‚àí1.0‚àí0.50.00.51.0\\nInferred User LeaningGab\\n0.00.20.4\\n‚àí1.0‚àí0.50.00.51.0Scored\\nFrac. of questionable\\nsources shared\\n00.250.50.751\\nFig. 4 : Distributions of users‚Äô political leaning for each platform. Each unique user\\ngets assigned a leaning score between ‚àí1 and +1, according to their posting activity.\\nThe bars are colored according to the number of ‚Äúquestionable‚Äù sources shared by\\nusers of a specific leaning. We notice how Facebook and Twitter have a polarized\\nuser base with two distinct groups, one sharing mostly reliable content and the other\\nsharing mostly content with low factual reporting. The situation is more homogeneous\\nregarding all of the fringe platforms, and the remaining two mainstream platforms.\\nAdditionally, on BitChute, a substantial share of content from hyper-extremist\\nsites is driven by a very small group of highly active users. In some cases, these users\\nare only a few dozen or hundreds, yet they are responsible for thousands of links,\\ncreating a disproportionate volume of extremist content on the platform. This finding\\naligns with the concept of ‚Äúvocal minorities‚Äù [48], where a small but highly active\\nsubset of users exerts an outsized influence on the platform by driving a significant\\nshare of its activity. Interestingly, users with a political leaning below 0 (left-leaning)\\nmake up approximately 30% of the total user base but share, on average, 130 fewer\\nlinks than right-leaning users (leaning above 0) and 217 fewer links than those with\\na leaning equal or above 0 .5. This suggests that left-leaning users are generally less\\nactive in sharing content, resulting in a smaller contribution to the circulation of news\\n10on the platform. These dynamics underscore an asymmetry in content production\\nand dissemination across ideological groups, further highlighting the influence of vocal\\nminorities in shaping platform-wide narratives.\\nThe observations regarding the diversity of the user bases can be quantified by\\nmeans of the variance of the political leaning distribution, œÉ2, reported in Table 1.\\nPlatforms such as Twitter, Facebook, YouTube, and BitChute exhibit high variance,\\nindicating a wide spread of user leanings and reflecting a diverse user base, while the\\nremaining platforms demonstrate more homogeneous user bases.\\nWhen considering the reliability of the sources shared, we observe that users with\\nright-leaning and extreme-right political orientations share a higher proportion of ques-\\ntionable content. However, this pattern aligns with established correlations reported\\nby MBFC and is not specific to our study. Therefore, we focus on broader patterns\\nof user base composition and political leanings rather than the relationship between\\npolitical orientation and source reliability.\\nConclusions\\nThis study introduced the concept of ‚Äúecho platforms‚Äù, i.e., entire social media plat-\\nforms that operate as self-contained echo chambers, reinforcing homogeneous beliefs\\nand isolating users from opposing viewpoints. Echo platforms represent an evolution of\\ntraditional echo chambers, where the platform structure fosters ideological uniformity.\\nOur framework categorizes platforms across three dimensions‚Äîplatform centrality,\\ncontent reliability, and user base homogeneity‚Äîproviding a systematic approach to\\nunderstanding these dynamics within the broader context of internet fragmentation.\\nAnalyzing a multi-platform dataset spanning mainstream social media (e.g., Face-\\nbook, Twitter) as well as alt-tech platforms (e.g., Gab, Parler), we revealed patterns\\nof content redirection, community insulation, and polarized news consumption. This\\nsystemic shift from segregated communities to platform-wide segregation highlights\\nthe role of moderation policies, user self-selection, and engagement-driven business\\nmodels in shaping the rise of echo platforms.\\nOur findings contribute to the discourse on digital polarization, showing how alt-\\ntech platforms attract marginalized or de-platformed communities from mainstream\\nspaces, creating distinct niches that intensify ideological separation. This underscores\\nthe need to reevaluate moderation strategies, as current policies may unintentionally\\nexacerbate the fragmentation of the information ecosystem by fostering the growth of\\ninsulated echo platforms. Such dynamics challenge the digital public sphere by limiting\\nconstructive dialogue and deepening ideological divides.\\nWhile our operational framework provides robust tools for identifying and ana-\\nlyzing echo platforms, the study has limitations. The dataset, though extensive,\\nreflects a specific period marked by significant political events, such as elections.\\nThis context offers valuable insights into platform behavior during critical societal\\nmoments but may only partially capture ongoing changes in the social media land-\\nscape. Future research could extend this work by integrating more recent data and\\nexploring cross-cultural or longitudinal variations in platform dynamics.\\n11Our results offer a foundation for examining the socio-political impacts of echo\\nplatforms, particularly in contexts where public discourse and social cohesion are at\\nrisk. Understanding and mitigating the societal effects of echo platforms is essential for\\ndeveloping policies that balance open expression with fostering a diverse and cohesive\\npublic sphere. This study underscores the critical role of echo platforms in shaping\\nthe modern information ecosystem, emphasizing their contribution to internet frag-\\nmentation and societal polarization. By offering a detailed quantitative framework,\\nwe aim to advance understanding of these platforms and inform strategies to mitigate\\nthe adverse impacts of digital fragmentation on society.\\nMatherial and Methods\\nData\\nIn this section, we describe the procedures for data collection and preprocessing,\\nalong with details about the datasets used in our study. During data preprocessing,\\nwe manually excluded URLs that were not relevant to our analysis. This included\\nself-links (i.e., a platform linking to itself), URLs directing to social media platforms\\nnot included in the datasets (e.g., Snapchat, TikTok), video streaming services (e.g.,\\nVimeo, Dailymotion), financial services (e.g., PayPal, Venmo), music streaming ser-\\nvices (e.g., Spotify, SoundCloud), and tech platforms or services (e.g., Google Suite,\\nStreamlabs). Additionally, we removed a range of URLs that did not fit these broad\\ncategories but were similarly uninformative for our purposes, such as links to Ama-\\nzon, Steam, Coinbase, and NASA.\\nFacebook: The URLs analyzed were extracted from 21 million Facebook posts\\ncollected using the CrowdTangle service. These posts were identified based on searches\\nusing predefined keyword lists: L1: Trump, trump, #donaldtrump, #trump and L2:\\nBiden, biden, #joebiden, #biden, spanning the period from May to November 2020.\\nReddit: We utilize URLs extracted from a comprehensive collection of posts\\npublished in the subreddit r/Politics between January and December 2020, totaling\\nnearly 4.7 million posts. These posts were collected via the Pushshift dataset [49].\\nA subreddit is a user-created community focused on specific topics and governed by\\nits own rules, where members can subscribe, share posts, comment, and upvote or\\ndownvote content.\\nTwitter: Our dataset is based on information retrieved from Flamino et al. [43]\\nand includes 174 million tweets posted between June 1 and November 3, 2020.\\nYouTube: We use URLs extracted from the descriptions of 270,000 YouTube\\nvideos, collected using the YouTube Data API between June and December 2020.\\nThe videos were identified through searches based on predefined keyword lists: L1:\\nTrump, trump, #donaldtrump, #trump and L2: Biden, biden, #joebiden, #biden.\\nFor each list, an additional search was conducted by crawling the network of videos\\nusing YouTube‚Äôs algorithm to identify related videos*. From the gathered dataset,\\n12we retained only videos containing Trump ‚Äîtrump (orBiden ‚Äîbiden , respectively)\\nin the title or description.\\nBitChute: BitChute is a British alt-tech video hosting platform known for its\\nlower moderation efforts compared to its more popular counterpart, YouTube. The\\nplatform focuses heavily on news and politics and is associated with a significant\\namount of hate speech in both videos and comment sections [50]. For our analysis,\\nwe use the MeLa BitChute Dataset [51], which provides a near-complete scrape of\\nthe platform from 2019 to 2021. From this dataset, we filter the data to retain only\\nthe 1.1 million videos uploaded in 2020, out of the original 3.3 million videos.\\nGab: Gab is an alt-tech microblogging platform structured similarly to Twitter\\nbut with minimal content moderation, promoting free speech and Christian values\\n[52, 53]. With its predominantly far-right user base, Gab has been described as a\\nsafe haven for neo-Nazis, members of the American alt-right, Trump supporters, and\\nconspiracy theorists [54, 55]. The platform has also been repeatedly linked to online\\nradicalization and real-world violent events [56].\\nThe data for this study was collected in two phases. From June 1, 2020, to Octo-\\nber 23, 2020, posts were downloaded using Gab‚Äôs general stream, capturing all posts\\ngenerated during this period. After October 23, due to the deprecation of the API\\nendpoint, data was collected using the timelines of 930,000 users identified in the first\\nphase. The dataset was then filtered using the keywords trump‚Äù or biden,‚Äù resulting\\nin 467,000 posts made between June 1 and December 1, 2020.\\nParler: Launched in 2018, Parler is a platform with functionalities similar to Twit-\\nter, marketed as a free-speech-focused alternative. It gained mainstream attention\\nfollowing the storming of Capitol Hill in 2021, as it was one of the platforms allegedly\\nused to incite and plan the attack [57, 58]. Shortly after, Parler was removed from\\nthe Google Play Store and Apple App Store and subsequently suspended by its host-\\ning provider, Amazon AWS. At the time of writing, Parler‚Äôs social media outlet has\\nannounced plans to relaunch its services, though the exact timeline remains unclear.\\nThe data used in this study comes from a large dataset comprising 183 million\\nposts made between 2018 and 2021, collected by Aliapoulios et al. [59]. For our\\nanalysis, we focused exclusively on the 13 million posts made during 2020.\\nScored: Scored (accessible via both https://scored.co and https://communities.\\nwin) emerged as an alternative to Reddit, hosting numerous communities that\\nwere banned from more prominent social networks, including c/TheDonald ,\\nc/GreatAwakening , and c/FatPeopleHate . For this study, we use the iDRAMA-\\nScored-2024 dataset [60], a comprehensive scrape of the platform since its inception\\nin 2020. This dataset includes a total of 6.2 million posts spanning nearly four years,\\nof which 1.6 million posts from 2020 are analyzed in our work.\\n13Voat: The now-defunct Voat, which was shut down in December 2020, served as\\nan alternative to Reddit, similar to Scored. Several communities banned from Red-\\ndit migrated to Voat, forming new subverses ‚Äîthe platform‚Äôs equivalent of subreddits.\\nNotable examples include v/fatpeoplehate, v/TheRedPill, and v/GreatAwakening.\\nFor this study, we use data from a collection of 2.3 million posts made on the\\nplatform between November 2013 and December 2020, compiled by Mekacher and\\nPapasavva [61]. From this dataset, we retain only submissions made in 2020, exclud-\\ning those posted in the subverse v/QRV due to its policy of anonymizing user IDs,\\nresulting in a total of 355,000 submissions.\\nPlatforms graph and null model\\nThe set of connections (URLs) between platforms can be modeled as a weighted\\ndirected graph G= (V, E, w ), where Vrepresents the set of nine social media plat-\\nforms, E‚äÜV√óVthe set of directed edges between platforms, and w:E‚ÜíR+the\\nweight of edges corresponding to the number of times one platform links to another.\\nGiven the size disparity between the considered data sets, quantifying the relevance\\nof links among nodes using only their weights would introduce a bias. Thus, we com-\\npute how such weights deviate from their expected values. These expected values can\\nbe computed using the weighted configuration model [62], a null model in which the\\nin- and out-strength distributions of the nodes are kept. According to the weighted\\nconfiguration model, the average weight of a link connecting two uncorrelated vertices\\nwith out-strength sout\\niand in-strength sin\\njcan be written as E[wij] =sout\\ni ¬∑sin\\nj\\nS, where\\nSrefers to the total weight of the network. Hence, we can compute the relationship\\nbetween the observed weights and their expectation as follows:\\nÀÜwi,j=wi,j\\nE[wij].\\nSuch quantity approximates the results we would obtain by performing a strength-\\npreserving randomization on the network. If higher than 1, the weights we obtain\\nindicate that a link from one platform to another occurs more often than we would\\nexpect at random, and vice versa.\\nLabeling of news sources\\nAs mentioned, we utilize Media Bias/Fact Check (MBFC, https://mediabiasfactcheck.\\ncom) to label news outlets based on their political bias and level of reliability regarding\\nfactuality reporting. MBFC is an independent fact-checking organization that rates\\nvarious news sources. The labeling utilized in this study, collected in October 2024, con-\\ntains political bias categories ranging from extreme-left to extreme-right, while certain\\nnews outlets are classified as ‚Äúquestionable‚Äù, indicating that they exhibit one or more\\nof the following, per MBFC: ‚Äúextreme bias, consistent promotion of propaganda/con-\\nspiracies, poor or no sourcing to credible information, a complete lack of transparency\\nand/or is fake news‚Äù. While not explicitly falling under the ‚Äúquestionable‚Äù definition\\nof MBFC, we also consider as such domains classified as ‚Äúconspiracy/pseudoscience‚Äù,\\n14given their inherent low credibility and absence of proper fact-checking. Furthermore,\\nwe manually labeled a small number of right-biased and/or extremist websites which\\naccounted for a significant portion of some alt-tech platforms‚Äô traffic, but that were\\nnot present on MBFC.\\nInferring accounts‚Äô leaning\\nTo compute an account‚Äôs leaning, we utilize the following algorithmic procedure: we\\nassign a score between ‚àí1 and +1 to each external domain, depending on its MBFC‚Äôs\\npolitical bias label. Namely, -1 for the extreme left, -0.66 for left, -0.33 for left-center, 0\\nfor least biased, 0.33 for right-center, 0.66 for right, and +1 for the extreme right. For\\nan account iwho shared nURLs towards external domains Ci={c1, c2, ..., c n}, each\\nURL cjis associated with one of these numeric values. The political leaning xiof the\\naccount iis then defined as the average of the political bias of all the domains shared:\\nxi‚â°Pn\\nj=1cj\\nn. (1)\\nThis returns a leaning score in the interval [ ‚àí1,1], where a value of ‚àí1 (+1) indicate\\nan extreme-left (extreme-right) leaning. This procedure, though simple, is grounded in\\npsychological theories such as selective exposure [63] and has proven to be an effective\\nestimator of users‚Äô political leaning [5, 43].\\n15SI Appendix Section\\nTable 3 : Timeframe and number of URLs collected (before and after processing)\\nfor each platform‚Äôs data set.\\nPlatform Timeframe # of URLs (unprocessed) # of URLs (processed)\\nFacebook 25/05/2020 - 15/11/2020 20M 10.7M\\nReddit 01/01/2020 - 31/12/2020 328k 320k\\nTwitter 01/06/2020 - 03/11/2020 189M 103M\\nYouTube 01/06/2020 - 16/12/2020 981k 512k\\nBitChute 02/04/2020 - 09/10/2020 3.2M 1.9M\\nGab 01/06/2020 - 01/12/2020 468k 453k\\nParler 01/01/2020 - 31/12/2020 14M 8.2M\\nScored 01/01/2020 - 31/12/2020 984k 883k\\nVoat 01/01/2020 - 25/12/2020 269k 215k\\nTable 4 : Table showing the name, number of URLs shared, number of URLs\\nshared toward left biased sources, fraction of left biased sources shared, cumula-\\ntive count and cumulative proportion of left biased sources shared for the eleven\\nVoat‚Äôs subverses responsible for sharing approximately 80% of the platform‚Äôs left\\nbiased content. Note how in this table we consider as ‚Äúleft biased‚Äù every source\\nclassified as either extreme-left biased, left biased, or left-center biased.\\nSubverse URLs left-leaning URLs frac. left cum. count cum. prop.\\n1. AnonAll 15822 9643 0 .609 9643 0 .298\\n2. news 31884 5931 0 .186 15574 0 .482\\n3. politics 20898 2627 0 .126 18201 0 .563\\n4. whatever 25841 2327 0 .090 20528 0 .635\\n5. GreatAwakening 35987 1535 0 .043 22063 0 .682\\n6. OccidentalEnclave 4865 1345 0 .276 23408 0 .724\\n7. Niggers 5621 553 0 .098 23961 0 .741\\n8. Worldnews 2289 550 0 .240 24511 0 .758\\n9. theawakening 12765 446 0 .035 24957 0 .772\\n10. technology 2780 442 0 .159 25399 0 .785\\n11. Australia 919 388 0 .422 25787 0 .797\\n16214692\\n69733\\n482041245616\\n10806\\n2069119979230792\\n541549\\n240505980\\n18721109664128\\n2939026919\\n44139957\\n582677\\n63225010453\\n213305046\\n23135\\n150328481381\\n34971\\n56432091\\n1824\\n381755955\\n293055169836\\n13603941\\n17339612032\\n1426141296594\\n2\\n159\\n9211\\n1124321125633\\n4488892\\n500414358\\n468211906733\\n1252524457080\\n00\\n000 0\\nScoredVoatParlerGabBitChuteYouTubeRedditTwitterFacebook\\nFacebookTwitterRedditYouTubeBitChuteGabParlerVoat\\nScored\\nToFromFig. 5 : Adjacency matrix describing the number of links from and to each of the nine\\ndifferent platforms.\\n1710.71\\n10.78\\n0.77\\n10.41\\n0.14\\n0.26\\n10\\n0\\n0.01\\n0\\n10.28\\n0.05\\n0.28\\n0.18\\n0.16\\n10.35\\n0.05\\n0.29\\n0.44\\n0.07\\n0.88\\n10.39\\n0.26\\n0.42\\n0.19\\n0.14\\n0.82\\n0.74\\n10.03\\n0.01\\n0.03\\n0.02\\n0.01\\n0.08\\n0.07\\n0.07\\n1 ScoredVoatParlerGabBitChuteYouTubeTwitterRedditFacebook\\nFacebookRedditTwitterYouTubeBitChuteGabParlerVoat\\nScoredFig. 6 : Matrix showing the weighted cosine similarity between the top-20 linked\\ndomains for each platform.\\n18References\\n[1] Gallup, K.: Indicators of news media trust. John S. and James L. Knight\\nFoundation Miami (2018)\\n[2] Nic, N., Fletcher, R., Kalogeropoulos, A., Levy, D.A., Nielsen, R.K.: Reuters\\ninstitute digital news report 2018. Reuters Institute for the Study of Journalism\\n39(2018)\\n[3] Voorveld, H.A., Van Noort, G., Muntinga, D.G., Bronner, F.: Engagement with\\nsocial media and social media advertising: The differentiating role of platform\\ntype. Journal of advertising 47(1), 38‚Äì54 (2018)\\n[4] Sangiorgio, E., Cinelli, M., Cerqueti, R., Quattrociocchi, W.: Followers do not\\ndictate the virality of news outlets on social media. PNAS nexus 3(7), 257 (2024)\\n[5] Cinelli, M., De Francisci Morales, G., Galeazzi, A., Quattrociocchi, W., Starnini,\\nM.: The echo chamber effect on social media. Proceedings of the National\\nAcademy of Sciences 118(9), 2023301118 (2021)\\n[6] Etta, G., Sangiorgio, E., Di Marco, N., Avalle, M., Scala, A., Cinelli, M., Quat-\\ntrociocchi, W.: Characterizing engagement dynamics across topics on facebook.\\nPlos one 18(6), 0286150 (2023)\\n[7] Bessi, A., Coletto, M., Davidescu, G.A., Scala, A., Caldarelli, G., Quattrociocchi,\\nW.: Science vs conspiracy: Collective narratives in the age of misinformation.\\nPloS one 10(2), 0118093 (2015)\\n[8] Terren, L.T.L., Borge-Bravo, R.B.-B.R.: Echo chambers on social media: A\\nsystematic review of the literature. Review of Communication Research 9(2021)\\n[9] Avalle, M., Di Marco, N., Etta, G., Sangiorgio, E., Alipour, S., Bonetti, A., Alvisi,\\nL., Scala, A., Baronchelli, A., Cinelli, M., et al. : Persistent interaction patterns\\nacross social media platforms and over time. Nature 628(8008), 582‚Äì589 (2024)\\n[10] Shandwick, W., Tate, P.: Krc research (2019). civility in america 2019: Solutions\\nfor tomorrow. Weber Shadwick 26(2019)\\n[11] League, A.-D.: Online hate and harassment report: the american experience 2020.\\nRetrieved September 25, 2021 (2020)\\n[12] Tahmasbi, F., Schild, L., Ling, C., Blackburn, J., Stringhini, G., Zhang, Y., Zan-\\nnettou, S.: ‚Äúgo eat a bat, chang!‚Äù: On the emergence of sinophobic behavior on\\nweb communities in the face of covid-19. In: Proceedings of the Web Conference\\n2021, pp. 1122‚Äì1133 (2021)\\n[13] Bail, C.A., Argyle, L.P., Brown, T.W., Bumpus, J.P., Chen, H., Hunzaker, M.F.,\\nLee, J., Mann, M., Merhout, F., Volfovsky, A.: Exposure to opposing views\\n19on social media can increase political polarization. Proceedings of the National\\nAcademy of Sciences 115(37), 9216‚Äì9221 (2018)\\n[14] Kubin, E., Von Sikorski, C.: The role of (social) media in political polarization: a\\nsystematic review. Annals of the International Communication Association 45(3),\\n188‚Äì206 (2021)\\n[15] Falkenberg, M., Galeazzi, A., Torricelli, M., Di Marco, N., Larosa, F., Sas, M.,\\nMekacher, A., Pearce, W., Zollo, F., Quattrociocchi, W., et al. : Growing polar-\\nization around climate change on social media. Nature Climate Change 12(12),\\n1114‚Äì1121 (2022)\\n[16] Cinus, F., Minici, M., Monti, C., Bonchi, F.: The effect of people recommenders\\non echo chambers and polarization. In: Proceedings of the International AAAI\\nConference on Web and Social Media, vol. 16, pp. 90‚Äì101 (2022)\\n[17] Del Vicario, M., Bessi, A., Zollo, F., Petroni, F., Scala, A., Caldarelli, G., Stanley,\\nH.E., Quattrociocchi, W.: The spreading of misinformation online. Proceedings\\nof the national academy of Sciences 113(3), 554‚Äì559 (2016)\\n[18] Diaz Ruiz, C., Nilsson, T.: Disinformation and echo chambers: how disinformation\\ncirculates on social media through identity-driven controversies. Journal of public\\npolicy & marketing 42(1), 18‚Äì35 (2023)\\n[19] Sunstein, C.R.: The law of group polarization. University of Chicago Law School,\\nJohn M. Olin Law & Economics Working Paper (91) (1999)\\n[20] Del Vicario, M., Vivaldo, G., Bessi, A., Zollo, F., Scala, A., Caldarelli, G., Quat-\\ntrociocchi, W.: Echo chambers: Emotional contagion and group polarization on\\nfacebook. Scientific reports 6(1), 37825 (2016)\\n[21] Quattrociocchi, W., Scala, A., Sunstein, C.R.: Echo chambers on facebook.\\nAvailable at SSRN 2795110 (2016)\\n[22] Hobolt, S.B., Lawall, K., Tilley, J.: The polarizing effect of partisan echo\\nchambers. American Political Science Review 118(3), 1464‚Äì1479 (2024)\\n[23] Garimella, K., De Francisci Morales, G., Gionis, A., Mathioudakis, M.: Political\\ndiscourse on social media: Echo chambers, gatekeepers, and the price of biparti-\\nsanship. In: Proceedings of the 2018 World Wide Web Conference, pp. 913‚Äì922\\n(2018)\\n[24] Grimmelmann, J.: The virtues of moderation. Yale JL & Tech. 17, 42 (2015)\\n[25] Ali, S., Saeed, M.H., Aldreabi, E., Blackburn, J., De Cristofaro, E., Zannettou, S.,\\nStringhini, G.: Understanding the effect of deplatforming on social networks. In:\\nProceedings of the 13th ACM Web Science Conference 2021, pp. 187‚Äì195 (2021)\\n20[26] Monti, C., Cinelli, M., Valensise, C., Quattrociocchi, W., Starnini, M.: Online\\nconspiracy communities are more resilient to deplatforming. PNAS nexus 2(10),\\n324 (2023)\\n[27] Mekacher, A., Falkenberg, M., Baronchelli, A.: The systemic impact of deplat-\\nforming on social media. PNAS nexus 2(11), 346 (2023)\\n[28] Cima, L., Trujillo, A., Avvenuti, M., Cresci, S.: The great ban: Efficacy and\\nunintended consequences of a massive deplatforming operation on reddit. In:\\nCompanion Publication of the 16th ACM Web Science Conference, pp. 85‚Äì93\\n(2024)\\n[29] Cava, L.L., Aiello, L.M., Tagarelli, A.: Drivers of social influence in the twitter\\nmigration to mastodon. Scientific Reports 13(1), 21626 (2023)\\n[30] Quelle, D., Bovet, A.: Bluesky: Network topology, polarisation, and algorithmic\\ncuration. arXiv preprint arXiv:2405.17571 (2024)\\n[31] Failla, A., Rossetti, G.: ‚Äúi‚Äôm in the bluesky tonight‚Äù: Insights from a year worth\\nof social data. PloS one 19(11), 0310330 (2024)\\n[32] Newell, E., Jurgens, D., Saleem, H., Vala, H., Sassine, J., Armstrong, C., Ruths,\\nD.: User migration in online social networks: A case study on reddit dur-\\ning a period of community unrest. In: Proceedings of the International AAAI\\nConference on Web and Social Media, vol. 10, pp. 279‚Äì288 (2016)\\n[33] Horta Ribeiro, M., Jhaver, S., Zannettou, S., Blackburn, J., Stringhini, G.,\\nDe Cristofaro, E., West, R.: Do platform migrations compromise content mod-\\neration? evidence from r/the donald and r/incels. Proceedings of the ACM on\\nHuman-Computer Interaction 5(CSCW2), 1‚Äì24 (2021)\\n[34] Russo, G., Verginer, L., Ribeiro, M.H., Casiraghi, G.: Spillover of antisocial behav-\\nior from fringe platforms: The unintended consequences of community banning.\\nIn: Proceedings of the International AAAI Conference on Web and Social Media,\\nvol. 17, pp. 742‚Äì753 (2023)\\n[35] Zannettou, S., Bradlyn, B., De Cristofaro, E., Kwak, H., Sirivianos, M., Stringini,\\nG., Blackburn, J.: What is gab: A bastion of free speech or an alt-right echo\\nchamber. In: Companion Proceedings of the The Web Conference 2018, pp. 1007‚Äì\\n1014 (2018)\\n[36] Schulze, H., Hohner, J., Greipl, S., Girgnhuber, M., Desta, I., Rieger, D.: Far-right\\nconspiracy groups on fringe platforms: A longitudinal analysis of radicalization\\ndynamics on telegram. Convergence: The International Journal of Research into\\nNew Media Technologies 28(4), 1103‚Äì1126 (2022)\\n[37] Cinelli, M., Etta, G., Avalle, M., Quattrociocchi, A., Di Marco, N., Valensise, C.,\\n21Galeazzi, A., Quattrociocchi, W.: Conspiracy theories and social media platforms.\\nCurrent Opinion in Psychology 47, 101407 (2022)\\n[38] Horta Ribeiro, M., Hosseinmardi, H., West, R., Watts, D.J.: Deplatforming did\\nnot decrease parler users‚Äô activity on fringe social media. PNAS nexus 2(3), 035\\n(2023)\\n[39] Winkel, T., et al.: Fringe platforms: An analysis of contesting alternatives to the\\nmainstream social media platforms in a platformized public sphere. PhD thesis,\\nUtrecht University (2023)\\n[40] L¬¥ evy, P.: Collective intelligence, a civilisation: Towards a method of positive inter-\\npretation. International Journal of Politics, Culture, and Society 18, 189‚Äì198\\n(2005)\\n[41] Howcroft, D., Fitzgerald, B., et al. : From utopia to dystopia: the twin faces\\nof the internet. In: Information Systems: Current Issues and Future Changes,\\nProceedings of IFIP WG8, vol. 2, pp. 49‚Äì70 (1998). Citeseer\\n[42] Stefanov, P., Darwish, K., Atanasov, A., Nakov, P.: Predicting the topical stance\\nand political leaning of media using tweets. In: Proceedings of the 58th Annual\\nMeeting of the Association for Computational Linguistics, pp. 527‚Äì537 (2020)\\n[43] Flamino, J., Galeazzi, A., Feldman, S., Macy, M.W., Cross, B., Zhou, Z., Serafino,\\nM., Bovet, A., Makse, H.A., Szymanski, B.K.: Political polarization of news media\\nand influencers on twitter in the 2016 and 2020 us presidential elections. Nature\\nHuman Behaviour 7(6), 904‚Äì916 (2023)\\n[44] Patriots.Win - Bias and Credibility. Media Bias/Fact Check. Accessed: 2024-11-\\n04. https://mediabiasfactcheck.com/patriots-win-bias-and-credibility/\\n[45] Patriots.win. Anti-Defamation League. Accessed: 2024-11-04. https://www.adl.\\norg/glossary/patriotswin\\n[46] De Francisci Morales, G., Monti, C., Starnini, M.: No echo in the chambers of\\npolitical interactions on reddit. Scientific reports 11(1), 2818 (2021)\\n[47] Monti, C., D‚ÄôIgnazi, J., Starnini, M., De Francisci Morales, G.: Evidence of\\ndemographic rather than ideological segregation in news discussion on reddit. In:\\nProceedings of the ACM Web Conference 2023, pp. 2777‚Äì2786 (2023)\\n[48] Mustafaraj, E., Finn, S., Whitlock, C., Metaxas, P.T.: Vocal minority versus\\nsilent majority: Discovering the opionions of the long tail. In: 2011 IEEE Third\\nInternational Conference on Privacy, Security, Risk and Trust and 2011 IEEE\\nThird International Conference on Social Computing, pp. 103‚Äì110 (2011). IEEE\\n[49] Baumgartner, J., Zannettou, S., Keegan, B., Squire, M., Blackburn, J.: The\\n22pushshift reddit dataset. In: Proceedings of the International AAAI Conference\\non Web and Social Media, vol. 14, pp. 830‚Äì839 (2020)\\n[50] Trujillo, M., Gruppi, M., Buntain, C., Home, B.: What is BitChute? Character-\\nizing the ‚Äúfree speech‚Äù alternative to YouTube. arXiv (2004)\\n[51] Trujillo, M.Z., Gruppi, M., Buntain, C., Horne, B.D.: The mela bitchute dataset.\\nIn: Proceedings of the International AAAI Conference on Web and Social Media,\\nvol. 16, pp. 1342‚Äì1351 (2022)\\n[52] Romm, T.: Silicon valley elite and social media‚Äôs role in hate and radicalization\\nthat led to gab. The Washington Post (2018). Accessed: 2024-11-18\\n[53] Forward, T.: How gab weaponizes christian extremism and antisemitism. The\\nForward (2022). Accessed: 2024-11-18\\n[54] LaFrance, A.: What is gab? The Atlantic (2018). Accessed: 2024-11-18\\n[55] Staff, F.N.: Social media platform gab‚Äôs website and twitter account taken down.\\nFox News (2020). Accessed: 2024-11-18\\n[56] Ribeiro, M.H., Blackburn, J., Bradlyn, B., De Cristofaro, E., Stringhini, G., Long,\\nS., Greenberg, S., Zannettou, S.: The evolution of the manosphere across the web.\\nIn: Proceedings of the International AAAI Conference on Web and Social Media,\\nvol. 15, pp. 196‚Äì207 (2021)\\n[57] Gais, H., Cruz, F.: Far-right insurrectionists organized capitol siege on parler.\\nSouthern Poverty Law Center (2021). Accessed: 2024-11-18\\n[58] News, A.: Experts: Echo chambers in apps like parler, gab contributed to capitol\\nattack. ABC News (2021). Accessed: 2024-11-18\\n[59] Aliapoulios, M., Bevensee, E., Blackburn, J., Bradlyn, B., De Cristofaro, E.,\\nStringhini, G., Zannettou, S.: A large open dataset from the parler social net-\\nwork. In: Proceedings of the International AAAI Conference on Web and Social\\nMedia, vol. 15, pp. 943‚Äì951 (2021)\\n[60] Patel, J., Paudel, P., De Cristofaro, E., Stringhini, G., Blackburn, J.: idrama-\\nscored-2024: A dataset of the scored social media platform from 2020 to 2023.\\nIn: Proceedings of the International AAAI Conference on Web and Social Media,\\nvol. 18, pp. 2014‚Äì2024 (2024)\\n[61] Mekacher, A., Papasavva, A.: ‚Äù i can‚Äôt keep it up.‚Äù a dataset from the defunct\\nvoat. co news aggregator. In: Proceedings of the International AAAI Conference\\non Web and Social Media, vol. 16, pp. 1302‚Äì1311 (2022)\\n[62] Serrano, M. ¬¥A., BoguÀú n¬¥ a, M.: Weighted configuration model. arXiv preprint cond-\\nmat/0501750 (2005)\\n23[63] Stroud, N.J.: Polarization and partisan selective exposure. Journal of communi-\\ncation 60(3), 556‚Äì576 (2010)\\n24',\n",
       " 'CTRAPS: CTAP Client Impersonation and API Confusion on FIDO2\\nMarco Casagrande\\nEURECOM\\nmarco.casagrande@eurecom.frDaniele Antonioli\\nEURECOM\\ndaniele.antonioli@eurecom.fr\\nAbstract\\nFIDO2 is the standard technology for single-factor and\\nsecond-factor authentication. It is specified in an open stan-\\ndard, including the WebAuthn and CTAP application layer\\nprotocols. We focus on CTAP, which allows FIDO2 clients\\nand hardware authenticators to communicate. No prior work\\nhas explored the CTAP Authenticator API, a critical protocol-\\nlevel attack surface that deals with credential creation, dele-\\ntion, and management. We address this gap by presenting the\\nfirst security and privacy evaluation of the CTAP Authenti-\\ncator API. We uncover two classes of protocol-level attacks\\non CTAP that we call CTRAPS. The client impersonation\\n(CI) attacks exploit the lack of client authentication to tamper\\nwith FIDO2 authenticators. They include zero-click attacks\\ncapable of deleting FIDO2 credentials, including passkeys,\\nwithout user interaction. The API confusion (AC) attacks\\nabuse the lack of protocol API enforcements and confound\\nFIDO2 authenticators, clients, and unaware users into calling\\nunwanted CTAP APIs while thinking they are calling legiti-\\nmate ones. For example, if a victim thinks he is authenticating\\nto a website, they are deleting their credentials. The presented\\neleven attacks are conducted either in proximity or remotely\\nand are effective regardless of the underlying CTAP transport\\n(USB, NFC, or BLE). We detail the eight vulnerabilities in the\\nCTAP specification, enabling the CTRAPS attacks. Six are\\nnovel and include unauthenticated CTAP clients and trackable\\nFIDO2 credentials. We release CTRAPS , an original toolkit,\\nto analyze CTAP and conduct the CTRAPS attacks. We con-\\nfirm the attacks‚Äô practicality on a large scale by exploiting six\\npopular authenticators, including a FIPS-certified one from\\nYubico, Feitian, SoloKeys, and Google, and ten widely used\\nrelying parties, such as Microsoft, Apple, GitHub, and Face-\\nbook. We present eight practical and backward-compliant\\ncountermeasures to fix the attacks and their root causes. We\\nresponsibly disclosed our findings to the FIDO alliance and\\nthe affected vendors.1 Introduction\\nFast IDentity Online v2 (FIDO2) is the de-facto standard\\nfor single-factor (passwordless) and second-factor (2FA) au-\\nthentication. Google, Dropbox, and GitHub [44] designed\\nFIDO to offer a practical and scalable solution for authen-\\ntication. FIDO has been widely adopted by industries and\\norganizations, including Google, Microsoft, and the US gov-\\nernment [29]. Market forecasts predict the FIDO market to\\nrapidly grow from USD 230.6 million in 2022 to USD 598.6\\nmillion in 2031 [60]. Yubico, a FIDO authenticator market\\nleader, sold more than 22 million YubiKey authenticators [66].\\nThis growth will continue because of the recent industry-\\nwide push towards single-factor passkey-based authentica-\\ntion [21, 30, 56].\\nFIDO2 involves three entities: an authenticator that gener-\\nates and asserts possession of authentication credentials (e.g.,\\npublic-private key pairs), a relying party that authenticates the\\nuser (e.g., challenge-response protocol based on credentials),\\nand a client who wants to authenticate to the relying party and\\nmanages the communication between the authenticator and\\nthe relying party. Typically, the authenticator is a dongle, the\\nrelying party is a web server, and the client is a web browser\\nor a mobile app.\\nThe authenticator and the client communicate using the\\nClient to Authenticator Protocol (CTAP) . CTAP works at the\\napplication-layer and is transported over Universal Serial Bus\\n(USB), Near Field Communication (NFC), or Bluetooth Low\\nEnergy (BLE). CTAP exposes to the client the CTAP Au-\\nthenticator API , usable to interact with the authenticator, e.g.,\\ncredential creation, management, and deletion. These API\\ncalls might require User Verification ( UV) and User Presence\\n(UP) authorization. The latest CTAP protocol specification is\\nthe version 2.2 [3].\\nThis work focuses on the CTAP protocol and its security\\nand privacy guarantees. There are only a few research studies\\nabout CTAP. The authors of [10] performed a provable secu-\\nrity analysis on CTAP, highlighting unauthenticated DH key\\nexchange. In a follow-up work [11], they proposed an imper-\\n1arXiv:2412.02349v1  [cs.CR]  3 Dec 2024sonation attack exploiting CTAP to register an authenticator\\nwith an arbitrary relying party. The authors in [34] propose a\\nMitM privacy leak attack on CTAP based on unauthenticated\\nDH. Other works target the authenticator with fault injection\\nand side channel physical attacks [43, 55]. The literature on\\nWebAuthn is extensive, featuring, among others, misbinding,\\nmisauthentication, session hijacking, cookie theft, social en-\\ngineering, and rogue authenticator attacks [38, 42, 46, 50, 65]\\n(see Section 10 for more FIDO2 related work).\\nNo prior work investigated the CTAP Authenticator API .\\nThis API is a critical protocol-level attack surface as it enables\\nthe creation, management, and deletion of FIDO2 credentials\\nand the administration of FIDO2 authenticators. FIDO2 cre-\\ndentials are essential for security and privacy as they authorize\\naccess to sensitive online services, including social media,\\nbanking, data sharing, e-commerce, etc. A protocol-level at-\\ntack on the CTAP Authenticator API would enable access\\nand tamper with any FIDO2 credential stored on any FIDO2\\nauthenticator, regardless of the authenticator‚Äôs hardware and\\nsoftware details. Hence, it is crucial to assess the API‚Äôs ex-\\npected security and privacy properties and whether they hold\\nthem in practice.\\nWe fill this gap by presenting the first security and privacy\\nassessment of the CTAP Authenticator APIs. We uncover two\\nclasses of protocol-level attacks on CTAP we call CTRAPS .\\nTheclient impersonation (CI) attacks exploit the lack of client\\nauthentication to tamper with a victim authenticator. Some of\\nthe attacks are zero-click (i.e., not require user interaction),\\nwhile others are one-click (i.e., require expected user inter-\\naction). The API confusion (AC) attacks abuse the lack of\\nprotocol API enforcements and confound a FIDO2 authen-\\nticator, a client, and an unaware user into calling unwanted\\nCTAP Authenticator APIs while thinking they are calling le-\\ngitimate ones (e.g., the user thinks he is authenticating to a\\nwebsite but he is instead deleting his credentials). In total, we\\npresent five CI and seven AC attacks.\\nThe attacks are conducted in proximity (malicious FIDO2\\ndevice close to the victim) or remotely (malicious app in-\\nstalled on the victim‚Äôs phone). The AC attacks require a MitM\\nposition, while the CI attacks target the authenticator. Unlike\\nprior work, they do notrequire physical access to the au-\\nthenticator, a compromised client, or side channel and fault\\ninjection [43, 47]. Moreover, the attacks are stealthy because\\nthey employ CTAP-compliant API calls and do not require\\nunexpected user interactions (unlike phishing [61] or other\\ndeception techniques [50]).\\nThe CTRAPS attacks have a critical andwidespread im-\\npact on the FIDO2 ecosystem. They are critical as they enable\\nthe violate security, privacy, and availability of FIDO2 de-\\nvices. For example, a CI or an AC attacker can (remotely)\\nfactory reset an authenticator, deleting all FIDO2 credentials\\nand locking out the victim from the related service. More-\\nover, despite targeting CTAP, the attacks have an impact on\\nFIDO2 relying parties, e.g., they can delete non-discoverablecredentials stored on the relying party using WebAuthn. They\\nare widespread as they exploit protocol-level vulnerabilities\\nin the CTAP application-layer protocol. Hence, they can be\\nconducted against any FIDO2 device regardless of the CTAP\\ntransport (USB, NFC, and BLE).\\nThe isolate eight vulnerabilities in the CTAP specification\\nenables the CTRAPS attacks. Six of them are novel within\\nFIDO2 and include unauthenticated CTAP clients, trackable\\nFIDO2 credentials, and weak authorization (of destructive\\nAPI calls). The vulnerabilities are severe as they affect any\\nFIDO2 authenticator and client implementing any CTAP ver-\\nsion, including the latest CTAP2.2 draft. They also indirectly\\naffect FIDO2 relying parties, as we will explain later. More-\\nover, we find a implementation flaw on Yubico authenticator\\nfirmware, allowing the leak of sensitive data and user tracking\\n(CVE-2024-35311). We disclosed it to Yubico, who fixed it.\\nWe present CTRAPS , a new toolkit to experiment with CTAP\\nand conduct the CTRAPS attacks. The toolkit has three mod-\\nules: CTAP testbed, malicious CTAP clients, and Wireshark\\ndissectors. The testbed allows virtual clients and relying par-\\nties to test a FIDO2 authenticator locally and safely over\\nCTAP. The malicious clients include CI and AC proximity\\nand remote attack implementations that can be tested on real-\\nworld devices. For example, it ships an Android app and Prox-\\nmark3 scripts to test the CI attacks over NFC. The dissectors\\nmodule includes an extended FIDO2 dissector for Wireshark,\\nadding new and valuable packet information such as status\\ncodes and support for credential management.\\nWe demonstrate the practicality of the attacks by suc-\\ncessfully evaluating them on popular FIDO2 authenticators,\\nclients, and relying parties and conducting them over differ-\\nent CTAP transports (USB, NFC). We attack six authentica-\\ntorsfrom Yubico (including a FIPS-compliant one), Feitian,\\nSoloKeys, and Google. We conducted the attacks over USB\\nand NFC. We also exploit ten relying parties offering passkeys\\nand second-factor authentication, including Microsoft, Apple,\\nGitHub, and Facebook.\\nTo fix the CTRAPS attacks and their flaws, we design\\neight countermeasures that are backward-compliant with the\\nCTAP standard. The fixes include CTAP client authentica-\\ntion, stricter authorization requirements for destructive APIs,\\nintroduce a dedicated PIN for destructive operations (e.g.,\\ncredential deletion), and rotate user identifiers and credentials\\nto mitigate user tracking. They are practical as they rely on\\nmechanisms already available on the authenticator (e.g., PIN\\nand LED) and do not require adding extra hardware (e.g., an\\nextra display).\\nWe summarize our contributions as follows:\\n‚Ä¢We perform the first assessment of the CTAP Authenti-\\ncator API. We unveil two classes of CTAP protocol-level\\nattacks: CI and AC. The attacks compromise the secu-\\nrity, privacy, and availability of the FIDO2 ecosystem.\\nFor instance, they (remotely) delete FIDO2 credentials,\\n2track users via FIDO2 credentials, and DoS authenti-\\ncators. They are enabled by eight CTAP protocol level\\nvulnerabilities, six of which are new.\\n‚Ä¢We provide a toolkit to evaluate the CTAP Authenticator\\nAPI surface and test our attacks locally in a virtual envi-\\nronment or on actual devices. We successfully evaluate\\nour attacks against popular FIDO2 devices. We exploit\\nsix authenticators, two transports, and ten relying parties.\\nThe affected vendors include key FIDO2 players like\\nGoogle, Apple, Microsoft, and Yubico.\\n‚Ä¢We fix the attacks and their root causes by propos-\\ning eight practical and backward-compliant counter-\\nmeasures. We responsibly disclosed our findings to the\\nFIDO2 Alliance and affected vendors.\\nResponsible Disclosure We responsibly disclosed our find-\\nings to the FIDO Alliance in November 2023 [7]. They ac-\\nknowledged our report, provided feedback in May 2024, and\\nshared it with their members. In December 2023, we reported\\nour findings to the affected authenticator manufacturers (i.e.,\\nYubico, Feitian, SoloKeys, and Google). Google assigned pri-\\nority P2 and severity S2 to our report. Yubico acknowledged\\nthe implementation bug we found, pushed a fix in production,\\npublished a security advisory [70], and created CVE-2024-\\n35311 [69]. The other manufacturers acknowledged the report\\nwithout commenting on it. We also contacted Apple and Mi-\\ncrosoft regarding their weak credential protection policy that\\nfacilitates user tracking and profiling. They responded that\\nour report has no security implications for their products.\\nEthics and Availability We conducted our experiments\\nethically. We evaluated our authenticators and accounts. We\\ndid not collect personal data or involve third parties. We will\\nopen source our contributions, including the CTRAPS toolkit,\\nafter responsible disclosure with the FIDO Alliance, the man-\\nufacturers, and the relying parties.\\n2 Background and System Model\\nWe introduce the FIDO2 standard, its underlying Client To\\nAuthenticator Protocol (CTAP), and our system model.\\n2.1 FIDO2\\nFIDO2 [4] is an open standard for user authentication based\\nonasymmetric cryptography and curated by the FIDO Al-\\nliance. Four entities compose the FIDO2 ecosystem: an au-\\nthenticator, a client, a user, and a relying party. In a typical\\nscenario, a user connects his authenticator to the client to\\naccess an online service hosted by a relying party.\\nThe FIDO2 specification includes the WebAuthn and CTAP\\napplication-layer protocols. WebAuthn provides a secure and\\nprivate communication channel between a relying party and a\\nclient, and its latest version is WebAuthnL2 [62]. CTAP, thefocus of this work, enables a secure and private connection\\nbetween an authenticator and a client via the CTAP Authen-\\nticator API. For example, calling MakeCred registers a new\\ncredential, and GetAssertion authenticates an existing one.\\nA FIDO2 credential is a key pair used to sign and ver-\\nify challenges by applying standard cryptographic tech-\\nniques, such as the Elliptic Curve Digital Signature Algorithm\\n(ECDSA). Access to the private key of a FIDO2 credential\\nis safeguarded by encryption using a credential master key\\nunique to each authenticator and securely stored within the\\nauthenticator‚Äôs Secure Element. FIDO2 credentials can be\\ndiscoverable or non-discoverable. Discoverable credentials,\\nalso known as passkeys, are stored on the authenticator and\\nused for passwordless authentication. Non-discoverable cre-\\ndentials are stored on the web by the relying party and used\\nfor multi-factor authentication.\\nA FIDO2 credential is bound to three identifiers: the cre-\\ndential identifier (CredId), the relying party identifier (RpId),\\nand the user identifier (UserId). CredId is derived from the cre-\\ndential master key and uniquely identifies a credential. Before\\ndeleting a credential, the client needs to specify a CredId. The\\nRpId identifies a relying party, usually coincides with its ori-\\ngin (e.g., login.microsoft.com ), and should be considered\\npublic. A relying party randomly generates a UserId when a\\nuser creates his first credential and associates the UserId to\\nall credentials generated by that user. At registration time, a\\nrelying party can attach additional data to a credential, includ-\\ning sensitive or personally identifying information, using the\\noptional CredBlob FIDO2 extension.\\n2.2 CTAP\\nAs part of the FIDO2 standard, CTAP has considerably\\nevolved over time. CTAP1, also known as FIDO U2F (Uni-\\nversal 2nd Factor), provides phishing-resistant 2FA. CTAP2.0\\nmaintains backward compatibility with CTAP1 while intro-\\nducing passwordless authentication. CTAP2.1 [1] adds the\\ncredential protection policy, discoverable credential manage-\\nment (i.e., the CredMgmt API), and biometric authentication.\\nThe draft for CTAP2.2 [3] is the latest available version, of-\\nfering new features such as support for hybrid authenticators\\nequipped with cameras to scan QR codes.\\nCTAP relies on two core user authorization mechanisms to\\nsecure API calls: (i) User Verification (UV) , which requires\\nthe user to enter a PIN or biometric data, and (ii) User Pres-\\nence (UP) , which requires the user to press a button on the\\nauthenticator or to bring it into the client‚Äôs NFC range. Table 1\\nshows the most common CTAP Authenticator APIs and their\\nUVandUPrequirements. We describe each API:\\nMC:MakeCred registers a new credential bound to an online\\naccount with a relying party.\\nGA:GetAssertion authenticates to a relying party by prov-\\ning possession of a credential.\\n3Table 1: CTAP Authenticator API entries, short names (SN),\\nUVandUPauthorization requirements and support for sub-\\ncommands. Yes1: depends on the client and relying party\\nconfiguration, Yes2: depends on API subcommand. In the\\nCTAP standard, MakeCred is called MakeCredential and\\nCredMgmt is called CredentialManagement .\\nCTAP API SN UV UP Subcmd\\nMakeCred MC Yes Yes No\\nGetAssertion GA Yes1Yes1Yes\\nCredMgmt CM Yes No Yes\\nClientPin CP Yes2No Yes\\nReset Re No Yes No\\nSelection Se No Yes No\\nGetInfo GI No No No\\nCM:CredentialMgmt manages the authenticator‚Äôs discover-\\nable credentials (e.g., enumerate, modify, and delete).\\nCP:ClientPin handles UVbased on a user PIN to be sub-\\nmitted via the client‚Äôs UI.\\nRe:Reset factory resets the authenticator (i.e., wipes all\\ndiscoverable and non-discoverable credentials by re-\\ngenerating the credential master key).\\nSe:Selection selects an authenticator to operate among\\nthe available ones.\\nGI:GetInfo returns the authenticator‚Äôs details (e.g., manu-\\nfacturer, transports, extensions, and settings).\\nCTAP offers other optional security and privacy mecha-\\nnisms. The authorization requirements for GetAssertion\\ndepend on the client and relying party configuration. A client\\ncan specify the option up=false to skip UP. At registration\\ntime, a relying party can enforce access control by specifying\\na credential protection policy via the optional CredProtect\\nextension. However, the default policy skips UV, weakening\\nprivacy protection. Authenticators may also feature additional\\nsecurity mechanisms unrelated to CTAP, such as the FIDO\\nauthenticator certification level [5] and the FIPS [51] certifi-\\ncation.\\nTheGetAssertion ,CredMgmt , and ClientPin APIs of-\\nfer multiple functionalities through API subcommands. For\\nexample, CredMgmt(GetCredsData) returns the amount of\\nstored discoverable credentials and CredMgmt(DelCreds)\\ndeletes all discoverable credentials. Compared to their origi-\\nnal API, some API subcommands have more relaxed require-\\nments. For example, ClientPin(KeyAgreement) requests\\nthe authenticator‚Äôs public key without requiring UV.\\nCTAP WebAuthn Relying \\nParty Authenticator Client UV UP\\nClient CTAP \\nUser \\nMitM \\nFigure 1: CTRAPS threat model. The user authenticates to\\nthe relying party using a client (e.g., browser) and an authenti-\\ncator (hardware dongle). The user, when needed, grants UP by\\npressing a button on the authenticator and UV by submitting\\na PIN to the client. We study two attacker models: (i) a client\\nimpersonation attacker targeting the authenticator over CTAP\\n(left), (ii) a MitM attacker in the CTAP channel between the\\nauthenticator and the client.\\n2.3 System Model\\nFigure 1 shows the standard FIDO2 system model [4], com-\\nposed by an authenticator, a client, and a relying party. The\\nuser connects the authenticator to the client to authenticate on\\na service hosted by the relying party. The entities support up\\nto CTAP2.2 and WebAuthnL2 (i.e., the latest and supposedly\\nmost secure FIDO2 protocols). We describe each entity in\\ndetail.\\nAuthenticator The authenticator is a FIDO2 roaming au-\\nthenticator: a physical device carried around by the user that\\ncan be connected to the client (e.g., a USB/NFC dongle). The\\nauthenticator runs a CTAP server that exposes the CTAP Au-\\nthenticator API. The API is accessible over USB, NFC, and\\nBLE, which are the standard CTAP transports. The authenti-\\ncator supports FIDO2‚Äôs UP(e.g., via a button press) and UV\\n(e.g., via a user PIN) user authorization mechanisms. It stores\\ndiscoverable credentials and the credential master key.\\nClient The client is a FIDO2 client handling the commu-\\nnication between the authenticator and the relying party. It\\nexposes a CTAP client to the authenticator and a WebAuthn\\nclient to the relying party. The client could be a web browser,\\na mobile app for Android [26] or iOS [27], or a command line\\ntool like the Yubico CLI [68].\\nRelying party The relying party is an online service that\\nrelies on FIDO2 passwordless or multi-factor authentication.\\nIt runs a WebAuthn server that responds to FIDO2 registration\\nand authentication requests over TLS from the client. The\\nrelying party stores non-discoverable credentials, and user and\\ncredential identifiers. Offline operations on the authenticator,\\nlike deleting discoverable credentials, indirectly affect the\\n4relying party by making the user unable to log into their\\nonline service.\\nUser The user owns an authenticator and a device running\\nthe client, e.g., a YubiKey and a laptop. He utilizes his authen-\\nticator to register FIDO2 credentials and authenticate to the\\nassociated relying party. To do so, he connects his authentica-\\ntor to the client and provides UVandUP, if necessary. The\\nuser locally manages the authenticator via the client without\\nconnecting to a relying party. For example, he can check his\\ndiscoverable credentials and change the authenticator‚Äôs PIN.\\n3 CTRAPS Client Impersonation Attacks\\n3.1 Introduction and Motivation\\nWe unveil the four CTRAPS Client Impersonation (CI) at-\\ntacks exploiting the CTAP Authenticator API. The attacks\\nfactory reset the authenticator via the Reset API, track the\\nuser via GetAssertion , lock the authenticator via ClientPi\\nn, and profile the authenticator via GetInfo . They exploit five\\nprotocol-level vulnerabilities described later in Section 5.1\\nand the unrealistic FIDO reference threat model we discuss\\nin Section 8.2.\\nThe attacks advance the state of the art. Prior CI attacks\\nrequired: (i) to trick the user to obtain authorization [38], (ii)\\na compromised CTAP client (e.g., malicious browser exten-\\nsion) [11], (iii) or physical access to the authenticator [65].\\nOur attacks instead are: (i) zero-click , as we bypass UVand\\nUPauthorizations which require user interaction; (ii) require\\nno client compromise as they are conducted from an attacker-\\ncontroller device (e.g., an NFC reader); and (iii) require no\\nphysical access but rely on a proximity-based attacker con-\\nducting the attacks over NFC. Next, we introduce the CI\\nattacker model and describe the attacks.\\n3.2 CI Attacker Model\\nThe CI attacker model assumes an adversary impersonating a\\nCTAP client to an authenticator, as shown in Figure 1. The\\nattacker is in proximity (e.g., an NFC reader) or can connect\\nto the authenticator remotely (e.g., a mobile app with Internet\\naccess). The attacker can send legitimate CTAP commands\\nto the authenticator. She cannot modify the authenticator‚Äôs\\nfirmware or compromise a legitimate FIDO2 client and rely-\\ning party. She has no physical access to the authenticator.\\nThe attacker goal is to compromise the authenticator secu-\\nrity,privacy , and availability by exploiting the CTAP Authen-\\nticator API (introduced in Section 2). For example, she wants\\nto tamper with discoverable credentials stored on the authen-\\nticator (security), track a user via the authenticator (privacy),\\nor DoS the authenticator (availability).\\nAuthenticator Attacker\\nReset, UP\\nNFC bypasses User Presence (UP)\\nDeletes\\nallcreds\\nResets\\nsettings\\nand data\\nResetOKFigure 2: CI1attack. Factory reset authenticator via Reset .\\nWhile in NFC range, the attacker calls the Reset API. Over\\nNFC, the authenticator skips UPand instantly factory resets,\\ndeleting its discoverable and non-discoverable credentials.\\n3.3 CI Attacks Description\\nWe describe the four CI attacks. We label them CI 1, CI 2, CI 3,\\nand CI 4.\\nCI1: Factory reset authenticator In CI 1, the attacker\\nabuses the Reset API to factory reset an authenticator, as\\nshown in Figure 2, deleting discoverable and non-discoverable\\ncredentials, PIN, user preferences, and stored data. The at-\\ntacker connects to the authenticator and, despite not authenti-\\ncating, issues a factory reset command (requiring UP). She\\nbypasses the UPcheck as, according to the CTAP standard,\\nconnecting over NFC implies user presence. This novel trick\\nresults in a zero-click factory reset over NFC. Instead, CI 1is\\na one-click factory reset when deployed over USB, as the UP\\nbypass is only available to the NFC transport. A factory reset\\nwipes out all credentials, as it erases the credential master key\\nnecessary for decryption. It also deletes the authenticator‚Äôs\\nsettings, including the PIN, user preferences, and stored data.\\nThen, the authenticator confirms the successful reset.\\nCI2: Track user from credentials In CI 2, instead of us-\\ningGetAssertion API for authentication, the attacker ex-\\nploits it to leak identifying information and track the user,\\nas shown in Figure 3. The attacker does not require UVas\\nshe only targets relying parties that register credentials using\\nthe weak and default CredProtect=UVOptional default pol-\\nicy, such as Microsoft and Apple. She does not require UP\\neither, as her GetAssertion command contains the up=false\\noption. As a result, she achieves a zero-click leak of all cre-\\ndential and user identifiers registered with specific relying\\nparties. With the identifiers, she fingerprints the users and\\ntracks them over multiple connections by performing CI 2\\neach time and looking for matching fingerprints. CI 2is ef-\\nfective even on credentials protected by stronger credential\\nprotection policies (i.e., CredProtect=UVRequired andCred-\\nProtect=UVOptionalWithCredIDList ), but requires UVor the\\nknowledge of credential identifiers.\\n5Authenticator Attacker\\nRpIdList using CredProtect=UVOptional\\nGA, RpIdList, up=false\\nGAOK, CredIdList, UserIdList\\nFingerprintList =\\nHASH(CredIdList, UserIdList)Figure 3: CI2attack. Track user from credentials via\\nGetAssertion . The attacker connects to the authenticator\\nand calls the GetAssertion API ( GAin the figure). She skips\\nUVby targeting relying parties using the weak and default\\nCredProtect default policy and skips UPby passing up=false .\\nThe authenticator returns a list of credential and user identi-\\nfiers used by the attacker to fingerprint the authenticator and\\ntrack the user.\\nCI3: Force authenticator lockout In CI 3, the attacker\\nabuses the ClientPin , which protects the authenticator from\\nPIN brute-forcing, to lock the authenticator or even force a\\nfactory reset. Through the GetPinToken subcommand, she\\nsubmits several wrong PIN guesses in a row to the authen-\\nticator. After three wrong guesses, the device enters a soft\\nlock mode preventing further actions until a reboot (i.e., leav-\\ning and re-entering a client‚Äôs NFC range or detaching and\\nre-attaching to a USB port). After a maximum number of\\nfailed PIN attempts (CTAP mandates eight), the authentica-\\ntor enters a hard lock mode that is only restorable through a\\nfactory reset, which wipes out all credentials and can lead to\\naccount loss.\\nCI4: Profile authenticator In CI 4, the attacker calls Ge\\ntInfo to leak the authenticator details as a stepping stone\\nto more advanced attacks, profile the user, and track him in\\nfuture connections, and assess whether the authenticator is\\nvulnerable to an implementation-specific attack like [69], The\\nleaked details include the manufacturer, model, and FIDO2\\nversion, and the supported algorithms, transports, options, and\\nextensions. The authenticator also discloses user settings, such\\nas FIDO2 being disabled over a specific transport.\\n4 CTRAPS API Confusion Attacks\\n4.1 Introduction and Motivation\\nWe present the seven CTRAPS API Confusion (AC) attacks\\ntaking advantage of a novel attack technique we define as API\\nconfusion .\\nAPI confusion tricks a client, an authenticator, and their\\nuser into calling an unwanted CTAP Authenticator API. The\\nunwanted API call must have the same or lower UVandUPrequirements of the confounded API. This technique is very\\neffective as it does not require social engineering [61] or other\\ndeception techniques [50] to trick the user into calling a bad\\nAPI. The user cannot detect ongoing API confusion because,\\nunlike prior attacks, he only performs expected actions (e.g.,\\ndoes not grant UVif the API he calls does not require it).\\nThe attacks exploit protocol-level vulnerabilities we outline\\nin Section 5.1 and the FIDO reference threat model in Sec-\\ntion 8.2.\\nThe AC attacks represent a new class of attacks that has\\nnot been explored. As shown in Table 7, prior work on CTAP\\neavesdropped on unencrypted CTAP traffic and exploited the\\nunauthenticated Diffie-Hellman in a MitM attack but did not\\ninspect the Authenticator API. Physical access and side chan-\\nnel attacks targeted the authenticator to leak its credential mas-\\nter key, while the remaining works put CTAP on the sidelines,\\ntargeting WebAuthn instead with authenticator rebinding and\\nsession hijacking. Next, we introduce our attacker model and\\ndescribe the AC attacks.\\n4.2 AC Attacker Model\\nThe AC attacker model assumes a man-in-the-middle (MitM)\\nattacker between the authenticator and the client, as shown in\\nFigure 1. The attacker is either in proximity to the authenti-\\ncator (e.g., a NFC skimmer) or can contact the authenticator\\nfrom remote (e.g., using a remotely controllable USB hub).\\nShe maintains stealthiness by not triggering unexpected be-\\nhaviors. For example, she calls APIs when the user is oper-\\nating the authenticator and does not require extra UVand\\nUPauthorizations. She cannot modify the authenticator‚Äôs\\nfirmware or compromise a legitimate FIDO2 client and rely-\\ning party. She has no physical access to the authenticator.\\nThe attacker goal is to violate the authenticator security ,\\nprivacy , and availability by exploiting the CTAP Authenti-\\ncator API (introduced in Section 2). For example, she wants\\nto leak and delete the discoverable credentials stored in the\\nauthenticator, including passkeys (security), track a user via\\nthe authenticator (privacy), or DoS the authenticator (avail-\\nability).\\n4.3 AC Technique and Combinations\\nThe seven AC attacks rely on the API confusion attack tech-\\nnique. The attacker intercepts a call to API A and changes (i.e.,\\nconfounds) it to API B . The adversary only requires that API B\\nhas the same or lower UVandUPauthorization requirements\\nthan API A . The AC technique has six steps:\\n1.The user calls API A through the client. The API might\\nrequire UVand/or UP.\\n2.If required by API A , the attacker obtains UVby exe-\\ncuting the CTAP PIN/UV authentication protocol v1\\n(viaClientPin ). The user inputs the PIN on the client,\\n6Table 2: There are 49 ways to perform AC against 7 CTAP\\nAuthenticator APIs. The user intends to call API A , instead is\\ntricked into calling API B .‚úì1: proximity-based attacker, ‚úì2:\\ndefault CredProtect=UVOptional if credential protection is\\nenabled, n/a: not applicable.\\nCM Re GA MC CP Se GI\\nMC ‚úì ‚úì ‚úì n/a ‚úì ‚úì ‚úì\\nGA ‚úì ‚úì n/a ‚úì ‚úì ‚úì ‚úì\\nCM n/a ‚úì1‚úì ‚úì1‚úì ‚úì ‚úì\\nCP ‚úì ‚úì1‚úì ‚úì1n/a ‚úì ‚úì\\nRe n/a n/a ‚úì2n/a ‚úì ‚úì ‚úì\\nSe n/a ‚úì ‚úì2n/a ‚úì n/a ‚úì\\nGI n/a ‚úì1‚úì2‚úì ‚úì ‚úì n/a\\nTotal 3 6 6 4 6 6 6\\nwhich encrypts it and submits it to the authenticator. The\\nauthenticator responds with an encrypted User Verifica-\\ntion Token (UVT) that will be attached to any API call\\nrequiring UV.\\n3.The attacker calls API B rather than API A based on the\\nAC combinations in Table 2.\\n4.If required by API A , the attacker obtains UPfrom the\\nuser, who is unable to realize he is under attack. The\\nattacker can only obtain UPonce, as multiple requests\\nwould alarm the user. This step is skipped over NFC as\\nproximity implies UP.\\n5.The authenticator executes API B and returns a success\\nmessage.\\n6.The attacker informs the victim via the CTAP client\\nthatAPI A was successfully executed with compatible\\nauthorizations,\\nThe AC strategy is effective on 7CTAP Authenticator\\nAPIS and provides 49ways to confound the victim as shown\\nin Table 2. Multiple ( API A ,API B ) pairs achieve the same\\ngoal. The amount of available pairs depends on their UVand\\nUPrequirements and, in the case of AC 3, also on the Cred-\\nProtect policy. The first column lists seven APIs the user\\nintends to call ( API A ), and the remaining columns represent\\nthe API called by the attacker ( API B ). For instance, AC 1is\\navailable whenever the user calls MakeCred ,GetAssertion ,\\norClientPin , confounding the call to CredMgmt . Some com-\\nbinations are only feasible by a proximity-based attacker or\\nunder the default CredProtect policy. An API cannot be con-\\nfounded with itself or APIs with incompatible authorization\\nrequirements.\\nAuthenticator Attacker User\\nAPI A, UV\\nUser VeriÔ¨Åcation (UV)\\nCM(GetCredsData) , UV\\nStoredCredsAmount\\nCM(EnumRps) , UV\\nRpIdList\\nCM(EnumCreds) , UV, RpIdList\\nCredIdList\\nCM(DelCreds) , UV, CredIdList\\nDeletes all\\ndisc. creds\\nCM(DelCreds) OK API AOKFigure 4: AC 1attack. Delete discoverable credentials at-\\ntack with proximity. The user intends to call API A , requiring\\nUVbut not necessarily UP. For example, GetAssertion ,\\nClientPin , orMakeCred . The attacker obtains UVfrom the\\nunsuspecting user. Instead of API A , she calls CredMgmt (CM\\nin the figure). She executes four CredMgmt subcommands,\\nwhich list and then delete all discoverable credentials on the\\nauthenticator.\\n4.4 AC Attacks Description\\nWe describe the seven AC attacks. We label them AC 1, AC 2,\\nAC3, AC 4, AC 5, AC 6, and AC 7. The attacks are related to the\\nsecond to last column of Table 2. AC 1exploits all possible\\nways to call CM, AC 2to call Re, and so on.\\nAC1: Delete discoverable credentials In AC 1, the attacker\\nabuses the CredMgmt API to delete the discoverable creden-\\ntials on the authenticator, as shown in Figure 4. The user in-\\ntends to call API A , which requires UVbut not necessarily UP,\\nsuch as GetAssertion ,ClientPin , orMakeCred . Instead,\\nthe attacker executes four separate CredMgmt subcommands,\\nnone of which require UP. First, she checks the existence of\\ndiscoverable credentials to erase (StoredCredsAmount) via Cr\\nedMgmt(GetCredsMetadata) . Second, she retrieves the list\\nof relying parties stored on the authenticator (RpIdList) via\\nCredMgmt(EnumRps) . Third, she uses RpIdList to retrieve\\nthe list of stored credential identifiers (CredIdList) via CredM\\ngmt(EnumCreds) . Fourth, she uses CredIdList to delete all\\ndiscoverable credentials via CredMgmt(DelCreds) . Finally,\\nshe falsely returns API A OK to the user.\\nAC 2: Factory reset authenticator In AC 2, the attacker\\nexploits the Reset API to factory reset the authenticator,\\nsimilar to CI 1. Since Reset over USB requires UP, but not\\nUV, an attacker can confound MakeCred ,GetAssertion , and\\nSelection into a Reset call. An attacker over NFC, able to\\nbypass UP, can also confound CredMgmt ,ClientPin , and G\\netInfo .\\n7AC 3: Track user from credentials In AC 3, the attacker\\nmisuses the GetAssertion API to leak unique identifiers\\nas fingerprints and track the user, similar to CI 2. She can\\nconfound MakeCred ,CredMgmt , and ClientPin into a GetAs\\nsertion call, if she wants to access credentials protected by\\ntheCredProtect=UVRequired orCredProtect=UVOptional\\nWithCredIDList policies. Additionally, the attacker can also\\nconfound Reset ,Selection , and GetInfo if she only wants\\nto access credentials protected by the weak CredProtect=UV\\nOptional default policy.\\nAC 4: Fill authenticator credential storage In AC 4, the\\nattacker repeatedly calls MakeCred to register new discover-\\nable credentials, until the authenticator‚Äôs credential storage is\\nfull. She exploits the rk=true option to enforce the generation\\nof discoverable credentials over non-discoverable ones. A full\\nstorage compromises the authenticator‚Äôs availability as the\\nuser cannot register new discoverable credentials.\\nAC 5: Force authenticator lockout In AC 5, the attacker\\nabuses the ClientPin API to lock the authenticator and force\\na mandatory factory reset, similar to CI 3. Although Clien\\ntPin requires UV, the attacker wants to fail multiple PIN\\nattempts (i.e., she does not need UV). Consequently, she can\\nconfound any API call into a ClientPin call, as she does not\\nneed authorization.\\nAC 6: Authenticator DoS In AC 6, the attacker calls the S\\nelection API to trigger an unwanted UPcheck to keep the\\nauthenticator busy and to deny availability. Since the attacker\\ncan detect when the busy state ends (e.g., the user pressed the\\nauthenticator‚Äôs button or 30 seconds have passed), she can\\nprolong the attack indefinitely.\\nAC 7: Profile authenticator In AC 7, the attacker invokes\\ntheGetInfo API to retrieve the authenticator‚Äôs details. Then,\\nsimilar to CI 4, she uses this information as a stepping stone to\\nother attacks, to track the user, or to check whether the authen-\\nticator is vulnerable to implementation-specific attacks [69].\\nNot requiring UVorUP, the attacker can confound any API\\ncall into a GetInfo call.\\n5 CTRAPS Vulnerabilities and Impact\\n5.1 Vulnerabilities\\nThe CTRAPS attacks are enabled by eight vulnerabilities we\\ndiscovered in the CTAP specification. Six of them are novel,\\nwhile V2 was discussed in the misbinding and misauthentica-\\ntion attacks of [64]. Still, this is the first work exploiting V2\\nvia AC. We describe them and how they map to the CI and\\nAC attacks presented in Sections 3 and 4.\\nV1: Unauthenticated CTAP client The CTAP client does\\nnot authenticate to the user, the authenticator, or the relying\\nparty. FIDO2 clients (and, by extension, CTAP clients) have\\nno identity, meaning that the authenticator cannot distinguish\\nan official client developed by its manufacturer from a third-\\nparty client. The authenticator has no choice but to trust anyconnecting client, including compromised ones.\\nV2: No authenticator feedback about API calls Despite\\nhaving an LED, the authenticator does not provide the user\\nwith visual feedback when invoking APIs or granting UV\\nandUP. The user cannot confirm whether the intended API\\nhas been called (or confounded) and which API utilized the\\ngranted UVandUPauthorizations.\\nV3: NFC range provides UPAuthenticators inside the\\nNFC range of a FIDO2 client automatically obtain UPwithout\\nthe user pressing a button on the authenticator. Bypassing UP\\ndecreases the security protections of MakeCredential ,G\\netAssertion , and Reset to only UVor no authorization\\nrequirement at all.\\nV4: Weak destructive APIs authorization Destructive\\nAPI calls, such as credential deletion ( CredMgmt ) or authen-\\nticator factory reset ( Reset ), and non-destructive ones, like\\nauthentication ( GetAssertion ) are authorized by the same\\nUVPIN. For example, the user intends to authenticate (non-\\ndestructive) and provides UVandUP, instead the attacker\\nfactory resets the authenticator (destructive).\\nV5: User trackable via CredId and UserId Discoverable\\ncredentials contain static andunique CredId and UserId, ex-\\nploitable to reliably track users. These values can be obtained\\nwithout UVorUPvia the GetAssertion API. We note that\\nthe more credentials are stored in the authenticator, the more\\nthis vulnerability is effective as each credential contributes to\\nthe user fingerprint.\\nV6:Reset does not require UVTheReset should en-\\nforce stricter authorization requirements. Despite being de-\\nstructive, the Reset API does only require UP. Anyone close\\nto the authenticator can obtain UPby pressing its button or\\nbeing in NFC range.\\nV7:CredMgmt does not require UPTheCredMgmt API\\ndoes notrequire UP, but only UV, to delete discoverable\\ncredentials. The user submits the PIN only once but can delete\\nany number of credentials. In contrast, creating N credentials\\nalso requires N UPchecks.\\nV8:Selection is usable for DoS TheSelection API\\ncan be used to continuously request UPchecks to the Authen-\\nticator and put it in an unresponsive state as the API is not\\nrate limited.\\nMapping to attacks Table 3 maps the eight vulnerabilities\\n(columns) to the eleven CTRAPS attacks. V1 is the root\\ncause of CI and AC attacks, as it allows an untrusted client\\nor a MitM attacker to connect to the authenticator without\\nauthenticating it. V2 provides stealthiness to AC attacks since,\\nwithout visual feedback, the user cannot confirm whether the\\nAPI he called has been confounded. Due to V3, CI 1and CI 2\\nover NCF require zero clicks instead of one (i.e., UPcheck).\\nV3 also unlocks several new API confusion combination, such\\nasGetInfo (no authorization requirement) into Reset (V3\\nbypasses UP.\\nV4 allows to perform the destructive CI 2, CI 3, AC 1, AC 2,\\nand AC 5even when the user calls a non-destructive API, such\\n8Table 3: Mapping the eight vulnerabilities (columns) to the\\nfour CI and seven AC attacks.\\nV1 V2 V3 V4 V5 V6 V7 V8\\nCI1‚úì ‚úì ‚úì ‚úó ‚úó ‚úì ‚úó ‚úó\\nCI2‚úì ‚úì ‚úì ‚úó ‚úì ‚úó ‚úó ‚úó\\nCI3‚úì ‚úì ‚úó ‚úó ‚úó ‚úó ‚úó ‚úó\\nCI4‚úì ‚úì ‚úó ‚úó ‚úó ‚úó ‚úó ‚úó\\nAC1‚úì ‚úì ‚úó ‚úì ‚úó ‚úó ‚úì ‚úó\\nAC2‚úì ‚úì ‚úì ‚úì ‚úó ‚úì ‚úó ‚úó\\nAC3‚úì ‚úì ‚úì ‚úó ‚úì ‚úó ‚úó ‚úó\\nAC4‚úì ‚úì ‚úì ‚úó ‚úó ‚úó ‚úó ‚úó\\nAC5‚úì ‚úì ‚úó ‚úì ‚úó ‚úó ‚úó ‚úó\\nAC6‚úì ‚úì ‚úó ‚úó ‚úó ‚úó ‚úó ‚úì\\nAC7‚úì ‚úì ‚úó ‚úó ‚úó ‚úó ‚úó ‚úó\\nasSelection . V5 enables the usage of identifiers as persis-\\ntent fingerprints, resulting in two user tracking attacks (i.e.,\\nCI2and AC 3). V6 allows for a zero-click factory reset attack\\n(i.e., CI 1) over NFC. V7 allows for a one-click credential dele-\\ntion attack (i.e., AC 1). V8 enables a persistent and reliable\\nDoS attack on the authenticator (i.e., AC 6).\\n5.2 Impact\\nThe eleven CTRAPS attacks break the security, privacy, and\\navailability of the FIDO2 ecosystem, with widespread and\\nsevere implications. We support our claims with the experi-\\nmental results in Section 7.\\nOur attacks exploit protocol-level CTAP vulnerabilities,\\nworking regardless of the transport and the implementation\\ndetails of the authenticator, the client, and the relying party.\\nThey threaten millions of authenticators in the wild, and their\\nrespective users, with destructive and scalable attacks. Being\\nat the protocol-level, the root causes are complex to fix, and\\nmost authenticators (e.g., Yubikeys) do not support firmware\\nupdates anyways, leaving them vulnerable forever.\\nOur attacks affect users, authenticators, relying parties, and\\nclients alike. For example, by erasing credentials, we remove\\nthe ability to perform web authentication from users and rely-\\ning parties, by locking the authenticator, we prevent its usage,\\nand by performing API confusion, we trick the client into be-\\nlieving that a confounded API call was executed legitimately\\ninstead.\\nAnyone can deploy our practical andlow-cost attacks, as\\nthey require minimal equipment, such as an NFC reader or a\\nsmartphone. However, their realistic outcome causes concrete\\nand immediate damage with limited or no user interaction\\nand notice. For instance, we lost access to our test Google\\nand Apple ID accounts because we could not pass 2FA after\\ndeleting our credentials with AC 1. No prior attack achieved\\nsimilar goals, such as credential tampering and user tracking.The CI attacks over NFC do not require compromising the\\nuser device or any interaction. They work out-of-the-box on\\nany NFC-enabled device. The AC attacks are stealthy and\\npersistent. They do not trigger abnormal behavior, only asking\\nforUVandUPwhen the user expects it. As a direct conse-\\nquence, the user will likely trigger the attacks on multiple\\noccasions, not realizing the lingering threat. CI and AC at-\\ntacks can even be combined . For example, the attacker can\\nobtain a fingerprint with AC 7and track the user with CI 2.\\n6 Implementation\\nWe present CTRAPS , a novel toolkit implementing the\\nCTRAPS attacks. It has three modules: a CTAP testbed (Sec-\\ntion 6.1), the malicious CTAP clients (Section 6.2), and the\\nWireshark dissectors (Section 6.3). We describe how we im-\\nplemented each module and their novelties.\\n6.1 CTAP Testbed\\nOur CTAP testbed is a Python3 module that includes a virtual\\nrelying party with a customizable WebAuthn server and a\\nvirtual client talking to a real authenticator over CTAP and\\na virtual relying party over WebAuthn. The testbed has two\\nbenefits: (i) It allows to perform experiments locally, safely,\\nand without an Internet connection, without interacting and\\ntampering with real relying parties. (ii) It allows to simulate\\ndifferent attack scenarios by realistically replicating client\\nand server configurations, including the credential protection\\npolicy ( CredProtect ). Our virtual relying party and client are\\nextensions of the python-fido2 [67], Yubico‚Äôs open-source\\nPython library for FIDO2.\\nThe virtual relying party is implemented as a customizable\\nWebAuthn server running on our testbed, and not on a network\\nlike an ordinary relying party. We extended existing code by\\nadding standard relying party templates and fast customiza-\\ntion of the server‚Äôs parameters. For example, we implemented\\na template imitating Microsoft relying party, including the\\nsame identifier (i.e., login.microsoft.com ). Options such as the\\ncredential protection policy and the attestation verification.\\nThe virtual relying party was instrumental in setting up the\\nauthenticator in the correct state for our CI and AC attacks\\n(e.g., registering credentials with a weak protection policy)\\nquickly and without involving real relying parties. A separate\\nmalicious client deploys the actual attacks.\\nThe virtual CTAP client offers a low-level API to interact\\nwith the authenticator and the virtual relying party. It can send\\nCTAP commands in any order with custom or even malformed\\npayload values. We extended the existing code by adding a set\\nof common and abnormal use cases useful for security testing\\nand vulnerability assessment. For example, authentication to\\na relying party or a mass credential registration that fills the\\nentire memory of the authenticator. Each use case includes\\nappropriate and configurable settings and capabilities, such as\\n9Figure 5: Our malicious Android CTAP client performing\\nzero-click CI 2over NFC, leaking relying party and credential\\nidentifiers.\\nthe CTAP authorization requirements, challenge, origin, and\\nCTAP rkandupoptions.\\nThe testbed requires the user to authorize direct access to\\nthe authenticator. Linux requires adding extra udev rules, Ma-\\ncOS asks to accept a notification on the screen, and Windows\\nneeds admin privileges. However, this behavior is expected by\\nthe user and does not hinder the effectiveness of our attacks\\n(i.e., the user approves the notification on MacOS as he is the\\none plugging in the authenticator and wanting to use it).\\n6.2 Malicious CTAP Clients\\nIn our toolkit, we develop three malicious CTAP clients, de-\\nploying the CI and AC attacks. We also release five video\\ndemonstrations of our attacks on real authenticators.\\nWe implemented the CI attacks on a malicious CTAP client\\nrunning on an Android app. Currently, only attacks over NFC\\nare fully functional, but we plan to add the USB transport in\\nthe future, as it only requires engineering effort.\\nThe app supports both a proximity and a remote CI attack\\nmode. In the proximity mode, the attacker controls the app\\nand constantly scans for authenticators to connect to and\\nexploit. For example, the app can perform CI 2to track the\\nuser with leaked identifiers, as shown in Figure 5. In the\\nremote mode, the app spoofs a legitimate NFC app, enticing\\nthe user to connect their authenticator (e.g., by asking for\\nFIDO2 authentication). We include in the app a CBOR parser\\nfor CTAP that we wrote.\\nThe app does not need root privileges and asks at runtime\\nfor the dangerous android.permission.NFC , required to\\naccess the android.nfc [24] API. However, this is not an\\nissue, as we are not trying to hide the app‚Äôs NFC capabilities.\\nThe app also needs the standard install-time android.permi\\nssion.INTERNET to exfiltrate the data collected through CI 2\\nand CI 4.\\nWe also deployed the CI attacks using a Proxmark3 [54],\\nan open-source and programmable development kit for NFCTable 4: Details about the six authenticators we attack. All\\nauthenticators support USB and NFC, except OpenSK, which\\nonly supports USB. FVer: firmware version, OSF: open-\\nsource firmware, DCr: discoverable credentials.\\nAuthenticator Manuf Year FVer OSF DCr\\nYubiKey 5 Yubico 2018 5.2.7 No 25\\nYubiKey 5 FIPS Yubico 2021 5.4.3 No 25\\nFeitian K9 Feitian 2016 3.3.01 No 50\\nSolo V1 SoloKeys 2018 4.1.5 Yes 50\\nSolo V2 Hacker SoloKeys 2021 2.964 Yes 50\\nOpenSK Google 2023 2.1 Yes 150\\n(RFID). By equipping the Proxmark3 with a long-range high-\\nfrequency antenna, we were able to extend the reach of our\\nattacks. The long-range antenna has an indicative range of\\n100 to 120 millimeters, as opposed to the 40 to 85 millimeters\\nof the built-in antenna. We developed the CI attacks in a\\ncustom Lua script using the Proxmark3 ISO14443 Type A\\nmodule (i.e., read14a ). The Proxmark communicates with\\nthe authenticator utilizing the same APDU commands we use\\nfor the Android app.\\nWe implemented the AC attacks in an Electron app simu-\\nlating a MitM attacker. The app runs a malicious CTAP client\\ndeveloped as a Javascript library. Our code imports the node-\\nhidmodule to access the USB HID traffic. It scans for local\\nHID devices, identifies the authenticator from their properties\\n(e.g., the product and manufacturer fields), and connects to it.\\nThe client sends binary data over USB to the authenticator,\\nachieving the same results as MitM attackers.\\n6.3 FIDO Wireshark Dissectors\\nWe extended the official Wireshark FIDO2 dissectors [71]\\nwith new and valuable features. We add support for the Cred\\nMgmt API. We include parsers for WAITING andPROCESSING\\nkeepalive status codes that identify when authenticators are\\nunavailable waiting for UP. We parse the authenticator‚Äôs capa-\\nbilities in the CTAPHID_INIT message, which helps test AC 7.\\nWe provide an improved way to display CTAP data when\\ndissecting CTAPHID (USB) and ISO7816/ISO14443 (NFC).\\nFinally, we add missing vendor and product identifiers to the\\ndissector tables. We release the dissectors as a Lua script (i.e.,\\nfido2-dissectors.lua ) that can be found in our toolkit.\\n7 Evaluation\\nWe successfully evaluated our eleven attacks against sixpopu-\\nlar authenticators from Yubico, Feitian, SoloKeys, and Google\\nsupporting CTAP over USB and NFC, and tenwell-known\\nrelying parties, including Microsoft, Apple, GitHub, and Face-\\nbook. Our evaluation includes relying parties because their\\n10Table 5: CI and AC attacks on six authenticators. The first column lists the authenticators‚Äô names. The remaining columns report\\nour four CI and seven AC attacks on CTAP. ‚úì: attack is effective on the authenticator, n/a: not applicable as the authenticator\\ndoes not implement the Selection API.\\nAuthenticator CI 1CI2CI3CI4AC1AC2AC3AC4AC5AC6AC7\\nYubiKey 5 ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì n/a ‚úì\\nYubiKey 5 FIPS ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì n/a ‚úì\\nFeitian K9 ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì n/a ‚úì\\nSolo V1 ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì n/a ‚úì\\nSolo V2 Hacker ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì\\nOpenSK ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì\\nCI1: Factory reset authenticator, CI2: Track user from credentials, CI3: Force authenticator lockout, CI4: Profile authenticator, AC1: Delete\\ndiscoverable credentials, AC2: Factory reset authenticator, AC3: Track user from credentials, AC4: Fill authenticator credential storage, AC5:\\nForce authenticator lockout, AC6:Authenticator DoS, AC7: Profile authenticator.\\ncredential protection policy affects our attacks (i.e., by requir-\\ningUV). At the same time, operations on the authenticator\\n(i.e., deleting credentials) indirectly affect relying parties due\\nto the user losing access to his online accounts.\\nWe tested the CI attacks using the malicious Android CTAP\\nclient and the Proxmark3, and the AC attacks using the Elec-\\ntron app that simulates a MitM position in the USB traffic\\n(for more details about our toolkit, refer to Section 6).\\nWe present our evaluation setup and results.\\n7.1 Setup\\nAuthenticators We evaluate sixpopular FIDO2 authentica-\\ntors. Table 4 shows their technical details. The YubiKey 5\\nNFC, YubiKey 5 NFC FIPS, and Feitian NFC K9 are closed-\\nsource and do not support firmware updates, The Solo V1,\\nSolo V2 Hacker, and Open Security Key (OpenSK) have an\\nopen-source firmware (OSF) that we updated to their latest\\nversion. The authenticators support USB and NFC, except for\\nOpenSK, which has an NFC module but supports only USB.\\nThe Solo V1 requires a button press to activate NFC. We did\\nnot find any FIDO2 authenticator supporting BLE.\\nThe authenticators store a maximum of 25 (Yubico), 50\\n(Feitian and SoloKeys), or 150 (OpenSK) discoverable cre-\\ndentials. The YubiKey 5 FIPS is FIPS140-2 compliant. Hence,\\nit should provide strong security guarantees. We run OpenSK\\non an NRF52840 dongle, but the attacks could be tested on\\nany board supporting OpenSK.\\nRelying parties Our list of relying parties covers perva-\\nsive and heterogeneous online services, including software as\\na service, social, gaming, cryptographic signing, authentica-\\ntion, and cloud storage. We registered our authenticators with\\ntenFIDO2 relying parties: Adobe, Apple, DocuSign, Face-\\nbook, GitHub, Hancock, Microsoft, NVidia, Synology, and\\nVault Vision. Some offer Single Sign-On (SSO), enabling ac-\\ncess to multiple services. For example, a single set of FIDO2\\ncredentials can log into Microsoft, OneDrive, Outlook, and\\nMinecraft. Consequently, erasing a single credential has awidespread effect on multiple online services.\\nCTRAPS toolkit We ran the CTAP testbed and the Elec-\\ntron app on a Dell Inspiron 15 3502 laptop (OSes: Ubuntu\\n22.04.3 LTS and Windows 11 Home) and on a MacBook Pro\\nM1 (OS: MacOs Ventura 13.4). We connected the authentica-\\ntor by plugging it into a USB port.\\nWe installed the malicious CTAP client Android app on a\\nrooted Google Pixel 2 (OS: Android 11), a non-rooted RealMe\\n11 Pro (OS: Android 14) and Xiaomi Redmi Plus 5 (OS:\\nAndroid 8.1). Root access was not required and did not affect\\nthe attack in any way. We equipped a Proxmark3 RDV4 with a\\nlong-range high-frequency antenna to extend the range of our\\nNFC attacks. We connected the authenticator to the Android\\napp and the Proxmark3 by placing it within NFC range.\\n7.2 Authenticators Results\\nTable 5 shows the evaluation results for the CI and AC attacks.\\nWe tested the CI and AC attacks in proximity (NFC) and re-\\nmotely (malicious app). We successfully ran the attacks on all\\ntested authenticators, including a recent and FIPS-compliant\\nYubiKey. Four AC6attacks are not applicable as the related au-\\nthenticators do not support the Selection API. As expected,\\nsince we attack CTAP at the protocol level, the attacks are\\neffective regardless of the CTAP transport (i.e., USB or NFC)\\nor the authenticator‚Äôs software and hardware.\\nWe also found a CredMgmt implementation vulnerability\\non the YubiKey 5 and YubiKey 5 FIPS, now tracked with\\nCVE-2024-35311 [69]. An authenticator should not execute\\nthe subcommand CredMgmt(EnumRpsGetNextRp) , unless C\\nredMgmt(EnumRpsBegin) was called first. This is relevant\\nbecause the latter requires UV, the former does not. How-\\never, Yubico failed to implement this requirement due to an\\nincorrect handling of the authenticator‚Äôs state. We exploit\\nthis flaw to perform a zero-click relying party leakage . We\\nrepeatedly call CredMgmt(EnumRpsGetNextRp) , which does\\nnot require UVand is not affected by CredProtect , to reveal\\nall relying parties, except one, linked to the discoverable cre-\\n11Table 6: CTRAPS attacks on ten relying parties. The first and second columns list the relying parties‚Äô names and identifiers.\\nThe third column highlights whether they register discoverable (Disc, DiscWeak) or non-discoverable (NonDisc) credentials.\\nWe indicate with DiscWeak a relying party using the default and weak CredProtect=UVOptional policy. Columns four, five,\\nand six specify the effect of each attack. n/a: the attack is not applicable because the relying party currently does not support\\ndiscoverable credentials.\\nRp RpId Cred Delete Creds Track User DoS Authenticator\\nAdobe adobe.com Disc AC 1, CI 1, AC 2CI2, AC 3 AC4, CI 3, AC 5, AC 6\\nApple apple.com DiscWeak AC 1, CI 1, AC 2CI2, AC 3 AC4, CI 3, AC 5, AC 6\\nDocuSign account.docusign.com NonDisc CI 1, AC 2 n/a CI 3, AC 5, AC 6\\nFacebook facebook.com NonDisc CI 1, AC 2 n/a CI 3, AC 5, AC 6\\nGitHub github.com Disc AC 1, CI 1, AC 2CI2, AC 3 AC4, CI 3, AC 5, AC 6\\nHancock hancock.ink Disc AC 1, CI 1, AC 2CI2, AC 3 AC4, CI 3, AC 5, AC 6\\nMicrosoft login.microsoft.com DiscWeak AC 1, CI 1, AC 2CI2, AC 3 AC4, CI 3, AC 5, AC 6\\nNVidia login.nvgs.nvidia.com Disc AC 1, CI 1, AC 2CI2, AC 3 AC4, CI 3, AC 5, AC 6\\nSynology account.synology.com Disc AC 1, CI 1, AC 2CI2, AC 3 AC4, CI 3, AC 5, AC 6\\nVault Vision auth.vaultvision.com Disc AC 1, CI 1, AC 2CI2, AC 3 AC4, CI 3, AC 5, AC 6\\ndentials stored on the authenticator. Given a sufficiently large\\namount of relying parties, this information can also be used\\nto fingerprint and track the user.\\nDepending on their complexity, the proximity CI and AC\\nattacks required 50 to 500 milliseconds within the NFC range.\\nFor example, AC 1takes the longest because it involves signif-\\nicantly more CTAP messages than any other attack. Typically,\\na smartphone‚Äôs NFC range is four centimeters or less [8], but,\\non authenticators, we only achieved up to two centimeters\\nof range. The Proxmark3 built-in antenna also reached up\\nto two centimeters, which we extended up to six and a half\\ncentimeters with a long-range antenna. However, prior work\\ndemonstrated that, with specialized equipment, the NFC range\\ncan be extended up to 50 centimeters [41].\\nWe confirmed on all six authenticators that, as prescribed\\nby CTAP, a factory reset over USB can only be executed if\\nthe device has been plugged into the USB port within the last\\nten seconds. Therefore, we CI 1and AC 2over USB features\\nthis additional constraint.\\n7.3 Relying Parties Results\\nTable 6 shows that the CTRAPS attacks directly or indirectly\\naffect all evaluated relying parties, even without sending We-\\nbAuthn messages. We exploit eight relying parties supporting\\ndiscoverable credentials, including two using a weak Cred-\\nProtect policy, and two employing non-discoverable ones.\\nAC1, CI 1, and AC 2prevent access to a relying party via\\ncredential deletion. CI 2and AC 3utilize the user identifiers a\\nrelying party provides to track users. AC 4, CI 3, AC 5, and AC 6\\nindirectly prevent relying parties from authenticating regis-\\ntered authenticators. Since AC 1and AC 4target discoverable\\ncredentials, they do not apply to relying parties registering\\nnon-discoverable ones, like DocuSign and Facebook.Among the relying parties supporting discoverable creden-\\ntials, we found that Microsoft and Apple rely on the weak and\\ndefault CredProtect policy. As a consequence, performing\\nCI2and AC 3on them does not require UV.\\n8 Discussion\\n8.1 Comparison with prior FIDO attacks\\nTable 7 compares our work with relevant attacks on FIDO2.\\nThe CI attacks are the first client impersonation attacks on\\nFIDO2 (CTAP2+). While the AC attacks are the first API con-\\nfusion attack on FIDO2. The CI attacks are low-cost as they\\ndo not require a compromised client or user device or prior\\nknowledge of user secrets (e.g., credential identifiers). The\\nAC attacks have a moderate cost, as they require a man-in-the-\\nmiddle position between the authenticator and the client. The\\nCTRAPS attacks have a higher impact than most other pre-\\nvious attacks, as, for example, they can permanently destroy\\nall credentials and track users. We now directly compare our\\nwork with a selection of the attacks from Table 7.\\nIn [50], the authors presented WebAuthn deception attacks\\nthrough a terminal-based malware, redirecting the user to an\\nattacker-controlled authentication page and stealing creden-\\ntials via a keylogger. Our attacker model is weaker , relying\\nsolely on a compromised (CTAP) client instead of a malware,\\nkeylogger, and malicious browser session.\\nIn [10] and [11], the researchers formally verified FIDO2\\nand then demonstrated the feasibility of client impersonation\\nby exploiting the unauthenticated CTAP ECDH also discussed\\nin [34]. They required a compromised client (i.e., a browser)\\nrunning on the user‚Äôs device to decrypt the authenticator‚Äôs\\nPIN necessary for UV. In contrast, we focus on bypassing\\nor stealing UVandUP, granted by the user to other CTAP\\n12APIs, without needing to decrypt the PIN or resorting to social\\nengineering, UI deception, or other manipulative tactics [42].\\nThe authors of [65] examined attacks involving local (i.e.,\\nbrowser extension) and physical (i.e., temporary access to\\nthe authenticator) adversaries, including misbinding, MitM,\\nand session hijacking. They identify two protocol-level flaws\\nin FIDO2: a lack of confidentiality and integrity and broken\\nclone detection. In comparison, we evaluate a proximity-based\\nattacker (e.g., an NFC reader) never considered before in\\nFIDO2. Moreover, we find six new FIDO2 protocol-level\\nissues in addition to theirs.\\n8.2 FIDO2 Reference Threat Model Issues\\nFIDO2 has a non-normative reference threat model [2] that in-\\ncludes security assumptions, goals, and threats against clients,\\nauthenticators, and relying parties. We found three issues (IS1,\\nIS2, and IS3) with their threat model:\\nIS1: Unclear security boundaries The threat model\\npresents six broad security assumptions but then violates them\\nwhen discussing threats. For example, SA-4 states that the\\nFIDO user device and applications involved in a FIDO oper-\\nation act as trustworthy agents of the user. This implies that\\nthe FIDO client (e.g., the user‚Äôs browser or mobile app) must\\nbe inherently trusted. However, the threat model includes\\nthreats breaking SA-4 like T-1.2.1: FIDO client corruption\\nthat identifies an attacker with code execution on a FIDO\\nclient. Instead, security assumptions should hold to enable a\\nsecurity analyst to draw security boundaries (i.e., differentiate\\nwhat we trust from what can be attacked).\\nIS2: Proximity threats are missing Despite FIDO support-\\ning proximity transports like NFC and BLE, their reference\\nmodel classifies proximity-based threats as physical access,\\neven though these two categories have significant differences.\\nFor example, compared to physical access, the range of a prox-\\nimity attack can be extended. Hence, our proximity attacks,\\nwhich do not require physical access, cannot be accurately\\ndescribed within this reference threat model.\\nIS3: Security goals are narrow The threat model has nar-\\nrow security goals based on [18] (2006) and [14] (2012). The\\nsecurity goals focus on web authentication but overlook FIDO\\nclients and roaming authenticators. For example, there are no\\nsecurity goals for the Authenticator API (i.e., addressing all\\nAC attacks) or discoverable credentials (i.e., addressing AC 1,\\nCI1, and AC 2).\\n9 Countermeasures\\nWe present the design and evaluation of eight practical\\nand backward-compliant countermeasures fixing the eleven\\nCTRAPS attacks and their related eight vulnerabilities. Each\\ncountermeasure addresses a vulnerability (e.g., C1 fixes V1)\\nand reduces the CTAP attack surface. The countermeasuresare implementable as amendments to the FIDO2 standard or\\nas FIDO2 extensions. We describe each countermeasure.\\nC1: Trusted CTAP clients We address V1 by recommend-\\ning that FIDO provide a list of trusted CTAP clients. FIDO\\noffers several certifications, including the FIDO Functional\\nCertification [6] which only attests the interoperability of\\nclients, servers, and authenticators. We suggest extending this\\ncertification also to cover the trustworthiness of CTAP clients.\\nFor instance, FIDO could implement a Software Bill Of Ma-\\nterials (SBOM) solution to monitor trusted CTAP clients and\\ntheir vulnerabilities [63].\\nC2: Authenticator visual feedback We address V2 by\\nrequiring the authenticator to provide visual feedback of the\\ncalled APIs. For instance, the authenticator‚Äôs LED could blink\\nonce for non-destructive API calls and twice for destructive\\nones. The CTAP wink command, which blinks the LED, must\\nbe disabled during this visual feedback.\\nC3: User interaction for UPover NFC We address V3 by\\nrequiring user interaction during UPchecks over NFC. For\\nexample, the user could press a button on the authenticator to\\ngrant UPover NFC, similar to UPchecks over USB.\\nC4: Dedicated PIN for destructive APIs We address V6\\nby introducing a dedicated PIN to authorize destructive API\\ncalls (e.g., CredMgmt andReset ) and by repurposing the\\ncurrent PIN to authorize non-destructive API calls (e.g., Se\\nlection andGetInfo ). The new PIN should have the same\\nor stricter requirements as the non-destructive PIN (i.e., four\\nto sixty-three Unicode characters [1]).\\nC5: Dynamic and UV-protected CredId and UserId We\\naddress V5 by implementing dynamic CredId and UserId and\\nmandating CredProtect=UVRequired . CredId and UserId\\nshould rotate after a set amount of logins (e.g., every ten lo-\\ngins) or a time interval (e.g., once per month). Hence, we\\nraise the bar for user profiling and tracking attacks on authen-\\nticators. Currently, the user can indirectly change a CredId by\\ncalling MakeCred to generate a new credential for his account,\\nreplacing the old one. However, the user cannot change the\\nUserId, which the relying party determines and, based on our\\nexperience, remains fixed to the user account.\\nC6:Reset must require UVWe address V6 by requiring\\nUVto call Reset . Hence, the user must validate a factory\\nreset by entering a valid PIN.\\nC7:CredMgmt must require UPWe address V7 by re-\\nquiring UPto call CredMgmt . Hence, the user must authorize\\ncredential deletion one by one, making it impossible to delete\\nmultiple credentials with a single API call.\\nC8:CredMgmt must require UPWe address V8 by en-\\nforcing temporal rate limiting on Selection calls to a maxi-\\nmum of three calls within two minutes. We are not expecting\\nissues with our rate limiting, akin to the limiting already exist-\\ning for ClientPin(GetPinToken) , as a client typically calls\\nSelection once per session.\\nUsability We believe that the stronger security granted by\\nour countermeasures is worth the inevitable usability trade-\\n13Table 7: Recent attacks on FIDO. We assign each attack a cost and impact. For example, the cost for a MitM attacker is Mid, and\\nfor a proximity-based attacker is Low. Similarly, hijacking a session has a Mid impact, and permanently destroying credentials\\nhas a High impact.\\nAttack Class Protocol Transp Surface Impl Reqs Cost Impact\\nCTAP MitM [34] DH MitM CTAP2.0 All ClientPin ‚úó MitM Mid Mid\\nPrivacy leak [34] Eavesdropping CTAP2.0 All MakeCred ‚úó n/a Low Low\\nAuth rebind [34] Auth rebind WebAuthn All Creds.create ‚úó n/a High High\\nParallel session [34] Session hijack WebAuthn All Creds.get ‚úó n/a Mid Mid\\nEvil maid [47] Phys access n/a n/a Auth TEE ‚úó Phys access High High\\nTitan phone imp [47] Impersonation U2F BLE Android ‚úì Proximity Low Mid\\nTitan key imp [47] Impersonation U2F BLE Google Acc ‚úì Proximity Low Mid\\nAuth imp [11] DH MitM CTAP2.0/2.1 USB ClientPin ‚úì Mal browser Mid Mid\\nWeb MitM [11] Session hijack WebAuthn USB Creds.get ‚úì Mal browser Mid Mid\\nRogue key [11] Auth rebind WebAuthn USB Creds.create ‚úì Mal browser Mid High\\nFIDOLA [50] Session hijack WebAuthn USB Creds.get ‚úì Malware High Mid\\nCTRAPS CI Impersonation CTAP2.0/2.1/2.2 All Auth API ‚úì Proximity Low High\\nCTRAPS AC API confusion CTAP2.0/2.1/2.2 All Auth API ‚úì MitM Mid High\\noffs. C1, C2, and C8 do not affect usability. C5 only introduces\\none additional UVandUPcheck every time the credential\\nand user identifier need to rotate out (e.g., once per month),\\nbarely affecting usability. On the other hand, C4 requires the\\nuser to remember a second PIN, and C6 and C7 add more\\nauthorization requirements to Reset andCredMgmt . C3 also\\nadds user interaction when connecting to the authenticator\\nover NFC.\\nAdding a display We do not consider adding a display\\nto a roaming authenticator an optimal solution as it is not\\nbackward-compliant . Millions of deployed authenticators\\nwould remain vulnerable. Moreover, it would require sig-\\nnificant hardware and software modifications, such as adding\\na secure display, a display controller firmware, and a battery,\\nthat would introduce usability, performance, and cost issues.\\n10 Related Work\\nAttacks on FIDO(2) Researchers demonstrated practical\\nattacks on older FIDO versions, such as authenticator re-\\nbinding, parallel sessions, and multi-user attacks [37, 46],\\nUSB HID man-in-the-middle attacks [16], BLE pairing [15],\\nrelying party public key substitution [58], bypassing push-\\nbased 2FA [38], real-time phishing [61], and side chan-\\nnel attacks [39, 55]. FIDO2 was also found vulnerable to\\nside-channel attacks [47] and rogue key or impersonation\\nthreats [11]. Moreover, attacks on lower layers trusted by\\nFIDO2 were presented including IV reuse on Samsung Key-\\nstore [59]. No prior attack investigated API confusion on\\nCTAP, including its latest version.\\nFormal Analysis The formal analysis and verification com-\\nmunity extensively researched FIDO. The community for-\\nmally verified FIDO‚Äôs Universal Authentication Framework(UAF) [28, 53], FIDO2 (including its privacy, revocation, at-\\ntestation, and post-quantum crypto) [10, 12, 13, 35]. Yubico\\nproposed a key recovery mechanism based on a backup au-\\nthenticator that was proven secure using the asynchronous\\nremote key generation (ARKG) primitive [31]. The formal\\nanalyses are notcovering our CI and AC attacks.\\nExtensions FIDO supports extensions to add optional fea-\\ntures in a backward-compliant way. For instance, FeIDO [57]\\nproposes an extension to recover a FIDO2 credential using\\nan electronic identifier. Extensions are not secure by default,\\nand researchers proposed a fix to protect them against MitM\\nattacks [17]. We suggest to update the CTAP specification\\nrather than implementing our countermeasures as FIDO ex-\\ntensions that would be optional and insecure by design.\\nEnhancements Researchers proposed (cryptographic) en-\\nhancements to FIDO protocols. In [32], the authors present a\\nhybrid post-quantum signature scheme for FIDO2 and tested\\nit using OpenSK [33] (which we exploit in this work). In [36],\\nthe authors propose a global key revocation procedure for\\nWebAuthn that revokes credentials without communicating to\\neach individual relying party WebAuthn server. True2F [23]\\npresented a backdoor-resistant FIDO U2F design, protect-\\ning the authenticator from a malicious browser by requiring\\nthe authenticator interaction during every authentication, and\\nfrom fingerprinting by rate limiting credential registration.\\nProposed enhancements are notaddressing our attacks, which\\nare effective regardless of the FIDO2 cryptographic primi-\\ntives.\\nUsability Researchers performed extensive usability stud-\\nies on FIDO U2F [19, 20, 22, 45], FIDO2 roaming authenti-\\ncators [25, 52], passkeys [40], and cross-site 2FA [48]. Our\\npaper is orthogonal to usability studies.\\nSurveys There are several FIDO survey papers. In [9] the\\nauthors describe the evolution of FIDO protocols, security\\n14requirements, and adoption factors. In [49], the authors sur-\\nveyed the adoption of passwordless authentication among a\\nlarge user base, considering users‚Äô perceptions, acceptance,\\nand concern with single-factor authentication without pass-\\nwords. Our paper is orthogonal to surveys.\\n11 Conclusion\\nNo prior work assessed the CTAP Authenticator API, a crit-\\nical API exposed by a client to an authenticator to manage,\\ncreate, and delete credentials. We address this gap by pre-\\nsenting the first security and privacy evaluation of the CTAP\\nAuthenticator API. We uncover two classes of protocol-level\\nattacks capable of abusing the API. The CI attacks spoof a\\nCTAP client to a victim authenticator to factory reset, track,\\nand DoS the authenticator. The AC attacks utilize a MitM\\nposition to change user CTAP API calls to an API desired\\nby the attacker, potentially destructive while stealing their\\nauthorization. We deploy the first CTAP client impersonation\\nin FIDO2, enabling an attacker to call CTAP APIs without\\nauthorization or user interaction. We also introduce a novel\\nattack strategy called API confusion, which changes, without\\nuser consent, the API called by the user to an API chosen by\\nthe attacker.\\nWe uncover eleven new proximity-based and remote at-\\ntacks that can severely impact millions of FIDO2 users. For\\nexample, our attacks delete FIDO2 credentials and master\\nkeys (security breach) and track users through their creden-\\ntials (privacy breach). The attacks are effective on the entire\\nFIDO2 ecosystem as they target eight vulnerabilities we dis-\\ncovered in the CTAP specification. These flaws include the\\nlack of CTAP client authentication and improper API autho-\\nrizations. CTRAPS attacks are low-cost, as they do not require\\nspecialized or expensive equipment, and stealthy, as they do\\nnot trigger unexpected user interactions.\\nWe develop the CTRAPS toolkit to test our attacks with a\\nlow-cost setup and on a large scale. It includes a CTAP testbed\\nwith a virtual relying party and a virtual client, a CTAP client\\nNFC impersonator (i.e., malicious Proxmark scripts and An-\\ndroid NFC app), and enhanced Wireshark dissectors for CTAP.\\nWe successfully exploit six authenticators and ten relying par-\\nties from leading FIDO2 players such as Yubico, Feitian,\\nGoogle, Microsoft, and Apple. We develop eight effective and\\nlegacy-compliant countermeasures to fix our attacks and their\\nroot causes.\\nWe share three lessons we learned about FIDO2 credential\\nstorage andpasswordless-ness , which are valuable for the\\ncurrent transition from single-factor authentication to 2FA\\nand passkeys [21, 56]: (i) Being stored on the authenticator,\\nFIDO2 discoverable credentials are protected from third-party\\ndata breaches. However, this introduces new attacks that work\\nexclusively on discoverable credentials (i.e., AC 1, CI 2, AC 3,\\nand AC 4); (ii) FIDO2 users cannot prevent attacks targeting\\ndiscoverable credentials, as they cannot choose the type ofcredentials they register and their protection policies, decided\\nby the relying party and the client instead. (iii) The FIDO2\\ncore message is to steer away from passwords because they\\nare vulnerable to phishing. However, digging deeper, we real-\\nized that FIDO2 still relies on phishable mechanisms, even for\\npasswordless authentication. For instance, a passwordless cre-\\ndential is protected by an alphanumeric PIN (i.e., a phishable\\nsequence the user must remember).\\nAcknowledgments\\nWork funded by the European Union under grant agreement\\nno. 101070008 (ORSHIN project). Views and opinions ex-\\npressed are however those of the author(s) only and do not\\nnecessarily reflect those of the European Union. Neither the\\nEuropean Union nor the granting authority can be held re-\\nsponsible for them. Moreover, it has been partially supported\\nby the French National Research Agency under the France\\n2030 label (NF-HiSec ANR-22-PEFT-0009) and the Apri-\\ncot/ENCOPIA ANR MESRI-BMBF project (ANR-20-CYAL-\\n0001).\\nReferences\\n[1]FIDO Alliance. CTAP 2.1 Proposed Standard with\\nErrata. https://fidoalliance .org/specs/\\nfido-v2 .1-ps-20210615/fido-client-to-\\nauthenticator-protocol-v2 .1-ps-errata-\\n20220621 .html , 2022.\\n[2]FIDO Alliance. FIDO Security Reference, Review\\nDraft 23 May 2022. https://fidoalliance .org/\\nspecs/common-specs/fido-security-ref-v2 .1-\\nps-20220523 .pdf, 2022.\\n[3]FIDO Alliance. CTAP 2.2 Review Draft 01.\\nhttps://fidoalliance .org/specs/fido-v2 .2-\\nrd-20230321/fido-client-to-authenticator-\\nprotocol-v2 .2-rd-20230321 .html , 2023.\\n[4]FIDO Alliance. FIDO Alliance Specifications Overview.\\nhttps://fidoalliance .org/specifications ,\\n2024.\\n[5]FIDO Alliance. FIDO Certified Products.\\nhttps://fidoalliance .org/certification/\\nfido-certified-products/ , 2024.\\n[6]FIDO Alliance. FIDO Functional Certification.\\nhttps://fidoalliance .org/certification/\\nfunctional-certification/ , 2024.\\n[7]FIDO Alliance. FIDO Security Secretariat. https://\\nfidoalliance .org/certification/secretariat/ ,\\n2024.\\n15[8]Android. Near Field Communication (NFC)\\nOverview. https://developer .android .com/\\ndevelop/connectivity/nfc , 2024.\\n[9]Anna Angelogianni, Ilias Politis, and Christos Xenakis.\\nHow many FIDO protocols are needed? Surveying the\\ndesign, security and market perspectives. arXiv preprint\\narXiv:2107.00577 , 2021.\\n[10] Manuel Barbosa, Alexandra Boldyreva, Shan Chen, and\\nBogdan Warinschi. Provable security analysis of FIDO2.\\nInAdvances in Cryptology‚ÄìCRYPTO 2021: 41st Annual\\nInternational Cryptology Conference, CRYPTO 2021,\\nVirtual Event, August 16‚Äì20, 2021, Proceedings, Part\\nIII 41 , pages 125‚Äì156, 2021.\\n[11] Manuel Barbosa, Andr√© Cirne, and Lu√≠s Esqu√≠vel.\\nRogue key and impersonation attacks on FIDO2: From\\ntheory to practice. In Proceedings of the 18th Interna-\\ntional Conference on Availability, Reliability and Secu-\\nrity. Association for Computing Machinery, 2023.\\n[12] Nina Bindel, Cas Cremers, and Mang Zhao. FIDO2,\\nCTAP 2.1, and WebAuthn 2: Provable security and post-\\nquantum instantiation. In 2023 IEEE Symposium on Se-\\ncurity and Privacy (SP) , pages 1471‚Äì1490. IEEE, 2023.\\n[13] Nina Bindel, Nicolas Gama, Sandra Guasch, and Eyal\\nRonen. To attest or not to attest, this is the question‚Äì\\nProvable attestation in FIDO2. Cryptology ePrint\\nArchive , 2023.\\n[14] Joseph Bonneau, Cormac Herley, Paul C Van Oorschot,\\nand Frank Stajano. The quest to replace passwords: A\\nframework for comparative evaluation of web authenti-\\ncation schemes. In 2012 IEEE symposium on security\\nand privacy , pages 553‚Äì567. IEEE, 2012.\\n[15] Christiaan Brand. Advisory: Security Issue with\\nBluetooth Low Energy (BLE) Titan Security Keys.\\nhttps://security .googleblog .com/2019/05/\\ntitan-keys-update .html , 2019.\\n[16] Thanh Bui, Siddharth Prakash Rao, Markku Antikainen,\\nViswanathan Manihatty Bojan, and Tuomas Aura. Man-\\nin-the-Machine: Exploiting Ill-Secured Communication\\nInside the Computer. In 27th USENIX security sympo-\\nsium (USENIX Security 18) , pages 1511‚Äì1525, 2018.\\n[17] Andre B√ºttner and Nils Gruschka. Protecting FIDO\\nExtensions Against Man-in-the-Middle Attacks. In In-\\nternational Workshop on Emerging Technologies for Au-\\nthorization and Authentication , pages 70‚Äì87. Springer,\\n2022.\\n[18] Tsai Chwei-Shyong, Lee Cheng-Chi, and Min-Shiang\\nHwang. Password Authentication Schemes: Current\\nStatus and Key Issues. International Journal of Network\\nSecurity , 2006.[19] St√©phane Ciolino, Simon Parkin, and Paul Dunphy. Of\\nTwo Minds about Two-Factor: Understanding Every-\\nday FIDO/U2F Usability through Device Comparison\\nand Experience Sampling. In Fifteenth Symposium on\\nUsable Privacy and Security (SOUPS 2019) , pages 339‚Äì\\n356, 2019.\\n[20] Jessica Colnago, Summer Devlin, Maggie Oates, Chelse\\nSwoopes, Lujo Bauer, Lorrie Cranor, and Nicolas\\nChristin. ‚ÄúIt‚Äôs not actually that horrible‚Äù Exploring\\nAdoption of Two-Factor Authentication at a University.\\nInProceedings of the 2018 CHI Conference on Human\\nFactors in Computing Systems , pages 1‚Äì11, 2018.\\n[21] Mike Hanley (GitHub CSO). Securing mil-\\nlions of developers through 2FA. https:\\n//github .blog/2024-04-24-securing-millions-\\nof-developers-through-2fa/ , 2024.\\n[22] Sanchari Das, Andrew Dingman, and L Jean Camp. Why\\nJohnny doesn‚Äôt use two factor a two-phase usability\\nstudy of the FIDO U2F security key. In International\\nConference on Financial Cryptography and Data Secu-\\nrity, pages 160‚Äì179. Springer, 2018.\\n[23] Emma Dauterman, Henry Corrigan-Gibbs, David Maz-\\ni√®res, Dan Boneh, and Dominic Rizzo. True2F:\\nBackdoor-resistant authentication tokens. In 2019 IEEE\\nSymposium on Security and Privacy (SP) , pages 398‚Äì\\n416, 2019.\\n[24] Android developers. Android NFC basics.\\nhttps://developer .android .com/develop/\\nconnectivity/nfc/nfc , 2023.\\n[25] Florian M Farke, Lennart Lorenz, Theodor Schnitzler,\\nPhilipp Markert, and Markus D√ºrmuth. You still use the\\npassword after all‚ÄìExploring FIDO2 Security Keys in\\na Small Company. In Sixteenth Symposium on Usable\\nPrivacy and Security (SOUPS 2020) , pages 19‚Äì35, 2020.\\n[26] Feitian. Feitian Android App. https:\\n//play .google .com/store/apps/details?id=\\ncom .ft.entersafe .iepassmanager , 2022.\\n[27] Feitian. Feitian iOS App. https://apps .apple .com/\\nus/app/iepassmanager/id1504200260 , 2022.\\n[28] Haonan Feng, Hui Li, Xuesong Pan, Ziming Zhao, and\\nT Cactilab. A Formal Analysis of the FIDO UAF Proto-\\ncol. In Network & Distributed System Security Sympo-\\nsium (NDSS‚Äô21) , 2021.\\n[29] FIDO Alliance. U.S. General Services Ad-\\nministration‚Äôs Rollout of FIDO2 on login.gov.\\nhttps://fidoalliance .org/u-s-general-\\nservices-administrations-rollout-of-fido2-\\non-login-gov/ , 2023.\\n16[30] FIDO Alliance. FIDO Alliance Publishes\\nNew Specifications to Promote User Choice\\nand Enhanced UX for Passkeys. https:\\n//fidoalliance .org/fido-alliance-publishes-\\nnew-specifications-to-promote-user-choice-\\nand-enhanced-ux-for-passkeys , 2024.\\n[31] Nick Frymann, Daniel Gardham, Franziskus Kiefer,\\nEmil Lundberg, Mark Manulis, and Dain Nilsson. Asyn-\\nchronous Remote Key Generation: An Analysis of Yu-\\nbico‚Äôs Proposal for W3C WebAuthn. In Proceedings of\\nthe 2020 ACM SIGSAC Conference on Computer and\\nCommunications Security (CCS) , pages 939‚Äì954, 2020.\\n[32] Diana Ghinea, Fabian Kaczmarczyck, Jennifer Pullman,\\nJulien Cretin, Rafael Misoczki, Stefan K√∂lbl, Luca Inv-\\nernizzi, Elie Bursztein, and Jean-Michel Picod. Hybrid\\npost-quantum signatures in hardware security keys. In\\n4th ACNS Workshop on Secure Crytographic Implmen-\\ntation , 2023.\\n[33] Google. OpenSK: a Rust Implementation of a\\nFIDO2 Authenticator. https://github .com/google/\\nOpenSK , 2024.\\n[34] Jingjing Guan, Hui Li, Haisong Ye, and Ziming Zhao.\\nA Formal Analysis of the FIDO2 Protocols. In Eu-\\nropean Symposium on Research in Computer Security\\n(ESORICS) , pages 3‚Äì21, 2022.\\n[35] Lucjan Hanzlik, Julian Loss, and Benedikt Wagner. To-\\nken meets wallet: Formalizing privacy and revocation\\nfor FIDO2. In 2023 IEEE Symposium on Security and\\nPrivacy (SP) , pages 1491‚Äì1508. IEEE, 2023.\\n[36] Lucjan Hanzlik, Julian Loss, and Benedikt Wagner. To-\\nken meets Wallet: Formalizing Privacy and Revocation\\nfor FIDO2. In 2023 IEEE Symposium on Security and\\nPrivacy (SP) , pages 1491‚Äì1508, 2023.\\n[37] Kexin Hu and Zhenfeng Zhang. Security analysis of an\\nattractive online authentication standard: FIDO UAF\\nprotocol. China Communications , 13(12):189‚Äì198,\\n2016.\\n[38] Mohammed Jubur, Prakash Shrestha, Nitesh Saxena,\\nand Jay Prakash. Bypassing push-based second\\nfactor and passwordless authentication with human-\\nindistinguishable notifications. In Proceedings of the\\n2021 ACM Asia Conference on Computer and Commu-\\nnications Security , pages 447‚Äì461, 2021.\\n[39] Michal Kepkowski, Lucjan Hanzlik, Ian Wood, and Mo-\\nhamed Ali Kaafar. How Not to Handle Keys: Timing\\nAttacks on FIDO Authenticator Privacy. In Proceedings\\non Privacy Enhancing Technologies , volume 4, pages\\n705‚Äì726, 2022.[40] Michal Kepkowski, Maciej Machulak, Ian Wood, and\\nDali Kaafar. Challenges with Passwordless FIDO2 in\\nan Enterprise Setting: A Usability Study. arXiv preprint\\narXiv:2308.08096 , 2023.\\n[41] Ziv Kfir and Avishai Wool. Picking Virtual Pockets\\nusing Relay Attacks on Contactless Smartcard. In\\nFirst International Conference on Security and Privacy\\nfor Emerging Areas in Communications Networks (SE-\\nCURECOMM‚Äô05) , pages 47‚Äì58, 2005.\\n[42] Dhruv Kuchhal, Muhammad Saad, Adam Oest, and\\nFrank Li. Evaluating the Security Posture of Real-World\\nFIDO2 Deployments. In Proceedings of the ACM confer-\\nence on computer and communications security (CCS) ,\\n2023.\\n[43] Ninja Lab. A Side Journey to Titan. https://\\nninjalab .io/a-side-journey-to-titan , 2024.\\n[44] Juan Lang, Alexei Czeskis, Dirk Balfanz, Marius\\nSchilder, and Sampath Srinivas. Security keys: prac-\\ntical cryptographic second factors for the modern web.\\nInInternational Conference on Financial Cryptography\\nand Data Security , pages 422‚Äì440. Springer, 2016.\\n[45] Leona Lassak, Annika Hildebrandt, Maximilian Golla,\\nand Blase Ur. It‚Äôs Stored, Hopefully, on an Encrypted\\nServer: Mitigating Users‚Äô Misconceptions About FIDO2\\nBiometric WebAuthn. In 30th USENIX Security Sympo-\\nsium (USENIX Security 21) , pages 91‚Äì108, 2021.\\n[46] Hui Li, Xuesong Pan, Xinluo Wang, Haonan Feng, and\\nChengjie Shi. Authenticator rebinding attack of the UAF\\nprotocol on mobile devices. Wireless Communications\\nand Mobile Computing , 2020:1‚Äì14, 2020.\\n[47] Victor Lomne. An Overview Of The Security\\nOf Some Hardware FIDO(2) Tokens. https://\\nwww .youtube .com/watch?v=hpOp9X4sMaE , 2022.\\n[48] Sanam Ghorbani Lyastani, Michael Backes, and Sven\\nBugiel. A systematic study of the consistency of two-\\nfactor authentication user journeys on top-ranked web-\\nsites. In 30th Annual Network & Distributed System\\nSecurity Symposium (NDSS‚Äô23) , 2023.\\n[49] Sanam Ghorbani Lyastani, Michael Schilling, Michaela\\nNeumayr, Michael Backes, and Sven Bugiel. Is FIDO2\\nthe Kingslayer of User Authentication? A Comparative\\nUsability Study of FIDO2 Passwordless Authentication.\\nInIEEE Symposium on Security and Privacy , pages\\n268‚Äì285, 2020.\\n[50] Ahmed Tanvir Mahdad, Mohammed Jubur, and Nitesh\\nSaxena. Breaching Security Keys without Root: FIDO2\\n17Deception Attacks via Overlays exploiting Limited Dis-\\nplay Authenticators. In Proceedings of the ACM confer-\\nence on computer and communications security (CCS) ,\\n2024.\\n[51] National Institute of Standards and U.S. Depart-\\nment of Commerce Technology. NIST Special\\nPublication 800-63B, Digital Identity Guide-\\nlines, Authentication and Lifecycle Management.\\nhttps://pages .nist .gov/800-63-3/sp800-\\n63b .html#-5112-memorized-secret-verifiers ,\\n2017.\\n[52] Kentrell Owens, Olabode Anise, Amanda Krauss, and\\nBlase Ur. User Perceptions of the Usability and Security\\nof Smartphones as FIDO2 Roaming Authenticators. In\\nSeventeenth Symposium on Usable Privacy and Security\\n(SOUPS 2021) , pages 57‚Äì76, 2021.\\n[53] Olivier Pereira, Florentin Rochet, and Cyrille Wiedling.\\nFormal analysis of the FIDO 1.x protocol. In Founda-\\ntions and Practice of Security: 10th International Sym-\\nposium, FPS 2017, Nancy, France, October 23-25, 2017,\\nRevised Selected Papers 10 , pages 68‚Äì82. Springer,\\n2018.\\n[54] Proxmark. Proxmark RFID Tool. https://\\nproxmark .com, 2024.\\n[55] Thomas Roche, Victor Lomn√©, Camille Mutschler, and\\nLaurent Imbert. A Side Journey to Titan. In 30th\\nUSENIX Security Symposium (USENIX Security 21) ,\\npages 231‚Äì248, 2021.\\n[56] Google Safety and Security. The begin-\\nning of the end of the password. https:\\n//blog .google/technology/safety-security/\\nthe-beginning-of-the-end-of-the-password/ ,\\n2023.\\n[57] Fabian Schwarz, Khue Do, Gunnar Heide, Lucjan Hanz-\\nlik, and Christian Rossow. FeIDo: Recoverable FIDO2\\nTokens Using Electronic IDs. In Proceedings of the\\n2022 ACM SIGSAC Conference on Computer and Com-\\nmunications Security , pages 2581‚Äì2594, 2022.\\n[58] Michael Scott. FIDO‚ÄìThat Dog Won‚Äôt Hunt. In Security\\nand Privacy in New Computing Environments: EAI Con-\\nference, SPNCE 2020 , pages 255‚Äì264. Springer, 2021.\\n[59] Alon Shakevsky, Eyal Ronen, and Avishai Wool. Trust\\nDies in Darkness: Shedding Light on Samsung‚Äôs Trust-\\nZone Keymaster Design. In 31st USENIX Security Sym-\\nposium (USENIX Security 22) , pages 251‚Äì268, 2022.\\n[60] Transparency Market Research. FIDO\\nAuthentication Market Forecast. https:\\n//www .transparencymarketresearch .com/fido-\\nauthentication-market .html , 2023.[61] Enis Ulqinaku, Hala Assal, AbdelRahman Abdou, Sonia\\nChiasson, and Srdjan Capkun. Is Real-time Phishing\\nEliminated with FIDO? Social Engineering Downgrade\\nAttacks against FIDO Protocols. In 30th USENIX Se-\\ncurity Symposium (USENIX Security 21) , pages 3811‚Äì\\n3828, 2021.\\n[62] W3C. Web Authentication: An API for accessing Public\\nKey Credentials - Level 2. https://www .w3.org/TR/\\nwebauthn-2 , 2021.\\n[63] Wei Wu, Pu Wang, Lei Zhao, and Wei Jiang. An Intel-\\nligent Security Detection and Response Scheme Based\\non SBOM for Securing IoT Terminal devices. In 2023\\nIEEE 11th International Conference on Information,\\nCommunication and Networks (ICICN) , pages 391‚Äì398,\\n2023.\\n[64] Peng Xu, Ruijie Sun, Wei Wang, Tianyang Chen, Yubo\\nZheng, and Hai Jin. Sdd: A trusted display of fido2 trans-\\naction confirmation without trusted execution environ-\\nment. Future Generation Computer Systems , 125:32‚Äì40,\\n2021.\\n[65] Tarun Kumar Yadav and Kent Seamons. A Security\\nand Usability Analysis of Local Attacks Against FIDO2.\\nIn31th Annual Network & Distributed System Security\\nSymposium (NDSS‚Äô24) , 2024.\\n[66] Yubico. Q3 Interim Report. https:\\n//investors .yubico .com/en/wp-content/\\nuploads/sites/2/2023/03/Q3-investor-\\nmorning-presentation-231110 .pdf, 2023.\\n[67] Yubico. Yubico FIDO2 Python Library. https://\\ngithub .com/Yubico/python-fido2 , 2023.\\n[68] Yubico. Yubikey Manager CLI. https://github .com/\\nYubico/yubikey-manager , 2023.\\n[69] Yubico. CVE-2024-35311. https://cve .mitre .org/\\ncgi-bin/cvename .cgi?name=CVE-2024-35311 ,\\n2024.\\n[70] Yubico. Security Advisory YSA-2024-02 FIDO Re-\\nlying Party Enumeration. https://www .yubico .com/\\nsupport/security-advisories/ysa-2024-02 ,\\n2024.\\n[71] z4yx. FIDO Wireshark protocol dissectors over\\nUSB HID. https://gist .github .com/z4yx/\\n218116240e2759759b239d16fed787ca , 2019.\\n18',\n",
       " 'Epinet for Content Cold Start\\nHong Jun Jeon\\nhjjeon@stanford.edu\\nStanford University\\nStanford, California, USASongbin Liu\\nMeta\\nNew York, New York, USA\\nsongbin@meta.comYuantong Li\\nMeta\\nNew York, New York, USA\\nyuantongli@meta.com\\nJie Lyu\\nMeta\\nNew York, New York, USA\\njlyu@meta.comHunter Song\\nMeta\\nNew York, New York, USA\\nhuntersong21@meta.comJi Liu\\nMeta\\nBellevue, Washington, USA\\nji.liu.uwisc@gmail.com\\nPeng Wu\\nMeta\\nNew York, New York, USA\\nwupeng@meta.comZheqing Zhu\\nMeta\\nBellevue, Washington, USA\\nbillzhu@meta.com\\nAbstract\\nThe exploding popularity of online content and its user base poses\\nan evermore challenging matching problem for modern recommen-\\ndation systems. Unlike other frontiers of machine learning such\\nas natural language, recommendation systems are responsible for\\ncollecting their own data. Simply exploiting current knowledge\\ncan lead to pernicious feedback loops but naive exploration can\\ndetract from user experience and lead to reduced engagement. This\\nexploration-exploitation trade-off is exemplified in the classic multi-\\narmed bandit problem for which algorithms such as upper confi-\\ndence bounds (UCB) and Thompson sampling (TS) demonstrate\\neffective performance. However, there have been many challenges\\nto scaling these approaches to settings which do not exhibit a con-\\njugate prior structure. Recent scalable approaches to uncertainty\\nquantification via epinets [ 22] have enabled efficient approxima-\\ntions of Thompson sampling even when the learning model is a\\ncomplex neural network. In this paper, we demonstrate the first\\napplication of epinets to an online recommendation system. Our\\nexperiments demonstrate improvements in both user traffic and\\nengagement efficiency on the Facebook Reels online video platform.\\nCCS Concepts\\n‚Ä¢Computing methodologies ‚ÜíOnline learning settings .\\nKeywords\\nRecommendation Systems, Thompson Sampling, Contextual Bandit,\\nEpinet\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than the\\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\\nand/or a fee. Request permissions from permissions@acm.org.\\nWebConf ‚Äô25, April 28‚ÄìMay 02, 2025, Sydney, Australia\\n¬©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 978-1-4503-XXXX-X/18/06\\nhttps://doi.org/XXXXXXX.XXXXXXXACM Reference Format:\\nHong Jun Jeon, Songbin Liu, Yuantong Li, Jie Lyu, Hunter Song, Ji Liu, Peng\\nWu, and Zheqing Zhu. 2024. Epinet for Content Cold Start. In Proceedings\\nof Make sure to enter the correct conference title from your rights confirmation\\nemail (WebConf ‚Äô25). ACM, New York, NY, USA, 9 pages. https://doi.org/\\nXXXXXXX.XXXXXXX\\n1 Introduction\\nWith the exponential growth of online content, the matching prob-\\nlem of user and content becomes evermore challenging. This de-\\nscribes the task of modern recommendation systems and fortu-\\nnately, it is farfrom a needle in the haystack problem: past user\\nengagement is often indicative of future engagement with a piece\\nof content. Furthermore, new content often still exhibits similarities\\nto that of the past. As a result, intelligent algorithm design ought\\nto enable systems to reliably recommend content which each user\\nindividually finds engaging.\\nWhile other areas of machine learning such as natural language\\nhave captured the attention of academics and industry practitioners\\nalike, the recommendation system problem remains a formidable\\nfrontier for algorithmic advancements. Unlike natural language,\\nthe recommendation system problem falls squarely in the bandit\\nlearning or (more generally) reinforcement learning frameworks.\\nBandit and RL problems differ from supervised learning in the\\ncrucial detail that the algorithm is tasked with collecting its own\\ndata. For recommendation systems, this entails that the algorithm\\nmust decide which content to recommend to each user and will only\\nobserve labels for the user/content pairs which it proposes. While\\nsimple to describe, bandit problems exhibit a deep challenge known\\nas the ‚Äúexploration vs exploitation trade-off‚Äù. At any moment in\\ntime, an effective algorithm will have to balance the importance\\nof accumulating short-term reward by exploiting information that\\nit already has collected, versus exploring the unknown to acquire\\nnew information.\\nWhile there are known algorithms which optimally solve the\\nmulti-armed bandit problem [ 9], the solutions are computationally\\nintractable in all but the simplest of problem instances. As a result,\\nthe past decade has seen an explosion of interest in efficient algo-\\nrithms which exhibit an effective trade-off between exploration andarXiv:2412.04484v1  [cs.IR]  20 Nov 2024WebConf ‚Äô25, April 28‚ÄìMay 02, 2025, Sydney, Australia Hong Jun Jeon, Songbin Liu, Yuantong Li, Jie Lyu, Hunter Song, Ji Liu, Peng Wu, and Zheqing Zhu\\nexploitation. Theoretical analyses of even the simplest multi-armed\\nbandit problem settings have demonstrated that naive exploration\\napproaches such as ùúñ-greedy and Boltzmann exploration result in\\nùëÇ(ùëá)regret, where ùëáis the number of environment interactions.\\nAs regret measures the difference between the reward of the algo-\\nrithm‚Äôs action and the optimal reward, linear regret implies that\\nthe agent will notidentify the optimal arm even with infinite en-\\nvironment interactions. Meanwhile, methods such as Thompson\\nSampling (TS) and Upper Confidence Bounds (UCB) can achieve\\nùëÇ(‚àö\\nùëá)regret, implying that these algorithms reliably identify the\\nbest bandit arm with repeated environment interaction. The key\\ninsight is that algorithms which effectively leverage their epistemic\\nuncertainty (posterior distribution for TS and confidence inter-\\nval for UCB) are able to provide fruitful strategies to combat the\\nexploration-exploitation trade-off. While the multi-armed bandit\\nproblem is an idealized setting which deviates from the realities of\\nmodern recommendation systems, it elucidates the unique benefits\\nwhich are afforded by algorithms which effectively represent and\\nleverage uncertainty.\\nThe challenge of adapting UCB and TS to modern recommenda-\\ntion systems lies in tractably representing uncertainty for complex\\nmodels involving neural networks. While posterior distributions ex-\\nhibit efficient updates in the presence of nice conjugate priors, this\\nis certainly not the case for likelihoods involving neural networks.\\nHowever several lines of work have attempted to establish approx-\\nimate methods of posterior sampling. Among the early works is\\nstochastic gradient langevin dynamics [ 29] which adds noise to the\\nstochastic gradient updates. Upon arriving at a local minima the\\nnoise produces samples surrounding the posterior mode. However,\\nthe algorithm is computationally onerous for online methods as it\\nrequires optimization to convergence at every timestep to produce\\nsamples from the posterior. There have also been various works\\nin the Bayesian Neural Network literature [ 5,15] which represent\\nuncertainty via updating a posterior distribution over parameters.\\nHowever these methods are again computationally onerous and\\nresult in inadequate posterior approximation (factorized distribu-\\ntions on network weights are a poor characterization of model\\nuncertainty). Other works have proposed MC Dropout as a poste-\\nrior sampling approximation but [ 12,20] have demonstrated that\\nthe quality of the posterior approximation can be very poor. Deep\\nensembles have been a reliable method to approximate posterior\\nsampling via sampling from the ensemble‚Äôs particles but suffer from\\nthe fact that compute requirements grow linearly in the size of the\\nensemble [ 16,18]. Most recently, [ 22] have proposed epinet , a com-\\nputationally tractable approach which attains ensemble level per-\\nformance with a fraction of the computational requirements. [ 37]\\nhave applied epinet to an offline recommendation system problem\\nbut whether this method can produce meaningful improvements\\nin an online production setting is still an open question.\\nIn this paper, we provide the first online experimental deploy-\\nment of epinet in a production recommendation system. Specifically,\\nwe use a Thompson Sampling algorithm with posterior samples\\nprovided by epinet to produce recommendations in the cold start\\nretrieval phase of Facebook‚Äôs Reels recommendation. Content cold\\nstart involves making recommendations for new content which\\ndoes not have many impressions yet. As a result it is a settingwhich requires effective navigation of the explore/exploit tradeoff.\\nOur experiments demonstrate improvements in overall traffic and\\nengagement efficiency metrics such as like rate and video view\\ncompletion rate across the cold start content corpus.\\n2 Related Works\\nIn recommendation systems broadly and especially in content cold\\nstart, researchers have leveraged the multi-armed bandit formula-\\ntion to design algorithms which effectively trade-off novel explo-\\nration of the content corpus and exploitation of information which\\nhas already been gathered [ 4,8,19]. However, as the size of the\\ncontent corpus and user base grows, the assumptions of stateless-\\nness and independent arms becomes increasingly unreasonable. As\\na result, many works have come use probabilistic models which ex-\\nhibit generalization across arms and contextualization which reflect\\nthe varying preferences of different users [ 10,27,28,32]. However,\\neven with these problem formulations, the design of algorithms\\nwhich effectively solve these problems has been an active area of\\nresearch.\\nAs aforementioned, the most popular algorithms which exhibit\\nnontrivial exploration are UCB [ 2,3] and TS [ 26]. However, these\\nalgorithms were developed for the standard multi-armed bandit\\nproblem involving independent arms and no notion of state. As\\na result, translating these algorithmic solutions to problem set-\\ntings involving contextualization and generalization has been a\\npressing challenge in recent years. In particular, with the dramatic\\nincreases in capabilities afforded by machine learning with deep\\nneural networks, many methods have attempted to adapt the ideas\\nfrom UCB and TS to be amenable to computation with neural net-\\nworks [ 1,6,17,21,31,34,35]. However, the major drawback with\\nthe above methods is that they are impractical when computational\\ncosts are factored in. Notably, with the rise of recommendation\\nsystems which leverage immense transformer models to inform\\npredictions [ 13,25,30,33], each flop of compute which is expended\\non resources outside of model or dataset size comes at a steep cost\\n[11,14]. To ameliorate these concerns surrounding computational\\nresources, Osband et al . [22] have proposed epinet , a computation-\\nally tractable approach which attains ensemble level performance\\nwith a fraction of the computational requirements. [ 37] have ap-\\nplied epinet to an offline recommendation system problem but our\\nwork marks the first application of this technology in an online\\nproduction recommendation system setting.\\n3 Problem Formulation\\nWe formalize the recommendation system problem as a non-stationary\\ncontextual bandit problem. Concretely, the non-stationary contex-\\ntual bandit problem can be identified by a tuple\\n(O,A,(Vùë°)ùë°‚ààN,(ùúìùë°)ùë°‚ààN,ùúå),\\nwhereOdenotes the observation set, Athe action set,(Vùë°)ùë°‚ààN\\ndenotes the random process which dictates the non-stationarity,\\n(ùúìùë°)ùë°‚ààNdenotes the random process which dictates the contexts ,\\nandùúådenotes the probability associated with an observation ùëú‚ààO.\\nIn the recommendation system problem, a context ùúÉùë°represents\\nthe raw features of a particular user, whileVùë°represents the rawEpinet for Content Cold Start WebConf ‚Äô25, April 28‚ÄìMay 02, 2025, Sydney, Australia\\nfeatures of available content to recommend to said user. We dive\\ninto the details of each component below.\\n(1)Action space: We take Ato be the fixed set {1,2,...,ùëÅ},\\nwhereùëÅdenotes the number of items to select from. At\\nthe retrieval stage of recommendation, the algorithm can\\nproposeùëÄitems per timestep. Therefore, for all ùë°, we let\\nrandom variable ùê¥ùë°:Œ©‚Ü¶‚ÜíAùëÄdenote the action taken by\\nour algorithm at time ùë°.\\n(2)Non-stationary item pool: We take (ùëâùë°)ùë°‚ààNto be the random\\nprocess which represents the changing item pool. For all ùë°,\\nùëâùë°=(ùúôùë°,1,ùúôùë°,2,...,ùúôùë°,ùëÅ)‚ààV whereùúôùë°,ùëñdenotes the raw\\nfeatures associated with item ùëñat timeùë°andVdenotes the\\nrange ofùëâùë°. Since we are focused on content cold start, the\\nitem pool is constantly being refreshed by adding newly\\ncreated content and removing matured content.\\n(3)Context: We take(ùúìùë°)ùë°‚ààNto be the random process which\\nrepresents the user context at each timestep. For all ùë°,ùúìùë°‚ààŒ®\\nconsists of the raw features associated with the user for\\nwhich we provide recommendations.\\n(4)Observation Space: We take Oto be set of observations\\nabout a user and a recommended item. secommendation\\nsystems often leverage multiple signals including binary\\nsignals such as whether the user liked or shared the item,\\nand real-valued signals such as the proportion of the video\\nwhich was completed. However, to make actions for the\\nfollowing timestep, the algorithm must have access to the\\nsubsequent item pool ùëâùë°+1and context ùúìùë°+1. Therefore, for\\nallùë°, we let random variable ùëÇùë°+1:Œ©‚Ü¶‚ÜíOùëÄ√óV√óŒ®denote\\nthe observation associated with action ùê¥ùë°, the subsequent\\nitem poolùëâùë°+1, and the subsequent context ùúìùë°+1.ùëÄagain\\ndenotes the number of items that were proposed in action\\nùê¥ùë°.\\n(5)Observation probability: We let ùúå(¬∑|ùëÇùë°,ùê¥ùë°)=P(ùëÇùë°+1‚àà\\n¬∑|ùëÇùë°,ùê¥ùë°)denote the probability of a subsequent observation\\nconditioned on the previous observation and action. Recall\\nthatùëÇùë°also contains the item pool ùëâùë°and the user context ùúìùë°.\\nGeneralization across users and items with similar features\\nis captured by ùúå.\\nWe assume that the algorithm designer has an concrete objective\\nin mind. This objective is represented by a (known) reward function\\nR:O‚Ü¶‚Üí‚Ñú+which maps an observation to a scalar objective value.\\nIn practice, this is a suitable weighting of the various labels (like,\\nshare, etc). We let random variable ùëÖùë°+1denote the sum of rewards\\nof allùëÄlabels observed in ùëÇùë°+1. The objective is to maximize the\\naverage long-term reward:\\n1\\nùëáùëá‚àí1‚àëÔ∏Å\\nùë°=0E[ùëÖùë°+1].\\n4 Exploration for Content Cold Start\\nCold start content consists of videos which have been shown to\\nfewer than 10000 users. Note that even if the video is shown to\\na user, this does not necessarily mean that the user meaningfully\\nengaged with it. In fact it is rather the contrary as for instance,\\nonly 1%of recommended content results in a like from the user.\\nWith this sparse feedback signal, the ability to learn efficientlybecomes an imperative. Algorithms which explore ineffectively\\nwill not only provide poor recommendations to the user, but will\\nalso consume valuable bandwidth which could have been allocated\\nto better learning about the user‚Äôs preferences.\\nThe multi-armed bandit problem, despite its simplicity, assesses\\na core capability for an intelligent agent: The ability to balance\\nimmediate reward with the acquisition of new information (explo-\\nration/exploitation tradeoff). To effectively direct exploration of\\nthe content corpus, an algorithm must know what it doesn‚Äôt know .\\nIn the Bayesian framework, uncertainty is modeled using the tools\\nof probability and represented via a posterior distribution . As the\\nalgorithm observes more data, this distribution will concentrate,\\nreflecting greater certainty . This uncertainty which is reducible\\nby observing more data is typically referred to as epistemic uncer-\\ntainty . Common algorithms such as Thompson sampling and upper\\nconfidence bounds represent epistemic uncertainty via a posterior\\ndistribution and a confidence interval respectively.\\nHowever many production recommendation systems keep only\\napoint estimate . A point estimate corresponds to a distribution\\nin which all probability mass is placed on a singleton. This does\\nnot effectively model epistemic uncertainty since the algorithm is\\nequally (and absolutely) certain across all time. This is especially\\nproblematic for cold start content since it consist of items with little\\nto no impression data. The point estimate model‚Äôs score for such\\ncontent is essentially uninformed. If it happens to underestimate\\nthe value of the content, it will never be proposed to the user.\\nThis could further exacerbate the issue referred to as popularity\\nbias, the tendency of recommendation systems to simply exploit\\nbased off of existing data and forego exploration. Exploration for\\nrecommendation systems can therefore be reduced to representing\\nepistemic uncertainty for complex modern systems involving deep\\nneural networks.\\n4.1 Thompson Sampling\\nIn this paper, we focus on representing epistemic uncertainty with\\napproximate posterior distributions. Therefore, we limit our algo-\\nrithmic focus to Thompson sampling, which samples from this\\nposterior distribution to select actions. In this section, we provide\\nan abstract overview of Thompson sampling to frame our eventual\\nalgorithm.\\nFor allùë°, Thompson sampling takes as input an approximate\\nposterior distribution ùëÉùë°(ùúå‚àà¬∑)and the previous observation ùëÇùë°,\\nand produces an action ùê¥ùë°. Recall that ùúåis the probability mea-\\nsure which dictates the observations in our contextual bandit envi-\\nronment. The following pseudocode abstractly depicts Thompson\\nsampling:\\nTherefore, approximate Thompson sampling methods differ in\\n1)how the specify the approximate prior distribution ùëÉ0(¬∑)and\\n2)how they approximate the Bayesian update. While simplifying\\nassumptions such as independent beta prior/posterior distributions\\nmay enable exact Bayesian updating, they greatly suffer from the\\ninability to account for information present in the user context ùúìùë°\\nand the item features ùëâùë°.WebConf ‚Äô25, April 28‚ÄìMay 02, 2025, Sydney, Australia Hong Jun Jeon, Songbin Liu, Yuantong Li, Jie Lyu, Hunter Song, Ji Liu, Peng Wu, and Zheqing Zhu\\nAlgorithm 1 Thompson Sampling\\n1:InitializeùëÉ0(¬∑)\\n2:ObserveùëÇ0\\n3:forùë°=0,1,2,...do\\n4: Sample ÀÜùúå‚àºùëÉùë°(¬∑|ùëÇùë°)\\n5: SelectùëÄactionsùëé1,...,ùëéùëÄ‚àà A which maximize\\nE[ùëÖùë°+1|ùê¥ùë°=ùëé,ùúå=ÀÜùúå]\\n6:ùê¥ùë°‚Üê(ùëé1,ùëé2,...,ùëéùëÄ)\\n7: ObserveùëÇùë°+1\\n8:ùëÉùë°+1‚Üê(Approximate)Bayesian Update(ùëÉùë°,ùê¥ùë°,ùëÇùë°+1)\\n9:end for\\n5 Approximate Thompson Sampling with\\nEpinet\\nIn this section we present our method which leverages epinet [ 22]\\nto model epistemic uncertainty in an approximate Thompson sam-\\npling algorithm. We begin with some background on epistemic\\nneural networks , a broad class of approximate posterior methods\\nwhich encompass epinet.\\n5.1 Epistemic Neural Networks\\n[23] defined a class of neural networks known as epistemic neural\\nnetworks (ENNs). Epistemic neural networks are specified by a\\nparameterized function class ùëìand a reference distribution ùëÉùëç. The\\noutputùëìùúÉ(ùëã,ùëß)depends both on the input ùëãand an epistemic index\\nùëßwhich is drawn from the reference distribution ùëÉùëç. To produce a\\nmarginal prediction for an input ùëã, an epistemic neural network\\nintegrates over its epistemic indices:\\nÀÜùëÉ(ùëå)=‚à´\\nùëßùëìùúÉ(ùëã,ùëß)ùëëùëÉùëç(ùëß).\\nMany existing methods of uncertainty modelling for neural net-\\nworks including BNNs, MC Dropout, and Deep Ensembles can be\\nexpressed as ENNs under a suitable reference distribution ùëÉùëç. We\\ninstantiate a few below.\\nExample 1. (Point Estimate) Consider a parameterized neural\\nnetworkùëîùúÉ(¬∑):‚Ñúùëë‚Ü¶‚Üí ‚Ñú . LetùëÉùëç(¬∑)beany distribution. Then\\nùëìùúÉ(ùëã,ùëß)=ùëîùúÉ(ùëã)is equivalent to a network which only keeps a point\\nestimate.\\nIn our experimentation, the baseline that we compare our algo-\\nrithm to is the point estimate. This is because it is the standard\\nmethod in machine learning and recommendation systems: provide\\npredictions/recommendations which are optimal according to the\\nlearned model. While this is a reasonable approach in supervised\\nlearning settings in which the algorithm does not have control\\nover the data that it observes, in a bandit/reinforcement learning\\nproblem, this exploitative behavior can result in feedback loops\\nsuch as popularity bias in recommendation systems.\\nExample 2. (MC Dropout) Consider a parameterized neural\\nnetworkùëîùúÉ(¬∑):‚Ñúùëë‚Ü¶‚Üí‚Ñú with dropout applied to the input layer. Let\\nùëÉùëç(¬∑)=Uniform\\x10\\n{0,1}ùëë\\x11\\n. Then,ùëìùúÉ(ùëã,ùëß)=ùëîùúÉ(ùëã‚äôùëß)is equivalent\\nto dropout.\\nWe do not directly compare performance against MC dropout as\\nit has been demonstrated theoretically and empirically to providepoor representations of epistemic uncertainty [ 20,22]. To see this,\\none need look no further than the fact that the variance of MC\\ndropout does not go to 0even as the number of observed examples\\nincrease to‚àû. Osband [20] identifies that this is because dropout\\napproximates aleatoric risk as opposed to epistemic uncertainty.\\nThis distinction is key as using the former to guide exploration is\\nincoherent while using the latter can result in efficient exploration.\\nExample 3. (Deep Ensembles) Consider a deep ensemble of\\nsizeùëÅcomprised of parameterized models ùëîùúÉ1(¬∑),ùëîùúÉ2(¬∑),...,ùëîùúÉùëÅ(¬∑):\\n‚Ñúùëë‚Ü¶‚Üí ‚Ñú . LetùëÉùëç(¬∑)=Uniform({1,...,ùëÅ}). Then,ùëìùúÉ(ùëã,ùëß)=\\nùëîùúÉùëß(ùëã)is equivalent to deep ensembles.\\nDeep ensembles have been shown to provide useful representa-\\ntions of epistemic uncertainty in bandit and reinforcement learning\\nsettings [ 18,24]. They have even demonstrated good performance\\nin offline recommendation system settings [ 36]. However, their\\nmajor drawback is their enormous computational overhead. The\\ncomputational resources scale linearly in the size of the ensemble.\\nEnsembles typically need on the order of 100particles to function\\neffectively, a figure which is computationally infeasible when each\\nbase model is operating at the scale of Facebook‚Äôs recommendation\\nsystem. As a result, we omit experimentation with ensembles in\\nfavor of Epinet, a method which promises ensemble-level perfor-\\nmance without ensemble-level computational resources [22].\\n5.2 Epinet\\nRecent work has demonstrated that epinet [ 22] can achieve perfor-\\nmance comparable to deep ensembles with hundreds of particles\\nwith only a small fraction of the computational overhead. We detail\\nthe design and implementation of epinet in this section.\\nTo integrate epinet into Meta‚Äôs existing cold start retrieval model,\\nwe create some mild modifications from the original outline of\\n[22] and [ 37]. The system consists of two neural networks: a user\\ntowerùëî(user)\\nùúâùë¢with parameters ùúâùë¢and an item tower ùëî(item)\\nùúâùëñwith\\nparametersùúâùëñ. The system is trained on several supervision signals\\nsuch as like, share, etc. Let ùêædenote the number of such signals.\\nThe item tower takes the raw features ùúôùë°,ùëéof an item and outputs\\nan embedding vector\\nùëî(item)\\nùúâùëñ(ùúôùë°,ùëé)\\n|         {z         }\\nitem embeddings‚àà‚Ñúùëë.\\nMeanwhile, the user tower takes the raw features ùúìùë°of the user\\ncontext and outputs an embedding matrix\\nùëî(user)\\nùúâùë¢(ùúìùë°)\\n|       {z       }\\nuser embeddings‚àà‚Ñúùëë√óùêæ.\\nThese user and item embeddings are fed into an overarch model\\nwhich includes the epinet. The overarch consists of a base mlp\\nand an epinet. The base mlp ùëîùúÉconsists of trainable parameters ùúÉ\\nbut does not depend on a sampled epistemic index ùëß. Meanwhile,\\nthe epinetùúéùúÇconsists of trainable parameters and takes as input a\\nsampled epistemic index ùëß. The base mlp and the epinet both take\\nas input the concatenated user and item embeddings as well as in\\ninteraction term which consists of the concatenated elementwiseEpinet for Content Cold Start WebConf ‚Äô25, April 28‚ÄìMay 02, 2025, Sydney, Australia\\nproduct of the item embedding with the ùêæuser embeddings. Hence,\\nfor allùë°, the input is hence a vector ùë•ùë°‚àà‚Ñúùëë(2ùêæ+1). The overarch\\noutput is as follows:\\nùëìùúÉ,ùúÇ(ùë•ùë°,ùëßùë°)\\n|       {z       }\\noverarch=ùëîùúÉ(sg[ùë•ùë°])\\n|       {z       }\\nbase mlp+ùúéùúÇ(sg[ùë•ùë°],ùëßùë°)\\n|           {z           }\\nepinet,\\nwhere sg[¬∑]denotes a stop gradient. The stop gradient has been\\nobserved to improve training stability [22].\\nThe additive form of epinet is motivated by two factors 1)func-\\ntional uncertainty estimation and 2)efficient uncertainty estimation.\\nFunctional uncertainty estimates emphasize that neural network\\nuncertainty is notover the model parameters, but rather the func-\\ntionthat the parameters induce. This is an important distinction as\\nmany different arrangements of the parameters result in the same\\nfunction (appropriate permutations of the neurons for instance).\\nEfficient uncertainty estimation is possible if the epinet is smaller\\nthan the base model and the computation for evaluating several\\nepistemic indices ùëß1,...,ùëßùëõcan be batched. For methods such as\\nMC dropout, deep ensembles, and BNNs, either the prohibitive size\\nof the models and/or the inability to perform batched computa-\\ntion makes uncertainty computation burdensome. We now provide\\nfurther architectural details of the epinet.\\n5.3 Prior Networks\\nIn the literature, it is common to further divide the epinet into a\\nlearnable component and a fixed component referred to as a prior\\nnetwork [22,37]. Dwaracherla et al . [7] have demonstrated that for\\ndeep ensembles, prior functions dramatically improve upon vanilla\\ndeep ensembles [ 16] particularly in low-data regimes. Concretely,\\nifùë•ùë°denotes the epinet input and ùëßùë°the epistemic index, the epinet\\ntakes the form\\nùúéùúÇ(ùë•ùë°,ùëßùë°)\\n|     {z     }\\nepinet=ùúéùêø\\nùúÇ(ùë•ùë°,ùëßùë°)\\n|     {z     }\\nlearnable+ùúéùëÉ(ùë•ùë°,ùëßùë°)\\n|      {z      }\\nprior net.\\nWe assume that the epinet produces a scalar output and that ùëßùë°ùëñùëñùëë‚àº\\nN(0,ùêºùëëùëß). While the embeddings are trained on several labels (like,\\nshare, etc.), for our experimentation, we limited training the epinet\\nto only a single label. We note that the prior network is nottrainable.\\nIn both deep ensembles and epinet, these prior functions appear to\\nbe crucial algorithmic additions to ensure sufficient initial diversity\\nwithin the represented distribution (variability in ùëß).\\nThe choice of architecture for the epinet is rather peculiar but\\nit draws inspiration from linear bandits . The extension to neural\\nnetworks provided in [ 23] is ad-hoc but they demonstrate good\\nperformance nonetheless. Deriving more appropriate forms for\\nsoftmax/sigmoidal outputs is a potential direction for future work.\\nThe learnable network is a standard multi-layered perceptron (MLP)\\nwith Glorot initialization:\\nùúéùêø\\nùúÇ(ùë•ùë°,ùëßùë°)=mlpùúÇ([ùë•ùë°,ùëßùë°])‚ä§ùëßùë°,\\nwhere mlpùúÇreturns an output in ‚Ñúùëëùëßand[ùë•ùë°,ùëßùë°]is a concatenation\\nofùë•ùë°andùëßùë°. Meanwhile, typical choices of the prior network include\\nùúéùëÉsampled from the same architecture as ùúéùêøbut with different\\nparameter initialization.5.4 Model Training\\nLetùëé‚ààA denote an item selected by action ùê¥ùë°and letùë¶ùë°+1be the\\nlabels associated with the outcome of showing item ùëéto userùúìùë°.\\nWe assume that ùë¶ùë°+1‚àà[0,1]ùêæand we let Àúùë¶ùë°+1‚àà[0,1]denote the\\nsingle task label that we provide to the epinet. We then sample a\\nsingle epistemic indices ùëßùë°fromN(0,ùêºùëëùëß).\\nL(ùúâùë¢,ùúâùëñ,ùúÉ,ùúÇ,ùúôùë°,ùëé,ùúìùë°,ùëßùë°)\\n=ùêæ‚àëÔ∏Å\\nùëò=1BCE\\x10\\nùë¶ùë°+1,ùëò, ùëî(item)\\nùúâùëñ(ùúôùë°,ùëé)‚ä§ùëî(user)\\nùúâùë¢(ùúìùë°)ùëò\\x11\\n|                                                      {z                                                      }\\nembedding loss\\n+BCE\\x10\\nÀúùë¶ùë°+1, ùëìùúÉ,ùúÇ(ùë•ùë°,ùëßùë°)\\x11\\n|                         {z                         }\\nepinet loss,\\nwhere BCE denotes binary cross entropy. In practice, we sample\\na minibatch of actions and average the loss across the minibatch\\nbefore taking a gradient step.\\n5.5 Epinet Thompson Sampling Algorithm\\nWith the above details in place, we now present the epinet Thomp-\\nson sampling algorithm.\\nAlgorithm 2 Epinet Thompson Sampling\\n1:Initialize trainable parameters ùúâùëñ\\n0,ùúâùë¢\\n0,ùúÉ0,ùúÇ0and fixed parame-\\nters for prior net.\\n2:ObserveùëÇ0\\n3:forùë°=0,1,2,...do\\n4: Sampleùëßùë°‚àºN( 0,ùêºùëëùëß)\\n5: Compute user embedding ùëî(user)\\nùúâùë¢\\nùë°(ùúìùë°)\\n6: forùëé=1,2,...,ùëÅ do\\n7: Compute item embeddings ùëî(item)\\nùúâùëñ\\nùë°(ùúôùë°,ùëé)\\n8: Compute overarch input ùë•ùë°,ùëéwith user and item em-\\nbeddings\\n9: end for\\n10: SelectùëÄactionsùëé1,...,ùëéùëÄ‚àà A which maximize\\nùëìùúÉùë°,ùúÇùë°(ùë•ùë°,ùëé,ùëßùë°)\\n11:ùê¥ùë°‚Üê(ùëé1,ùëé2,...,ùëéùëÄ)\\n12: ObserveùëÇùë°+1\\n13:(ùúâùë¢\\nùë°+1,ùúâùëñ\\nùë°+1,ùúÉùë°+1,ùúÇùë°+1) ‚Üê ( ùúâùë¢\\nùë°,ùúâùëñ\\nùë°,ùúÉùë°,ùúÇùë°) ‚àíùõº¬∑\\n‚àáL(ùúâùë¢\\nùë°,ùúâùëñ\\nùë°,ùúÉùë°,ùúÇùë°,ùúôùë°,ùëé,ùúìùë°,ùëßùë°,ùë¶ùë°+1)\\n14:end for\\nNote that prior distribution is represented by the initial parame-\\nters of the various involved neural networks. For epinet, this prior\\nis a distribution over neural network functions . Sampling from this\\ndistribution involves first sampling an epistemic index ùëßùë°from the\\nreference distribution and then running a forward pass with ùëßùë°held\\nfixed. Meanwhile, the approximate posterior update is performed\\nvia gradient descent on the objective. In the following section,\\nwe outline the details of our online experimentation with epinet\\nThompson sampling for cold start retrieval.WebConf ‚Äô25, April 28‚ÄìMay 02, 2025, Sydney, Australia Hong Jun Jeon, Songbin Liu, Yuantong Li, Jie Lyu, Hunter Song, Ji Liu, Peng Wu, and Zheqing Zhu\\nuser tower \\nitem tower overarch ewtr\\nlike\\nshare \\nvvs\\newtruser embeddings \\nitem embeddings overarch input sg\\nepistemic index user features item features \\nFigure 1: The above diagram depicts the training setup of our recommendation system. Note that the epinet is part of the\\noverarch component.\\n5.6 Recommendation Pipeline\\nSource \\n1Source\\n2Source\\n3Source \\nN-2Source \\nN-1Source \\nN\\nRanking \\nUser Our \\nAlgorithm \\nFigure 2: The above diagram depicts the content cold start\\nfunnel. Each source filters a large batch of video content\\nand passes the results to a blender before it is ranked. Videos\\nwhich achieve sufficient rank are presented to users and mon-\\nitored for engagement. Our algorithm impacts the proposals\\nof one of the sources which feed into the ranking algorithm.\\nThe Facebook Reels recommendation system involves various phases\\nin which recommendation proposals are generated, pooled, and\\nthen ranked before they are served to the user. Since our project\\nexplores content cold start, our method operates at the proposal\\nstage of recommendation. Before the proposals are presented to\\nthe ranker, they are blended with the proposals of other sources\\nwhich will be running their own separate recommendation algo-\\nrithm. Therefore, even if our algorithm suggests a piece of content\\nto a particular user, it will not be served to that user unless theranker predicts that our content is more promising than that of\\nother sources. However, quota is set aside for cold start content\\nto mitigate popularity bias. The application of epinets to the rank-\\ningstage of recommendation remains an interesting direction for\\nfuture research.\\n6 Experiments\\nWe conducted an online A/B test over a period of 5 days to test\\nthe effectiveness of our proposed method. For contextual bandit\\nexperiments, this is a standard duration as it doesn‚Äôt take long for\\nresults to converge. We run our test on Facebook‚Äôs Reels recommen-\\ndation system which serves billions of users each day. Our method\\nis applied at the retrieval stage of recommendation. The generator\\nwhich we deploy our experiment serves around 120million users\\nper day. For the test and control arms, we allocate groups of around\\n12million users.\\n6.1 Implementation Details\\nThe user/item embeddings are trained to minimize loss on 4dif-\\nferent tasks: watch score (ws), like, share, and video view seconds\\n(vvs). Watch score is defined below:\\nws=\\uf8f1\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4 \\uf8f2\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f31if video length <10seconds and user completed\\nvideo more than once\\n1if video length‚â•10seconds and <20seconds and\\nuser completed video\\n1if video length‚â•20seconds and user watched at\\nleast 20seconds\\n0otherwise.Epinet for Content Cold Start WebConf ‚Äô25, April 28‚ÄìMay 02, 2025, Sydney, Australia\\nLike and share are a binary signals which are 1if the user liked/shared\\nthe video and 0otherwise. Vvs is defined below:\\nvvs=\\uf8f1\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4 \\uf8f2\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f30 if video viewed for <10seconds\\n1\\n9if video viewed for ‚â•10seconds and <20seconds\\n......\\n1 if video viewed for ‚â•90seconds.\\nWhile the user and item embeddings are trained on the 4above\\nlabels, the overarch is only trained on ws. The control arm only\\nmaintains a point estimate and does notinclude an overarch com-\\nponent. Therefore, it is trained to minimize BCE loss on the above\\nsignals and recommends content greedily with respect to its cur-\\nrent point estimate. Both models are trained every hour, initialized\\nfrom the most recent checkpoint. They are trained on a pool of new\\ndata which is aggregated across allother generators. Therefore, the\\ndata contributed by our algorithm to the aggregated pool is very\\nminiscule. This brings about two concerns 1)data leakage between\\ntreatment arms, 2)dilution of data collected by our method by data\\nfrom other generators. However, upon inspection the embeddings\\nfor each user were sufficiently different. As a result, there is 1)no\\nmeaningful generalization to users across treatment groups and 2)\\nless concern of dilution since each the data from other generators\\nis sufficiently disjoint in embedding space. We hypothesize that\\ntheses are the reasons that we observe significant performance\\nimprovements with our method despite the above concerns.\\nEach embedding is of dimension ùëë=128and the epistemic index\\ndimensionùëëùëß=5. In the overarch model, we use 2-hidden layer\\nMLPs with hidden dimensions [384,256]for both the epinet and the\\nbase mlp with glorot initialization. As aforementioned, we sample\\nonly a single epistemic index for both training and inference.\\n6.2 Experimental Results\\nWe now outline the results of our online experiments. A recom-\\nmendation counts as an ‚Äúimpression‚Äù if it is displayed on the user‚Äôs\\nscreen for at least 250milliseconds. Since we are interested in\\nthe performance of cold start content, we measure performance\\non content with fewer than 10000 impressions. We further strat-\\nify performance by grouping the statistics by video impression\\ncount buckets of[0 : 100,100 : 200,200 : 400,400 : 1000,1000 :\\n2000,2000 : 3000,3000 : 4000,4000 : 5000,5000 : 10000]. We report\\nimprovements across 3efficiency metrics: like per impression, video\\ncompletion per impression, and watch score per impression. Video\\ncompletion is 1if the user viewed the video to completion and 0\\notherwise.\\nFigures 3, 4, and 5 depict the percentage change of our epinet\\nalgorithm in comparison to the control group for the various afore-\\nmentioned efficency metrics. We plot the 95%confidence intervals\\nfor each metric and impression count bucket. Hence, a change is sig-\\nnificant if the error bar does not cross past 0. Figure 3 demonstrates\\nlarge improvements in like per impression, especially for content\\nwith lower impression counts. This suggests that via intelligent\\nexploration, the system can more reliably explore cold start con-\\ntent and serve videos that the users enjoy. Figure 4 demonstrates\\nsignificant improvement in video view completion per impression,\\nindicating that the epinet algorithm is successfully able to recom-\\nmend content which users enjoy enough to watch to completion.Finally, Figure 5 depicts positive, though less significant, results for\\nwatch score per impression.\\nLike/Impression vs Impression Count \\nImpression Count \\nFigure 3: We depict percentage change in like per impres-\\nsion between our method and the control. We group videos\\nby their impression counts. We notice the most significant\\nbenefits in the videos with lower impression count which is\\npromising for content cold start.\\nVideo Completion/Impression vs Impression Count \\nImpression Count \\nFigure 4: We depict percentage change in video view comple-\\ntion per impression between our method and the control. We\\ngroup videos by their impression count. We notice improved\\nvideo view completion across all impression counts.WebConf ‚Äô25, April 28‚ÄìMay 02, 2025, Sydney, Australia Hong Jun Jeon, Songbin Liu, Yuantong Li, Jie Lyu, Hunter Song, Ji Liu, Peng Wu, and Zheqing Zhu\\nWatch Score/Impression vs Impression Count \\nImpression Count \\nFigure 5: We depict percentage change in watch score per\\nimpression between our method and the control. We group\\nvideos by their impression count.\\nFinally, we analyze how our method reallocates recommenda-\\ntions across the impression count buckets. Figure 6 shows that our\\nmethod dramatically boosts impressions for content with fewer\\nimpressions and modestly pulls traffic from content with more im-\\npressions. We also note that our method demonstrates an overall\\n17% boost in impressions when aggregated across content of all\\nimpression counts.\\nImpressions vs Impression Count \\nImpression Count \\nFigure 6: We depict the percentage change in impressions for\\ncontent grouped by the impression count for the content. Our\\nmethod dramatically improves impressions for videos with\\nfewer impressions and modestly pulls traffic from videos\\nwith higher impression count.\\nThe results of this empirical investigation suggest that applying\\nepinets to approximate Thompson sampling can provide concrete\\nimprovements in an online production system.7 Conclusion\\nOur work marks the first empirical investigation of epinets in an on-\\nline production recommendation system. The investigation demon-\\nstrated that epinets can provide an effective solution to managing\\nthe exploration-exploitation trade-off in content cold start. Interest-\\ning future extensions of this work include larger-scale experiments\\nand application of these ideas to later stages (ranking) in the rec-\\nommendation pipeline. Extensions from the contextual bandit to a\\nreinforcement learning setting also serves as an interesting direc-\\ntion for future research.\\nReferences\\n[1]Marc Abeille and Alessandro Lazaric. 2017. Linear thompson sampling revisited.\\nInArtificial Intelligence and Statistics . PMLR, 176‚Äì184.\\n[2] Rajeev Agrawal. 1995. Sample mean based index policies by o (log n) regret for\\nthe multi-armed bandit problem. Advances in applied probability 27, 4 (1995),\\n1054‚Äì1078.\\n[3] P Auer. 2002. Finite-time Analysis of the Multiarmed Bandit Problem.\\n[4]St√©phane Caron and Smriti Bhagat. 2013. Mixing bandits: A recipe for improved\\ncold-start recommendations in a social network. In Proceedings of the 7th Work-\\nshop on Social Network Mining and Analysis . 1‚Äì9.\\n[5]Armen Der Kiureghian and Ove Ditlevsen. 2009. Aleatory or epistemic? Does it\\nmatter? Structural safety 31, 2 (2009), 105‚Äì112.\\n[6]Vikranth Dwaracherla, Xiuyuan Lu, Morteza Ibrahimi, Ian Osband, Zheng Wen,\\nand Benjamin Van Roy. 2020. Hypermodels for exploration. arXiv preprint\\narXiv:2006.07464 (2020).\\n[7]Vikranth Dwaracherla, Zheng Wen, Ian Osband, Xiuyuan Lu, Seyed Mohammad\\nAsghari, and Benjamin Van Roy. 2022. Ensembles for Uncertainty Estimation:\\nBenefits of Prior Functions and Bootstrapping. arXiv:2206.03633 [cs.LG] https:\\n//arxiv.org/abs/2206.03633\\n[8]Cr√≠cia Z Fel√≠cio, Kl√©risson VR Paix√£o, Celia AZ Barcelos, and Philippe Preux. 2017.\\nA multi-armed bandit model selection for cold-start user recommendation. In\\nProceedings of the 25th conference on user modeling, adaptation and personalization .\\n32‚Äì40.\\n[9]John C Gittins. 1979. Bandit processes and dynamic allocation indices. Journal of\\nthe Royal Statistical Society Series B: Statistical Methodology 41, 2 (1979), 148‚Äì164.\\n[10] Nicolas Gutowski, Tassadit Amghar, Olivier Camp, and Fabien Chhel. 2019. Global\\nversus individual accuracy in contextual multi-armed bandit. In Proceedings of\\nthe 34th ACM/SIGAPP Symposium on Applied Computing (Limassol, Cyprus) (SAC\\n‚Äô19). Association for Computing Machinery, New York, NY, USA, 1647‚Äì1654.\\nhttps://doi.org/10.1145/3297280.3297440\\n[11] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,\\nTrevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes\\nWelbl, Aidan Clark, et al .2022. Training compute-optimal large language models.\\narXiv preprint arXiv:2203.15556 (2022).\\n[12] Jiri Hron, Alexander G de G Matthews, and Zoubin Ghahramani. 2017. Variational\\nGaussian dropout is not Bayesian. arXiv preprint arXiv:1711.02989 (2017).\\n[13] Jun Hu, Wenwen Xia, Xiaolu Zhang, Chilin Fu, Weichang Wu, Zhaoxin Huan,\\nAng Li, Zuoli Tang, and Jun Zhou. 2024. Enhancing sequential recommendation\\nvia llm-based semantic embedding learning. In Companion Proceedings of the\\nACM on Web Conference 2024 . 103‚Äì111.\\n[14] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess,\\nRewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.\\nScaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020).\\n[15] Alex Kendall and Yarin Gal. 2017. What uncertainties do we need in bayesian\\ndeep learning for computer vision? Advances in neural information processing\\nsystems 30 (2017).\\n[16] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. 2017. Sim-\\nple and Scalable Predictive Uncertainty Estimation using Deep Ensembles. In\\nAdvances in Neural Information Processing Systems , I. Guyon, U. Von Luxburg,\\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), Vol. 30.\\nCurran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2017/\\nfile/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf\\n[17] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. 2010. A contextual-\\nbandit approach to personalized news article recommendation. In Proceedings of\\nthe 19th international conference on World wide web . 661‚Äì670.\\n[18] Xiuyuan Lu and Benjamin Van Roy. 2017. Ensemble sampling. Advances in neural\\ninformation processing systems 30 (2017).\\n[19] Hai Thanh Nguyen and Anders Kofod-Petersen. 2014. Using multi-armed bandit\\nto solve cold-start problems in recommender systems at telco. In Mining Intel-\\nligence and Knowledge Exploration: Second International Conference, MIKE 2014,\\nCork, Ireland, December 10-12, 2014. Proceedings . Springer, 21‚Äì30.Epinet for Content Cold Start WebConf ‚Äô25, April 28‚ÄìMay 02, 2025, Sydney, Australia\\n[20] Ian Osband. 2016. Risk versus uncertainty in deep learning: Bayes, bootstrap\\nand the dangers of dropout. In NIPS workshop on bayesian deep learning , Vol. 192.\\nMIT Press.\\n[21] Ian Osband and Benjamin Van Roy. 2015. Bootstrapped Thompson Sampling and\\nDeep Exploration. arXiv:1507.00300 [stat.ML] https://arxiv.org/abs/1507.00300\\n[22] Ian Osband, Zheng Wen, Seyed Mohammad Asghari, Vikranth Dwaracherla,\\nMorteza Ibrahimi, Xiuyuan Lu, and Benjamin Van Roy. 2023. Epistemic neural\\nnetworks. Advances in Neural Information Processing Systems 36 (2023), 2795‚Äì\\n2823.\\n[23] Ian Osband, Zheng Wen, Seyed Mohammad Asghari, Vikranth Dwaracherla,\\nMORTEZA IBRAHIMI, Xiuyuan Lu, and Benjamin Van Roy. 2023. Epistemic\\nNeural Networks. In Advances in Neural Information Processing Systems , A. Oh,\\nT. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (Eds.), Vol. 36.\\nCurran Associates, Inc., 2795‚Äì2823. https://proceedings.neurips.cc/paper_files/\\npaper/2023/file/07fbde96bee50f4e09303fd4f877c2f3-Paper-Conference.pdf\\n[24] Chao Qin, Zheng Wen, Xiuyuan Lu, and Benjamin Van Roy. 2022. An analysis of\\nensemble sampling. Advances in Neural Information Processing Systems 35 (2022),\\n21602‚Äì21614.\\n[25] Zhaopeng Qiu, Xian Wu, Jingyue Gao, and Wei Fan. 2021. U-BERT: Pre-training\\nuser representations for improved recommendation. In Proceedings of the AAAI\\nConference on Artificial Intelligence , Vol. 35. 4320‚Äì4327.\\n[26] William R Thompson. 1933. On the likelihood that one unknown probability\\nexceeds another in view of the evidence of two samples. Biometrika 25, 3-4 (1933),\\n285‚Äì294.\\n[27] Aleksandr Vorobev, Damien Lefortier, Gleb Gusev, and Pavel Serdyukov. 2015.\\nGathering Additional Feedback on Search Results by Multi-Armed Bandits with\\nRespect to Production Ranking. In Proceedings of the 24th International Conference\\non World Wide Web (Florence, Italy) (WWW ‚Äô15) . International World Wide\\nWeb Conferences Steering Committee, Republic and Canton of Geneva, CHE,\\n1177‚Äì1187. https://doi.org/10.1145/2736277.2741104\\n[28] Lu Wang, Chengyu Wang, Keqiang Wang, and Xiaofeng He. 2017. BiUCB: A\\nContextual Bandit Algorithm for Cold-Start and Diversified Recommendation.In2017 IEEE International Conference on Big Knowledge (ICBK) . 248‚Äì253. https:\\n//doi.org/10.1109/ICBK.2017.49\\n[29] Max Welling and Yee W Teh. 2011. Bayesian learning via stochastic gradient\\nLangevin dynamics. In Proceedings of the 28th international conference on machine\\nlearning (ICML-11) . Citeseer, 681‚Äì688.\\n[30] Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. 2021. Empowering\\nnews recommendation with pre-trained language models. In Proceedings of the\\n44th international ACM SIGIR conference on research and development in informa-\\ntion retrieval . 1652‚Äì1656.\\n[31] Pan Xu, Zheng Wen, Handong Zhao, and Quanquan Gu. 2020. Neural contex-\\ntual bandits with deep representation and shallow exploration. arXiv preprint\\narXiv:2012.01780 (2020).\\n[32] Yisong Yue and Carlos Guestrin. 2011. Linear submodular bandits and their\\napplication to diversified retrieval. Advances in Neural Information Processing\\nSystems 24 (2011).\\n[33] Jiaqi Zhai, Lucy Liao, Xing Liu, Yueming Wang, Rui Li, Xuan Cao, Leon Gao, Zhao-\\njie Gong, Fangda Gu, Michael He, et al .2024. Actions speak louder than words:\\nTrillion-parameter sequential transducers for generative recommendations. arXiv\\npreprint arXiv:2402.17152 (2024).\\n[34] Weitong Zhang, Dongruo Zhou, Lihong Li, and Quanquan Gu. 2020. Neural\\nthompson sampling. arXiv preprint arXiv:2010.00827 (2020).\\n[35] Dongruo Zhou, Lihong Li, and Quanquan Gu. 2020. Neural contextual bandits\\nwith ucb-based exploration. In International Conference on Machine Learning .\\nPMLR, 11492‚Äì11502.\\n[36] Zheqing Zhu and Benjamin Van Roy. 2023. Deep Exploration for Recommenda-\\ntion Systems. In Proceedings of the 17th ACM Conference on Recommender Systems\\n(Singapore, Singapore) (RecSys ‚Äô23) . Association for Computing Machinery, New\\nYork, NY, USA, 963‚Äì970. https://doi.org/10.1145/3604915.3608855\\n[37] Zheqing Zhu and Benjamin Van Roy. 2023. Scalable neural contextual bandit for\\nrecommender systems. In Proceedings of the 32nd ACM International Conference\\non Information and Knowledge Management . 3636‚Äì3646.',\n",
       " 'Depression detection from Social Media Bangla\\nText Using Recurrent Neural Networks\\nSultan Ahmed\\nDepartment of Information Systems\\nUniversity of Maryland Baltimore County\\nMaryland, USA\\nIL66977@umbc.eduSalman Rakin\\nDepartment of CSE\\nBangladesh University\\nof Engineering & Technology\\nDhaka, Bangladesh\\n0417052033@grad.cse.buet.ac.bdMohammad Washeef Ibn Waliur\\nDepartment of CSE\\nUniversity of Dhaka\\nDhaka, Bangladesh\\nwasheef123@gmail.com\\nNuzhat Binte Islam\\nDepartment of URP\\nJahangirnagar University\\nDhaka, Bangladesh\\nmounuzhat35@gmail.comBillal Hossain\\nDepartment of CSE\\nJagannath University\\nDhaka, Bangladesh\\nbillal13027@gmail.comDr. Md. Mostofa Akbar\\nDepartment of CSE\\nBangladesh University\\nof Engineering & Technology\\nDhaka, Bangladesh\\nmostofa@cse.buet.ac.bd\\nAbstract ‚ÄîEmotion artificial intelligence is a field of study that\\nfocuses on figuring out how to recognize emotions, especially in\\nthe area of text mining. Today is the age of social media which has\\nopened a door for us to share our individual expressions, emo-\\ntions, and perspectives on any event. We can analyze sentiment\\non social media posts to detect positive, negative, or emotional\\nbehavior toward society. One of the key challenges in sentiment\\nanalysis is to identify depressed text from social media text that\\nis a root cause of mental ill-health. Furthermore, depression\\nleads to severe impairment in day-to-day living and is a major\\nsource of suicide incidents. In this paper, we apply natural\\nlanguage processing techniques on Facebook texts for conducting\\nemotion analysis focusing on depression using multiple machine\\nlearning algorithms. Preprocessing steps like stemming, stop\\nword removal, etc. are used to clean the collected data, and\\nfeature extraction techniques like stylometric feature, TF-IDF,\\nword embedding, etc. are applied to the collected dataset which\\nconsists of 983 texts collected from social media posts. In the\\nprocess of class prediction, LSTM, GRU, support vector machine,\\nand Naive-Bayes classifiers have been used. We have presented\\nthe results using the primary classification metrics including F1-\\nscore, and accuracy. This work focuses on depression detection\\nfrom social media posts to help psychologists to analyze sentiment\\nfrom shared posts which may reduce the undesirable behaviors\\nof depressed individuals through diagnosis and treatment.\\nIndex Terms ‚ÄîEmotion Artificial Intelligence, Depression De-\\ntection, Machine Learning, Facebook posts, Bangla language,\\ndeep learning, Bag-Of-Words, Social media post\\nI. I NTRODUCTION\\nText is the most important means of communication in\\ntoday‚Äôs world. Popular online social networking sites such as\\nFacebook, Twitter, MySpace, etc. are mainly text-based. The\\nrapid growth of Social Media has created enough opportunities\\nto share information across time and space. Users are now\\ncomfortable contributing more to the content of social media\\nwebsites and posting their own material.\\nThe emergence of internet-based media sources has resulted\\nin the availability of substantial user data for the emotionalanalysis of text and images. Through a multitude of social\\nmedia platforms such as Twitter, Facebook, and Instagram,\\nindividuals tend to express their emotions, opinions, and daily\\nlives [14] today. This expression can be conveyed through\\nimages, videos, and text primarily. Due to the pervasiveness\\nand accessibility of these social media platforms, there is\\nan abundance of user data available for exploratory analysis.\\nTextual data, as the most prevalent form of communication,\\npossesses a number of characteristics that make it the optimal\\ndata source for emotion AI data analysis.\\nPeople are increasingly utilizing social media to share and\\nexpress their happiness, anxiety, sorrow, curiosity, etc. through\\ntextual content. Therefore, social media has emerged as a data\\nsource for analyzing people‚Äôs expressions [9]. The majority\\nof individuals in our country use Facebook to express their\\nemotions. If an automated system is able to identify the\\ndepressive language in social media posts, it will be possible\\nto identify depressed individuals within our network before\\nthey enter a malignant phase of depression.\\nThe classical approach to feature extraction for depression\\nanalysis from textual data is to identify unique stylometric\\nfeatures of written texts. The underlying assumption here is\\nthat each author has unique writing styles that are relatively\\nfixed and barely change with time. So we can use stylometric\\nfeatures to uniquely identify the writing style of the author [3].\\nAlong with the stylometric feature, we have used TF-\\nIDF vectorizer and Word Embedding approach to identify\\ndepression from textual data.\\nIn this work, we are interested in identifying the depression\\nof an author given a text. The Depression Identification(DI)\\nproblem has numerous applications in the emotional artificial\\nintelligence field. We are interested in addressing the DI\\nproblem for social media Bangla text. To the best of our\\nknowledge, only one paper has previously addressed this\\nproblem.arXiv:2412.05861v1  [cs.HC]  8 Dec 2024Although depression identification has been widely studied\\nin different languages, it is still understudied in the Bangla\\nlanguage. Bangla language is one of the most widely spoken\\nand culturally rich languages. This language is the 7thmost\\nspoken language [4] of the world and the native language of\\nBangladesh. However, this is not the only reason to study DI\\nproblems in the Bangla language. The problems associated\\nwith the Bangla language and the relatively under-developed\\nfield of Bangla Natural Language Processing (NLP) makes it\\nmore challenging to study such problems for Bangla.\\nIn this work, we will follow the following steps to study the\\nDI problem. We will manually create a dataset from Facebook\\ncontaining depressive posts and non-depressed posts. We will\\nthen pre-process the data and extract features from the data\\nin 3 ways. One is called the Bag-of-words technique. Another\\none is computing stylometric features to capture the writing\\nstyle of the author. The third one is using a word embedding\\napproach to convert text into a feature vector.\\nIn recent years, deep learning-based recurrent neural models\\nare used to automate depression information extraction due to\\ntheir performance in building models. These models do not\\nrequire to be provided with pre-defined handpicked features.\\nInstead, they can learn useful features from the data by\\nthemselves [6].\\nIn this work, we have used deep learning recurrent models\\nto automate depression detection from Facebook textual data.\\nSpecifically, we have used LSTM and GRU models from\\ndeep learning recurrent neural network models. From the\\ntraditional model, we have used SVM and NB models. Then\\nthe performance of the deep learning model is compared to\\ntraditional machine learning models.\\nThe rest of this paper is organized as follows: Section II\\noverviews the related works of the depression identification\\nproblem. In section III, we have proposed our detailed solu-\\ntion. Section IV presents the experimental results. Finally, in\\nsection V , we conclude our findings with a discussion of the\\nobtained observations and the future directions of this work.\\nII. R ELATED WORKS\\nIn [25], the authors conducted a depression analysis in Chi-\\nnese. In their endeavor, Psychological and Machine Learning\\nknowledge were combined. The authors opted for Psycholo-\\ngists who assisted 90 depressive and 90 non-depressed Micro-\\nblog users in collecting a total of 6013 micro-blogs. Their\\nmodel‚Äôs precision was 80\\nAbdul et al. [23] proposed using a Long Short-Term\\nMemory Recurrent Neural Network (LSTM-RNN) to ana-\\nlyze Bangla social media posts for depression. They gathered\\nBangla tweets from Twitter in order to compile the dataset\\nrequired for this endeavor. This dataset contained 1968 tweets,\\nwhich was insufficient for a deep learning model to perform\\nadequately. In order to improve the performance of the model\\nwith this short dataset, the data were stratified so that one\\ndepressive text was followed by one non-depressive text. They\\nexperimented with dividing the dataset into 80 percent for\\ntraining, 10 percent for validation, and 10 percent for modeltesting. They optimized four hyperparameters of the trained\\nmodel (LSTM size, batch size, number of epochs, and number\\nof layers) for maximum classification accuracy. The evaluation\\nof the model revealed an accuracy of 86.3%.\\nAuthors performed the Gated Recurrent Neural Network\\nalgorithm on the same dataset in another work [24] to predict\\ndepressive Bangla text. As in previous work, They worked\\nwith the hyper-parameters of the GRU model and achieved\\napproximately 75% classification for this task.\\nBillah et al. [5] collected depressed and non-depressed\\nposts from Facebook manually and applied SGD classifier,\\nMultinomial Na ¬®ƒ±ve Bayes, Logistic Regression, and Linear\\nSVC to detect social media post whether it was depressive\\nor not. During the treatment of patients, Psychologists prefer\\nsome linguistic features that may help to detect depression.\\nFor example, depressed people usually use words like ‚Äúme‚Äô,\\n‚ÄúI‚Äù, ‚Äúmyself‚Äù etc. which actually represent their self-centered\\nthinking focusing on themselves rather than other people.\\nThe authors collected these types of posts to enrich their\\ndataset which consisted of 1000 texts having depressive and\\nnot depressive texts. They applied several pre-processing steps\\nto clean the data like punctuation removal, normalization, tok-\\nenization, etc. They applied Unigram, Bigram, and Emoticon\\nfeatures in their dataset. They had achieved the highest 77.9%\\nclassification accuracy for the SGDC classifier.\\nHassan et al. [10] developed an automated system to detect\\ndepression levels of people from social media posts. They re-\\nmoved the stop words and applied N-grams, POS tagging, and\\nNegation feature extraction techniques to transform the text\\ninto a word vector. Finally, SVM, Na ¬®ƒ±ve Bayes, and Maximum\\nEntropy are applied to the dataset to classify depressive tweets\\nwhere SVM showed the highest 91% accuracy.\\nIII. M ETHODOLOGY\\nThis section presents a detailed overview of three feature\\nextraction techniques. One is the Bag-Of-Words feature and\\nanother is the Stylometric feature approach and the third one\\nis the word embedding approach. We first label the dataset\\nand then compute the feature after pre-processing the data.\\nThe feature is then fed into the machine learning model. Then\\nwe provide the architecture of the model. Fig 1 presents an\\noverview of our proposed solution.\\nA. Dataset Creation\\nLike any other TC problem, the first step in the depression\\ndetection problem is to build a large dataset. We have collected\\n981 texts from Facebook manually where the number of\\ndepressed class labels is 592 and the number of non-depressed\\nclass labels is 391 which is listed in Table I.\\nTABLE I: Data Distributions\\nClass Label Number of posts\\nDepressed Post 391\\nNon-depressed Post 592\\n2Fig. 1: Steps followed in our solution\\nWe have taken the following facts in our concern during data\\ncollection: i) collecting the status of people who have com-\\nmitted suicide, ii) Collecting the status of people who usually\\nshare depressive posts on social media, and iii) Collecting the\\nstatus of people who share pleasant posts. For the labeling of\\ndata, we have taken 2 people‚Äôs opinions to correctly determine\\nthe correct class label of data.\\nB. Pre-processing\\nThe post obtained from Facebook is noisy and often con-\\ntains a lot of unnecessary information. Here URL, image,\\ntags etc. are present in the text. So we apply various pre-\\nprocessing steps and have filtered out all characters except\\nBangla characters. Then we tokenize our texts and remove\\nstop words from the text. We collect Bangla stop words from\\nGithub repository [20] as mentioned in research work [22].\\nElongated words often contain some context in identifying\\nemotion from the text. We express our feelings in elongated\\nwords. For example, ‚ÄùGreaaat news!!!‚Äù has more feelings than\\n‚ÄùGreat news!!!‚Äù. To maintain the context of the text, we do\\nnot apply lemmatization.\\nC. Word Vector Formation\\nWe use traditional machine learning models in our proposed\\nsolution. To use these models, we need to convert our text to a\\nword vector. Conversion to word vector from text is done using\\nTF-IDF features and Sentiment/Emotion Features approaches.\\n1) Stylometric Features Approach:\\nStylomtric features are features that capture the writing\\nstyle of both depressed and non-depressed authors. We have\\nalready computed a large set of stylometric features based\\non existing works of [7, 17]. These features are categorized\\ninto four types: lexical features, structural features, syntactic\\nfeatures and content-specific features. These four categories of\\nstylometric features have been widely used in research work\\nof [1, 2]. Table II lists all the 141 features we computed as\\nstylometric features.\\nLexical features are the most common set of stylometric\\nfeatures that is intended for stylistics and text readabilityanalysis. These features also signifies language assessment,\\nfirst and second language acquisition. Lexical features consist\\nof word-based and character-based features. These features are\\nbasically concerned with usage frequency of individual letters,\\nvocabulary richness, entropy measure, consecutive occurrence\\nof words etc.\\nSyntactic features are primarily intended for identifying\\nwriting formation patterns such as the usage of punctuation\\nmarks. These features include total number of comma, colon,\\nquestion marks and exclamation marks etc.\\nStructure based features focuses on the way of organi-\\nzation of the layout of a text by an author. Organization of\\narticles represents different habitual facts of an author such as\\nparagraph length and use of greetings. As online texts have less\\ncontent information but richer stylistic information, so these\\nhabits are seen to be more prominent in these texts in bearing\\nstrong authorial evidence of personal writing styles. We have\\ncomputed 8 structure-related features as shown in Table II.\\nFinally, the content-specific features represents domain-\\nspecific terms. From the study of [15], it has been shown that\\nthese features are important in the author‚Äôs writing pattern\\nformation. For these features, we have first got the feature\\nwords as suggested for the Arabic language in [3]. Then\\nwe prepared the Bangla feature words by translating these\\nArabic words using Google Translator service API. We have\\ntranslated Arabic words into 5 categories: Economy, Pol-\\nicy, Social, Sport, and Negative. This translation resulted in\\nmany duplicates, flaws, and inconsistencies in the translated\\nlexicons. We have cleared all of these issues by manually\\ninspecting the lexicons. In this way, we have prepared 5\\ncontent-specific features for the Bangla language listed in\\nTable II.\\nFor each text of the user, the feature extractor produced\\na 141 dimension vector to represent the values of the 141\\nfeatures. As these feature sets contain information on the\\nwriting style of a user measured by various methods, the\\nfeature values we computed could range from 0 to any positive\\nvalue.\\nAs we want to ensure all features are treated equally in the\\nclassification process, we have normalized the features using\\nthe max-min normalization method to ensure all feature values\\nare between 0 and 1:\\nx‚Ä≤\\nij=xij‚àímin(xj)\\nmax(xj)‚àímin(xj)\\nwhere xijis the jth feature in the ith example, min( xj) and\\nmax(xj) are the minimum and maximum feature values of the\\njth feature separately.\\n2) TF-IDF Count Vectorizer Approach:\\nTF-IDF stands for Term Frequency Inverse Document\\nFrequency. TF-IDF converts the text of the user into a\\nmeaningful number of vectors to fit in the machine learning\\nmodel. In any document, the term frequency is the number of\\noccurrences of a term. On the other hand, document frequency\\nrepresents the number of documents containing of that term.\\nTerm frequency indicates the importance of a specific term\\n3TABLE II: Stylometric Features\\nCharacter Based Lexical Features\\nFeature Name Feature Count Feature Description\\nTotal characters =‚áíC F1 Alphabet, digits, special characters\\nTotal number of letters/C F2 A- \\x84\\nTotal number of digits/C F3 0- 9\\nTotal number of white-space characters/C F4 white sapce\\nTotal number of tab space characters/C F5 tab space\\nTotal number of elongation characters/C F6 total word that pronounced as long\\nTotal number of dependent vowel characters/C F7¬∑¬∑¬∑F17 a, i, \\x8c, u, \\x8e, \\x90, e, \\x94, \\x96\\nTotal number of special characters/C F18¬∑¬∑¬∑F41 <<,>>, $, %, &, *, (, ), >,<,{,}, [, ], , +,\\n-, =,ÀÜ, ,\\\\\\\\, Àú, ‚Äî\\nTotal number of individual characters/C F42¬∑¬∑¬∑F91 A- \\x84\\nWord Based Lexical Features\\nFeature Name Feature Count Feature Description\\nTotal number of words F92 Total number of all words in the text\\nAverage word length F93 The average number of characters per word\\nTotal Unique words F94 Words that appear once in a text\\nHapex legomena F95 Words that occurs only one\\nHapax Dislegomena F96 Words that occurs only twice\\nTotal no. of Short words F97 1-3 character words\\nTotal no. of Long words F98 Words longer than 6 characters\\nWord length frequency distribution F99¬∑¬∑¬∑F114 word length from 0 to 15\\nYules K measure F115 Measure of vocabulary richness\\nTotal Digit word F116 words that have digits\\nTotal Consecutive occurrence F117 no of words with 3 consecutive duplicate charac-\\nters\\nSyntactic Features\\nFeature Name Feature Count Feature Description\\nTotal single quotes/C F118 ‚Äô\\nTotal comma/C F119 ,\\nTotal periods/C F120 .\\nTotal colons/C F121 :\\nTotal semi colons/C F122 ;\\nTotal question marks/C F123 ?\\nTotal exclamation marks/C F124 !\\nTotal double quotes/C F125 ‚Äù\\nStructural Features\\nFeature Name Feature Count Feature Description\\nTotal number of lines F126 line splitted by .\\nTotal number of paragraphs F127 in the case of pressing enter\\nTotal number of sentences F128\\nTotal number of blank lines F129 in the case of pressing enter\\nTotal number of non blank lines F130\\nTotal length of non blank lines F131\\nAverage length of non blank lines F132\\nAverage number of words per sentence F133\\nAverage number of words per paragraph F134\\nAverage number of character per sentence F135\\nAverage number of character per paragraph F136\\nContent Specific Features\\nFeature Name Feature Count Feature Examples\\nTotal economic phrases F137 rajY tHibl (State funds), eka¬Æpain (Com-\\npany), ibineyag (Investment)\\nTotal policy phrases F138 ra√´Rd\\x8et (Ambassador), ibS\\x90¬¢xl (Chaotic),\\nJ√î(Care)\\nTotal Social phrases F139 nar\\x8c Aizkar (Women‚Äôs rights), bKS(race), Jub\\ngRup(youth group)\\nTotal Sports phrases F140 exla (Sports), fuTbl (Football), pdk (Medal)\\nTotal Negative phrases F141 pirtYag kora (abandon), G\\x90Na(hatred),\\npolayon (escape)\\n4TABLE III: Example Sentence for TF-IDF Vectorizer\\nText No. Example\\nText1 I love natural language processing.\\nText2 I like data processing.\\nText3 I like text processing.\\nTABLE IV: Example Sentence for TF Vectorizer\\nText No. I love like natural language data text processing\\nText1 1 1 0 1 1 0 0 1\\nText2 1 0 1 0 0 1 0 1\\nText3 1 0 1 0 0 0 1 1\\nTABLE V: Example Sentence for IDF Vectorizer\\nNo. I love like natural language data text processing\\nWord 0 0.477 0.18 0.477 0.477 0.477 0.477 0\\nTABLE VI: Example Vector for TF-IDF Vectorizer\\nText No. I love like natural language data text processing\\nText1 0 0.477 0 0.477 0.477 0 0 0\\nText2 0 0 0.18 0 0 0.477 0 0\\nText3 0 0 0.18 0 0 0 0.477 0\\nin a document. Document frequency indicates how common\\nthe term is in [13]. TF-IDF gives higher weights to terms\\nappearing frequently in the given document and rarely in other\\ndocuments.\\nWe have implemented the TF-IDF count vectorizer from\\nthe scikit-learn library. Since, TF-IDF generates higher di-\\nmensional feature vectors, we need dimensional reduction\\ntechniques. We can do these in several ways. The first one\\nis to reduce the number of extracted terms. Another one is\\nto use stemming. In our work, we have used the reduction\\nof extracted terms approach. We set the number of extracted\\nterms as 1000.\\nFor Example, let us take an example. Suppose, we have\\n3 texts which have to be converted to vectors using TF-IDF\\ncount vectorizer as shown in the table Table III.\\nFor converting these texts into vectors, we have to first\\nidentify unique words and count how many times these words\\noccur in each text. This is shown in Table IV\\nThen we have to compute inverse document frequency (IDF)\\nusing the following formula.\\nid fi=logn\\nd fi\\nwhere d firepresents how many documents contain the term\\ni and n is the total number of documents. We have calculated\\nthe inverse document frequency for each work and shown it\\nto Table V\\nNow we will multiply the TF matrix with IDF score to getthe vectorized form of each text which is shown in Table VI.\\nWe have converted all texts into vectors. These vectors can be\\nfed into any machine-learning algorithm.\\n3) Word Embedding Representation Approaches:\\nTraditionally, the bag-of-words (BOW) model is used to\\ntransform the text into feature vectors in text classification. In\\nthis model, a text is represented as the bag (multiset) of its\\nwords, disregarding grammar and even word order but keeping\\nmultiplicity. Authors in [3] followed the BOW model with a\\nset of hand-crafted rules to prepare the feature set.\\nHowever, motivated by the recent the success of deep\\nlearning models in text classification, we have used word\\nembeddings as features instead of applying the BOW model.\\nWord embeddings are a distributed representation of text that\\nallows words with similar meanings to have a similar repre-\\nsentation. In this approach, individual words are represented\\nas real-valued vectors in a predefined vector space. Authors\\nin [ mikolovÀôzweig ] have shown that word embeddings can\\ncapture syntactic and semantic regularities employing the\\nvector offsets between word pairs sharing a particular relation.\\nBelow, we explain how Word2vec is used to generate word\\nembeddings.\\na) Word2vec:\\nWord2vec, an efficient algorithm proposed by Google [16],\\ncan learn a standalone word embedding from a text cor-\\npus efficiently maintaining the contextual meaning of words.\\nWord2vec has two model architectures to produce an em-\\nbedding representation of words. One is Continuous Bag of\\n5Words(CBOW) and another is Skip Gram (SG). We have\\nimplemented both of these model architectures as mentioned\\nin [16].\\nCBOW Model takes the context of each word as the input\\nand tries to predict the word corresponding to the context.\\nSG predicts the surrounding window of context words based\\non the current single word. The word vector prediction is not\\ninfluenced by the order of context words. Figure 2 shows the\\narchitecture of the Skip-Grams and Continuous Bag of Words\\nmodel. Below we have described in detail how CBOW and\\nSkip-Gram Model is used to generate word vector from the\\ntext.\\ni) CBOW Model: CBOW model predicts the current single\\nword from a specified window of surrounding context\\nwords. To generate word embedding, we first select a\\nvocabulary of size V from our dataset. Then, we create a\\none-hot encoded vector for each of these words in V. For\\na window size C, the sliding window method generates\\nfeatures for every single word in each training sample to\\ntrain the CBOW model using the generated features.\\nThen, for each training sample, we feed forward one\\nhot encoded vector of input words to a Neural Network\\nconsisting of one hidden layer with N nodes. The output\\nlayer has V nodes with a softmax activation function. The\\noutput word vector is then compared against the actual\\nword vector. The weight of each layer is updated according\\nto the error and backpropagation happens. Figure 2(a)\\nshows the architecture of the Continuous Bag of Words\\nmodel.\\nExample 1. ‚ÄùHave a great day‚Äù- take the word ‚Äúgreat‚Äù\\nas an example. Assuming windows size as one, training\\nfeatures are- ([Have, great], a), ([a, day], great). CBOW\\nmodel learns to predict ‚Äôday‚Äô in the context given ‚Äôa‚Äô and\\n‚Äôgreat‚Äô as the input.\\nOur vocabulary is {Have, a, great, day }. The encoded one\\nhot vectors are: Have= {1, 0, 0, 0 }, a={0, 1, 0, 0 }, great= {0,\\n0, 1, 0}, day={0, 0, 0, 1 }.\\nFor this training feature, ([a, day], great), the CBOW model\\nfirst feeds forward the one hot encoded vector of ‚Äôa‚Äô and\\nday.WV N is multiplied with these vectors. Then after\\ndoing the average, we get the hidden layers vector. W‚ÄòNV\\nis the weight matrix that maps the hidden layer outputs to\\nthe final output layer. This output layer provides the word\\nvector of great. This word vector of great is then compared\\nto the actual word vector of great and WV NandW‚ÄòNV\\nvectors are updated.\\nii) Skip-Gram model: Skip-Gram model is a neural network that\\ncan generate word embedding without any labeled data. This\\nneural network does so by creating a ‚Äúfake‚Äù task to train. We\\naren‚Äôt interested in the input and output of this neural network\\nrather than the goal is to learn the weights of the hidden\\nlayer that are actually the ‚Äúword vectors‚Äù of the corresponding\\nwords.\\nThe fake task for the Skip-gram model would be, given a\\nword, we‚Äôll try to predict its neighboring words. We‚Äôll definea neighboring word by the window size ‚Äî a hyper-parameter.\\nGiven a sentence: ‚ÄúThe quick brown fox jumps over the lazy\\ndog.‚Äù and a window size of 2, if the target word is brown, its\\nneighboring words will be ( the, quick, fox, jumps). Our input\\nand target word pair would be (brown, the), (brown, quick),\\n(brown, fox), (brown, jumps). Within the sample window,\\nproximity of the words to the source word does not play\\nany role. So the, quick, fox, and jumps will be treated the\\nsame while training. Figure 2(b) shows the architecture of\\nContinuous Bag of Words model.\\nWe will first select a vocabulary of size V from our dataset.\\nThen we create a one-hot encoded vector of dimension 1xV\\nfor each of these words in V . So the dimensions of the input\\nvector in Skip-Gram model will be 1xV . The single hidden\\nlayer will have dimension VxN, where N is the size of the\\nword embedding and is a hyper-parameter. The output from\\nthe hidden layer would be of the dimension 1xN, which we\\nwill be fed into an softmax layer. There will be C vector in the\\noutput layer. The dimension of each vector in output layer will\\nbe 1xV , where each value in the vector will be the probability\\nscore of the target word at that position.\\nFor training samples corresponding to a source word, we will\\nback propagate in one back pass. So for brown, we will\\ncomplete the forward pass for all 4 target words ( the, quick,\\nfox, jumps). We will then calculate the errors vectors[1xV\\ndimension] corresponding to each target word. We will now\\nhave 4 1xV error vectors and will perform an element-wise\\nsum to get a 1xV vector. The weights of the hidden layer will\\nbe updated based on this cumulative 1xV error vector.\\nBoth Skip-gram and CBOW models can be used on a large\\ndataset to learn word embedding in a short time like billions of\\nwords in hours. This can be achieved in a low computational\\ncomplexity. Practically, Skip-gram gives better word represen-\\ntations for small data-set. CBOW provides better performance\\nand is more suitable when the datasets is large [16]. Further\\ndetails of the Skip-Grams and Continuous Bag of Words model\\ncan be found in [21]\\nb) Word Embedding using Word2vec:\\nAs per research study in [16], Skip-gram and Continuous\\nBag of Words, two architectures of Word2vec model, can be\\nused on any large corpus of text to learn the word embedding.\\nAs Word2vec model can capture a lot of information main-\\ntaining semantic, conceptual and contextual relation, we have\\nlearned the embedding vector of each word from user post\\nfrom Facebook of our dataset using this CBOW and Skip-\\nGram model.\\nFor example, let us have two sentences in our dataset. [I\\nhave a book, I love to eat mango]. So first we split the\\nsentence and generated a two-dimensional vector. The two-\\ndimensional vector will be [ [I, have, a, book], [I, love, to,\\neat, mango]]. Then we pass this two-dimensional vector to the\\nword2vec model. Skip-Gram and CBOW model generates the\\nword vector from this two-dimensional dataset using window-\\nsize as 5. The size of the word vector is 300. We have used\\nthe Gensim package to implement the Word2vec model. This\\n6model returns a 300-dimensional vector for each of these\\nwords: I, have, a, book, love, to, eat, mango. We save these\\nword embedding and later used these embeddings as the\\nweight of the embedding layer.\\nc) Classification:\\nFor a labeled sentence ‚ü®s, c‚ü©, we first encoding each word\\nof sentence s using a unique number. For example, if 1 is\\nassigned to ‚Äùcat‚Äù, 2 to ‚Äùmat‚Äù, and so on, then encoding of the\\nsentence ‚ÄùThe cat sat on the mat‚Äù as a dense vector will be\\nlike [5, 1, 4, 3, 5, 2].\\nThen we multiply this encoding vector with the embeddings\\nof words present in s to form the hidden representation of\\ns. These sentence representations are used to train a linear\\nclassifier. Specifically, we use the softmax function to compute\\nthe probability distribution over the classes in C.\\nFor example, Lets say our training sample consists of two\\nsentences: ‚ÄùI want to see you again.‚Äù and ‚ÄùFeeling better to see\\nyou again‚Äù. If we encode these phrases by assigning each word\\na unique integer number, then our phrases could be rewritten\\nas: [0, 1, 2, 3, 4, 5] , [6, 7, 2, 3, 4, 5]. We pass this training\\nsamples to embedding layer which can be defined as below:\\nEmbedding(8, 2, input_length=6)\\nThe first argument (8) is the number of distinct words in\\nthe training set. The second argument (2) indicates the size of\\nthe embedding vectors. The input-length argumet determines\\nthe size of each input sequence.\\nThe word embedding of each of these words present in our\\ndataset is got from Word2vec model and is set as weight of\\nembedding layer. Embedding layer can be thought as the table\\nused to map integers to embedding vectors. Lets say, currently\\nembedding layer has embedding vectors defined in Table VII:\\nSo according to these embeddings, our second training\\nphrase will be represented as:\\n[[4.1, 2.0], [2.1, 3.2], [1.0, 3.1], [0.3, 2.1], [2.2, 1.4], [0.7,\\n1.7]]\\nAfter passing through embedding layer, each of our training\\nsamples will be converted into 2-dimensional vector which is\\ngone into LSTM model.\\nword index Embedding\\nI 0 [1.2, 3.1]\\nwant 1 [0.1, 4.2]\\nto 2 [1.0, 3.1]\\nsee 3 [0.3, 2.1]\\nyou 4 [2.2, 1.4]\\nagain 5 [0.7, 1.7]\\nFeeling 6 [4.1, 2.0]\\nbetter 7 [2.1, 3.2]\\nTABLE VII: Sample weight of Embedding Layer\\nThe output from LSTM layer goes to flatten layer which\\nconverts the two dimensional vector into 1-dimensional vector.\\nIf the 2-dimensional vector of second training phrase is passed\\nthrough flatten layer, then the layer will give output: [4.1, 2.0,\\n2.1, 3.2, 1.0, 3.1, 0.3, 2.1, 2.2, 1.4, 0.7, 1.7]. The output fromflatten layer is passed through dense layer.\\nDense layer the regular deeply connected neural network\\nlayer which computes output = activation(dot(input, kernel)\\n+ bias). Dense layer is providing two valued 1 dimensional\\nvector. First value depicts the probability of being male and\\nsecond value depicts the probability of being female. Softmax\\nis used as activation function in dense layer whose computa-\\ntion is done using the following formula:\\nSoftmax (xi) =exp(xi)P\\njexp(xj)\\nD. Model Architecture\\nFor two types of features, we have implemented deep\\nlearning models and traditional machine learning models to\\ndetect depression in the text. In the deep learning model, we\\nimplement LSTM and GRU models. In the traditional machine\\nlearning model, we implement SVM, and NB models.\\n1) Model with Stylometric Features:\\nWe have prepared word vectors by Styometric features\\ndiscussed in Section-IIIC. We pass these vectors to the\\nLSTM layer having 300 nodes. The output of the LSTM\\nlayer is passed to a dense layer. Softmax [18] is used\\nas an activation function. The optimizer is RMSprop and\\nbinary cross entropy [19] is used as a loss function. The\\nsame process is repeated for Gated Recurrent Unit(GRU),\\nSVM, and NB models. We have noted down each model‚Äôs\\naccuracy, F1-score. Fig. 3(a) shows the architecture of the\\nLSTM, GRU, SVM, and NB model with the stylometric\\nfeature.\\n2) Model with TF-IDF feature:\\nWe have implemented deep learning models like LSTM,\\nGRU, and traditional machine learning algorithms such\\nas Support Vector Machine(SVM) and Naive Bayes (NB)\\nto detect depression from the text. We have used Term\\nFrequency Inverse Document Frequency (TF-IDF) to gen-\\nerate a feature set from the text. 2 grams is used in the TF-\\nIDF vectorizer. We have used the linear kernel function\\nin the Support Vector Machine model. The function of\\nkernel is to take data as input and transform it into the\\nrequired form. For each model, we have noted down\\naccuracy and F1-score.\\n3) Model with Word Embedding feature:\\nFor any sentence S with classification C, we have done\\nthe necessary prepossessing as discussed in Section-IIIC.\\nThen these sentences are passed through a tokenizer\\nwhich can produce a one-hot encoding vector of length\\n100. Only the top 1000 most frequent words are taken as\\nvocabulary. The first 100 words are taken for sentences\\nhaving more than 100 words. Shorter text is padded\\nwith zeros. After that, these vectors are fed into an\\nembedding layer. The weights of the embedding layer\\nare initialized with word2vec embedding weights. We\\ninitialize the embedding layer using trainable and non-\\ntrainable properties. Non-trainable property freezes the\\n7(a) CBOW model architecture\\n (b) Skip-Gram model architecture\\nFig. 2: Architecture for CBOW and Ski-Gram Model\\nEmbedding layer so that the pre-trained weights are not\\nupdated during the training. The output dimension of the\\nembedding layer is 300 as it is the vector length of each\\nword in the word2vec model. The sequence of 100 words\\nis then passed through an LSTM layer. The output of the\\nLSTM layer is passed to a dense layer which is used\\nto detect depression in the text. Softmax [18] is used in\\nthe dense layer as an activation function. The optimizer\\nis RMSprop and binary cross entropy [19] is used as\\na loss function. The same process is repeated for all\\nthe remaining deep-learning models. Figure 3(c) shows\\nthe architecture of our models with the word embedding\\nfeatures.\\nIV. E XPERIMENTAL EVALUATION\\nIn this section, we have evaluated the performance of our\\nproposed methods for depression detection on Facebook data\\nsets. We compare the performance of deep learning algorithms\\nwith traditional machine learning algorithms like Support\\nVector Machine(SVM) and Naive Bayes(NB).\\nA. Experimental Setup\\nWe have employed Python Keras framework with Tensor-\\nflow as a framework to implement deep learning models for\\ntraining, tuning, and testing. We have also used Gensim pack-\\nage for word2vec model implementation. Scikit-learn package\\nis used to implement traditional machine learning algorithms.\\nExperimental evaluation was conducted on a machine with an\\nIntel Core i7 processor with 1.8GHz clock speed and 8GB\\nRAM. The machine has also an Nvidia GeForce MX150 with\\n2GB memory and therefore Tensorflow based experiments\\nhave fully utilized GPU instructions. Considerable speed canbe achieved in Tensorflow based experiments by adding a GPU\\nas shown in [8].\\nB. Performance Evaluation and Parameter Tuning\\nWe have studied the efficiency and scalability of our pro-\\nposed methods by varying model architectures and feature set\\nvectors. We have measured performance of recurrent neural\\nnetwork, LSTM model by generating word vector using stylo-\\nmetric feature method and using word2vec method. Moreover,\\nwe have evaluated the performance of different word2vec\\nmodel by initializing embedding layer weights with them\\nand consider both trainable and non-trainable weight matrix.\\nTrainable weight matrix denotes that at the time of training,\\nthe embedding layer weights will be updated. Non-trainable\\nweight matrix denotes that the embedding layer weights will\\nnot be updated at the time of training.\\nC. Result Analysis\\n1) Performance of stylometric feature:\\nWe present the performance of stylometric feature along\\nwith traditional machine learning and deep learning al-\\ngorithms in Table VIII. From Table VIII, we can see that\\ndeep learning algorithm like LSTM performs better than\\ntraditional machine learning algorithms such as SVM &\\nNB. We have used stylometric features that can capture\\nthe writing style of different authors. NB outperforms\\nthe SVM model because the NB model with stylometric\\nfeatures can predict depression based on probabilistic\\ndistribution. Again, LSTM having feedback connections\\ncan process the entire sequence of data. Thus LSTM\\nwith stylometric features outperforms traditional machine\\nlearning algorithms like SVM and NB.\\n8(a) Model Architecture for Stylometric Feature\\n(b) Model Architecture for TF-IDF\\n(c) Model Architecture for Word Embedding\\nFig. 3: Model ArchitectureTABLE VIII: Performance measure for Stylometric Feature:\\nModel Feature Vector Accuracy F1-score\\nLSTM Styoletric Feature 76.42% 34.35%\\nGRU Styoletric Feature 74.58% 34.36%\\nSVM Styoletric Feature 72.95% 31.91%\\nNB Styoletric Feature 75.96% 37.30%\\n2) Performance of word embedding feature:\\nWe have experimented using word embedding as a feature\\nwith both deep learning and traditional machine learning\\nalgorithms. The performance comparison is noted down\\nin Table IX. The performance of NB & SVM mod-\\nels with word embedding features outperforms LSTM\\nand GRU models. NB models can predict class labels\\nbased on probabilistic distribution. Also, we have used\\nWord2vec [16] which can generate word vectors based\\non the context. Thus the performance of NB model in\\ncombination with the word embedding feature is better\\nthan deep learning models.\\n3) Performance of TF-IDF feature:\\nThe performance of the TF-IDF feature vector in accor-\\ndance with deep learning algorithms like LSTM, GRUand traditional machine learning algorithms such as SVM,\\nand NB is presented in Table X. Here, the NB model\\noutperforms SVM, GRU, and LSTM models. TF-IDF\\nfeature representation, a bag of words(BOW) feature, is\\nused to assign weight to a word based on the number of\\ntimes it appears in the text. As the NB model predicts the\\noutput class based on the probability of the given vector,\\nso NM model with the TF-IDF feature outperforms\\nanother model. The highest accuracy is 75.96% got by\\nthe NB model with the TF-IDF feature.\\n4) Performance among different models:\\nWe present the performance of different machine learning\\nmodel associated with the feature vector in Figure 4. This\\nfigure is a summarising table of Table VIII, IX, and X.If\\nwe take best performance of machine learning algorithm\\n9TABLE IX: Performance measure for Word Embedding Feature:\\nModel Feature Vector Accuracy F1-score\\nLSTM Word Embedding Feature 49.33% 24.45%\\nGRU Word Embedding Feature 41.93% 23.23%\\nSVM Word Embedding Feature 55.32% 22.22%\\nNB Word Embedding Feature 62.94% 23.16%\\nTABLE X: Performance measure for TF-IDF Feature:\\nModel Feature Vector Accuracy F1-score\\nLSTM TF-IDF Feature 60.91% 29.41%\\nGRU TF-IDF Feature 57.86% 33.6%\\nSVM TF-IDF Feature 65.92% 41.74%\\nNB TF-IDF Feature 65.99% 43.7%\\nTABLE XI: Comparison of Proposed Method with Previous Works\\nLanguage Proposed by Methodology Feature Extraction Accuracy\\nBangla [23] LSTM model None 86.3%\\nEnglish [11] KNN, SVM, DT LIWC 73%\\nBangla [5] SGD Classifier, Logistic\\nRegressionUnigram and Emoticons\\nwith TF-IDF values74.57%\\nBangla [24] GRU None 75.7%\\nEnglish [9] SVM, Na ¬®ƒ±ve Bayes, ME N-gram, POS, Negation\\nChecker91%\\nBangla Proposed Method LSTM, GRU, SVM, NB Stylometric Feature,\\nTF-IDF Feature, Word\\nEmbedding Feature76.42%\\nin terms of feature, then we will get Figure 4. From\\nFigure 4, we can see that deep learning model LSTM\\nmodel outperforms traditional machine learning models\\nsuch as SVM & NB. LSTM with the stylometric feature\\nis slightly better than NB with the stylometric feature.\\nThis is expected as LSTM can save previous state. NB\\nis slightly better than SVM in terms of accuracy since\\nthe NB can predict class labels based on the simpler\\ncomputation of text frequency to compute the posterior\\nprobability of each class. On the other hand, SVM works\\nbased on the computation of hyperplane equations which\\nseparates data into classes perfectly [12]. The highest\\naccuracy and F1-score got by our proposed methods are\\n76.42% and 34.35% accordingly.\\n5) Training and Validation Accuracy for LSTM: As the\\nstylometric feature with the LSTM model outperforms all\\nother models, we have drawn a plot on training accuracy\\nvs validation accuracy for the LSTM model. From the\\nfigure 5, we can see, with an increase in epochs, both\\ntraining and validation accuracy increase. But validation\\naccuracy is higher than training accuracy in this case. The\\nreason is that during random splitting, the examples that\\nare set in the validation set are easier to guess than in the\\ntraining set.\\n6) Comparison of Proposed Method with Previous Works:\\nFig. 4: Performance compare among different models\\nIn Table XI, we have shown our proposed methodology\\nand previous works done on depressed port detection\\nfrom social media text both in English and Bangla\\nlanguage. As the number of previous works done on\\ndepressed post detection from social media text in the\\nBangla language is very low, among [23], [5], [24]\\nand our proposed methodology which works are done\\nfor Bangla language, our work is showing comparatively\\nbetter performance.\\n10Fig. 5: Training and Validation Accuracy for LSTM\\nV. C ONCLUSION & F UTURE WORK\\nDepression detection from social media text is one of\\nthe research work done in various languages. In the Bangla\\nlanguage, there is no standard dataset for depression detection.\\nWe have made our dataset public so that future work can be\\ndone using this dataset. In this work, we have applied deep\\nlearning models and traditional machine learning models to\\ndetect depression from Bangla text. Three types of features\\nsuch as stylometric feature, TF-IDF feature, and Word embed-\\nding feature are used. Among them, LSTM with stylometric\\nfeatures shows the highest accuracy which is 76.42%. This\\nwork will help us to detect depressed people within our\\nnetwork before they go under a malignant stage of depression.\\nAfter detection, we can suggest them go to a psychiatrist for\\nbetter treatment of their mental disorder. Also, we can help\\nto prevent them from doing any life-threatening occurrence\\nowing to mental disorders. This work is done with a very\\nlimited dataset. In future work, the dataset will be increased.\\nAlso, we will implement more traditional and deep learning\\nmodels in order to predict depression from the text so that\\nbetter comparison can be done.\\nREFERENCES\\n[1] A. Abbasi and H. Chen. ‚ÄúApplying Authorship Analysis\\nto Arabic Web Content‚Äù. In: Intelligence and Security\\nInformatics, Berlin, Heidelberg . 2005.\\n[2] A. Abbasi and H. Chen. ‚ÄúApplying authorship analysis\\nto extremist-group Web forum messages‚Äù. In: IEEE\\nIntelligent Systems 20.5 (2005), pp. 67‚Äì75.\\n[3] K. Alsmearat, M. Al-Ayyouba, R. Al-Shalabi, and G.\\nKanaanbt. ‚ÄúAuthor gender identification from Arabic\\ntext‚Äù. In: Journal of Information Security and Appli-\\ncations 35.8 (2017), pp. 85‚Äì95.[4] Bengali language. Bengali language ‚Äî Wikipedia,\\nThe Free Encyclopedia . https : / / en . wikipedia . org /\\nwiki/Bengali language. [Online; accessed 04-February-\\n2020]. 2020.\\n[5] Masum Billah and Enamul Hassan. ‚ÄúDepression de-\\ntection from Bangla Facebook status using machine\\nlearning approach‚Äù. In: Int. J. Comput. Appl 975 (2019),\\np. 8887.\\n[6] B. Bsir and M. Zrigui. ‚ÄúEnhancing Deep Learning\\nGender Identification with Gated Recurrent Units Ar-\\nchitecture in Social Text‚Äù. In: Computaci ¬¥on y Sistemas\\n22.3 (2018), pp. 757‚Äì766.\\n[7] M. Corney, O. de Vel, A. Anderson, and G. Mohay.\\n‚ÄúGender-preferential text mining of e-mail discourse‚Äù.\\nIn:Computer Security Applications Conference(CSAC),\\nLas Vegas, USA, Dec 9-13 . 2002.\\n[8] Alexander G. de G. Matthews, Mark van der Wilk,\\nTom Nickson, Keisuke Fujii, Alexis Boukouvalas, Pablo\\nLe¬¥on-Villagr ¬¥a, Zoubin Ghahramani, and James Hens-\\nman. ‚ÄúGPflow: A Gaussian Process Library using Ten-\\nsorFlow‚Äù. In: Journal of Machine Learning Research\\n18.40 (2017), pp. 1‚Äì6. URL: http://jmlr.org/papers/v18/\\n16-537.html.\\n[9] Anees Ul Hassan, Jamil Hussain, Musarrat Hussain,\\nMuhammad Sadiq, and Sungyoung Lee. ‚ÄúSentiment\\nanalysis of social networking sites (SNS) data using\\nmachine learning approach for the measurement of\\ndepression‚Äù. In: 2017 international conference on in-\\nformation and communication technology convergence\\n(ICTC) . IEEE. 2017, pp. 138‚Äì140.\\n[10] Anees Ul Hassan, Jamil Hussain, Musarrat Hussain,\\nMuhammad Sadiq, and Sungyoung Lee. ‚ÄúSentiment\\nanalysis of social networking sites (SNS) data using\\nmachine learning approach for the measurement of\\ndepression‚Äù. In: 2017 international conference on in-\\nformation and communication technology convergence\\n(ICTC) . IEEE. 2017, pp. 138‚Äì140.\\n[11] Md Rafiqul Islam, Muhammad Ashad Kabir, Ashir\\nAhmed, Abu Raihan M Kamal, Hua Wang, and An-\\nwaar Ulhaq. ‚ÄúDepression detection from social network\\ndata using machine learning techniques‚Äù. In: Health\\ninformation science and systems 6 (2018), pp. 1‚Äì12.\\n[12] R. Kusumawati, A. D‚Äôarofah, and P. A. Pramana.\\n‚ÄúComparison Performance of Naive Bayes Classifier\\nand Support Vector Machine Algorithm for Twitter‚Äôs\\nClassification of Tokopedia Services‚Äù. In: Journal of\\nPhysics Conference Series 1320 (2019), p. 012016.\\n[13] Luthfi Ramadhan. TF-IDF Simplified . https : / /\\ntowardsdatascience . com / tf - idf - simplified -\\naba19d5f5530. [Online: accessed 30-October-2022].\\n[14] M.Rambocas and J. Gama. ‚ÄúMarketing Research: The\\nRole of Sentiment Analysis‚Äù. In: The 5th SNA-KDD\\nWorkshop11. University of Porto . 2013.\\n[15] C. Martindale and D. McKenzie. ‚ÄúOn the utility of con-\\ntent analysis in author attribution:The Federalist‚Äù. In:\\nComputers and the Humanities 29.04 (1995), pp. 259‚Äì\\n11270. URL: https://link.springer.com/article/10.1007/\\nBF01830395.\\n[16] T. Mikolov, K. Chen, G. Corrado, and J. Dean. ‚ÄúEf-\\nficient Estimation of Word Representations in Vector\\nSpace‚Äù. In: arXiv preprint arXiv:1301.3781 . 2013.\\n[17] F. A. Otoom, E. E. Abdullah, S. Jaafer, A. Hamdallh,\\nand D. Amer. ‚ÄúTowards author identification of Arabic\\ntext articles‚Äù. In: International Conference on Infor-\\nmation and Communication Systems (ICICS), Irbid,\\nJordan, April 1-3 . 2014.\\n[18] O. Sharma. ‚ÄúA New Activation Function for Deep\\nNeural Network, Faridabad, India‚Äù. In: International\\nConference on Machine Learning, Big Data, Cloud and\\nParallel Computing (COMITCon), Feb 14-16 . 2019.\\n[19] Shipra Saxena. Binary Cross Entropy/Log Loss for\\nBinary Classification . https : / / www . analyticsvidhya .\\ncom / blog / 2021 / 03 / binary - cross - entropy - log - loss -\\nfor-binary-classification/. [MARCH 3, 2021].\\n[20] stopwords-iso. Stopwords Bengali . https://github.com/\\nstopwords - iso / stopwords - bn. [Online: accessed 31-\\nMay-2021].\\n[21] L. Svoboda and T. Brychc ¬¥ƒ±n. ‚ÄúEnriching Word Embed-\\ndings with Global Information and Testing on Highly\\nInflected Language‚Äù. In: Computaci ¬¥on y Sistemas 23.03\\n(2019), pp. 773‚Äì783. URL: https://www.cys.cic.ipn.mx/\\nojs/index.php/CyS/article/view/3268.\\n[22] N. I. Tripto and M. E. Ali. ‚ÄúDetecting Multilabel Senti-\\nment and Emotions from Bangla YouTube Comments,\\nSylhet, Bangladesh‚Äù. In: International Conference on\\nBangla Speech and Language Processing (ICBSLP),\\nSept 21-22 . 2018.\\n[23] Abdul Hasib Uddin, Durjoy Bapery, and Abu Shamim\\nMohammad Arif. ‚ÄúDepression Analysis from Social\\nMedia Data in Bangla Language using Long Short Term\\nMemory (LSTM) Recurrent Neural Network Tech-\\nnique‚Äù. In: 2019 International Conference on Computer,\\nCommunication, Chemical, Materials and Electronic\\nEngineering (IC4ME2) . 2019, pp. 1‚Äì4. DOI: 10.1109/\\nIC4ME247184.2019.9036528.\\n[24] Abdul Hasib Uddin, Durjoy Bapery, and Abu Shamim\\nMohammad Arif. ‚ÄúDepression analysis of bangla social\\nmedia data using gated recurrent neural network‚Äù. In:\\n2019 1st International Conference on Advances in Sci-\\nence, Engineering and Robotics Technology (ICASERT) .\\nIEEE. 2019, pp. 1‚Äì6.\\n[25] Xinyu Wang, Chunhong Zhang, Yang Ji, Li Sun, Leijia\\nWu, and Zhana Bao. ‚ÄúA depression detection model\\nbased on sentiment analysis in micro-blog social net-\\nwork‚Äù. In: Trends and Applications in Knowledge Dis-\\ncovery and Data Mining: PAKDD 2013 International\\nWorkshops: DMApps, DANTH, QIMIE, BDM, CDA,\\nCloudSD, Gold Coast, QLD, Australia, April 14-17,\\n2013, Revised Selected Papers 17 . Springer. 2013,\\npp. 201‚Äì213.\\n12',\n",
       " 'Use of diverse data sources to control which\\ntopics emerge in a science map\\nJuan Pablo Bascur1, Rodrigo Costas1, and Suzan Verberne2\\n1Centre for Science and Technology Studies, Leiden University,\\nThe Netherlands\\n2Leiden Institute of Advanced Computer Science, Leiden\\nUniversity, The Netherlands\\nDecember 2024\\nAbstract\\nTraditional science maps visualize topics by clustering documents, but\\nthey are inherently biased toward clustering certain topics over others. If\\nthese topics could be chosen, then the science maps could be tailored for\\ndifferent needs. In this paper, we explore the use of document networks\\nfrom diverse data sources as a tool to control the topic clustering bias of a\\nscience map. We analyze this by evaluating the clustering effectiveness of\\nseveral topic categories over two traditional and six non-traditional data\\nsources. We found that the topics favored in each non-traditional data\\nsource are about: Health for Facebook users, biotechnology for patent\\nfamilies, government and social issues for policy documents, food for\\nTwitter conversations, nursing for Twitter users, and geographical entities\\nfor document authors (the favoring in this latter source was particularly\\nstrong). Our results show that diverse data sources can be used to control\\ntopic bias, which opens up the possibility of creating science maps tailored\\nfor different needs.\\n1 Introduction\\nScience maps are a form of visualization that provides a content overview of a\\ncollection of academic documents. They are typically used for literature analysis\\n[45], field delimitation, research policy, and enhanced document browsing [5]. A\\ntypical practice to create science maps is first to create a network of academic\\ndocuments where the links are an aspect of the documents (e.g. bibliographic\\nmetadata), then to cluster together the documents that are well connected, and\\nfinally to summarize the contents of these clusters. In other words, the map is\\na set of clusters that emerge from document connections, and what a cluster\\nrepresents is inferred from its documents.\\n1arXiv:2412.07550v1  [cs.DL]  10 Dec 2024In our previous work [6] we evaluated the extent to which a science map can\\nplace the documents of a topic inside clusters that contain mostly documents\\nof that same topic (i.e. to create clusters about the topic), a concept we refer\\nto as clustering effectiveness. There, we found that the clustering effectiveness\\nchanges depending on the kind of topic, or in other words, that the maps have\\na bias toward clustering certain kinds of topics more effectively than others.\\nFor example, we found that in maps based on citation links or text similarity,\\ntopics related to diseases are well clustered while topics related to geographical\\nlocations are not. This bias can prove inconvenient for science map users if their\\ntopics of interest do not align with the topic bias of the map, because then their\\ntopics would not be well represented by the map. For example, a science map\\nuser that wishes to find research about a given country will find non or few\\nclusters about this country, leading to the wrong conclusion that there is little\\nresearch about this country. In the current paper, we explore if the topic bias of\\na map can be changed by using different data sources to connect the documents\\nin the networks, which in the prior example means that the science map user can\\nselect a data source that is more effective at creating clusters about countries.\\nThe traditional data sources to create science maps are citation links net-\\nworks and text similarity networks, but to achieve our goal we explore other,\\nnon-traditional data sources. Most of our non-traditional data sources create\\nnetworks where two or more academic documents are connected with an element\\nexternal to the document (e.g. a patent that cites two documents), and for this\\nreason we will refer to these sources as external sources. Our topics are based\\non MeSH terms, and we group the topics into topic categories to facilitate our\\nanalysis. We measure the topic bias of a network as how well a topic is clustered\\n(i.e. clustering effectiveness) over several clustering solutions, each of them with\\ndifferent cluster sizes. Each of these clustering solutions is analogous to a very\\nsimple science map. We use the topic bias of text similarity networks as our\\nreference to compare how the topic bias changes in other networks.\\nOur research question is: Which topic categories benefit from using each\\nexternal source? We operationalize this benefit in two ways: First, if the clus-\\ntering effectiveness of the topic category in the network of the external source\\nis higher than the effectiveness of the same topic category in the text similarity\\nnetwork, and second, if the clustering effectiveness of the topic category in the\\nexternal source is higher than the other topic categories in the same external\\nsource. We will consider both operationalisations to address our research ques-\\ntion, but give more importance to the first one because it serves the needs of\\nscience map users more directly.\\nOur contributions are: (1) We present an expanded and improved analy-\\nsis method for evaluating the clustering effectiveness of a topic; (2) With this\\nmethod, we provide a large-scale analysis of eight different sources (two tradi-\\ntional and six external), twenty one networks of up to four million documents,\\nnearly three thousand clustering solutions, and seventeen topic categories, each\\none usually composed of between fifty and three hundred topics (values vary\\nbetween networks); (3) With this analysis, we show that topic bias can be\\nchanged using external sources, and also which are topics categories favored for\\n2each of the external source. This knowledge expands the customization options\\nof science maps.\\n2 Background\\nIn this section we explore several topics related to our paper, provide literature\\nexamples for each of them, and explore how our paper relates to the most\\nrelevant ones.\\n2.1 Interaction of academic documents with non-academic\\nelements\\nTraditionally, policy makers analyze scientific production to evaluate scientific\\nimpact, but they also are interested in evaluating its societal, technological and\\npolicy-making impact. For societal impact, the impact of publications on so-\\ncial media has been suggested as a proxy [40], and we highlight the company\\nAltmetric [2, 14, 12], which collects mentions to academic documents online,\\nincluding social media. For technological impact, patents are used [26]. Policy-\\nmaking impact is a more recent field of study, and we highlight the company\\nOverton [34, 15], which collects ample datasets of policy documents and their\\nreferences [12]. We also highlight the company Dimensions [19], which collects\\nthe connections of academic documents to citations, clinical trials, patents, pol-\\nicy documents, grants and datasets.\\n2.2 Science maps based on diverse sources\\nScience maps of academic documents typically use networks of citations links or\\ntext similarity [36], but both Janssens, Gl¬® anzel, and De Moor [21] and Ahlgren\\net al. [1] proposed networks that combine both citation links and text similar-\\nity. Also, Costas, de Rijcke and Marres [10] proposed a conceptual framework\\nfor analyzing the interaction between documents and social media by creating\\nnetworks of co-occurrence. Their framework is our source of inspiration for us-\\ning external sources to improve science maps and also for how we build the\\nnetworks of external sources. The main difference between their networks and\\nour networks is that in their networks co-occurrence is explicitly included in the\\nweight of the edges, while in our networks it is implicit by building the network\\nwith both the documents and the elements where the documents co-occur, an\\napproach similarly to the work of Yun, Ahn and Lee [44].\\nAn alternative method to create science maps is to create a network where\\nthe clusters are not made of academic documents, so to obtain a different per-\\nspective on the academic data. Keywords can be used to identify the topics\\nwithin a collection of documents, connecting the keywords by the documents\\nwhere they co-occur [25]. This has a slightly different functionality from identi-\\nfying topics using document clusters, like to study the evolution of topics over\\ntime [39]. Authors can be to used identify scientific collaborations, connecting\\n3the authors either by their co-authorships [28] or their citations [38]. Patents\\ncan be used to identify technological developments, connecting the patents by\\ntheir cited documents [24]. By their nature, networks of elements that co-occur\\nwith academic documents can be turned into networks of documents that co-\\noccur with these elements. For example, Tang and Colavizza [43] created two\\nnetworks using the same data, one of documents cited by the same Wikipedia\\narticle, and one of Wikipedia articles citing the same document. In this ex-\\nample, the co-occurrences where explicit, but Carusi and Bianchi [8] created a\\nbipartite network of authors and journals where the co-occurrences where im-\\nplicit. This allowed them to create clusters for both the authors and the journals\\nusing the same network with a method they called co-clustering. In our paper\\nthe external source networks are also bipartite, but our methodology will only\\nfocus on clustering the academic documents, not the external source elements.\\n2.3 Criticisms to maps of science\\nThere are several criticisms of the capacity of science maps to represent topics.\\nGl¬® aser [16] reported that expert based evaluation of maps is usually inconclusive.\\nHeld, Laudel and Gl¬® aser [17] found that the science maps were unable to have\\nboth at the same time one topic per cluster and one cluster per topic. Held\\nand Velden [18] found that clusters represent individual species instead of a\\nbiological field. Hric, Darst and Fortunato [20] made a strong criticism of the\\ncapacity of any kind of clustering algorithm in any kind of network to create\\nclusters where all the cluster nodes belong to a given category. Because of\\nthe failure of science maps to properly cluster all topics, topic wise evaluation\\nof science maps aims to make a more granular evaluation of the clustering and\\nidentify which topics get more effectively clustered, instead of making an overall\\nstatement about the quality of the map. This area of research has been sparsely\\nexplored by the literature. As far as we know, beyond our prior work [6], the\\nonly topical analyses that exist are the expert based evaluations of science maps\\nand, to a lesser extent, the exploration of the epistemic function of intra- and\\ninter-cluster citations performed by Seitz et al. [31].\\n2.4 Comparing clustering solutions of different networks\\nDifferent networks generate different science maps, and there have been several\\nattempts to compare the clustering solutions of different networks. Xu et al.[42]\\nidentified overlapping communities between the clusters of two networks with\\nthe same nodes. Xie and Waltman [41] did something similar, but using topic\\nmodeling instead of text similarity networks. ÀáSubelj, Van Eck and Waltman [33]\\nevaluated the quality of the clusters generated by different clustering algorithms\\nfrom the same network. Their method evaluated if the topics of the clusters\\ncorrespond to the topics of the field experts, and also evaluated attributes of the\\nclustering, like clustering stability, computing time, and cluster size. Waltman\\net al. [36] compared clustering solutions from different networks with the same\\nnodes using an additional network as reference to calculate the accuracy of the\\n4clusters. For an example that does not use clustering, Ba and Liang [3] identified\\noverlapping edges between two networks with the same nodes. In our prior work\\n[6], we compared the clustering effectiveness per topic by evaluating the extent\\nto which topic documents are in few clusters and the extent to which these\\nsame clusters only contain topic documents. In the current paper we refine this\\nmethod so its results are easier to interpret.\\n3 Methods\\nIn this section we describe how we obtained and cleaned the data, created the\\nnetworks and clusters, evaluated the clustering effectiveness, and compared the\\ntopic categories.\\n3.1 Core academic documents\\nThis is the set of documents that we used in the evaluation of clustering effec-\\ntiveness, and each network has a different subset of these documents depending\\non the data available for each external source. We selected all Web of Science\\ndocuments from the CWTS local database published between the years 2016\\nand 2019 that have a PubMed id (which is necessary to have MeSH terms) and\\nthat have a noun phrase in the title or abstract sections. The latter condition\\nwas added to have high quality text similarity networks, and the noun phrases\\nwere identified using the method developed by Waltman and van Eck [37]. We\\nchoose this range of years so as to have enough connections between the doc-\\numents and the external source elements, especially with patents because they\\ntake multiple years to accumulate, and also because in these years Twitter be-\\ncame popular for sharing academic documents while not being the years of the\\nCoronavirus pandemic. The external source elements are not limited by this\\ntime period. In total, our core set contains 4,142,511 documents.\\n3.2 External sources networks\\nThe external source networks are built the following way: For each external\\nsource, we first define what the nodes of this source mean (e.g. academic doc-\\nument authors, facebook users, etc. . . ), which we will refer to as the external\\nsource ‚Äúelements‚Äù. Then we select core academic documents and external source\\nelements that we will use in the network, such that all the documents are con-\\nnected to at least one element and all the elements are connected to at least two\\ndocuments. We use the ‚Äúat least two documents‚Äù threshold so that we do not\\nhave documents without any indirect connections with other documents (there\\nare no direct connections between documents). Then we create a network with\\nthese documents and elements where the edges that connect them are undi-\\nrected and have weight value 1, the document nodes have weight value 1 and\\nthe element nodes have weight value 0. We give this weight value to the element\\nnodes so that the clustering algorithm does not take these nodes into account\\n5when calculating the quality of a cluster. We will refer to these networks as\\nthe ‚ÄúPure‚Äù networks of an external source, to distinguish them from the mixed\\nand the text similarity networks of an external source (see below). It is worth\\nmentioning that this network creation design creates a bipartite network (only\\ndocument to non-document edges), while in science mapping literature it is\\nmore common to represent these relations a co-occurrence network (only doc-\\nument to document edges with no non-document nodes, and the weight value\\nof the edge is the number of non-document elements in common between the\\ndocuments). We use bipartite networks because they represent these relations\\nwith more computational efficiency than co-occurrence networks.\\nWe used the following external sources. All databases are the local version\\nfrom CWTS:\\nDocuments authors (AUTHOR): The external source elements are the\\nauthors of academic documents, and the connections are to these documents.\\nThe data comes from the disambiguated authors database of CWTS [13]. This\\nnetwork has 3,977,303 core academic documents, 2,710,012 external source ele-\\nments and 19,820,564 edges.\\nFacebook users (FACEBOOK): The external source elements are the\\nFacebook users (i.e. accounts), and the connections are to the documents they\\nhave posted web links to. The data comes from the Altmetric [2] Facebook\\ndatabase. This network has 596,783 core academic documents, 44,811 external\\nsource elements and 1,231,887 edges.\\nTwitter users (TWUSER): The external source elements are the Twitter\\nusers (i.e. accounts), and the connections are to the documents that their tweets\\nhave web links to. The data comes from the Altmetric [2] Twitter database.\\nThis network has 2,364,304 core academic documents, 1,495,275 external source\\nelements and 27,981,494 edges.\\nTwitter conversations (TWCONV): The external source elements are\\nthe Twitter conversations, and the connections are to the documents that its\\ntweets have web links to. A Twitter conversation is an original (non-reply) tweet\\nplus all the tweets that directly or indirectly reply to it. The data comes from\\nthe Altmetric [2] Twitter database. This network has 227,212 core academic\\ndocuments, 493,049 external source elements and 1,175,624 edges.\\nPatents families (PATENT): The external source elements are patent\\nfamilies, and the connections are to the documents cited by the patents of the\\npatent family. A patent family is made up of an initially submitted patent, plus\\nderivative patents (like updates or new application) and versions of the patent\\nsubmitted in different countries. The data comes from the PATSTAT database\\n[22] and we only use invention patents. This network has 98,278 core academic\\ndocuments, 41,714 external source elements and 175,693 edges.\\nPolicy documents (POLICY): The external source elements are policy\\ndocuments, and the connections are to the documents cited by the policy doc-\\numents. A policy document is a document written primarily for policy makers,\\nand includes documents such as memos and guidelines from governments and\\nthink tanks. The data comes from the Overton database [34]. This network has\\n311,867 core academic documents, 64,951 external source elements and 651,099\\n6edges.\\n3.3 Text similarity networks\\nWe use the topic bias of text similarity networks in our experiments as a ref-\\nerence to compare how the topic bias changes in other networks. We chose\\nthis source because it is traditionally used for the creation of science maps and\\nalso because it is less computationally demanding to create and cluster than\\nthe citation network, which is relevant because we created a reference network\\nfor each external source. The method to measure text similarity was the co-\\nsine similarity between the embedding of the text of two documents. The text\\nof a document is its concatenated title and abstract, and the embedding is\\nextracted using the Python implementation of Sentence BERT (SBERT) [30]\\nwith the ‚Äúallenai-specter‚Äù model [9], which is a model specifically trained with\\nscientific literature.\\nFor each external source, we create a text similarity network that contains\\nthe same academic core documents as the Pure network, which we will refer\\nto as the ‚ÄúBERT‚Äù network, and we also create a network that combines both\\nnetworks, which we will refer to as ‚ÄúMixed‚Äù network. To create the BERT\\nnetwork of a source we first make the academic documents into nodes with\\nweight value 1. Then, we calculate the text similarity between all pairs of\\ndocuments and only keep the 20 highest pairs per document. These values\\nbecome the weights of the undirected edges between the nodes, and if there\\nare two edges between two nodes then we merge them and sum their weights.\\nFinally, we multiply all the edge weight values by a factor such that the sum\\nof all edge weight values in a network is the same for the BERT and the Pure\\nnetworks. To create the Mixed network of a source we use the Pure network and\\nadd to it the edges from the BERT network. The purpose of the step where we\\nmultiply the edge weight values by a factor is to bring this network to the same\\nmagnitude as the Pure network, which has two goals: To make the edges that\\ncame from the BERT and Pure network have the same magnitude of influence in\\nthe clustering of the Mixed network, and to use the same clustering Resolution\\nvalues for the BERT and Pure networks, which is just convenient.\\n3.4 Citation network\\nThere are not many science maps studies published using SBERT for text sim-\\nilarity because it is a recently developed method, making our results difficult\\nto compare to the literature. To solve this, we also evaluated the topic bias\\nof a network that is built based on a method well researched in the literature\\nand presented it next to the other external source networks. This well pub-\\nlished method is the extended direct citation [36], which is a citation network\\nthat includes connections to academic documents that are not part of the core\\nacademic documents. The Pure citation network includes all the core academic\\ndocuments as nodes with weight value 1 and the citations between each other as\\nundirected edges with weight value 1. It also includes the non-core documents\\n7from Web of Science that have citation links to at least two core academic\\ndocuments as nodes with weight value 0, and these links as undirected edges\\nwith weight value 1. These non-core documents are usually documents from\\noutside the time period or that do not have a PubMed id. This network has\\n4,142,511 core academic documents, 18,960,516 non-core academic documents\\nand 217,907,980 edges. The Mixed and BERT citation networks are created\\nthe same way as for the external sources (the BERT network uses only the core\\nacademic documents).\\n3.5 Clustering\\nTo cluster we used the Leiden algorithm [35], which is typically used in science\\nmaps. This algorithm requires the user to set a parameter, the ‚ÄúResolution‚Äù,\\nwhich has an effect on the size of the clusters (higher Resolution, smaller clus-\\nters). We clustered each network several times using a wide range of Resolution\\nvalues, using a different value each time. We decided on the Resolution values\\nrange on a network wise basis, and our criteria for this range was for the highest\\nvalue to create a clustering solution where most clusters have only one node,\\nand for the lowest value to create a clustering solution where most of the nodes\\nbelong to a single cluster. We clustered a number of Resolution values that\\nallowed us to keep the running time manageable (between 70 and 140 Resolu-\\ntion values per network), using the Python implementation of the library Igraph\\n[11] and the Leiden algorithm. All the clustering solutions are used during the\\nevaluations and comparisons.\\n3.6 Topics and topic categories\\nOur topics are the tree nodes in the MeSH hierarchical tree of MeSH terms, and\\nthe topic documents of a given topic are the documents labeled with the tree\\nnode of a topic. MeSH terms are a controlled vocabulary thesaurus from the\\nNational Library of Medicine (NLM) used for indexing PubMed, and are semi-\\nautomatically annotated to documents by the NLM [27]. We use MeSH terms\\ninstead of other alternatives because of their extensive system of hierarchical\\ntopics, high number of annotated documents, and high quality of annotations.\\nThe MeSH terms are organized in a hierarchical tree where almost each MeSH\\nterm maps to one or more nodes in the tree, but each tree node maps to a single\\nMeSH term. The tree is composed of 16 branches, and the tree nodes in the\\nlower levels are subtopics of the tree nodes in the higher levels. We refer to a\\ntree node using their MeSH term name followed by their tree node identity (e.g\\nHead [A01.456] ). The reason why we base our topics on the tree nodes of the\\nMeSH terms instead of just using the MeSH terms themselves is to facilitate\\nthe expansion and filtering of topics in the next steps of the methodology (see\\nbelow). We obtained the MeSH terms annotated for each document, plus the\\nmetadata of the MeSH terms themselves, including their tree nodes, from the\\nin-house CWTS database of PubMed and MeSH (version from 2024).\\n8Our topic categories are the MeSH tree branches, and all the tree nodes in\\nthe branch are topics that belong to the topic category. We use branches as\\ntopic categories because they are epistemological categories (e.g., organisms),\\nwhich are the kind categories commonly used for topical analysis of clusters\\n[31, 6]. There are 3 branches that we decided to, instead of using them as\\ntopic categories, use their highest level tree nodes as topic categories, because\\nwe think these tree nodes work better than their branches as topic categories.\\nThe branches that we replaced with their higher level tree nodes are Disciplines\\nand Occupations [H] ,Anthropology, Education, Sociology, and Social Phenom-\\nena [I] andTechnology, Industry, and Agriculture [J] . We also removed the\\nfollowing topic categories due to having too few topics: Humanities [K] ,Publi-\\ncation Characteristics [V] ,Human Activities [I03] , and Non-Medical Public and\\nPrivate Facilities [J03] . In the end, we used the 17 topic categories in Table 1.\\nTable 1: List of topic categories used in the current paper.\\nTopic Categories\\nAnatomy [A]\\nOrganisms [B]\\nDiseases [C]\\nChemicals and Drugs [D]\\nAnalytical, Diagnostic and Therapeutic Techniques, and Equipment [E]\\nPsychiatry and Psychology [F]\\nPhenomena and Processes [G]\\nNatural Science Disciplines [H01]\\nHealth Occupations [H02]\\nSocial Sciences [I01]\\nEducation [I02]\\nTechnology, Industry, and Agriculture [J01]\\nFood and Beverages [J02]\\nInformation Science [L]\\nNamed Groups [M]\\nHealth Care [N]\\nGeographicals [Z]\\nTo have good topics, we would like each topic to be annotated on all the\\ndocuments related to it, but the NLM typically only annotates up to fifteen\\nMeSH terms per document, which means that the more generic MeSH terms\\nare not annotated. To fix this, we expanded the topics annotated on a document\\nusing the already annotated MeSH terms and the MeSH tree. We transformed\\neach of the MeSH terms into all of their corresponding MeSH tree nodes, and\\nthen we added all the MeSH tree nodes upstream in the MeSH tree from the\\ncurrent MeSH tree nodes. For example, if a document had the MeSH term Scalp ,\\nwe transformed this MeSH term into its tree node version ( Scalp [A01.456.810] ),\\nand added the upstream tree nodes ( Head [A01.456] ,Body Regions [A01] ) to\\nthe document.\\n9To improve the reliability of our evaluation we filter our topics. We do this\\nfiltering process for each external source because they use different sets of core\\nacademic documents. Our first filter criterion is by topic size (i.e. number of\\ndocuments with the topic) because the size of a topic can affect its clustering\\neffectiveness. We group the topics by size into Size bins, which go from a value\\n(excluding it) to double that value (including it), starting at 40 (e.g. 41-80,\\n81-160, 161-320, . . . [ X+ 1]-[2 X]). We use 40 for reasons explained in the next\\nsubsection. We filter out the Size bins that have less than half the number of\\ntopics than the Size bin with most topics, and also filter out the topics that\\nbelonged to these filtered out Size bins. The Size bins that we keep per source\\nare shown in Table 2.\\nTable 2: Size bins per source after filtering.\\nSource Size Bins\\nPatents families 41-80; 81-160; 161-320\\nPolicy documents 41-80; 81-160; 161-320\\nFacebook users 41-80; 81-160; 161-320; 321-640\\nTwitter conversations 41-80; 81-160; 161-320; 321-640\\nTwitter users 81-160; 161-320; 321-640; 641-1,280\\nDocuments authors 161-320; 321-640; 641-1,280; 1,281-2,560\\nCitations 161-320; 321-640; 641-1,280; 1,281-2,560\\nOur second filter criterion is redundancy (i.e. two topics share a substantial\\nnumber of documents) because it can distort our results. To filter by redun-\\ndancy, we first identify the topics within the same topic category that are re-\\ndundant with each other. We define two topics as being redundant if they have\\na Jaccard similarity of 0.5 or higher (calculated from their number of shared\\ndocuments). We group the redundant topics using the agglomerative hierar-\\nchical clustering algorithm with the Complete Linkage method and Jaccard\\ndistance, with 0.5 as threshold. Then, we filter out each but the smallest topic\\nfrom each group, which in our experience tends to also be the topic that best\\nrepresents the other topics in the group. For example, if there is a group of re-\\ndundant topics made up of Canidae [B01.050.150.900.649.313.750.250.216] and\\nDogs [B01.050.150.900.649.313.750.250.216.200] , we believe that these topics\\nare better represented by the latter than the former. In cases where a group\\nhad more than one smallest topic, we selected the one with the tree node at the\\nlowest level in the tree. After filtering topics, we also filter the topic categories\\nthat contain too few topics in a Size bin. We choose this threshold manually per\\nexternal source, but it is always at least between 5 and 10 topics. It is worth\\nmentioning that in our prior work [6] we defined two topics as being redundant\\nif they had Jaccard similarity 0.9 or higher, so in the current paper we are being\\nsubstantially stricter at ensuring the quality of the data.\\n103.7 Evaluation\\n3.7.1 Clustering effectiveness\\nTo find out which topics are better represented by the clustering of the networks,\\nwe use the concept of clustering effectiveness that we introduced in our prior\\nwork [6]. The unit to measure the clustering effectiveness is ‚ÄúPurity‚Äù, which\\nis, for a set of selected clusters, which fraction of their documents belong to a\\ngiven topic. In mathematical terms, Purity is defined as:\\nPurity =PN\\ni=1|Di‚à©DM|\\nPN\\ni=1|Di|(1)\\nHere, Ndenotes the number of selected clusters, Didenotes the documents in\\nselected cluster iandDMdenotes the topic documents of the topic. The higher\\nPurity, the more effective the clustering. Purity is bounded between values zero\\nand one, with Purity value one meaning that the selected clusters only contain\\ntopic documents. We calculate Purity for each clustering solution and topic, but\\ninstead of selecting all the clusters that contain topic documents to calculate\\nPurity, we only select a subset of these clusters. To do this, we sort all the\\nclusters that contain topic documents from the highest to the lowest number of\\ntopic documents, with ties won by the smallest cluster. Then, we choose the\\nthreshold of the minimum number of topic documents that we want the set of\\nselected clusters to contain, and then select clusters in the sorted order until we\\nreach this threshold. We call this value Coverage, and it is a fraction of the total\\nnumber of topic documents. In our paper we calculate Purity for three Coverage\\nvalues: 0.25, 0.50 and 0.75. We only compare Purity values calculated using the\\nsame Coverage value. In reference to the prior subsection, the reason why Size\\nbins start at 40 is because at Coverage 0.25 the value of the threshold is only 10\\ndocuments, which we set as the minimum to have a meaningful academic topic.\\nIn our concept of clustering effectiveness, the number of selected clusters\\n(NSC) also plays a role. In a science map, finding clusters related to a topic\\nrequires effort, so the smaller the NSC, the higher the cluster effectiveness. Also,\\na high NSC is correlated with smaller clusters, which itself is correlated with\\nhigher Purity because smaller clusters allow a more fine selection of the clusters.\\nFor example, if all clusters in a clustering solution are size one, then the value of\\nPurity is also one because all the selected clusters contain only topic documents.\\nTo control for the effect of NSC over Purity, we only compare Purity values when\\nthey have the same NSC.\\n3.7.2 Topic Purity profiles\\nIn our research question, we operationalized the concept of ‚Äùbenefit‚Äù in two\\nways, and in this subsection we will explain we will present our results such to\\naddress both ways. The first operationalisation was if the clustering effectiveness\\nof the topic category in the external source (either the Pure or Mixed network)\\nis higher than the same topic category in text similarity (the BERT network).\\n11We answer this question by comparing the clustering effectiveness of each topic\\nbetween these networks, and we represent the clustering effectiveness of a topic\\nfor a given network as a series of Purity and NSC values that we will refer to\\nas the topic ‚ÄúPurity profile‚Äù. The different Purity and NSC values come from\\neach of the clustering solutions generated for the network. The Purity profile of\\na topic is the Purity for each NSC value, the the NSC values are a consecutive\\nsequence of integers that go from 1 to N, and Nis:\\nN=‚åäS‚àóCov\\n5‚åã (2)\\nHere, Sis the size of the topic, Covis the coverage value, and function ‚åäx‚åãmeans\\nrounded down to the nearest integer. Therefore, the number of NSC values in\\na Purity profile depends on the size of the topic. The denominator value five\\nensures that the average number of topic documents per selected cluster is five\\nor more, so to limit the NSC to a value that could make sense in a science map\\ncontext. As for the Purity values, if there is more than one Purity value for a\\ngiven NSC, we only use the highest one. If there is no Purity value for the NSC\\nvalue one, we use Purity value zero. If there is any other Purity value missing\\nfor a given NSC value, we estimate it by linear regression between the Purity\\nvalues of the two nearest NSC values with known Purity. Figure 1 is an example\\nof how the Purity profile of a topic looks like.\\nTo compare two topic Purity profiles, we compare their Purity at each NSC\\nvalue (they must have the same NSC values), and say that one Purity profile is\\nhigher than another if they have a higher Purity in at least half of their NSC\\nvalues. Figure 2A shows an example diagram of how we calculate these results.\\nWe refer to the fraction of topics in a topic category that are higher than in\\nBERT as the ‚Äúabsolute Purity difference‚Äù of this topic category. This value\\nanswers the first operationalisation of our research question, and it indicates\\nthe extent to which the topics of a topic category achieve higher Purity in the\\nPure or Mixed network than in the BERT network. For example, if the absolute\\nPurity difference of a topic category in the Pure network of an external source\\nis 0.25, it means that a quarter of its topics have higher Purity in the Pure\\nnetwork than the BERT network.\\n3.7.3 Topic category Purity profiles\\nThe second operationalization was if the clustering effectiveness of the topic\\ncategory in the external source (either the Pure, Mixed or BERT network) is\\nhigher than the other topic categories in the same external source. To compare\\ntopic categories within a network we create a topic category Purity profile for\\neach of their Size bins, and only compare profiles with the same Size bin. To\\ncreate the Purity profile of a topic category at a given Size bin, we first obtain\\nits Purity and NSC values for each Resolution. However, Purity and NSC values\\nare only defined for topics, not for topic categories, so we define the Purity and\\nNSC values of a topic category at a given Size bin and Resolution values as the\\nmedian Purity and NSC of the topics of that topic category in that Size bin and\\n12Figure 1: Example of a Purity profile. This is the Purity profile of the topic\\nBacillus thuringiensis [B03.510.460.410.158.218.800] for the Policy documents\\nBERT network calculated using Coverage 0.50. This topic has 60 topic docu-\\nments among the core documents used by the Policy networks, which for this\\nCoverage value means that the Purity is calculated after selecting clusters that\\ncontain at least 30 topic documents. So for example, if we assume that the\\nselected clusters contain exactly 30 topic documents, from the figure we can\\nsay that at different Resolution values the network can place 30 out the 60\\ntopic documents in one cluster containing 150 documents (30 /0.2), two clusters\\ncontaining 75 documents (30 /0.4), and four clusters containing 50 documents\\n(30/0.6). Using lower Coverage values or topics with more topic documents\\ntends to achieve higher Purity by the last NSC value.\\nat that Resolution. Then, to build the topic category Purity profile, we use the\\nPurity and NSC values following the same protocol that we did for building the\\nPurity profiles of topics, with the difference that instead of calculating the Nof\\nthe Purity profile (i.e. the highest NSC value in the profile) using Sas the size of\\na topic, we calculate the Nusing Sas the average between the lower and upper\\nbound of a Size bin (e.g. for Size bin 41-80, S= 60, and if Cov = 0.25, then\\nN= 3). As a note, we want to mention that we considered using topic category\\nPurity profiles instead of topic Purity profiles for the first operationalization,\\nbut we found that the results from this approach provided us with less nuanced\\ninformation than the one we ultimately used. However, as a general rule, if\\nhalf or more of the topic Purity profiles in a topic category were higher in the\\nexternal source than in BERT (absolute Purity difference >0.5), then their\\ndifferent Size bin topic category Purity profiles were also higher than in BERT.\\nTo compare the topic categories Purity profiles of a given network with each\\nother, we do not calculate which one is higher as we did for the topic Purity\\nprofiles, but instead we calculate how often their Purity is higher than each\\nother. We do this because we want to consider all the topic categories at the\\nsame time, and also because which one is better tends to change frequently\\nacross the NSC, probably because same of them have very similar clustering\\neffectiveness. Therefore, for each Size bin in a network, we consider all the topic\\n13categories Purity profiles at the same time, and for each NSC value (they all\\nhave the same NSC values because they have the same Size bin and Coverage)\\nwe identify which topic categories are among the top third highest Purity value\\nat this NSC value. Then, for each topic category we report for how many (as a\\nfraction) NSC values it is among the top third, averaged over all the Size bins.\\nFigure 2B shows a diagram of how we calculate these results. For example, if\\nthe top third count of a topic category in the Pure network of an external source\\nis 0.25, it means that, on average across the Size bins, it is among the top third\\nhighest Purity topic categories of the Pure network for a quarter of the NSC.\\nWe defined the top value in relative terms (as a third) because different external\\nsources have a different number of topic categories.\\nThe top third count already answers the second operationalisation, but we\\nwould like to go one step further to know how the external source is different\\nfrom text similarity. To do this, we subtract the Pure or Mixed network top\\nthird count from the BERT top third count to obtain a value that we refer to\\nas the ‚Äúrelative Purity difference‚Äù of that topic category. This value indicates\\nthe extent to which a topic category achieves higher Purity than the other topic\\ncategories in the Pure or Mixed networks, but not so in the BERT network. For\\nexample, if the relative Purity difference of a topic category in the Pure network\\nof an external source is 0.25, it means that in the Pure network the top third\\ncount of that topic category is 0.25 higher than in the BERT network.\\n3.7.4 Summary of comparisons methods\\nWe compare the clustering effectiveness of topic categories for an external source\\nusing two metrics: The absolute Purity difference, which indicates the difference\\nbetween the Pure or Mixed networks against the BERT network for this topic\\ncategory, and relative Purity difference, which indicates the difference of how\\nthis topic category relates to the other topic categories within the Pure or Mixed\\nnetworks against how it relates in the BERT network. Both the absolute and\\nrelative Purity differences are important to understand which topic categories\\nbenefit from using each external source. The absolute difference is the most\\nimportant because it indicates if a topic does better or worse than in BERT,\\nwhile the relative difference only ranks topic categories within the network.\\nHowever, the Purity could increase with better methods for creating the clusters\\nand networks, especially considering that we did not focus on achieving high\\nPurity. In such cases, the relative difference can suggest which topic categories\\ncan achieve high Purity after refining the methods, even if we achieved a low\\nabsolute difference.\\n4 Results\\nFrom now on, we will refer to specific networks of an external source using the\\nfollowing prefixes: ‚Äúb‚Äù for the BERT network, ‚Äúm‚Äù for the Mixed network, and\\n‚Äúp‚Äù for the Pure network, so for example ‚ÄúmTwconv‚Äù is the Mixed network\\n14Figure 2: Diagram on the representation of results. A: How to calculate from\\ntopic Purity profiles if a topic has higher clustering effectiveness than BERT\\nin the Pure or the Mixed network. In this example, a topic has higher Purity\\nthan BERT for the Mixed network, but not so for the Pure network. B: How\\nto calculate from topic category Purity profiles the number of NSC that a topic\\ncategory is in the top third Purity of a network. In this example, the topic\\ncategories A, B and C achieve a top third count of 0.7, 0.3 and 0, respectively\\nof the Twitter conversations. In this section we will present the results of our\\nexperiments (which are reported in Table 3 and summarized in Table 4). We will\\nreview these results going over each external source, focused on which networks\\ndid the best per topic category and the magnitude of this performance, which is\\nalso summarized in Table 5. We will limit our discussion of the topic category\\nOrganisms [B] because most external sources showed an improvement on it,\\nwhich suggest that BERT is particularly bad at it. We will also limit our\\ndiscussion of the Coverage because the three Coverage values produced roughly\\nthe same result, with very few exceptions. For the topic categories that look\\ninteresting, we explore if there is a common theme among their high Purity\\ntopics. We also explore their topic category Purity profiles (Figure 3) to see if\\nthey are ‚Äúcompetitive‚Äù, which means that its Purity profile is close or higher to\\nBERT, and therefore, a science map created using this network might achieve\\na clustering effectiveness similar or higher than a BERT network for this topic\\ncategory.\\n4.1 Citations\\nAs Table 4 shows, mCitation did better than BERT, and it was the best network\\noverall, which aligned with prior results in the literature where networks that\\ncombines citations and text outperforms both [7, 36]. pCitation also did better\\n15Table 3: Detail of the results of each network. For each topic category, we show\\nthe top third count and the absolute Purity difference at each Coverage value.\\nValues zero are not shown. Dots mean that the topic category was not included\\nin the experiment due to having too few topics per Size bin, as explained in the\\nfiltering process.\\n16Table 4: Summary of the results for each network. This table shows the abso-\\nlute and relative Purity difference, but only the highest of the three Coverage\\nvalues. All values are derived from Table 3. ‚ÄúM‚Äù means the Mixed network\\nand ‚ÄúP‚Äù means the Pure network. Light green and dark green means that the\\ntopic category has at least 0.2 and 0.5 absolute Purity difference, respectively.\\nOne and two plus signs mean that the topic category has at least 0.2 and 0.5\\nrelative Purity difference, respectively. The relative Purity difference is calcu-\\nlated from the top third count in Table 3. Dots mean that the topic category\\nwas not included in the experiment due to having too few topics per Size bin,\\nas explained in the filtering process.\\n17Table 5: Best networks per topic category from Table 4. We selected the network\\nwhere the topic category had the highest absolute difference, relative difference,\\nor a combination of both if possible. We gave more importance to absolute\\ndifference than to relative difference (i.e., in Table 4, dark green is better than\\ntwo plus symbols) because it is more important for our research question. The\\nmagnitude, shown by a number of stars, indicates the value of the best networks\\ndifferences: Zero stars means either one plus symbol, two plus symbols, or light\\ngreen. One star means both one plus symbol and light green. Tw o stars mean\\neither dark green or light green with two plus symbols. Three stars mean dark\\ngreen with two plus symbols. In the abbreviations, ‚Äúb‚Äù stands for the BERT\\nnetwork, ‚Äúm‚Äù for the Mixed network, and ‚Äúp‚Äù for the Pure network. ‚ÄòTw‚Äô stands\\nfor Twitter, and ‚Äòconv‚Äô stands for conversations.\\nCategory Best Networks Magnitude\\nAnatomy mTwconv\\nOrganisms mPatents, pFacebook, pAuthor **\\nDiseases pPolicy, mTwconv\\nChemicals mPatents, pPatents, mPolicy, pPolicy, mTwconv\\nAnalytical mFacebook, mTwconv\\nPsychiatry pPolicy, mTwconv, pTwconv, pAuthor\\nPhenomena pPatents, mTwconv\\nNatural Sc. mTwconv, pTwconv\\nHealth Occ. pFacebook **\\nSocial Sci. mTwconv, pTwconv, pTwauthor\\nEducation -\\nTechnology mPatents *\\nFood and B. mTwconv **\\nInformatio. mTwconv, pTwauthor\\nNamed Grou. pFacebook **\\nHealth Car. mTwconv, pTwauthor\\nGeographic pAuthor ***\\n18Figure 3: Examples of Purity of several topic categories for different networks.\\nThese examples are referenced in the results section. All Purity profiles are\\ncalculated for Size bin 161-320 and Coverage 0.50. It is worth reminding that\\nthe Purity profiles of topic categories are made of the average Purity and NSC of\\nthe topics of that Size bin and topic category at each of the clustering solutions.\\nHowever, a way to interpret them is to imagine they are the Purity profile of a\\ntopic that represents all topics of the topic category. This imaginary topic would\\nhave the average size of the Size bin (240 documents), and therefore each NSC\\nwould contain at least 120 topic documents. It is important to mention that,\\neven as they all have the same Size bin, the Purity values should not be compared\\nbetween different external sources because each source has a different set of core\\nacademic documents. This is why we only compare networks that belong the\\nthe same external source, as they have the same set of core documents.\\n19than the other external sources, specially with Chemicals and Drugs [D] , but\\nfor most topic categories it did not match BERT performance (i.e. absolute\\nPurity difference >0.5), which shows that BERT is a good reference source\\nfour our analysis (with the exception of the topic category Organisms [B] ). The\\nperformance of BERT over pCitation is also interesting because in our prior work\\n[6] we compared citation networks (using the same network creating method)\\nwith text similarity networks based on the BM25 text similarity metric (a metric\\nthat matches and weights the words in common between documents), and found\\nthat they had similar clustering effectiveness. This suggests that BERT does\\nbetter than BM25, which makes sense because BERT is a much more refined\\nmethod, but we did not test this comparison directly. The fact that most\\nnetworks outperform BERT at Organisms [B] could be because BERT is an\\nembedding representation of the text, which means that it learns the context of\\nwords. Given that the words that surround the name of organisms can be very\\nsimilar, BERT might struggle distinguishing them, so for this topic category it\\nmight be better to do text vectorization by term frequency (like BM25) instead\\nof embeddings.\\n4.2 Twitter conversations\\nThe network mTwconv had the best overall performance (after citation net-\\nworks) because it has at least 0.2 absolute Purity difference in every category.\\nWe believe this happens because conversations are more topically narrow than\\nthe elements of the other external sources. Also, mTwconv was the best with\\ntopic category Food and Beverages [J02] , which we believe can be due to con-\\nversations about nutrition on Twitter. It is interesting that, on the other hand,\\npTwconv no topic category with absolute Purity difference 0.2 or higher, and\\nthat the topic categories in mTwconv with the greatest improvement ( Food and\\nBeverages [J02] andGeographicals [Z] ) are not the same as in pTwconv (which\\nareNatural Science Disciplines [H01] ,Social Sciences [I01] andNamed Groups\\n[M]). This low absolute difference and difference in topic categories suggests that\\npTwconv needs the support of bTwconv to make a good clustering, which might\\nbe related to the low number of edges in the network (e.g. pTwconv has about\\ntwo edges per external source element, while pTwauthor has about twenty).\\nThe profile of Food and Beverages [J02] andGeographicals [Z] in mTwconv is\\nslightly higher than bTwconv (Figure 3), which suggests that this mTwconv is\\nvery competitive. On the other hand, the profiles in pTwconv are substantially\\nlower, which is unfortunate because this is one of the networks that had the\\ngreatest improvements in Natural Science Disciplines [H01] andSocial Sciences\\n[I01].\\n4.3 Document Authors\\nThe network pAuthor was the best for the topic category Geographicals [Z] ,\\nalthough it did poorly for the other topic categories. We believe that it was the\\nbest due to document authors having stable interests over time about given geo-\\n20graphical regions. Figure 3 shows that Geographicals [Z] achieve a substantially\\nhigher profile in pAuthor than in bAuthor or mAuthor, making it very compet-\\nitive. This is especially interesting given that, based on our prior work [6], the\\ntopic category Geographicals [Z] is the worst topic category for text similarity\\nand citation networks by a substantial margin. Document authors are already\\nused to create science maps, but unlike our paper, the clusters in those maps\\nthend to be made up of authors instead of documents, and the edges represent\\nhow many documents the authors have written together [23].\\n4.4 Facebook users\\npFacebook did well with topic category Named Groups [M] , especially its top-\\nics about medical personnel, and was the best with Health Occupations [H02] ,\\nespecially for its topics medical specialties and nursing. This suggests that the\\nusers of Facebook are very interested in sharing documents related to health\\nadvice, which makes sense because it has a lot of support groups for people that\\nsuffer certain diseases where they share advice. The profile of mFacebook in\\nthese topic categories was higher than pFacebook, and for Health Occupations\\n[H02] it was about half that of bFacebook (Figure 3), so we believe mFace-\\nbook to be competitive for Health Occupations [H02] . It is worth mentioning\\nthat the highest topic profiles of pFacebook within the topic categories Named\\nGroups [M] andHealth Occupations [H02] were much higher than in bFace-\\nbook. Therefore, if there was a topic category that was exclusively composed\\nof topics medical personnel, specialties and nursing, then this topic category\\nwould certainly have a much higher profile for pFacebook and mFacebook than\\nfor bFacebook. This shows that the topic categories that we use in the current\\npaper might be insufficient to capture the benefit of the external sources.\\n4.5 Policy documents\\npPolicy did well in the topic categories Named Groups [M] andGeographicals\\n[Z], and it was one of the few that had an improvement in Psychiatry and Psy-\\nchology [F] , although the improvement was small. We found that the theme\\nthat unifies the topics with a high Purity profile in each topic category were: In\\nPsychiatry and Psychology [F] , topics relevant to the government (e.g. combat\\ndisorders) or society (e.g. social phobia). In Named Groups [M] , medical pro-\\nfessions and vulnerable groups (e.g. undocumented immigrants, persons with\\nmental disabilities, minors). In Geographicals [Z] , American states and global\\nsouth countries. The high Purity profile in the former two topic categories are\\nabout government and social issues, which makes sense given that this is policy\\ndocuments, while the topics from the latter one are relevant to the American\\ngovernment, which makes sense because the database has a better coverage\\nof policy documents from the Anglo-Saxon world [29]. The profile of Named\\nGroups [M] andPsychiatry and Psychology [F] in pPolicy is substantially lower\\nthan in bPolicy, but the oposite is true in the profile of Geographicals [Z] (Fig-\\nure 3). However, the Purity of the profile of Geographicals [Z] is still extremely\\n21low, making it not very useful for science map users. Interestingly, the profile\\nfor mPolicy is lower than for either pPolicy and bPolicy, which is uncommon,\\nsuggesting that in this topic category the BERT and Pure networks do not\\ncomplement each other to create better clusters.\\n4.6 Patent families\\nmPatents did well with the topic categories Chemicals and Drugs [D] , specially\\nits topics about biochemical elements, and Technology, Industry, and Agriculture\\n[J01] , specially its topics about chemical components. This suggests that this\\nnetwork does well for topics about Biotechnology, which is likely related to the\\ninventions proposed in the patents. For the profile of Chemicals and Drugs [D]\\nandTechnology, Industry, and Agriculture [J01] (Figure 3), mPatents is about\\nhalf that of bPatents, which we believe is enough for mPatents to be competitive.\\nOn the other hand, pPatents did poorly in absolute Purity difference.\\n4.7 Twitter Authors\\nThe network pTwauthor was one of best for Social Sciences [I01] andHealth\\nCare [N] , specially for topics about nursing. The reason for this topic to have\\na high clustering effectiveness is likely to be related to being on of the most\\nshared topics in social media [14], but we also believe that it is due to a sub-\\nstantial number of Twitter users sharing documents exclusively related to nurs-\\ning. Unfortunately, neither pTwauthor or mTwauthor had topic categories with\\nabsolute Purity difference higher than 0.2, and the pTwauthor profile in these\\ncategories was substantially lower than in bTwauthor (Figure 3), suggesting\\nthat pTwauthor is not competitive. Considering how well mTwconv did, this\\nsuggests that Twitter networks are more helpful for science maps when the net-\\nworks are based on conversations instead of users, even as the second are more\\ncommonly used [10]. This could be because Twitter users tweet about multiple\\ntopics, while conversations are likely to be more topically focused. pTwauthor\\nalso did much worse than pFacebook, which is the other network with social\\nmedia users as nodes, and we believe this can be due to Twitter having a high\\nnumber of bot accounts that share academic documents automatically, at least\\ncompared with Facebook.\\n4.8 Twitter networks versus the other networks\\nWe noticed that the Pure Twitter networks (pTwconv and pTwauthor) provide\\na very different perspective from the other sources. On one hand, if we ignore\\nOrganisms [B] , these are the networks with the highest number of topic cate-\\ngories with a high relative Purity difference. On the other hand, these are the\\nnetworks that achieved the highest improvement for topic category Natural Sci-\\nence Disciplines [H01] , which is a category that science map users expect to see\\nin science maps and that the traditional sources for science maps are not good\\nat showing [6]. We believe that the perspective of Twitter is due to a dichotomy\\n22on how science is organized: On one have, we have the social construction of\\nhow people think science should be organized, represented by Twitter and Face-\\nbook, and on the other hand we have organization that emerges from practical\\nuses of science, represented by all the other sources.\\n5 Discussion\\nIn this section we will discuss the high level ideas, strengths and weaknesses\\nof our work. One of our most important results is that the external sources\\ntend to cluster some topic categories better than others, and that these topic\\ncategories are different between sources. This suggests that external sources\\nprovide complementary perspectives on how to group documents together, and\\nthat these perspectives also have a meaning. These different perspective are\\nnot only useful to create science maps, like in this paper, but they could poten-\\ntially be applied in other areas to reveal how society perceives and engages with\\nscience. For example, that the Twitter perspective is very different from the\\nother networks, or that Facebook users share health science, or that document\\nauthors are conservative about the geographical area they publish about. Also,\\neven as the external sources tend to not outperform BERT in most topic cate-\\ngories, this was not the goal of the paper, and it is possible that an alternative\\nmethod for constructing science maps could reach this goal.\\nA strength of our research is the clustering effectiveness evaluation method,\\nwhich is a substantial improvement over the clustering effectiveness evaluation\\nmethod we used in our prior work [6] because our new approach is much easier to\\ninterpret. We used to have two metrics to evaluate effectiveness, Purity and the\\ninverse clustering count, while now we only have Purity. We also used to only\\nbe able to compare clustering effectiveness between clustering solutions with\\nthe same documents and similar cluster sizes, while now we can compare the\\nclustering solutions of several Resolution values across networks with different\\ndocuments. In the prior work we also did not have Purity profiles, which provide\\na very intuitive description of the quality of the topic clusters that a user would\\nexperience in a science map. On the other hand, our evaluation method misses\\nsome of the nuance of our last work. For example, we did not evaluate if some\\nsources are better than others at different cluster sizes (our prior work and Xie\\nand Waltman [41] found that citations are better than text for smaller clusters).\\nA limitation of our work is that we performed our experiments on clustering\\nsolutions that are less sophisticated than science maps used by researchers. For\\nexample, some science map methodologies have a minimum size for clusters, and\\nclusters smaller than this size are merged with other clusters [37]. We did not do\\nthis, and as a consequence, when the nodes of a cluster are all equally connected\\nby a few hub nodes in the network, reducing the size of the cluster by increasing\\nthe Resolution will turn random nodes of this cluster into singletons. This is a\\nproblem because, if this node is a topic document, then Purity would decrease\\nat higher NSC, creating very confusing results for some topics that do not reflect\\nthe cluster effectiveness that would be observed in a science map. We observed\\n23this situation mostly in the Twitter users source, where some documents were\\nshared by only one or two users. We did not attempt to prevent this situation\\nbecause doing so would increase the complexity of our experimental design.\\nAnother limitation of our research is that our Mixed networks combine a non-\\nbipartite network (the BERT networks, non-bipartite because the links go from\\ndocument to document) with a bipartite network (the Pure networks, bipartite\\nbecause the links go from document to external source element). There are\\nstudies that use either of these types of networks for creating science maps,\\nbut there are no studies about combining them, which could have unintended\\neffects in the map. The closest there is in the literature is the extended citation\\nnetworks, where there are links from document to document and from document\\nto non-core document, but not from non-core document to non-core document.\\nAlso, bipartite networks are not very common in science mapping, and it is\\nmore common to, instead of having the unit of co-occurrence in the network\\n(in our case, the external source element), to represent the co-occurrence in the\\nedge weight [32]. The method we used to combine the networks into the Mixed\\nnetwork is also relatively straightforward, and the only modification that we\\nmake is that the sum of edges weights in both networks must be the same. We\\ncan imagine alternative modifications, for example making all the edges that\\ncame out from a node to add up to the same value. We did not explore these\\nalternatives to not further complicate our analyses, but future research could\\nexplore how to create better Mixed networks for a given external source.\\nAnother limitation is that the data sources that we used might not be avail-\\nable for researchers that use science maps. For example, the API of Twitter\\nused to be free for researchers but now is paid. We believe our results are still\\nrelevant because new sources of data can open up in the future, which can also\\nbe evaluated to identify which topic categories benefit from them.\\n6 Conclusions\\nThe topical bias of science maps limits their usefulness for topical analyses.\\nIn the current paper we have explored different data sources for creating aca-\\ndemic documents networks that represent different document relations, with\\nthe purpose of finding sources that can change the topical bias of a science\\nmap. Our method of analysis was comparing the clustering effectiveness of dif-\\nferent MeSH topic categories within a network and between networks, using a\\nmethodology that we refined from our prior work. We explored traditional sci-\\nence maps data sources (text similarity and citation links) and non-traditional\\ndata sources based on the co-occurrence of academic documents on another ele-\\nment (policy document, patent families, Facebook users, Twitter conversations,\\nTwitter users, and document authors), which we referred to as external sources.\\nOur comparisons were between networks that use either text similarity, external\\nsources, or a mix of both.\\nWe found that different external sources can be used to favor the emergence\\nof different topics, and the following combinations had a particularly strong ef-\\n24fect: Health for Facebook users, biotechnology for patent families, government\\nand social issues for policy documents, food for Twitter conversations, nursing\\nfor Twitter users, and most strongly geographical entities for document authors.\\nWe also found that Twitter conversations work particularly well when combined\\nwith text similarity and that our text similarity metric (sentence BERT) seems\\nto perform better than the similarity metrics used in prior work (like BM25),\\nexcept for topics related to organisms. Also, the favored topic categories are not\\naffected by changing the percentage of the topic documents used in the evalua-\\ntion, as shown by the similarity between the different Coverage values. Finally,\\nthe best topic categories in the Twitter networks were very different from the\\nother networks, which means that Twitter (and potentially other similar social\\nmedia platforms, like the new BlueSky or Mastodon) might provide different\\nperspectives for the study of the organization of scientific knowledge, getting\\nus closer to latent representations of how society perceives and interacts with\\nscience.\\nOur results show that external sources of academic document networks can\\nbe used to control topic bias, which opens up the possibility of creating science\\nmaps tailored for different needs. The most direct way of applying our discover-\\nies is to create science maps biased toward different topics using these external\\nsources. However, with the exception of document authors and their high clus-\\ntering effectiveness for geographical entities, most external sources need to be\\nused in combination with text similarity sources to achieve a high clustering ef-\\nfectiveness relative to traditional sources, and it is still an open question which\\nis the best method for combining them into a single network. The clusters of ex-\\nternal sources could also be used beyond science maps, for example to identify\\npotential misuse of scientific publications (e.g. in misinformation strategies),\\nor to identify societal connections or sensitivities that are not reflected in the\\nacademic world (e.g. connecting papers of diets and health concerns).\\n7 Supplementary material\\nThe data and the code used to create the results is available at a Zenodo repos-\\nitory [4]\\nReferences\\n[1]P. Ahlgren ,Y. Chen ,C. Colliander , and N. J. van Eck , Enhancing\\ndirect citations: A comparison of relatedness measures for community de-\\ntection in a large set of pubmed publications, Quantitative Science Studies\\n1no. 2 (2020), 714‚Äì729.\\n[2]Altmetric.com , About us, https://www.altmetric.com/about-us/ ,\\nAccessed: 2024-11-01.\\n25[3]Z. Ba andZ. Liang , A novel approach to measuring science-technology\\nlinkage: From the perspective of knowledge network coupling, Journal of\\nInformetrics 15no. 3 (2021), 101167.\\n[4]J. P. Bascur , Use of diverse data sources to control which topics emerge\\nin a science map. Supplementary material, November 2024. https://doi.\\norg/10.5281/zenodo.14170722 .\\n[5]J. P. Bascur ,S. Verberne ,N. J. van Eck , and L. Waltman , Aca-\\ndemic information retrieval using citation clusters: In-depth evaluation\\nbased on systematic reviews, Scientometrics 128no. 5 (2023), 2895‚Äì2921.\\n[6]J. P. Bascur ,S. Verberne ,N. J. van Eck , and L. Waltman , Which\\ntopics are best represented by science maps? An analysis of clustering\\neffectiveness for citation and text similarity networks, 2024. Available at\\nhttps://arxiv.org/abs/2406.06454 .\\n[7]K. W. Boyack andR. Klavans , A comparison of large-scale science mod-\\nels based on textual, direct citation and hybrid relatedness, Quantitative\\nScience Studies 1no. 4 (2020), 1570‚Äì1585.\\n[8]C. Carusi andG. Bianchi , Scientific community detection via bipar-\\ntite scholar/journal graph co-clustering, Journal of Informetrics 13no. 1\\n(2019), 354‚Äì386.\\n[9]A. Cohan ,S. Feldman ,I. Beltagy ,D. Downey , and D. S. Weld ,\\nSpecter: Document-level representation learning using citation-informed\\ntransformers, 2020. Available at https://arxiv.org/abs/2004.07180 .\\n[10]R. Costas ,S. de Rijcke , and N. Marres , ‚ÄúHeterogeneous couplings‚Äù:\\nOperationalizing network perspectives to study science-society interactions\\nthrough social media metrics, Journal of the Association for Information\\nScience and Technology 72no. 5 (2021), 595‚Äì610.\\n[11]G. Csardi andT. Nepusz , The igraph software package for complex\\nnetwork research, InterJournal, Complex Systems 1695 (2006). Available\\nathttps://igraph.org .\\n[12]P. Dorta-Gonz ¬¥alez ,A. Rodr ¬¥ƒ±guez-Caro , and M. I. Dorta-\\nGonz ¬¥alez , Societal and scientific impact of policy research: A large-scale\\nempirical study of some explanatory factors using altmetric and overton,\\nJournal of Informetrics 18no. 3 (2024), 101530.\\n[13]C. A. D‚ÄôAngelo andN. J. Van Eck , Collecting large-scale publication\\ndata at the level of individual researchers: A practical proposal for author\\nname disambiguation, Scientometrics 123(2020), 883‚Äì907.\\n[14]Z. Fang ,R. Costas ,W. Tian ,X. Wang , and P. Wouters , An exten-\\nsive analysis of the presence of altmetric data for web of science publications\\nacross subject fields and research topics, Scientometrics 124no. 3 (2020),\\n2519‚Äì2549.\\n26[15]Z. Fang ,J. Dudek ,E. Noyons , and R. Costas , Science cited in policy\\ndocuments: Evidence from the overton database, 2024. Available at https:\\n//arxiv.org/abs/2407.09854 .\\n[16]J. Gl ¬®aser , Opening the black box of expert validation of bibliometric\\nmaps, in Lockdown Bibliometrics: Papers not submitted to the STI Con-\\nference 2020 in Aarhus , 2020, pp. 27‚Äì36.\\n[17]M. Held ,G. Laudel , and J. Gl ¬®aser , Challenges to the validity of topic\\nreconstruction, Scientometrics 126(2021), 4511‚Äì4536.\\n[18]M. Held andT. Velden , How to interpret algorithmically constructed\\ntopical structures of scientific fields? A case study of citation-based map-\\npings of the research specialty of invasion biology, Quantitative Science\\nStudies 3no. 3 (2022), 651‚Äì671.\\n[19]D. W. Hook ,S. J. Porter , and C. Herzog , Dimensions: Building con-\\ntext for search and evaluation, Frontiers in Research Metrics and Analytics\\n3(2018), 23.\\n[20]D. Hric ,R. K. Darst , and S. Fortunato , Community detection in\\nnetworks: Structural communities versus ground truth, Physical Review E\\n90no. 6 (2014), 062805.\\n[21]F. Janssens ,W. Gl ¬®anzel , and B. De Moor , A hybrid mapping of\\ninformation science, Scientometrics 75no. 3 (2008), 607‚Äì631.\\n[22]B. Kang andG. Tarasconi , Patstat revisited: Suggestions for better\\nusage, World patent information 46(2016), 56‚Äì63.\\n[23]S. Kumar , Co-authorship networks: A review of the literature, Aslib Jour-\\nnal of Information Management 67no. 1 (2015), 55‚Äì73.\\n[24]K.-K. Lai andS.-J. Wu , Using the patent co-citation approach to estab-\\nlish a new patent classification system, Information processing & manage-\\nment 41no. 2 (2005), 313‚Äì330.\\n[25]K. Lee andS. Lee , Knowledge structure of the application of high-\\nperformance computing: A co-word analysis, Sustainability 13no. 20\\n(2021), 11249.\\n[26]M. Meyer , Does science push technology? Patents citing scientific litera-\\nture, Research policy 29no. 3 (2000), 409‚Äì434.\\n[27]National Institutes of Health , Medical subject headings, Available\\nathttps://www.nlm.nih.gov/mesh/meshhome.html , Accessed: 2024-11-\\n01.\\n[28]M. E. Newman , Coauthorship networks and patterns of scientific collab-\\noration, Proceedings of the national academy of sciences 101 no. suppl 1\\n(2004), 5200‚Äì5205.\\n27[29]H. Pinheiro ,E. Vignola-Gagn ¬¥e, and D. Campbell , A large-scale vali-\\ndation of the relationship between cross-disciplinary research and its uptake\\nin policy-related documents, using the novel overton altmetrics database,\\nQuantitative Science Studies 2no. 2 (2021), 616‚Äì642.\\n[30]N. Reimers andI. Gurevych , Sentence-bert: Sentence embeddings using\\nsiamese bert-networks, 2019. Available at https://arxiv.org/abs/1908.\\n10084 .\\n[31]C. Seitz ,M. Schmidt ,N. Schwichtenberg , and T. Velden , A case\\nstudy of the epistemic function of citations‚Äîimplications for citation-based\\nscience mapping, in Proceedings of the 18th International Conference of the\\nInternational Society for Scientometrics and Informetrics (ISSI) , 2021.\\n[32]H. Small , Co-citation in the scientific literature: A new measure of the\\nrelationship between two documents, Journal of the American Society for\\ninformation Science 24no. 4 (1973), 265‚Äì269.\\n[33]L.ÀáSubelj ,N. J. Van Eck , and L. Waltman , Clustering scientific pub-\\nlications based on citation relations: A systematic comparison of different\\nmethods, PloS one 11no. 4 (2016), e0154404.\\n[34]M. Szomszor andE. Adie , Overton: A bibliometric database of policy\\ndocument citations, Quantitative Science Studies 3no. 3 (2022), 624‚Äì650.\\nhttps://doi.org/10.1162/qss_a_00204 .\\n[35]V. A. Traag ,L. Waltman , and N. J. Van Eck , From Louvain to Lei-\\nden: Guaranteeing well-connected communities, Scientific reports 9no. 1\\n(2019), 1‚Äì12.\\n[36]L. Waltman ,K. W. Boyack ,G. Colavizza , and N. J. van Eck , A\\nprincipled methodology for comparing relatedness measures for clustering\\npublications, Quantitative Science Studies 1no. 2 (2020), 691‚Äì713.\\n[37]L. Waltman andN. J. Van Eck , A new methodology for constructing a\\npublication-level classification system of science, Journal of the American\\nSociety for Information Science and Technology 63no. 12 (2012), 2378‚Äì\\n2392.\\n[38]F. Wang ,C. Jia ,X. Wang ,J. Liu ,S. Xu ,Y. Liu , and C. Yang ,\\nExploring all-author tripartite citation networks: A case study of gene\\nediting, Journal of Informetrics 13no. 3 (2019), 856‚Äì873.\\n[39]Q. Wang ,B. Zou ,J. Jin , and Y. Wang , Studying the linkage patterns\\nand incremental evolution of domain knowledge structure: A perspective\\nof structure deconstruction, Scientometrics (2024), 1‚Äì26.\\n[40]K. Williams , What counts: Making sense of metrics of research value,\\nScience and Public Policy 49no. 3 (2022), 518‚Äì531. https://doi.org/\\n10.1093/scipol/scac004 .\\n28[41]Q. Xie andL. Waltman , A comparison of citation-based clustering\\nand topic modeling for science mapping, arXiv preprint arXiv:2309.06160\\n(2023).\\n[42]S. Xu ,J. Liu ,D. Zhai ,X. An ,Z. Wang , and H. Pang , Overlapping the-\\nmatic structures extraction with mixed-membership stochastic blockmodel,\\nScientometrics 117(2018), 61‚Äì84.\\n[43]P. Yang andG. Colavizza , A map of science in wikipedia, in Companion\\nProceedings of the Web Conference 2022 , 2022, pp. 1289‚Äì1300.\\n[44]J. Yun ,S. Ahn , and J. Y. Lee , Return to basics: Clustering of scientific\\nliterature using structural information, Journal of Informetrics 14no. 4\\n(2020), 101099.\\n[45]M. Zitt , Meso-level retrieval: IR-bibliometrics interplay and hy-\\nbrid citation-words methods in scientific fields delineation, Sciento-\\nmetrics 102 no. 3 (2015), 2223‚Äì2245. https://doi.org/10.1007/\\ns11192-014-1482-5 .\\n29',\n",
       " 'ConvMesh: Reimagining Mesh Quality\\nThrough Convex Optimization\\nAlexander Valverde1\\n1UC Santa Cruz\\nDecember 12, 2024\\nAbstract\\nMesh generation has become a critical topic in recent years, form-\\ning the foundation of all 3D objects used across various applications,\\nsuch as virtual reality, gaming, and 3D printing. With advancements\\nin computational resources and machine learning, neural networks\\nhave emerged as powerful tools for generating high-quality 3D ob-\\nject representations, enabling accurate scene and object reconstruc-\\ntions. Despite these advancements, many methods produce meshes\\nthat lack realism or exhibit geometric and textural flaws, necessitat-\\ning additional processing to improve their quality.\\nThis research introduces a novel approach that leverages convex\\nconstraints to enhance existing meshes by refining their texture and\\ngeometry. By focusing on a sparse set of point clouds from both the\\noriginal and target meshes, this method demonstrates significant im-\\nprovements in mesh quality with minimal data requirements. To eval-\\nuate the approach, the classical dolphin mesh dataset from Facebook\\nAI was used as a case study, with optimization performed using the\\nCVXPY library. The results reveal promising potential for stream-\\nlined and effective mesh refinement.\\nIntroduction\\nMachine learning techniques have been widely employed to generate new\\nmeshes from ellipsoidal shapes [7]. However, in certain cases, these neu-\\nral methods fail to produce satisfactory results due to a lack of detail in\\nspecific regions of the object. Even with extensive training, the generated\\n1arXiv:2412.08484v1  [cs.GR]  11 Dec 2024mesh may still exhibit artifacts that neural approaches alone cannot resolve.\\nThis limitation suggests that mathematical methods, such as convex opti-\\nmization‚Äîwhich focuses on minimizing a function to reach a global mini-\\nmum‚Äîcould provide a more effective solution.\\nFor this project, I selected a classic mesh developed by the Facebook\\nTeam (Mesh from PyTorch3D Tutorial), representing a dolphin. The goal\\nis to deform a spherical object to achieve a close approximation (or even\\nan identical replication) of the dolphin mesh. To accomplish this, I employ\\nstochastic gradient descent to adjust the vertices and faces, aiming to refine\\nthe representation. The model was trained for 2000 epochs, resulting in the\\nfollowing mesh:\\nFigure 1: Original dolphin mesh.\\n2Figure 2: Generated dolphin mesh generated after 2000 epochs of training\\ndeforming a sphere.\\nPrevious Work\\nAlthough the body of documentation on this topic is not extensive, several\\nresearchers have applied optimization methods to mesh generation, a field\\nthat has seen significant advancements over the decades. One of the pioneer-\\ning contributions came from Hoppe et al., who introduced techniques in mesh\\noptimization [3]. Their work presented a method for generating a mesh M\\nof the same topological type as an initial mesh M0. They demonstrated the\\neffectiveness of mesh optimization in applications such as surface reconstruc-\\ntion from unorganized point clouds and mesh simplification. Their approach\\ninvolved minimizing an energy function E(K, V ), where Krepresents the\\nconnectivity of the mesh, and Vis the set of vertex positions.\\nIn another project, Feldman [2] explored methods to improve mesh quality\\nusing Laplacian smoothing. This approach focused on optimizing the quality\\nmetrics of the mesh to produce a more structured and uniform mesh. Feld-\\nman‚Äôs work highlighted the practical applications of smoothing techniques\\nin improving the usability and performance of meshes, particularly for finite\\nelement simulations.\\n3Methods\\nA wide range of standard convex programming forms can be employed, in-\\ncluding least squares (LS), linear programming (LP), quadratic program-\\nming (QP), semidefinite programming (SDP), second-order cone program-\\nming (SOCP), geometric programming (GP), and smooth convex program-\\nming (smooth CP). However, selecting the most suitable form for a given\\nproblem is not always straightforward. In some cases, it may even be nec-\\nessary to implement a custom solver. As noted by Grant et al., disciplined\\nconvex programming (DCP) offers a transparent and reliable framework for\\nproblem formulation.\\nDCP focuses on systematically transforming nonconvex problems into\\nconvex ones by introducing new functions to encapsulate nonlinearities. Its\\ncore is defined by a ‚Äúgrammar‚Äù that ensures each transformation step pre-\\nserves convexity, thus guaranteeing that the final problem remains within the\\nconvex domain [1].\\nFor practical application, convex optimization often needs to scale to high\\ndimensions. The problem considered here involves 7,684 variables and 15,360\\nconstraints, qualifying it as a large-scale convex optimization instance. To\\ntackle it effectively, we define the appropriate convex optimization problem\\nand employ both disciplined convex programming (DCP) and disciplined\\nquasiconvex programming (DQCP), along with a Splitting Conic Solver\\n(SCS) [1]. These methods are particularly suitable for large-scale scenar-\\nios.\\nFor this specific case, CVXPY translates the problem into a santard form\\neploying a ‚ÄùReduction chain‚Äù. First, it converts the DCP-compliant problem\\ninto a conic form. Second, handles any special attributes that need to be\\ntranslated into standard constraints. Third, rewritres the problem data into\\nthe matrix form required by SCS. Finally, prepares the data for the SCS\\nsolver.\\nSCS is a first-order method especially for large-scale scale conic scenarios.\\nThis solver classifies constraints into different cones (zero, linear, second-\\norder, semidefinite) [5]. It does not rely on any explicit algorithm parameters,\\nand the per-iteration cost of the method is the same as applying the splitting\\nmethod to the primal or dual alone. The SCS method involves applying an\\noperator splitting method to the homogeneous self-dual embedding of the\\noriginal optimization problem to find a point in the intersection of an affine\\nset and a convex cone. It solves a system of linear equations and projects\\nthe point onto the cone.\\n4Definitions\\nFor this problem is necessary to set some definitions of the elements involved\\nin the solution of it.\\nLet:\\n‚Ä¢nbe the number of vertices in the source mesh.\\n‚Ä¢Vorig={vorig,1, vorig,2, . . . , v orig,n} ‚äÇR3represent the original vertex\\npositions of the source mesh.\\n‚Ä¢Vtarget ={vtarget ,1, vtarget ,2, . . . , v target ,n} ‚äÇR3represent the target vertex\\npositions, to which the new positions should be aligned.\\n‚Ä¢V={v1, v2, . . . , v n} ‚äÇR3be the optimization variables representing\\nthe updated vertex positions.\\n‚Ä¢Edenote the set of edges in the mesh, where each edge connects two\\nvertices ( i, j).\\n‚Ä¢Œ¥ > 0 is a small positive constant defining the maximum allowable\\ndistance between connected vertices for smoothness.\\n‚Ä¢Œª >0 is the weighting factor for the regularization and alignment terms,\\nrespectively.\\n‚Ä¢¬Øt=1\\nnPn\\ni=1vtarget ,iis the mean of the target vertex positions.\\nConstraints\\nFor all edges ( i, j)‚ààE, enforce smoothness by limiting the distance between\\nconnected vertices:\\n‚à•vi‚àívj‚à•2‚â§Œ¥ (1)\\nVariables:\\nvi‚ààR3fori= 1,2, . . . , n (2)\\nComplete Optimization Problem\\nmin\\nVnX\\ni=1‚à•vi‚àí¬Øt‚à•2+ŒªnX\\ni=1‚à•vi‚àívtarget ,i‚à•2\\n2 (3)\\nvi‚ààR3, i= 1,2, . . . , n (4)\\n5Results\\nThe optimization process ran for over two hours using Python libraries such\\nas CVXPY, PyTorch3D, and Torch. By employing the convex optimization\\nframework, the updated mesh exhibited significant improvements over the\\ninitial neural network-generated version. We employ a value of Œ¥= 0.1 and\\na weighting factor Œª= 0.3.\\nFigure 3: Comparison between the original dolphin mesh (top), the generated\\nmesh (middle), and the optimized mesh (bottom).\\nThe optimized mesh shows a better representation of the dolphin shape\\nwith smoother edges and a more elongated figure, particularly in areas like\\nthe head. However, some minor inconsistencies remain, such as slight defor-\\nmations at the dorsal fin and the tail. These issues could be mitigated by\\nintroducing additional loss terms to enforce edge and normal consistency.\\n6Figure 4: Different views of the new mesh\\nComparison of Metrics: Initial vs. Iter 225\\nTable 1 presents the comparison of key metrics between Iter 0 (initial iter-\\nation) and Iter 225 (final iteration) during the optimization process. These\\nmetrics include the primal residual (Pri Res), dual residual (Dua Res), the\\ngap between primal and dual objectives (Gap), and the objective value (Ob-\\njective). The significant decrease in these values from the initial to the 225th\\niteration indicates the progress of the optimization algorithm toward conver-\\ngence.\\nTable 1: Comparison of Key Metrics\\nMetric Initial (Iter 0) Iter 225\\nPri Res 0.488 1.550000e-07\\nDua Res 15.300 9.030000e-08\\nGap 274.000 5.540000e-07\\nObjective 153.000 1.240000e+01\\n‚Ä¢Pri Res: Indicates the distance from satisfying the primal feasibility\\nconditions\\n‚Ä¢Dua Res: Indicates the distance from satisfying the dual feasibility\\nconditions\\n‚Ä¢Gap: Difference between the primal and dual objectives (lower is bet-\\nter)\\n7Table 2: Summary of CVXPY Problem\\nMetric Value\\nProblem Status optimal\\nOptimal Value 12.43618\\nCompilation Time (s) 76.8\\nSolver Time (s) 5694.0\\n‚Ä¢Objective: The value of the objective function being optimized.\\nThe Numerical solver modifies the initial variables to 30733 and the con-\\nstraints to 3375381. This happened because in a mesh there are many vertices\\nand edges that are connected, therefore, is necessary to set all the possible\\nconstraints because each change affects neighborhoods.\\nTable 3: Cone Details\\nCone Type Details\\nPrimal zero / Dual free variables 7686\\nLinear variables 15360\\nSOC variables 61440\\nPSD variables 3290895\\nThe table above illustrates the distribution of each type of variable that\\nthe solver used to solve the problem.\\nPrimal Perspective: These variables are exactly zero, satisfying linear\\nequalities that set them to specific values.\\nDual Perspective: These variables have no restrictions and are used to\\nrepresent equality constraints.\\nLinear Variables: These variables must lie in a nonnegative domain,\\nmeaning they must be greater than or equal to zero.\\nSecond-Order Cone (SOC) Variables: These variables are more com-\\nplex, as they can represent norm bounds [4], which generalize simple linear\\ninequalities.\\nPositive Semidefinite (PSD) Variables: These variables are matrix-\\nvalued and must form a positive semidefinite matrix. PSD matrices are often\\nused to enforce structural conditions in optimization problems.\\nFor this specific case, PSD has a greater value of variables because they\\noperate in a higher-dimensional space, involving complex structural con-\\nstraints. Due to the properties of PSD, it requires more variables and con-\\nstraints compared to simple linear constraints. PSD enforces that a matrix is\\n8symmetric and all eigenvalues are nonnegative. In general, PSD can encode\\nrelationships and structural conditions that may not be possible with other\\nconstraints. A mesh has a complete structure that is connected everywhere;\\ntherefore, it is necessary to maintain those conditions [6].\\nDiscussion\\nConclusions\\nIn conclusion, the results highlight the potential of convex optimization for\\nimproving neural mesh generation. While the updated mesh retains the\\noverall geometry of the target, future work could focus on refining the loss\\nfunction to enhance alignment in detailed areas. Is important to mention\\nthat this new mesh was optimized only on 500 points from the original mesh,\\ntherefore, indicates how with a low-dimensional representation is possible to\\nachieve these results.\\nFuture Work\\nThe proposed method requires further evaluation, particularly in terms of\\nsolver time, as it currently takes over an hour to generate the corresponding\\nmesh. Future research should focus on exploring alternative approaches that\\nleverage GPUs and vectorization techniques. These methods could poten-\\ntially reduce computation time while maintaining the accuracy and quality\\nof the solution. Additionally, investigating algorithmic optimizations and\\nparallel processing strategies may also contribute to achieving faster results.\\nReferences\\n[1] Stephen Boyd and Lieven Vandenberghe. Convex Optimization . Cam-\\nbridge University Press, 2004.\\n[2] B. Feldman. Convex optimization for simplicial mesh improvement, 2005.\\nUnpublished project report.\\n[3] H. Hoppe, T. DeRose, T. Duchamp, J. McDonald, and W. Stuetzle.\\nMesh optimization. In Proceedings of the 20th Annual Conference on\\nComputer Graphics and Interactive Techniques (SIGGRAPH ‚Äô93) , pages\\n19‚Äì26, 1993.\\n9[4] Roger A. Horn and Charles R. Johnson. Matrix Analysis . Cambridge\\nUniversity Press, New York, 2nd edition, 2013.\\n[5] Brendan O‚ÄôDonoghue, Eric Chu, Neal Parikh, and Stephen Boyd. Conic\\noptimization via operator splitting and homogeneous self-dual embed-\\nding. Journal of Optimization Theory and Applications , 169(3):1042‚Äì\\n1068, 2016.\\n[6] Danica Sutherland. Positive semi-definite matrices. Lecture Notes, 2024.\\n[7] N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, and Y. Jiang. Pixel2mesh:\\nGenerating 3d mesh models from single rgb images. In Proceedings of the\\nEuropean Conference on Computer Vision (ECCV) , pages 52‚Äì67, 2018.\\nAppendix\\nThe source code and implementation details for this study are publicly avail-\\nable on GitHub. Access the repository at the following link: ConvMesh\\nGitHub Repository.\\n10',\n",
       " 'Detecting Visual Triggers in Cannabis Imagery: A CLIP-Based Multi-Labeling\\nFramework with Local-Global Aggregation\\nLinqi Lu, Xianshi Yu, Akhil Perumal Reddy\\nUniversity of Wisconsin-Madison\\n[llu84, xyu384, perumalreddy]@wisc.edu\\nAbstract\\nThis study investigates the interplay of visual and tex-\\ntual features in online discussions about cannabis edibles\\nand their impact on user engagement. Leveraging the CLIP\\nmodel, we analyzed 42,743 images from Facebook (March 1\\nto August 31, 2021), with a focus on detecting food-related\\nvisuals and examining the influence of image attributes such\\nas colorfulness and brightness on user interaction. For tex-\\ntual analysis, we utilized the BART model as a denoising au-\\ntoencoder to classify ten topics derived from structural topic\\nmodeling, exploring their relationship with user engage-\\nment. Linear regression analysis identified significant pos-\\nitive correlations between food-related visuals (e.g., fruit,\\ncandy, and bakery) and user engagement scores, as well\\nas between engagement and text topics such as cannabis le-\\ngalization. In contrast, negative associations were observed\\nwith image colorfulness and certain textual themes. These\\nfindings offer actionable insights for policymakers and reg-\\nulatory bodies in designing warning labels and marketing\\nregulations to address potential risks associated with recre-\\national cannabis edibles.\\n1. Introduction\\nCannabis edibles are now legally available in certain re-\\ngions; however, their rising prevalence has raised signifi-\\ncant concerns regarding their impact on both public health\\nand the online environment. The consumption of edible\\ncannabis products has been associated with a spectrum of\\nadverse health effects, including cognitive impairment, el-\\nevated heart rate, heightened anxiety, and addiction risks\\n[6, 8]. Furthermore, the visual representation of cannabis-\\nrelated imagery‚Äîcharacterized by vibrant colors, candy-\\nlike appearances, and bakery-inspired themes‚Äîhas shown\\na propensity to evoke specific emotional and perceptual re-\\nsponses [5]. Aside from being aesthetic, visual triggers\\nmight influence online expressions and shape societal at-\\ntitudes toward cannabis products, especially among youngsocial media users. By normalizing cannabis use through\\nonline marketing using visual elements like foodie cues,\\nyounger audiences may become desensitized to the risks\\nof marijuana use. Given these dynamics, it is crucial to\\nsystematically investigate the interplay between visual el-\\nements in edible cannabis-related online content and user\\nengagement. Such insights are invaluable for guiding gov-\\nernmental bodies and public health organizations in crafting\\neffective warning labels and regulating marketing strategies\\nto mitigate risks associated with cannabis overuse. Building\\nupon this context, we posed the following research ques-\\ntions:\\nRQ1: How effective is the CLIP model in detecting food-\\nrelated visual elements (e.g., candy, bakery, and fruit) in\\nsocial media images, and what is its performance after fine-\\ntuning improvement?\\nRQ2: What is the relationship between food-related visual\\nfeatures and user engagement?\\nRQ3: How do low-level image attributes, specifically col-\\norfulness and brightness, correlate with user engagement on\\nsocial media platforms?\\nRQ4: How are textual topic frames, such as discussions\\nabout marijuana legalization, associated with user engage-\\nment in social media posts?\\n2. Methods\\n2.1. Data Collection and Processing\\nThe dataset was extracted from Facebook through the uti-\\nlization of the Crowd Tangle API. The data is from March\\n1, 2021, to August 31, 2021. A pre-established list of search\\nkeywords, which encompassed terms such as ‚Äùcannabis,‚Äù\\n‚Äùedibles,‚Äù ‚Äùkush,‚Äù and ‚Äùmarijuana,‚Äù was deployed to iden-\\ntify posts related to recreational edible marijuana. Then,\\nthe image-level collection was based on the post-level infor-\\nmation, which resulted in approximately 20,000 images per\\nmonth. For data filtering, we filtered out noise by searching\\nthe keyword through the account name and account descrip-\\ntion to keep the cannabis-related marketing posts. To en-\\nhance the precision of our collected posts and ensure imagearXiv:2412.08648v1  [cs.CY]  22 Nov 2024relevance, we implemented a blacklist to exclude irrelevant\\ncontent. This blacklist contained terms generally associated\\nwith non-edible cannabis consumption, such as ‚Äôblunt‚Äô and\\n‚Äôdab,‚Äô as well as phrases primarily related to smoking, va-\\nping, or topical applications, like ‚Äôburning one down‚Äô and\\n‚ÄôEnail/e-nail.‚Äô Additionally, it included mentions of ‚Äôhemp‚Äô\\nin conjunction with ‚Äôflax,‚Äô ‚Äôchia,‚Äô or ‚Äôcereal,‚Äô to further re-\\nfine our dataset.\\n2.2. Engagement Score\\nIn CrowdTangle API, user engagement is quantified using a\\nscoring system, where a post‚Äôs actual interactions are com-\\npared to a calculated benchmark based on similar past posts.\\nScores above 1.0x signify overperformance, while scores\\nbelow -1.0x indicate underperformance. This system ad-\\njusts for posts significantly underperforming their bench-\\nmarks, assigning negative scores to emphasize limited en-\\ngagement. Additionally, posts marginally over or under the\\nexpected interaction threshold receive scores between 0x\\nand 1x, providing a refined measure of engagement. This\\nmethodology ensures a straightforward yet detailed assess-\\nment of user interaction in an academic context.\\n2.3. Data Summaries\\nThere are 42,743 images in the dataset after data cleaning\\nand removing duplicates.\\nData Composition: Approximately 95% of the dataset is\\ncomprised of link or photo data, suggesting a diverse range\\nof media content.\\nImage Text: A significant observation is that approxi-\\nmately 75% of the image text is null, indicating that a sub-\\nstantial portion of the images lacks textual information.\\nMessage Length Distribution: Analysis of message length\\nreveals that nearly 80% of the messages contain fewer than\\n100 words. This suggests that a majority of the content is\\nconcise and to the point.\\nTemporal Distribution: Both the diurnal cycle and week-\\nday distribution exhibit a balanced pattern. This implies that\\nthe dataset is evenly distributed across different times of the\\nday and days of the week.\\n3. Extract Features From Images and Texts\\n3.1. Colorfulness and Brightness\\nThe image colorfulness function analyzes the colorfulness\\nof an input image. It achieves this by splitting the image\\ninto its Red (R), Green (G), and Blue (B) components. Sub-\\nsequently, it calculates the differences between the Red and\\nGreen channels (rg) and the difference between 0.5 times\\nthe sum of Red and Green and the Blue channel (yb). The\\nfunction then computes the mean and standard deviation for\\nboth rg and yb. By combining these statistics using a spec-\\nified formula, it derives a ‚Äùcolorfulness‚Äù metric for the im-\\nFigure 1\\nFigure 2\\nage, emphasizing both mean and standard deviation com-\\nponents [2]. The final result is returned as a measure of the\\nimage‚Äôs colorfulness.\\nThe brightness function is designed to evaluate the aver-\\nage brightness of an input image. It begins by converting\\nthe image to the HSV color space. From the resulting HSV\\nimage, the function extracts the Value channel, which rep-\\nresents the brightness information. The average brightness\\nis then calculated by computing the mean of the values in\\nthe extracted Value channel. This function provides a quan-\\ntitative measure of the overall brightness of the input im-candy bakery fruit human weed text\\n% 0.5 1.6 1.41 40.8 15.0 58.8\\nTable 1. Percentage of images labeled with each visual element.\\nage, useful for understanding the luminance characteristics\\nof the image in terms of its HSV representation.\\n3.2. Visual Object Detection\\nWe utilized a pre-trained visual language model, Con-\\ntrastive Language-Image Pre-Training (CLIP) [4], to auto-\\nmatically detect visual elements in each image. The selec-\\ntion of these visual elements was guided by our research\\nquestions and insights from existing literature [7]. The iden-\\ntified visual elements include candy, bakery, fruit, human,\\nweed , and text. This task was formulated as a multi-label\\nclassification problem, as each image could contain zero,\\none, or multiple visual elements (e.g., an image could si-\\nmultaneously include both human andcandy ).\\nCLIP was chosen for this task due to its ability to sup-\\nport user-defined labels, a feature not commonly available\\nin other pre-trained image classification or object detection\\nmodels. CLIP assigns labels by calculating embeddings for\\nboth pre-specified text queries and the image itself. The co-\\nsine similarity between these embeddings is then computed\\nfor each query-image pair. These similarity scores are sub-\\nsequently used to infer image labels. For instance, proba-\\nbilities for each label can be derived using a softmax acti-\\nvation function applied to the similarity scores for a given\\nimage. Detailed information on the specific queries used is\\nprovided in Appendix 5.1.\\nTo enhance the performance of CLIP in multi-label\\nclassification, we employed the Local-Global Aggregation\\n(LGA) procedure proposed by [1]. This method extends the\\nstandard CLIP approach by incorporating similarity scores\\ncomputed for local patches of each image in addition to the\\noverall image-level score. The local and global scores are\\nthen aggregated to generate a final similarity score for each\\nquery-image pair, referred to as the LGA score.\\nThresholding was applied to the LGA scores to assign\\nlabels to each image. Thresholds were determined indi-\\nvidually for each of the six visual elements by referencing\\nmanual annotations created for a randomly sampled subset\\nof images (details provided below). For each element, the\\nthreshold maximizing the F1 score for binary classification\\nwas selected. The distribution of visual elements after ap-\\nplying these thresholds is summarized in Table 1. The re-\\nsults indicate that certain elements, such as human andtext,\\nare substantially more prevalent than others, such as candy\\nandfruit, in our dataset.3.3. Performance of CLIP Multi-Labeling\\nPerformance Evaluation. Before integrating the CLIP-\\ngenerated image labels into downstream analyses, we eval-\\nuated their accuracy by comparing them to manual anno-\\ntations from a randomly sampled subset of images. The\\nevaluation revealed a critical limitation: the absence of ex-\\nplicit negative classes in the token list led to frequent mis-\\nclassification. For instance, when labeling an image of a\\npet using tokens such as ‚Äòweed,‚Äô ‚Äòtext,‚Äô ‚Äòcandy,‚Äô ‚Äòbakery,‚Äô\\n‚Äòfruit,‚Äô ‚Äòhuman,‚Äô and ‚Äòothers,‚Äô the model often misclassified\\nthe image as ‚Äòhuman,‚Äô assigning it a high probability score.\\nHowever, when the token ‚Äòpet‚Äô was added to the list, the\\nmodel no longer misclassified the image as ‚Äòhuman.‚Äô This\\nresult highlighted the importance of including tokens that\\ncapture unrelated or ambiguous categories to improve the\\nrobustness of CLIP‚Äôs multi-labeling process.\\nToken Refinement Approach. To address the identified\\nlimitations, we applied an image captioning model to all\\ncollected images, extracted common topics from the gen-\\nerated captions, and incorporated irrelevant or ambiguous\\ntopics into the token list for the ‚Äòothers‚Äô category. This re-\\nfinement was intended to mitigate misclassification and im-\\nprove the model‚Äôs ability to identify relevant visual cues.\\nManual Annotation for Validation. We manually an-\\nnotated a random sample of 1,049 images to validate the\\nCLIP-generated labels. Three undergraduate annotators la-\\nbeled each image following a detailed codebook, achieving\\nhigh inter-coder reliability (Krippendorf‚Äôs Œ± > 0.85). The\\nmanually annotated dataset served as a benchmark to eval-\\nuate the performance of the CLIP model and guided refine-\\nments to the labeling process.\\nFine-Tuning for Improvement. Based on the man-\\nual annotations, we identified opportunities to improve\\nthe model‚Äôs performance. To achieve this, we fine-tuned\\nthe pre-trained CLIP model using the manually annotated\\ndataset.\\nPerformance of CLIP multi-labeling. Before using the\\nCLIP-generated image labels in our preliminary analysis,\\nwe examined the performance of the LGA scores by com-\\nparing them with manual labels of the 1049 images. Fig-\\nure 3 shows that, the LGA scores performs reasonably well\\nin multi-labeling our images. This is demonstrated by 0.9\\nplus AUC (Area under the ROC Curve) for almost all vi-\\nsual elements, with human manual labels as reference. The\\nresults in Figure 3 also shows that the LGA procedure is\\nnecessary when performing multi-labeling on our data, as\\ndemonstrated by the large drop of AUC when the LGA pro-\\ncedure is not used.\\n3.4. Identify Topics in Post Texts\\nWe utilized the pre-trained Bidirectional and Auto-\\nRegressive Transformers (BART) [3] model to extract top-\\nics from the post texts, where ten topics were selected us-Figure 3. Evaluation of CLIP model performance in image label-\\ning. The figure presents the Area Under the Curve (AUC) scores,\\nusing manual annotations as the reference standard. The results\\ncompare the performance of the CLIP model with Local-Global\\nAggregation (LGA) scores (orange) against the baseline perfor-\\nmance of the CLIP model without LGA (blue), where similarity\\nscores for the entire image were thresholded directly.\\n1. Cannabis Ingredients\\n2. Policies and Legalization of Cannabis\\n3. Positive feelings of cannabis\\n4. Cannabis and Social Justice\\n5. Cannabis Trade and Economics\\n6. Cannabis Prevention Campaign\\n7. Edibles and Cannabis\\n8. Health Risks Associated with Cannabis\\n9. Cannabis for Pain Management\\n10. Mental Health and Cannabis\\nTable 2. STM-generated Topics as Covariates.\\ning Structural Topic Modeling (STM) on our post text cor-\\npus. Table 2 lists the ten selected topics, which ranged\\nfrom cannabis ingredients to policies and the legalization\\nof cannabis. Similar to the image labeling process, we ad-\\ndressed a multi-labeling problem for post texts, as one post\\ntext could pertain to zero, one, or multiple topics. For con-\\nvenience, the BART API from Huggingface was employed\\nto create scores for each topic-text pair. These BART scores\\nwere directly utilized in the downstream analysis as ten nu-\\nmerical features of each image (after mapping post texts to\\nimages). Figure 4 (Appendix 5.2) illustrates the distribution\\nof the scores across the ten topics in our data, indicating that\\nsome topics appeared much more frequently than others.\\n4. Analyze association with online activities\\nThis study focused on the association between different vi-\\nsual elements in the social media environment and online\\ninteractions in response to the corresponding social media\\npost. Specifically, we performed Linear Regression to ad-dress this goal, using users‚Äô online interaction as the re-\\nsponse. The following sections discuss how we created an\\nengagement score for each post to quantify its related online\\ninteractions.\\n4.1. Quantification of Online Interactions.\\nWe measured the volume of the online activities following\\nan image post by normalizing the volume with the aver-\\nage amount of activities incited by the corresponding Face-\\nbook account. This normalization is performed to address\\nthe large variation in the popularity of different accounts.\\nSpecifically, for each image, we identify the account which\\nposted it. We then take the last 100 image posts from that\\naccount and calculate the average number of interactions\\nat each age (15 minutes old, 60 minutes old, 5 hours old,\\netc.). Next, we compared the pattern of interactions fol-\\nlowing the image post we are interested in that average and\\nmultiplied the difference by the weights in each dashboard.\\nThis comparison generated an engagement score for the im-\\nage post. A score over 1 means the post is performing better\\nthan average from that account, and at that point in time (10\\nminutes after posting, 1 hour after posting, etc). A negative\\nscore means it is performing worse than the average.\\n4.2. Preliminary Results\\nThe results (see Table 3) fitted a linear regression model\\nto predict engagement score based on image and text fea-\\ntures. These findings revealed significant positive associ-\\nations between online activities and the presence of visual\\ncues such as candy andweed visual cues. A negative asso-\\nciation was observed for textwithin images. Notably, the\\ncolorfulness of post images was significantly negatively as-\\nsociated with online engagement. Regarding textual topics,\\npolicies and legalization of cannabis ,cannabis and social\\njustice , and positive feelings about cannabis were signifi-\\ncantly positively associated with online activities. In con-\\ntrast, topics such as cannabis for pain management and\\nmental health and cannabis demonstrated significant neg-\\native associations with online engagement.\\n5. Discussion and Conclusion\\nThis study highlights key associations between visual cues\\nin social media posts and user engagement. Specifically,\\nwe identified a positive correlation between images featur-\\ning candy and weed visuals and online interactions, while\\ntext-heavy images showed a negative association. Inter-\\nestingly, the colorfulness of images was negatively asso-\\nciated with user engagement, contrary to findings in other\\nvisual marketing contexts. In terms of textual content, top-\\nics related to policies, social justice, and positive sentiments\\nabout cannabis were positively correlated with engagement,\\nwhereas topics concerning pain management and mental\\nhealth showed negative relationships. These findings haveestimation P >|t| %95 CI\\nIntercept -10.1942 0.000 (-11.829, -8.560)\\ntype (reference=photo)\\nlink -2.3146 0.000 (-3.065, -1.564)\\nlive video complete 7.8269 0.077 (-0.858, 16.512)\\nlive video scheduled 3.3386 0.397 (-4.390, 11.067)\\nnative video 16.7042 0.000 (12.114, 21.295)\\nstatus -0.1534 0.909 (-2.790, 2.483)\\nvideo -3.4102 0.040 (-6.670, -0.150)\\nyoutube 3.1502 0.027 (0.354, 5.947)\\ndiurnal cycle (reference=Morning)\\nAfternoon 2.4860 0.000 (1.571, 3.401)\\nEvening 1.3554 0.008 (0.357, 2.354)\\nNight -0.5146 0.340 (-1.572, 0.542)\\ncolorfulness score -0.0336 0.000 (-0.046, -0.021)\\nbrightness score -0.0065 0.074 (-0.014, 0.001)\\nis weekend -0.7678 0.069 (-1.595, 0.060)\\nPost image labels\\ncandy 5.8403 0.017 (1.047, 10.634)\\nbakery -0.8872 0.512 (-3.539, 1.765)\\nfruit 2.5166 0.079 (-0.291, 5.324)\\nhuman 0.4942 0.177 (-0.223, 1.212)\\nweed 2.5978 0.000 (1.544, 3.652)\\ntext -0.9217 0.012 (-1.643, -0.200)\\nPost text topic scores\\nCannabis Ingredients 0.3542 0.683 (-1.348, 2.057)\\nPositive feelings of cannabis 3.3647 0.000 (1.860, 4.869)\\nCannabis Trade and Economics -0.9708 0.471 (-3.613, 1.671)\\nEdibles and Cannabis -1.5927 0.071 (-3.323, 0.138)\\nCannabis for Pain Management -4.8443 0.000 (-7.219, -2.470)\\nPolicies and Legalization of Cannabis 3.1464 0.002 (1.176, 5.117)\\nCannabis and Social Justice 2.5947 0.019 (0.418, 4.771)\\nCannabis Prevention Campaign 1.8193 0.198 (-0.949, 4.587)\\nHealth Risks Associated with Cannabis 3.2779 0.003 (1.144, 5.412)\\nMental Health and Cannabis -3.9959 0.000 (-5.802, -2.190)\\nTable 3. Linear Regression Results. P-values that are smaller than\\n0.05 have been marked with boldface.\\nimportant implications for regulating cannabis marketing,\\nparticularly in ensuring that content does not unintention-\\nally appeal to youth or vulnerable populations. Current reg-\\nulations in the United States fall short in effectively restrict-\\ning youth-targeted content in the marketing of cannabis-\\ninfused edibles. To address this, it is imperative to establish\\nstricter and more uniform national guidelines that mandate\\nresponsible packaging and marketing practices to minimize\\nthe risk of youth initiation or accidental consumption. Pol-\\nicymakers can leverage these findings to develop more in-\\nformed and effective regulatory frameworks.\\nDespite its contributions, this study has limitations that war-\\nrant further investigation. First, the analysis was limited\\nto Facebook posts over a six-month period in 2021, poten-\\ntially excluding evolving trends and patterns across other\\nplatforms and timeframes. Future studies could expand the\\nscope to include longitudinal data and diverse social media\\nplatforms, such as Instagram or TikTok, where visual con-\\ntent plays a central role. Second, while this study focused\\non specific visual and textual features, it did not account\\nfor audience-specific factors such as demographic or psy-chographic differences, which could mediate the observed\\nassociations. Future research could incorporate audience\\nsegmentation to understand how different groups engage\\nwith cannabis-related content. Future research could ex-\\nplore these downstream effects, providing a more compre-\\nhensive understanding of how online cannabis marketing in-\\nfluences public health outcomes.\\nOverall, this study underscores the complex dynamics of\\nonline cannabis marketing, highlighting the interplay be-\\ntween visual and textual content in shaping user engage-\\nment. The findings offer valuable guidance for policymak-\\ners and researchers alike in addressing the challenges posed\\nby cannabis marketing, fostering a safer and more responsi-\\nble digital landscape.\\nReferences\\n[1] Rabab Abdelfattah, Qing Guo, Xiaoguang Li, Xiaofeng\\nWang, and Song Wang. Cdul: Clip-driven unsupervised learn-\\ning for multi-label image classification. In Proceedings of\\nthe IEEE/CVF International Conference on Computer Vision ,\\npages 1348‚Äì1357, 2023. 3\\n[2] David Hasler and Sabine E Suesstrunk. Measuring colorful-\\nness in natural images. In Human vision and electronic imag-\\ning VIII , pages 87‚Äì95. SPIE, 2003. 2\\n[3] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvinine-\\njad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov,\\nand Luke Zettlemoyer. Bart: Denoising sequence-to-sequence\\npre-training for natural language generation, translation, and\\ncomprehension. In Proceedings of the 58th Annual Meeting\\nof the Association for Computational Linguistics , pages 7871‚Äì\\n7880, 2020. 3\\n[4] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\\ning transferable visual models from natural language supervi-\\nsion. In International conference on machine learning , pages\\n8748‚Äì8763. PMLR, 2021. 3\\n[5] Muna Sharma and Yilang Peng. How visual aesthetics and\\ncalorie density predict food image popularity on instagram:\\nA computer vision analysis. Health Communication , pages\\n1‚Äì15, 2023. 1\\n[6] Yuyan Shi, Ying Cao, Ce Shang, and Rosalie Liccardo Pacula.\\nThe impacts of potency, warning messages, and price on pref-\\nerences for cannabis flower products. International Journal of\\nDrug Policy , 74:1‚Äì10, 2019. 1\\n[7] Andy SL Tan, Erica Weinreich, Alisa Padon, Mirtala Sanchez,\\nKyle M Snyder, Anna Vasilyeva, Simon Sandh, Emily Gold-\\nmann, Melody Goodman, and Danielle C Ompad. Presence of\\ncontent appealing to youth on cannabis-infused edibles pack-\\naging. Substance use & misuse , 57(8):1215‚Äì1219, 2022. 3\\n[8] L Welty, A Harrison, K Abram, N Olson, D Aaby, and K\\nMcCoy. Key substance use and mental health indicators in\\nthe united states: results from the 2016 national survey on\\ndrug use and health. Substance Abuse and Mental Health Ser-\\nvices Administration. Retrieved. College of Health Sciences ,\\n106(5):128, 2019. 1Detecting Visual Triggers in Cannabis Imagery: A CLIP-Based Multi-Labeling\\nFramework with Local-Global Aggregation\\nSupplementary Material\\nAppendix A\\nSearch Keyword\\nkief OR mmj OR mmot OR weed brownie OR weedbrownie\\nOR weedbrownies OR Blunt OR budder OR Burning one\\ndown OR Butane honey oil OR Cannabis OR cannabis but-\\nter OR cannabis candy OR cannabis edible OR cannabis in-\\nfused OR cannabutter OR Cbd OR Cheeching OR Chillum\\nOR Concentrates OR Cosmic brownie OR Dab OR dab life\\nOR dabber OR dabbers OR dabbin OR Dablife OR dabs\\nOR dank cookie OR dank edible OR edibles OR enail OR\\ne-nail OR ganja OR hash butter OR hash cookie OR hash\\noil OR hemp OR high edible OR indica OR infused mari-\\njuana OR kush OR kush butter OR kush cookies OR kush\\ninfused OR Legalize marijuana OR legalized marijuana OR\\nlegalizing marijuana OR Live resin OR magic brownie OR\\nmagic brownies OR Marihuana OR Marijuana OR Mar-\\nijuana brownie OR marijuana butter OR Marijuana cake\\nOR Marijuana candy OR marijuana cookie OR marijuana\\nedible OR Marijuana gummies OR medibles OR Medical\\ncannabis\\nAppendix B\\nCodebook\\n‚Ä¢ Candy (0=No 1=Yes)\\n‚Ä¢ Bakery (0=No 1=Yes)\\n‚Ä¢ Fruit (0=No 1=Yes)\\n‚Ä¢ Human (0=No 1=Yes)\\n‚Ä¢ Colorfulness (score)\\n‚Ä¢ Brightness (score)\\n‚Ä¢ Text (0=No 1=Yes)\\nAppendix C\\n5.1. Details of Using the CLIP\\nWe used the resnet-50x64 model of the CLIP. The embed-\\ndings of each of the seven queries are calculated by the\\nCLIP model, and the average of the seven embeddings are\\nused when calculating the similarities with image embed-\\ndings.\\n[‚Äôitap of a .‚Äô, ‚Äôa bad photo of the\\n.‚Äô, ‚Äôa origami .‚Äô, ‚Äôa photo of the\\nlarge .‚Äô, ‚Äôa in a video game.‚Äô, ‚Äôart\\nof the .‚Äô, ‚Äôa photo of the small .‚Äô]\\nWe added dogto our visual element list when construct-\\ning queries. This is because, empirically, we found thatthere are a decent amount of images with dogs in our image\\ndata and the CLIP model tends to give these images high\\nsimilarity scores w.r.t. the human label. While applying\\nthe CLIP model, we added the dog label to bring down the\\ncalculated probabilities for human features, so as to prevent\\nhuman features from being erroneously detected in images\\nthat have dog (but no human).\\n5.2. Distribution of topic scores for post texts\\nFigure 4 shows distribution of post text topic scores.\\nFigure 4. Distribution of topic scores for post texts as assigned by\\napplying the BART model.',\n",
       " 'Exploration of the Dynamics of Buy and Sale of Social\\nMedia Accounts\\nMario Beluri\\nmabe00023@stud.uni-saarland.de\\nSaarland UniversityBhupendra Acharya\\nbhupendra.acharya@cispa.de\\nCISPASoheil Khodayari\\nsoheil.khodayari@cispa.de\\nCISPA\\nGiada Stivala\\ngiada.stivala@cispa.de\\nCISPAGiancarlo Pellegrino\\npellegrino@cispa.de\\nCISPAThorsten Holz\\nholz@cispa.de\\nCISPA\\nAbstract\\nThere has been a rise in online platforms facilitating the\\nbuying and selling of social media accounts. While the trade\\nof social media profiles is not inherently illegal, social me-\\ndia platforms view such transactions as violations of their\\npolicies. They often take action against accounts involved in\\nthe misuse of platforms for financial gain. This research con-\\nducts a comprehensive analysis of marketplaces that enable\\nthe buying and selling of social media accounts.\\nWe investigate the economic scale of account trading\\nacross five major platforms: X,Instagram ,Facebook ,Tik-\\nTok, and YouTube . From February to June 2024, we iden-\\ntified 38,253 accounts advertising account sales across 11\\nonline marketplaces, covering 211 distinct categories. The\\ntotal value of marketed social media accounts exceeded $64\\nmillion, with a median price of $157 per account. Addition-\\nally, we analyzed the profiles of 11,457 visible advertised\\naccounts, collecting their metadata and over 200,000 profile\\nposts. By examining their engagement patterns and account\\ncreation methods, we evaluated the fraudulent activities com-\\nmonly associated with these sold accounts. Our research\\nreveals these marketplaces foster fraudulent activities such\\nas bot farming, harvesting accounts for future fraud, and\\nfraudulent engagement. Such practices pose significant risks\\nto social media users, who are often targeted by fraudulent\\naccounts resembling legitimate profiles and employing so-\\ncial engineering tactics. We highlight social media platform\\nweaknesses in the ability to detect and mitigate such fraud-\\nulent accounts, thereby endangering users. Alongside this,\\nwe conducted thorough disclosures with the respective plat-\\nforms and proposed actionable recommendations, including\\nindicators to identify and track these accounts. These mea-\\nsures aim to enhance proactive detection and safeguard users\\nfrom potential threats.\\n1 INTRODUCTION\\nWith the widespread use of social media platforms and the\\ngrowing number of users, fraudsters are increasingly exploit-\\ning platforms and their users with various social engineeringtricks [ 28,30,51]. According to the Federal Trade Commis-\\nsion (FTC), scams originating from social media resulted in\\nreported losses totaling $2.7 billion between January 2021\\nand June 2023 [ 44]. The report also highlighted that fraud\\noriginating from social media accounted for higher monetary\\nlosses compared to other methods of contact.\\nScammers abuse social media users through sophisticated\\nfraudulent schemes, often organized in a complex manner,\\nand leverage a large number of fake accounts to add layers\\nof sophistication to their attacks [ 63]. Beyond traditional\\nabuses such as phishing [ 20], investment fraud [ 42], and im-\\npersonation [ 11], scammers are found to engage in various\\nfraudulent schemes such as clickbait and engagement farm-\\ning to generate revenue [ 29]; spreading manipulated news\\nfor political or financial gain [ 10]; using deepfakes or genera-\\ntive AI for romance scams or pig-butchering [ 35]; advertising\\ncounterfeit goods [ 62]; executing shipping scams [ 48]; fraud-\\nulent recruitment [ 47]; blackmail [ 41] and sextortion [ 46];\\nand influence campaigns to manipulate public opinion or\\ntrends [ 45]. These fraudulent schemes have escalated in scale\\noperations in the last years, leading to unprecedented ex-\\nploitation of social media platforms and their users.\\nScammers likely find social media as an ideal platform for\\nexploitation due to its ease of account creation compared\\nto fake website setups. Account setup in social media pro-\\nfiles lacks steps such as purchasing domains and certificates\\nor ensuring the domain does not get flagged. Social media\\naccounts offer greater immunity compared to traditional at-\\ntack vectors like email, phone, or websites. With the rise\\nof websites that facilitate the purchase of social media ac-\\ncounts, scammers gain quick and credible entry points to ex-\\nploit others. These pre-established accounts often come with\\nout-of-the-box public metrics such as followers, likes, and\\ninteractions, making them appear authentic and trustwor-\\nthy to potential victims. This perceived legitimacy enables\\nscammers to carry out fraudulent activities with reduced\\nsuspicion. Furthermore, acquiring accounts with an existing\\naudience allows scammers to bypass the effort of building fol-\\nlowers organically, enabling them to scale operations rapidly.arXiv:2412.14985v1  [cs.CR]  19 Dec 2024These accounts also help evade platform detection by pos-\\ning as genuine users, making it more challenging for social\\nmedia platforms to identify and block malicious activities.\\nOur research is motivated by combining the above three\\nkey perspectives: (i) the growing number of social media\\nusers, (ii) the expansion and scale of scams on social media\\nbeyond traditional attacks, and (iii) the rise of websites sell-\\ning social media accounts, which allow fraudsters to bypass\\neffort, save time, scale operations, and evade platform detec-\\ntion. In this paper, we analyze the marketplaces that allow\\nbuying social media accounts. Our analysis provides a com-\\nprehensive study of the marketplaces and the accounts being\\nsold, including their public engagement metrics and scam\\noperations. Our work addresses the critical gap in under-\\nstanding what happens when such accounts are purchased\\nand how they are subsequently misused.\\nIn this work, we perform a comprehensive evaluation\\nof data collected from 11 marketplaces and over 38,000 ac-\\ncounts listed for sale. We analyze the processes sellers use to\\ncreate accounts on these marketplaces, analyze how social\\nmedia accounts are advertised and set up, and evaluate pro-\\nfile engagement metrics exploited for abuse. Furthermore,\\nwe quantify the impact and categorize the types of abuse\\nassociated with these accounts. Our work represents the first\\nlarge-scale analysis of 11 marketplaces selling social media\\naccounts of five platforms: X(formerly known as Twitter),\\nInstagram ,Facebook ,TikTok , and YouTube . More specifically,\\nwe identified 38,253 accounts on five social platforms which\\nresulted in over $64 million value for sale. We performed the\\ntracking and comprehensive evaluation of 11,457 accounts\\nthat provided the links pointing to their respective social me-\\ndia platforms. Through evaluating account creation and post\\nengagement from all five social media platforms, we shed\\nlight on how these accounts are later misused to target users.\\nAdditionally, we offer recommendations to mitigate such\\nscams. In summary, we make following key contributions.\\n‚Ä¢We conduct the first large-scale empirical study of\\nmarketplaces involved in selling social media accounts,\\nuncovering fraudsters targeting over 210 categories\\nas part of their scams.\\n‚Ä¢We provide a comprehensive evaluation of profile\\nengagement and account creation setups, identifying\\nthe operational scale and abuse categories in which\\nthese accounts are exploited to target platforms and\\ntheir users.\\n‚Ä¢Finally, we propose recommendations and distill in-\\nsights to combat such processes, aiming to prevent\\nthe emerging threats posed by these marketplaces\\nand sold accounts.\\nTo foster research, we share our code [ 56] related to mar-\\nketplaces that were publicly advertised for selling. However,for data protection reasons, the data related to the study are\\nonly shared with interested academics, abused entities, or\\nresearchers upon request.\\nEthical Consideration and Data Disclosure. Our research\\ndoes not involve direct interaction with scammers or indi-\\nviduals, and relies solely on publicly available data. During\\ndata collection, we ensured that no engagement with human\\nsubjects occurred; our methodology was entirely passive and\\nlimited to publicly available data. Additionally, we disclosed\\nrelevant information, such as social media profiles, to all five\\nsocial media platforms involved. For account-selling market-\\nplaces, we ensured ethical compliance by refraining from\\nbypassing CAPTCHAs, paywalls, evading automation trig-\\ngers, and avoiding any direct interactions with social media\\nprofiles during automated data collection. Our findings were\\nshared with all five social media platforms that we studied.\\nWe received positive feedback from X, expressing further\\ninterest in future collaboration.\\n2 BACKGROUND AND RELATED WORK\\nThe proliferation of social media platforms has brought trans-\\nformation changes to communication, entertainment, and\\nbusiness. However, alongside these benefits, social media has\\nalso become a fertile ground for abuse, fraud, and malicious\\nactivity. Bad actors exploit the platforms‚Äô vast user bases\\nfor illicit purposes such as account trading, spam, scams,\\nand phishing, often causing harm to legitimate users and\\nundermining trust in these platforms. Understanding the\\nmechanisms and economic incentives behind these malicious\\nactivities is critical for designing effective countermeasures.\\nOne growing area of concern is the emergence of fraudu-\\nlent marketplaces that facilitate the trade of compromised\\nor fake social media accounts. These accounts are used to\\namplify malicious campaigns, manipulate engagement met-\\nrics, or perpetrate fraud at scale. While previous studies have\\nexplored the broader cybercrime economy [ 22,53,57], little\\nattention has been paid to the life cycle of these marketplaces\\nand their accounts‚Äîfrom their sale to their eventual involve-\\nment in abuse, which we address in this work. We provide\\na framework for understanding the engagement and abuse\\npatterns of these accounts after they are traded. Our work\\nprovides a comprehensive study of identifying accounts that\\nare solid across various marketplaces and tracking the ac-\\ncount‚Äôs engagement and abuse categories. In the following,\\nwe discuss previous works related to our study and highlight\\nour unique nature of work addressing the research gap.\\nCybereconomics and Fraudulent Marketplaces. Some of\\nthe prior work that relates to ours focused on identifying ma-\\nlicious services or merchants and evaluating their business\\nmodel and product offering over time [ 22,53,57]. Stringh-\\nini et al. [ 53] analyzed the operations of Twitter Account\\n2Markets, which generate revenue by exploiting networks of\\nfollowers, often through artificial inflation of follower counts\\nor using compromised accounts to distribute promotional\\nor abusive content. Similarly, DeKoven et al. [ 22] studied\\nthe for-profit services that drive traffic to manipulate the\\nuser‚Äôs perception, while Thomas et al. [ 57] studied the role\\nof the underground market in contributing towards abusive\\nbehavior such as scams and spams. However, none of these\\nstudies focus on identifying the accounts that are being sold\\nand later tracking them to understand the maliciousness.\\nDetection of fake accounts. Another line of work sim-\\nilar to ours focused on the detection of fake accounts on\\nsocial media, for example by constructing ‚Äúsocial profiles‚Äù\\nof users, allowing to detect discrepancies of the regular be-\\nhavior (e.g., [ 19,23,50]) or by developing anomaly detection\\nalgorithms (e.g., [ 58,61]). Further research developed de-\\ntection techniques based on the characteristics of Twitter\\naccounts and posts (e.g., [ 21,31,53,55]), on the connections\\nbetween profiles (e.g., [ 38]), or on the combination of mul-\\ntiple features (e.g., [ 13,64]). Finally, Kurt et al. [ 57] studied\\npatterns in the naming and registration processes of Twitter\\naccounts, deriving patterns allowing to detection of abusive\\nbulk registration of profiles. Our study differs from previous\\nwork, mainly in understanding the origination of the social\\nmedia accounts that are later abused in scale.\\nSpam, Scam, and Phishing. Miscreants use social media\\nplatforms to spread spammy, malicious, or scam content [ 12,\\n19,54], leveraging a post‚Äôs content[ 24], visual appearance[ 52],\\nor the reputability of a profile[ 20,24], putting legitimate\\nusers at risk. Previous works measured the number of spam\\ntweets and URLs, finding tweets containing over 2 million\\ndistinct URLs pointing to blacklisted scams, phishing, and\\nmalware over the period of two months [ 25], and showing\\nthat most accounts spreading malicious tweets are likely\\ncompromised [ 25], although new accounts are also regis-\\ntered specifically with this purpose. A similar study [ 24]\\nconducted on Facebook confirmed this phenomenon, observ-\\ning how compromised accounts are used to contact victim\\nusers posting URLs leading to advertisements, phishing, and\\ndrive-by downloads. Our work complements such studies by\\nfocusing on social media and various types of scams.\\n3 EVALUATION SETUP\\nIn this section, we provide detailed information on evaluation\\nsetup and data collection process. Our evaluation framework\\nconsists of three main modules, as illustrated in Figure 1.\\nInitially, we identify marketplaces that advertise the buying\\nand selling of social media accounts ( ‚ûä). Once identified, we\\ncurate these marketplaces based on the feasibility of data\\ncollection and proceed with semi-automated steps to gather\\nadvertised accounts. We then query the respective socialmedia platforms to collect publicly available engagement\\nand profile metadata linked to these advertised accounts\\n(‚ûã). Finally, we track and analyze the collected marketplaces\\nand social media accounts to uncover the mechanics and\\noperations behind these scams ( ‚ûå). Below, we provide a\\ndetailed explanation of each of the three modules.\\n3.1 Collect Marketplaces\\nThe market for buying and selling social media accounts is\\ndivided into public and underground markets. Public mar-\\nkets are accessible through standard internet searches and\\noperate with a semblance of legitimacy, often hiding behind\\nthe guise of marketing services. In contrast, underground\\nmarkets are clandestine, often accessible only via specific\\nforums or onion directories on the dark web. These under-\\nground markets operate in secrecy to avoid detection and\\nenforcement actions.\\nFor data collection, we initiated our investigation through\\nGoogle searches and a review of previous academic papers\\nlisting account-selling websites or underground markets [ 17,\\n22,32‚Äì34,43,55,57]. This preliminary research provided a\\nfoundation, which was further expanded by tracking post-\\nings in publicly accessible forums and onion directories that\\nlist underground market sites. This dual approach ensured\\na comprehensive understanding of both market types. This\\nresulted in a comprehensive list of 58 websites and nine\\npersonal contact points (emails, phone numbers, telegram\\nhandlers). We focused on trading channels where social me-\\ndia account handles were publicly visible, excluding others\\nfrom further automated data collection, as reported in Table 9\\nin the Appendix.\\n3.2 Data Collection\\nOur data collection relied on two primary sources: (i)ac-\\ncounts advertised by sellers on various marketplaces and (ii)\\nfor each account with a visible social media profile link, we\\nqueried the respective social media platforms to gather asso-\\nciated profile metadata and engagement posts. We provide\\nfurther details below.\\nPublic Marketplace Account Collection. We developed\\na JavaScript-enabled web crawler to automatically extract\\naccount-selling offers related to popular social media plat-\\nforms. These include X,Instagram ,YouTube , and TikTok .\\nThe crawler is implemented using Python , with Selenium\\nfor browser automation and the Chrome DevTools Protocol\\nfor fine-grained page control and interaction.\\nEach marketplace may have multiple listings covering\\nvarious social media platforms, such as Instagram and Twit-\\nter. For each marketplace, we manually identified the seed\\nURLs for different listings and initialized our crawler with\\n3Collect \\nMarketplaces\\nAbuse \\nTracking \\n&\\nAnalysis\\nUnderground \\nMarketplaces\\nPublic\\nMarketplaces\\nMarketplaces\\nSocial \\nMedia \\nPlatforms\\nSemi-Automated\\nData \\nCollection\\n1\\n3\\n2Figure 1: Evaluation Setup ‚Äì Our evaluation setup com-\\nprises three main modules. Initially, we collect market-\\nplaces that sell social media accounts based on man-\\nual search ( ‚ûä). Through semi-automation, we collect\\ndata from various marketplaces related to advertised\\naccounts and further query social media APIs to col-\\nlect data related to the advertised account ( ‚ûã); finally,\\nwe evaluate the collected data by analyzing the mar-\\nketplace, and their affiliated social media accounts in-\\ncluding scam tracking and abusive elements associated\\nwith such accounts ( ‚ûå).\\nthese URLs. Given a seed URL, the crawler employs a depth-\\nfirst strategy: it visits a listing page, clicks on each offer to\\nreach the offer webpage, and collects its details. This pro-\\ncess continues until all offers on a listing page are covered.\\nThe crawler then moves to the next listing page and repeats\\nthe process, stopping only when no new offers or listing\\npages are found. For each advertised account for sale, we\\ncollect displayed information such as offer URL, title, seller\\ninformation, price, payment methods, social media account\\nhandles, account properties (such as the number of likes\\nand followers), and the offer description. Out of 58 trad-\\ning markets, 11 contain selling offers with publicly visible\\nsocial media account handles, which we focused on. In Ta-\\nble 1, we display each marketplace, seller, and advertised\\naccount detail. In total, we collected 38,253 URLs from the\\n11 marketplaces, out of which 11,457 URLs display accounts\\nlinked to respective social media platforms. Among these\\nmarketplaces, 35% (13,665/38,253) of accounts resulted from\\nAccsmarket as the highest number of accounts, and the low-\\nest accounted from FameSeller 109 accounts, lesser than 1%\\nof the total accounts found. We identified, 5/11 marketplaces,\\nSocialTradia ,TooFame ,SwapSocials ,Surgegram , and BuySocia\\nomitting the public display of seller‚Äôs information.\\nProfile Metadata Collection. Of 38,253 advertised accounts\\nfrom open marketplaces, 29% (11,457) of the accounts for\\nsale advertised visible links pointing to their respective so-\\ncial media profile. For each of these social media accounts,\\nwe collected each profile‚Äôs public profile metadata, includ-\\ning user profile names, descriptions, account creation dates,Table 1: Overview of the public marketplace sellers and\\nadvertised social media accounts for sale. Among these\\n11 marketplaces, Accsmarket was found to have the\\nhighest number of advertised accounts, and FameSeller\\nwas the lowest.\\nPublic Marketplace Sellers Accounts\\nAccsmarket 2,455 13,665\\nFameSwap 6,617 8833\\nZ2U 240 6,417\\nSocialTradia - 4,020\\nInstaSale 251 1,950\\nMidMan 304 1282\\nTooFame - 695\\nSwapSocials - 530\\nSurgeGram - 205\\nBuySocia - 547\\nFameSeller 77 109\\nTotal 9,944 38,253\\nand engaging posts, from visible accounts linked to the ad-\\nvertised seller‚Äôs marketplace page. For this, we utilized the\\nrespective API services [ 14‚Äì16,39,40,59,60] of the social\\nmedia platforms. In Table 2, we present a detailed breakdown\\nof the collected social media accounts and their correspond-\\ning posts. Our findings show that YouTube accounts had the\\nhighest number of visible account profiles linked, account\\n54% (6,271/11,457) of the total visible accounts, whereas the\\nlowest count resulted from Facebook , 5% (649/11457) from\\nour overall visible accounts.\\nUnderground Forum Account Collection. We analyzed\\naccounts sold on underground markets accessed via the Tor\\nnetwork. Initially, we manually inspected these markets to\\nconfirm their accessibility and available goods. Many mar-\\nkets referenced in related research were inaccessible due to\\ntakedowns, lack of directory listings, or timeout errors, while\\nothers were non-English, or did not sell digital goods. This\\nnarrowed our focus to four underground markets. We ex-\\npanded our dataset by adding 16 more underground forums\\nfound in onion directories, which sold social media-related\\ngoods at the time of this first inspection. All inspected mar-\\nkets required user registration and implemented complex,\\nsite-specific, non-standard CAPTCHAs . Additionally, navi-\\ngation was restricted: attempts to access pages not linked\\nwithin the current page resulted in blocks. Due to these lim-\\nitations, we collected all account sale data manually. Data\\ncollection followed two criteria: (i)browsing forum sections\\ndedicated to accounts or social media, or (ii)using forum\\nsearch functionalities with keywords like [account/s | pro-\\nfile/s] [name of social media] . In both cases, we recorded data\\n4Table 2: Overview of the social media data collection. In\\nthe table, we display social media accounts advertised\\nfor sale. The accounts that are listed referencing the\\nsocial media platform account profile we name them\\nas visible accounts.\\nSocial Visible Visible All\\nMedia Accounts Accts. Posts Accounts\\nInstagram 2,023 4,207 12,658\\nYouTube 6,271 3,411 9,087\\nTiktok 1700 25,131 8,973\\nFacebook 649 7,407 4,216\\nX 814 165,427 3,319\\nTotal 11,457 205,583 38,253\\nfrom the first five pages of results, up to 25 postings per\\nsocial media platform.\\nOf the 20 markets in our initial dataset, eight did not sell\\nsocial media-related goods, and four offered services like\\nlikes and followers but no accounts, leaving a final dataset of\\neight underground markets. In the second manual inspection,\\nwe collected for each posting the URL, title, textual content,\\nauthor‚Äôs username, publication date, number of replies, price,\\nquantity sold, and a screenshot. Differences in forum models\\nand GUIs meant that not all fields were consistently available\\nacross forums. For example, some forums did not display the\\ndate when a message was posted, or disallowed comments\\nunder the listings.\\n3.3 Tracking and Analysis\\nThe third module analyzes the data collected from market-\\nplaces and analyzes the engaged posts from the advertised\\naccounts. This includes aspects such as the intricacies of sell-\\ners‚Äô advertisements, public engagement with social media\\nprofiles, and abusive elements such as scam tactics and the\\noperations of scammers targeting social media users.\\nWe present our findings as follows: an overview of market-\\nplaces in Section 4; profile creation and engagement analysis\\nin Section 5; scam clustering and abuses in Section 6; track-\\ning and network analysis on Section 7, efficacy and abuse\\ncontrol in Section 8 and finally we provide recommendation\\nto fight against such scam in Section 9.\\n4 ANATOMY OF MARKETPLACES\\nWe conducted a comprehensive analysis of both open and\\nunderground marketplaces involved in the buying and sell-\\ning of social media accounts. Our motivation to study both\\ntypes of marketplaces was to understand a broader spectrumof account trade ecosystems‚Äîranging from visible, main-\\nstream practices to hidden, and illicit operations. We provide\\ndetailed insights for each section below.\\n4.1 Anatomy of Public Marketplaces\\nIn this section, we outline how sellers set up their profiles in\\npublic marketplaces to advertise their accounts. Specifically,\\nwe analyze into categories, account monetization, verifica-\\ntion, descriptions, public metrics, and account pricing. We\\nprovide further detailed information as below.\\nSeller. We identified 9,949 sellers across 11 marketplaces.\\nThe highest number of sellers composite from FameSwap\\nwith 6,617 sellers, while 5/11 marketplaces BuySocia ,Social-\\nTradia ,SurgeGram ,SwapSoul , and TooFame lacked sufficient\\nseller information. The median number of seller accounts\\nwas 77. Regarding seller nationality, 29,420 sellers did not\\ndisclose their country of origin, while 8,833 sellers repre-\\nsented 138 countries. Among these, the top five countries\\nwere the United States (2,683 sellers), Ethiopia (844), Pakistan\\n(596), the United Kingdom (382), and Turkey (366). In Figure 2,\\nwe showcase the cumulative growth and activity of listings\\nacross the data collection iterations. Our observations sug-\\ngest that accounts are replenished to align with supply and\\ndemand, ensuring readiness for future sales opportunities.\\nCategories Analysis. Out of 38,253 accounts, 22% (8,775)\\nwere found to lack any categorical representation. Among\\nthe remaining 29,478 advertised accounts, 212 unique cat-\\negories were identified. The top five categories were Hu-\\nmor/Memes (5,056 accounts), Luxury/Motivation (2,292), Games\\n(1,062), Fashion/Style (1,678), and Reviews/How-to (1,420). The\\nmedian account size for these categories was 3.\\nVerified Accounts. Out of 38,253 accounts, we identified\\n185 with verified social media statuses, all of which were\\nYouTube accounts. However, these accounts did not provide\\nURLs linking to their respective YouTube channels. It is likely\\nthat sellers use this strategy to attract potential buyers.\\nAccount Monetization. We identified 164 accounts report-\\ning monthly revenue generation ranging from $1 to $922,\\nwith a median value of $136 and a total combined revenue of\\n$42,019 per month. Some sellers provided additional details\\nabout income sources and the potential benefits buyers could\\ngain from purchasing these accounts. In total, 1,020 sellers\\ndisclosed unique income sources. The top three narratives in-\\ncluded: (i)generic ad-based revenue (335 sellers), (ii)Google\\nAdSense (73 sellers), and (iii)video accounts with premium\\nmemberships or channel monetization (73 sellers). Examples\\nof these narratives are provided below:\\nThe account generates income by selling promotion plans to\\nnft and crypto projects. You can sell tweets, retweets or some\\n5Figure 2: In this graph, we present the cumulative and\\nactive listings advertised by sellers across 11 open mar-\\nketplaces over our data collection iterations between\\nFeb 2024 to Jun 2024. The decline in active listings sug-\\ngests that some accounts went offline, possibly due to\\nsuccessful sales or the seller‚Äôs decision to take them\\noffline. At the same time, the continuous growth in cu-\\nmulative listings, despite the dip in active ones, reflects\\nthe replenishment of inventory to maintain higher\\nstock levels and meet supply and demand needs.\\ncombos of boths. You can also sell weekly, middle or long term\\ncampaigns. A revenue-share is also a smart option. I can teach\\nyou everything to help you make income with my account.\\nYou can monetise your content by selling promo videos or\\nputting different watermarks on your Shorts videos for money.\\nAccount Description. Out of 38,253 accounts, 63% (24,293)\\nincluded descriptions about the accounts. Through manual\\nevaluation based on keyword analysis, we identified eight\\ndistinct strategies used in these descriptions: (i)listings la-\\nbeled as authentic (784 accounts), (ii)listings labeled with\\n\"fresh and ready\" accounts (157), (iii)listings promoting busi-\\nness adaptability (122), (iv)real user accounts with activity\\n(116), and (v)offers with original email included. Examples\\nof a description are shown below:\\nNo shout outs have ever been done on the account. So the\\naccount is fresh and ready for whatever purposes you need ‚Äì\\nCPA, product promotion + sales, drop shipping, traffic genera-\\ntion, or simply you want to own an Instagram page with real\\nand active users. Save yourself time and energy of starting a\\nnew account and growing it (which can take months). Enjoy\\nthe convenience and time saved.Table 3: Payment methods supported by different plat-\\nforms.\\nPayment Methods\\nAccsmarket\\nFameSwap\\nZ2U\\nSocialTradia\\nInstaSale\\nMidMan\\nTooFame\\nSwapSocials\\nSurgeGram\\nBuySocia\\nFameSeller\\nTraditional\\nVisa ‚úì ‚úì\\nPayDirekt ‚úì\\nGPay Visa ‚úì\\nDLocal ‚úì\\nAppota Visa ‚úì\\nPrepaid Vouchers\\nNeoSurf ‚úì\\nCrypto\\nBTC ‚úì ‚úì ‚úì\\nETH ‚úì ‚úì ‚úì ‚úì\\nLiteCoin ‚úì\\nTether ‚úì\\nBNB ‚úì\\nMatic ‚úì ‚úì\\nDash\\nExchanges\\nCoinbase ‚úì ‚úì\\nAirWallex ‚úì\\nDigital Wallets\\nPayPal ‚úì ‚úì\\nTrustly ‚úì\\nSkrill ‚úì\\nWeChat ‚úì\\nAliPay ‚úì\\nPayssion ‚úì\\nEscrow-Based\\nTrustap ‚úì ‚úì\\nPayer ‚úì\\nUnknown ‚úì‚úì ‚úì ‚úì ‚úì\\nSelling TikTok account with over 2.1 million followers and a\\nviral video with 69 million views and 13.5 million likes. The\\naccount averages millions of views per video. This account\\nhas proven to be highly engaging and has attracted a large\\nfollowing. If you are interested in purchasing this account,\\nplease free to make an offer.\\nAccount Followers. Advertised accounts often share their\\nfollower counts. We found that 40% (15,358) of accounts\\ndisplayed follower information. The median follower counts\\nfor each social media platform were as follows: X(3077),\\nInstagram (26,998), TikTok (20,807), YouTube (25,700) and\\nFacebook (76,050).\\n6Figure 3: An example of advertised seller accounts on\\nFameSwap marketplace with exceptionally high prices,\\nthe reasons behind such elevated prices remain unclear.\\nThe account shows the follower count close million,\\nand the price at $50 million.\\nAccount Prices. The median advertised prices for social me-\\ndia accounts were as follows: Facebook ($14), X($17), Insta-\\ngram ($298), TikTok ($755), and YouTube ($759). The overall\\nsum of all advertised account prices totaled $64,228,836, with\\na median value of $7,573,348 across the platforms. Among\\nthe five platforms, TikTok had the highest total at $12,760,408,\\nwhile Facebook had the lowest at $145,937. Although the rea-\\nsons behind the exceptionally high pricing of some accounts\\nremain unclear, 345 accounts were identified with prices\\nexceeding $20,000. These accounts had a median price of\\n$45,000, a maximum of $5,000,000, and contributed a total\\nsum of $38,040,411. An example of such a price account is\\nshown in Figure 3.\\nSupported Payment Methods. We analyzed the payment\\nmethods supported for buyers across 11 marketplaces. In Ta-\\nble 3, we present a detailed breakdown of these payment\\nmethods by the marketplace. Our findings indicate that cryp-\\ntocurrency and digital wallets are preferred over traditional\\npayment providers. This preference is likely rooted in their\\nwidespread adoption, enhanced anonymity, and reduced po-\\ntential for disputes compared to traditional payment meth-\\nods. In Appendix A, we provide additional detail in payment\\nextraction and security implications.\\n4.2 Anatomy of Underground Marketplaces\\nOur investigation into underground markets for social media\\naccounts began with an initial list of eight marketplaces:\\nDark Matter [3],Kerberos [4],Nexus [6],Torzon Market [8],\\nWe The North [9],Black Pyramid [2],ARES Market [1], and\\nMGM Grand [5]. However, at the time of our in-depth datacollection, we observed that two (namely ARES Market and\\nMGM Grand ) did not have any account for sale, leaving six\\nmarkets for analysis. These provided valuable insights into\\nthe structure and dynamics of this illicit trade.\\nCharacteristics of the Marketplaces. We collected a total\\nof 65 posts from six platforms, related to four social networks.\\nThe Nexus market offers the largest amount of accounts (37),\\nfollowed by We The North (15). The remaining four lists five\\nor fewer accounts each, suggesting a lack of requests for\\nthis specific digital good. Listings in these markets describe\\naccounts for sale emphasizing characteristics like follower\\ncounts, and engagement metrics (likes and views), specifying\\nwhether they are organic or bots, whether accounts are aged,\\nand whether they are empty or populated with content. Posts\\ncan either sell single accounts or a bulk package, sometimes\\ncreating a mismatch between the listing price and the price\\nper account. The six marketplaces displayed varying levels of\\nactivity and specialization. Kerberos had two sellers offering\\n51 accounts, primarily for TikTok andX, indicating a focus\\non bulk sales. The remaining markets offered one account\\nper post. Dark Matter hosted five posts offering accounts\\nforYouTube ,TikTok , and X, from three sellers. Nexus , the\\nmost active market with 37 posts from four sellers, catered\\ntoInstagram ,X, and TikTok . In the Torzon market, two sellers\\nlisted four accounts across Instagram ,TikTok , and YouTube ,\\nand in Black Pyramid two sellers offered two YouTube ac-\\ncounts in two posts. We The North , with 15 posts from one\\nsingle seller, exclusively targeted TikTok , emphasizing its\\nprominence in the underground trade. Among the sellers,\\nwe identified two using the same username across platforms,\\nsuggesting cross-platform operations to maximize visibility.\\nStructure and Content of Listings. Listings generally fea-\\ntured concise descriptions, with post lengths averaging be-\\ntween 14 and 123 words depending on the market. Sellers\\nincluded contact details such as Telegram handles or website\\nlinks for payment and fulfillment, as marketplaces do not\\nhandle transactions directly. Posts also frequently outlined\\ndelivery logistics, guarantees, and disclaimers about seller li-\\nability, such as for lost credentials. On the other hand, listings\\nalmost never reported the handle of the advertised product\\n(observed only once). Comment threads often included buyer\\nfeedback, trial requests, or \"bumps\" from sellers to increase\\nvisibility. Occasionally, buyers left testimonials confirming\\nsuccessful transactions.\\nPatterns of Similarity Across Listings. A significant pat-\\ntern in our analysis is the high degree of similarity observed\\nacross some posts, with word similarity ranging from 88%\\nto 100%. This repetition often involves the same username\\n(or seller) reusing identical content for multiple posts, either\\non a single platform or across different platforms. Interest-\\ningly, the phenomenon is even more pronounced between\\n7sellers with distinct account names, particularly within the\\nsame marketplace, and to a lesser extent across different\\nmarketplaces.\\nTikTok-related offerings on the Nexus market exhibit the\\nmost notable cases of textual reuse. We carried out a case-\\ninsensitive similarity analysis after removing numbers and\\npunctuation. For instance, we identified the same author\\nusing identical body text for two different posts (100% sim-\\nilarity), seven posts from three distinct sellers with highly\\nsimilar content (average similarity of 98%), and two posts by\\nthe same seller on separate platforms, with identical text. Ad-\\nditionally, we found a single instance of two distinct authors\\nposting identical text on different platforms. Altogether, 12\\nof the 42 posts analyzed displayed such high similarity, with\\nall cases linked to just three authors. This consistency may\\nsuggest a coordinated effort rather than random duplication.\\nSimilar patterns were also identified for other platforms,\\nthough less frequently, with 2 out of 13 reused posts linked\\nto Instagram (also involving the Nexus marketplace), 1 out\\nof 3 for Twitter, and 3 out of 7 for YouTube.\\n4.3 Comparative Summary\\nOur observations on underground and open marketplaces\\nshowed that (i)underground forums are restricted via dark-\\nweb; (ii)sellers are not very informative and often operate\\nunder pseudonyms, whereas those on open marketplaces\\ntypically disclose limited identity information; (iii)the num-\\nber of listings advertised in underground were less than 100,\\nwhile the listings on open marketplaces found to be over\\n38K; (iv)listings on underground markets are ultimately\\nsimilar to forum posts, resulting in sparse or missing infor-\\nmation about account details (likes, followers); (iv)payments\\non underground markets were never handled by the plat-\\nform but agreed upon on a different channel between buyer\\nand seller, also via escrow methods, while payment methods\\non open marketplaces were rather flexible in types of pay-\\nment methods; (v)the prices on underground markets can\\nbe unclear when purchasing in bulk, or in case of private\\nbargaining or auctions, while the open marketplace found to\\ncontain pricing detail; and furthermore we observe there are\\nno buyer-seller mediatory transactions involved thus buyer\\nmay find unprotected during purchase at underground mar-\\nketplaces.\\nFor the rest of the paper, our analysis of social media\\nprofiles will be based on the public marketplaces.\\n5 ACCOUNT SETUP AND ENGAGEMENT\\nIn the previous section, we analyzed the anatomy of mar-\\nketplaces and explored how accounts are advertised for sale.Table 4: Followers - In this table we present followers\\nminimum, media, and maximum count based on the\\npublicly marketed available social media accounts that\\ncontain visible profile URLs to respective social media\\nplatforms. We queried each social media account and\\nobtained public metrics such as followers. This indi-\\ncates that accounts for sale often harvest large numbers\\nof followers.\\nSocial Media Min Median Max\\nTikTok 0 1 6,893\\nX 55 2,752 1,078,130\\nFacebook 115 27,669 5,239,529\\nInstagram 1032 8,362 6,288,290\\nYouTube 0 8,460 20,500,000\\nAll 0 7,830, 20,500,000\\nIn this section, we focus on understanding how the strate-\\ngic preemptive tailoring of profiles aligns with market de-\\nmand. Based on our findings, these accounts are meticulously\\ncrafted to target specific categories by leveraging factors such\\nas naming conventions, descriptions, geo-locations, account\\nsetup types, and creation dates. Our analysis reveals that\\nthe preemptive tailoring of profiles aims to mimic organic\\nprofiles, drive engagement, and build a substantial subscriber\\nbase. We provide detailed observations below\\nAccount Name and Description. We observe profiles fre-\\nquently adopt terms and themes associated with popular\\nindustries and interests, likely to attract a wide audience\\nor to foster trust and credibility for malicious use. This in-\\ncludes, for example, (i)trendy terms such as crypto or NFTS\\n(e.g., Crypt Hunter), (ii)names implying expertise or sta-\\ntus (e.g., Mr. NFT expert), (iii)personalization appealing to\\nspecific demographics (e.g., Kajal Kumar), (iv)profile with\\nthe adult or sensitive theme (e.g., Massage in Riyadh), and\\n(v)mix of unrelated names, emojis, or terms from regional\\nand local languages (e.g., „Åæ„Çì„Å°„Ç´„Éì„Ç¥„É≥ ). The account\\nnaming inclusion of financial and gaming terms indicates\\nlikely targeting of users interested in fast wealth-building or\\nentertainment.\\nLocation. We identified 3,236 profiles that listed 140 unique\\nlocations as part of their profile address, although location\\nentries are optional on social media profiles. Among these,\\nthe top five countries represented are the US(1,242), India\\n(470), Pakistan (222), South Korea (156), and Bangladesh (114).\\nThis indicates that the US is the preferred location for ac-\\ncount creation, potentially making the profiles appear more\\nrelatable and trustworthy to victims based on their origin.\\n8Figure 4: Date of account creation - In this graph, we\\ndisplay visible social media account profiles from 5\\nsocial media platforms based on their date of creation\\ndate. We identify that 30% of accounts were created be-\\nfore 2020, and less than 0.5% of accounts from YouTube\\nwere created between 2006 to 2010.\\nAffiliated Categories. Our observation showed that social\\nmedia profiles were often tagged with platform-specific cat-\\negories based on their relevance. We identified 288 distinct\\ncategories associated with 1,171 accounts. The top five cate-\\ngories include (i) Brand and Business (751), (ii) Entities (349),\\n(iii)Interests and Hobbies (322), (iv) Digital Assets & Crypto\\n(334), and (v) Events (219). Since categories like business, in-\\nterests, and assets naturally attract public engagement due\\nto their economic relevance, such accounts are likely to be\\nin high demand in marketplaces for purchase.\\nAccount Types. Social media accounts by default are un-\\nverified and lack restrictive settings such as protected or\\nprivate modes. We identified three account types across five\\nsocial media platforms: (i)business profiles marketed as enti-\\nties (193), (ii)verified accounts (669), and (iii)accounts with\\ncontrolled settings, including private (65) and protected (5)\\nmodes. This indicates that accounts for sale are predomi-\\nnantly unverified or standard profiles, with relatively fewer\\nbusiness or restricted accounts available.\\nAccount Creation. Account creation dates provide insight\\ninto the age of social media profiles. In Figure 4, we present\\nthe CDF of account creation dates across five social media\\nplatforms. Our analysis reveals that over 70% of accounts\\nwere created within the last 3.5 years, while less than 25%\\nwere created between 2005 and 2020. Among the platforms\\nanalyzed, TikTok profiles were created between 2017 and\\n2024, while X,Instagram , and Facebook accounts date back to\\n2010. Notably, less than 0.5% of YouTube accounts were cre-\\nated between 2006 and 2010. This suggests that the majorityof accounts advertised on these marketplaces are relatively\\nnew, with a smaller portion representing older, more estab-\\nlished profiles.\\nFollowers. Followers on social media platforms represent\\nindividuals subscribed to an account to receive notifications\\nand view its content in their news feeds. Our analysis across\\nfive social media platforms shows that the median number of\\nfollowers for accounts on sale exceeds 7,000, with the highest\\nfollower count surpassing 20 million. In Table 4, we present\\nthe minimum, median, and maximum follower counts for\\nthese accounts. This indicates that accounts marketed on\\nsuch marketplaces are often highly engaged and likely em-\\nploy engagement farming techniques to attract a substantial\\nnumber of followers.\\n6 SCAM POST ANALYSIS\\nWe perform a comprehensive evaluation to detect scam pat-\\nterns given the 205K collected posts of 11.4K social media\\naccounts (see Table 2). Our objective in analyzing these posts\\nis to understand how fraudsters attract victims by perform-\\ning various social engineering tricks. For this, we applied\\ntopic modeling techniques to group them into distinct clus-\\nters, and later performed a manual qualitative analysis of all\\nresulting clusters to identify the scam clusters. In total, we\\nidentified six clusters performing fraudulent activities via\\nposts.\\nTechnical Setup. Beginning with the collected posts, we\\nfocus on those based on English text, for which we rely on\\nthe CLD2 library [ 18], and remove stop words using the\\nBERTopic library [ 27]. Then, we extract embeddings for\\neach post using the all-mpnet-base-v2 sentence transformer\\nmodel [ 7,49]. Lastly, we use HDBSCAN [ 36] and UMAP [ 37]\\nfor clustering, followed by the KeyBERT model [ 26] to iden-\\ntify potential scam posts and refine topic representations\\nwithin each cluster. We then manually analyze the resulting\\nclusters, arranged by their size, to identify types of scam\\noffers, and provide details on security risks. We exclude clus-\\nters that do not contain scams from our study.\\nScam Findings. Starting with the dataset of 205K posts, we\\napplied our methodology outlined above to automatically\\ngroup the posts into 86 distinct clusters. From each cluster,\\nwe randomly selected and manually analyzed 25 sample posts\\nto assess whether the content within the cluster was related\\nto scams. As a result of this vetting process, we identified 16\\nclusters containing scam-related content, which we further\\ncategorized into six overarching scam types.\\nUsing this approach, we identified a total of 18.7K scam\\nposts across over 3.7K distinct scammer accounts from five\\nsocial media platforms: Facebook ,Instagram ,Tiktok ,X, and\\nYoutube . Table 5 provides a detailed breakdown of identi-\\nfied scam accounts and posts, and Table 6 presents a quick\\n9Table 5: Summary of scam accounts and scam posts\\nidentified across major social media platforms. No-\\ntably, YouTube had the highest number of scam ac-\\ncounts, while X led in scam-related posts, highlighting\\nsignificant variations in the scale of fraudulent activity\\nacross platforms.\\nSocial Media Scam Accounts Scam Posts\\nFacebook 512 3,838\\nInstagram 525 3,271\\nTiktok 461 3,034\\nX 610 6,988\\nYoutube 1,661 1,661\\nTotal 3,769 18,792\\nTable 6: Type and popularity of fraudulent offers across\\nscammer‚Äôs social media posts - This table presents six\\nscam categories identified through post clustering. Our\\nfindings reveal that scammers frequently exploit trend-\\ning topics and financial schemes, such as crypto/NFTs,\\nwhile traditional scams like phishing, product fraud,\\nadult content, and impersonation remain common.\\nCategory Accounts Posts\\nFinancial Scams 2,649 8,903\\n- Crypto Scams 2,352 8,218\\n- NFT and Giveaway Scams 163 389\\n- Financial Consulting 81 133\\n- Emotional Exploitation (Charity) 53 163\\nPhishing 933 2,293\\n- Through Popular Content/Challenges/Trends 725 1,749\\n- Through Chat Communication 208 544\\nProduct/Service Fraud 701 2,009\\n- Product Promotion Scams 296 739\\n- Fake Travel Deals 131 357\\n- Vehicle Sale/Rental Fraud 101 279\\n- Sports Betting and Merchandise Scams 129 451\\n- Fake Education-related Offers 44 183\\nAdult Content 244 466\\n- Provocative and Catphishing Lures 244 466\\nImpersonation 188 392\\n- Public Figures 53 133\\n- Fake Tech Support 135 259\\nEngagement Bait 2,300 4,597\\n- Like/Follow/Subscribe Requests 1,509 2,999\\n- Greetings and Motivational Phrases 791 1,598\\noverview of the scam categories. Below, we provide an in-\\ndepth analysis of the identified scam types.Financial Scams. Financial scams are one of the most\\npervasive forms of fraudulent activity on social media, char-\\nacterized by their focus on exploiting users‚Äô financial in-\\nterests or vulnerabilities. These scams are perpetrated by\\n2,649 accounts producing 8,903 posts. A major subcategory\\nis crypto scams, which involve promises of high returns\\non cryptocurrency investments, fake trading platforms, and\\nfraudulent initial coin offerings. These scams leverage the ris-\\ning popularity of digital assets to deceive users and account\\nfor 2,352 accounts and 8,218 posts. Similarly, NFT and give-\\naway scams capitalize on the emerging non-fungible token\\nmarket by promoting fake NFT projects or false giveaways,\\nengaging 163 accounts and 389 posts. Financial consulting\\nscams target users seeking financial advice, with scammers\\nimpersonating consultants to extract sensitive information\\nor money; this subcategory is responsible for 81 accounts\\nand 133 posts. Finally, emotional exploitation scams, such\\nas fake charity campaigns, manipulate users‚Äô goodwill by\\nsoliciting donations for fabricated causes, with 53 accounts\\nand 163 posts contributing to this deceitful practice.\\nEngagement Bait. Engagement bait scams exploit users‚Äô\\ndesire for connection and social media algorithms that re-\\nward interactions. These scams involve 2,300 accounts gener-\\nating 4,597 posts designed to maximize user engagement un-\\nder false pretenses. Like/follow/subscribe requests, the most\\ncommon type, are generated by 1,509 accounts through 2,999\\nposts. These requests often promise rewards or exclusive con-\\ntent in return for likes or follows but deliver nothing of value.\\nSimilarly, greetings and motivational phrases‚Äîposted by 791\\naccounts through 1,598 posts‚Äîcapitalize on users‚Äô emotional\\nresponses to generic but engaging content. While appearing\\nharmless, these tactics often serve as precursors to more\\ndeceptive practices by increasing scammers‚Äô visibility and\\nreach.\\nPhishing Scams. Phishing scams are highly deceptive\\nand aim to extract sensitive personal information such as lo-\\ngin credentials, financial data, or identification details. These\\nscams involve 933 accounts across 2,293 posts. One vari-\\nant, phishing through popular content, challenges, or trends,\\nmimics viral posts to lure users into clicking malicious links,\\nwith 725 accounts producing 1,749 posts. Another common\\nform, phishing through chat communication, involves scam-\\nmers directly messaging users while posing as trusted enti-\\nties, accounting for 208 accounts and 544 posts. These scams\\nexploit users‚Äô trust and curiosity, often leading to compro-\\nmised accounts or financial losses.\\nProduct/Service Fraud. Product and service fraud in-\\nvolves the false advertising of goods or services that do not\\nexist, luring users with appealing offers. This category com-\\nprises 701 accounts generating 2,009 posts. Service and prod-\\nuct promotion scams, executed by 296 accounts through 739\\nposts, mislead users with fake products, often using urgency\\n10to compel immediate purchases. Fake travel deals target vaca-\\ntioners with unrealistically cheap travel packages, involving\\n131 accounts and 357 posts. Vehicle sale/rental fraud, often\\nassociated with nonexistent cars or rentals, is perpetuated by\\n101 accounts through 279 posts. Additionally, sports betting\\nand merchandise scams, conducted by 129 accounts through\\n451 posts, exploit sports fans with promises of exclusive\\nmerchandise or fixed betting outcomes.\\nAdult Content Scams. Adult content scams exploit the\\nintimate nature of social media interactions to deceive users,\\noften involving provocative imagery or fabricated romantic\\nadvances. These scams are carried out by 244 accounts across\\n466 posts. A typical scheme involves catfishing, where scam-\\nmers pretend to be romantic interests to extract money, gifts,\\nor sensitive information from their targets. These scams prey\\non users‚Äô emotions and can escalate into extortion or identity\\ntheft.\\nImpersonation. Impersonation scams rely on mimicking\\ntrusted entities, such as public figures or technical support\\nservices, to deceive users. This category includes 188 ac-\\ncounts generating 392 posts. Public figure impersonation,\\ncarried out by 53 accounts through 133 posts, involves scam-\\nmers posing as celebrities or influencers to promote fake\\nproducts or investment schemes. Similarly, fake tech support\\nscams, conducted by 135 accounts through 259 posts, imper-\\nsonate legitimate support agents to trick users into granting\\nremote access to their devices or paying for unnecessary\\nservices. These scams exploit trust and authority to gain\\nvictims‚Äô compliance.\\n7 TRACKING AND NETWORK ANALYSIS\\nOur network analysis of visible profiles analyzes how ac-\\ncount formations are linked to various other social media\\nprofiles enabling us to understand the scale of the operations.\\nWe provide network evaluation below.\\nCluster Formation. To identify cluster formations, we se-\\nlected profile metadata attributes such as names, descriptions,\\nemail addresses, websites, and phone numbers. Using these\\nattributes, we automated the clustering process to group ac-\\ncounts from each social media platform into buckets contain-\\ning at least two or more unique UserIDs. Accounts without\\nmatching attributes across multiple profiles were categorized\\nas singletons. After the automated clustering, for each cluster\\nof the cluster, we perform a manual inspection to validate\\nthe legitimacy of the groupings based on these attributes.\\nOur findings are summarized below.\\nFindings. Our findings indicate that fewer than 5% of ac-\\ncounts were part of coordinated clustered campaigns. The\\nremaining 95% of accounts showed no significant correlation\\nwith other social media profiles based on their visible profile\\nmetadata. In Table 7, we detail the clustering results for eachFigure 5: Three examples of the profile descriptions of\\nadvertised social media accounts.\\nsocial media platform, including cluster attributes, cluster\\nsizes (minimum, median, and maximum), the total number\\nof clusters identified, number of cluster accounts, singleton,\\nand overall cluster accounts percentage from the dataset of\\neach social media profiles from their respective platforms.\\nAcross the five social media platforms, a total of 203 clusters\\nwere identified, with the highest number of cluster compos-\\nite from YouTube , and the lowest number of clusters from\\nTikTok . Our observation showed that one of the clusters from\\nInstagram consists of 46 social media accounts. The median\\nand minimum cluster size across all platforms was 2, while\\nthe total median number of clusters identified across the five\\nsocial media platforms was 35, containing a median of 89\\naccounts per cluster.\\nWe provide three illustrative examples of clustering based\\non the profile descriptions of advertised social media ac-\\ncounts in Figure 5. The first example illustrates the seller\\nharvesting 1K accounts each of those accounts having 100K\\nX (Twitter) followers and asking users to communicate via\\nan external communication channel (Telegram), indicating a\\ncovert and significant scale of operations designed to engage\\nvictims privately. In the second example below, an account\\nadvertises free giveaways related to NFTs, which are used as\\nbait to lure users into scams under the guise of community\\nengagement. The third example shows an account targeting\\nbusinesses or entities, offering high-quality profiles to attract\\nbuyers in the guise of established business or promotional\\npurposes. Thus, these show a diversity of operations, ranging\\nfrom large-scale scams to targeted strategies for monetiz-\\ning social media accounts, and tactics employed by sellers\\nbeyond the originated marketplaces.\\n11Table 7: Network Cluster Detail - In this table we provide the network analysis of social media that contain shared\\nattributes such as name, description, biography, email, phone,e or website. Based on their profile data analysis, we\\ncluster the accounts by these attributes and present the clustering evaluation. Our results highlight that a single\\ncluster from Instagram consists of as many as 46 social media accounts linked, whereas the smallest number of\\nclusters consists of TikTok .\\nSocial Media Cluster Attributes Min Max Median Clusters Cluster Accts. Singleton Overall Cluster Acts.\\nTikTok Description 2 22 4 3 26 1,674 1.5%\\nYouTube Name 2 3 2 97 195 6,076 3.1%\\nInstagram Biography 2 46 2 31 152 1,871 7.5%\\nFacebook Email/Phone/Website 2 4 2 37 81 568 12.48%\\nX Name/Description 2 7 2 35 89 725 19.93%\\nAll - 2 46 2 203 543 10,914 4.7%\\n8 EFFICACY AND ABUSE CONTROL\\nIn this section, we analyze social media accounts that were\\nactioned upon by platforms and evaluate the efficacy of block-\\ning such accounts.\\nDetection Overview. We analyzed the active status of 11,457\\nsocial media profiles using API responses from the respec-\\ntive platforms. These responses provided explanations for\\naccount actions, such as accounts on Xbeing labeled as ei-\\nther Forbidden orNot Found . The Forbidden status indicates\\nthat the account was banned due to policy violations, while\\nNot Found suggests that the account owner either changed\\ntheir UserID or voluntarily deleted the account. On Insta-\\ngram , the status appears as Page Not Found , while TikTok ,\\nYouTube , and Facebook display messages like Profile/channel\\ndoes not exist . We suspect that accounts labeled as Not Found\\norDoes not exist are likely associated with scammer or abuse\\nprofiles. Anecdotally, accounts either go offline intentionally\\nafter successfully executing scams or are taken down by the\\nplatform for violating policies during the operation of scams.\\nWe classify both scenarios under the efficacy of social me-\\ndia platforms in addressing and deactivating such accounts\\nconservatively.\\nFindings. Out of the 11,457 accounts analyzed, the overall\\nefficacy of social media platforms in blocking these accounts\\nwas 19.71% (2,259 accounts). A detailed breakdown of in-\\nactive accounts and their percentages across platforms is\\nprovided in Table 8. Among the five platforms, TikTok and\\nInstagram demonstrated the highest detection efficacy at\\n48%, whereas YouTube andFacebook showed the lowest effi-\\ncacy at just 5%. Our analysis revealed that blocked accounts\\nfrequently featured names associated with trends like crypto ,\\nNFTs ,beauty ,luxury ,animals , or miscellaneous word com-\\nbinations. This suggests that detection efforts are largely\\nfocused on accounts leveraging popular or trending topics.\\nAlthough TikTok andInstagram exhibited relatively higherTable 8: Detection Efficacy - In this table we present,\\nthe blocking effectiveness of social media platforms\\nthat were advertised for sale in open marketplaces.\\nOur observations showed that TikTok andInstagram\\nhad overall 50% of the blocking while X,Facebook , and\\nYouTube blocking lower than 20% of the advertised\\naccounts.\\nSocial Visible Inactive Blocking\\nMedia Accounts Accounts Efficacy\\nYouTube 6,271 315 5.02\\nFacebook 649 37 5.70\\nX 814 152 18.67\\nInstagram 2,023 939 46.41\\nTikTok 1,700 816 48\\nAll 11,457 2,259 19.71\\nblocking efficacy, given more than 70% of overall visible ac-\\ncounts were created within the last 3.5 years‚Äîthis period is\\nsubstantial for scammers to cause significant harm or abuse\\nto online users and platforms. Therefore, while platforms\\nalready shown taking proactive detection efforts on these\\naccounts show some promise, the overall efficacy still high-\\nlights a concerning gap in addressing and preventing such\\nthreats effectively.\\n9 RECOMMENDATIONS\\nOur study shows that accounts that are advertised for sell-\\ning at these marketplaces undergo preemptive tailoring for\\nfuture fraud and abuses. Thus the ecosystem of buying and\\nselling social media profiles fosters cybercriminals to operate\\nat scale, making it easy to obtain accounts to launch various\\ncybercrime activities. Throughout these processes, various\\nplatforms (e.g., social media, payment vendors) and users are\\n12exploited. With that, we would like to provide recommenda-\\ntions for the three parties below.\\nSocial Media Platforms. We recommend social media plat-\\nforms apply stricter and multi-level authenticity that dis-\\ncourages trading of accounts. This includes but is not limited\\nto(i)monitoring referral headers that are directed from mar-\\nketplaces that buy and sell social media profiles, and (ii)\\nperforming behavioral monitoring of accounts such as rapid\\nfollower growth, change of location, or IP addresses that\\nmay indicate a likelihood of engagement or account farm-\\ning. Additionally, we encourage social media platforms to\\nrun public awareness campaigns highlighting the risks of\\naccount trading, which may involve compromised or illicitly\\nobtained accounts, and to communicate the consequences of\\nplatform penalties.\\nPayment and Transaction Monitor. We recommend that\\npayment services such as PayPal ,cryptocurrency exchanges ,\\nwallet providers , and similar vendors implement robust fraud\\ndetection systems to flag transactions linked to the trading of\\nsocial media accounts. For example, during account verifica-\\ntion or onboarding for payment services, a thorough analysis\\nshould be conducted to determine the intended use of the ser-\\nvice. Similarly, payment platforms should monitor and flag\\naddresses associated with marketplaces facilitating account\\nsales. Establishing strict paywall transaction monitoring and\\nreporting mechanisms would enhance the detection and pre-\\nvention of fraudulent activities in this context.\\nLaw Enforcement and Policy Makers. Currently, the trad-\\ning of social media profiles operates in a grey area. While\\nsocial media platforms view such activities as violations of\\ntheir terms and conditions, such violations result in account\\nbans, which are not explicitly illegal under current laws. This\\nlack of regulation creates a gap in both oversight of social\\nmedia account trading and consumer protection. We rec-\\nommend that law enforcement agencies and policymakers\\nexplicitly ban the sale of social media profiles by incorpo-\\nrating clear prohibitions in legal frameworks. This is partic-\\nularly critical as purchased accounts are often misused for\\nmalicious purposes. Collaborative efforts with social media\\ncompanies and DNS sinkholes should be enforced to identify\\nand take down domains associated with marketplaces facili-\\ntating account sales. Additionally, we propose establishing\\nrobust consumer protection measures. This should include\\npenalties for individuals or organizations found engaging in\\nthe buying or selling of social media accounts, especially in\\ncases where such practices are likely in the future, use for\\nexploit or defraud others.\\n10 LESSONS LEARNED\\nIn this section, we summarize the main findings of our study\\nand discuss their wider implications.The Hidden Scale and Economics of Account Sales. This\\npaper provides the first large-scale empirical analysis of 38K\\nsocial media accounts listed for sale, revealing a total market\\nvalue exceeding $64M, with median prices differing across\\nplatforms (e.g., Instagram: $298, TikTok: $755), providing\\nkey insights into the economic drivers of this illicit market.\\nOld Accounts, New Tricks: Creation Patterns as Fraud\\nTools. We provide a novel timeline of account creation, re-\\nvealing that 30% of sold accounts were created pre-2020,\\nleveraging their longevity to evade detection. Conversely,\\naccounts created in the past 3.5 years still dominate scam\\nactivity (‚àº70%), suggesting that scammers quickly adapt to\\nplatform changes and user trends.\\nPlaybooks of Deceit: Fraud Strategies in Marketplaces.\\nThrough analysis of both public and underground market-\\nplaces, we identify coordinated fraud strategies, including\\nhigh textual similarity (up to 100%) across scam listings, in-\\ndicating shared playbooks among fraud networks.\\nThe Anatomy of Scams: Types and Tactics. We categorize\\n18.7K scam posts into six distinct types, including financial\\nscams, phishing, and impersonation. This clustering provides\\nactionable insights into how fraudsters operate across plat-\\nforms. Fraudulent accounts target specific categories (e.g.,\\ncrypto, gaming, luxury) with tailored narratives to exploit\\nniche communities, demonstrating high levels of operational\\nprecision.\\nEngagement Metrics Boost Fraudulent Credibility. By\\nanalyzing engagement metrics from 11,457 accounts, we\\ndemonstrate how these metrics are exploited to enhance the\\nperceived legitimacy of fraudulent accounts. We observed\\nthat accounts are pre-configured with characteristics such as\\nhigh follower counts and strategic descriptions to enhance\\ntheir appeal before sale.\\nProfiling Seller Activity Across Platforms. We identify\\npatterns in seller activity, including cross-marketplace op-\\nerations, and show how sellers replenish listings to align\\nwith supply-demand dynamics. Cross-platform activities, in-\\ncluding identical seller profiles on the dark web and public\\nmarketplaces, highlight a merging of traditionally separate\\nfraud ecosystems.\\nSocial Media Detection Gaps. Our results show that, de-\\nspite platform efforts, only 19.7% of identified fraudulent\\naccounts were actioned upon by social media platforms, un-\\nderscoring the critical need for enhanced detection method-\\nologies.\\n11 CONCLUDING REMARKS\\nIn conclusion, the rise of online marketplaces for trading\\nsocial media accounts presents significant risks to platform\\nintegrity and user safety. While not inherently illegal, these\\n13transactions violate the policies of platforms like X, Insta-\\ngram, Facebook, TikTok, and YouTube, and fuel fraudulent\\nactivities. Our analysis, conducted from February to June\\n2024, identified 38,253 accounts advertised for sale across\\n11 marketplaces and 211 distinct categories, representing a\\ntotal value exceeding $64 million, with a median price of\\n$120 per account. We examined 11,457 visible advertised\\naccounts and collected metadata along with over 200K asso-\\nciated posts. This data revealed fraudulent practices such as\\nbot farming, account harvesting for future scams, and decep-\\ntive engagement manipulation. These fraudulent accounts\\noften impersonate legitimate profiles, leveraging social en-\\ngineering tactics to exploit unsuspecting users. Platforms\\ncurrently face challenges in detecting and mitigating these\\nthreats, leaving users vulnerable to attacks. To address these\\nissues, we provided detailed disclosures to the respective\\nplatforms and proposed practical recommendations includ-\\ning indicators to identify and track fraudulent accounts given\\nthe scam patterns and tactics we discovered in our research.\\nACKNOWLEDGEMENTS\\nThe authors gratefully acknowledge funding from the Ger-\\nman Federal Ministry of Education and Research (BMBF\\ngrant 16KIS1900 ‚ÄúUbiTrans‚Äù). The authors used GPT-4 and\\nGrammarly to revise the text of most sections to correct\\ntypos, grammatical errors, and awkward phrasing.\\nREFERENCES\\n[1]Ares market. https://sn2sfdqay6cxztroslaxa36covrhoowe6a5xug6wlm\\n6ek7nmeiujgvad.link/.\\n[2]Black pyramid. http://blackpyoc3gbnrlvxqvvytd3kxqj7pd226i2gvfyhy\\nsj24ne2snkmnyd.onion/.\\n[3]Dark matter. http://darkmmro6j5xekpe7jje74maidkkkkw265nngjqxrv\\n4ik7v3aiwdbtad.onion/.\\n[4]Kerberos market. http://kerberqtg7xpofsc3w47nvjd52sys6hqdejk3h7f\\nz6kbqhyqrds3xgqd.onion/.\\n[5]Mgm market. https://mgmsanjqxo4svh35yqkxxe5r54z2xc5tjf6r3ichxd\\n3m2rwcgabf44ad.xyz/.\\n[6]Nexus market. http://nexusabcdkq4pdlubs6wk6ad7pobuupzoomoxi\\n6p7l32ci4vjtb2z7yd.onion/.\\n[7]Sentence Transformer all-mpnet-base-v2. https://huggingface.co/sen\\ntence-transformers/all-mpnet-base-v2.\\n[8]Torzon market. http://sglgj2fytneccvyn6n4u3pacj4zhdhscfoptnhxxes\\n3uvljmontru2yd.onion/.\\n[9] We the north. http://hn2paw7zaahbikbejiv6h22zwtijlam65y2c77xj2y\\npbilm2xs4bnbid.onion/.\\n[10] Abdelnabi, S., and Fritz, M. {Fact-Saboteurs}: A taxonomy of\\nevidence manipulation attacks against {Fact-Verification}systems. In\\nUSENIX Security (2023).\\n[11] Acharya, B., Lazzaro, D., L√≥pez-Morales, E., Oest, A., Saad, M.,\\nCin√†, A. E., Sch√∂nherr, L., and Holz, T. The imitation game: Ex-\\nploring brand impersonation attacks on social media platforms. In\\nUSENIX Security (2024).\\n[12] Acharya, B., Saad, M., Cin√†, A. E., Sch√∂nherr, L., Nguyen, H. D.,\\nOest, A., Vadrevu, P., and Holz, T. Conning the crypto conman:\\nEnd-to-end analysis of cryptocurrency-based technical support scams.InIEEE Symposium on Security and Privacy (IEEE S&P) (2024).\\n[13] Aggarwal, A., and Kumaraguru, P. What they do in shadows:\\nTwitter underground follower market. In Annual Conference on Privacy,\\nSecurity and Trust (PST) (2015).\\n[14] Apify . Apify instagram scraper api. https://apify.com/apify/instagra\\nm-scraper, 2024.\\n[15] Apify . Facebook scraper. https://apify.com/streamers/facebook-\\nscraper, 2024.\\n[16] Apify . Youtube scraper. https://apify.com/streamers/youtube-scraper,\\n2024.\\n[17] Bitaab, M., Cho, H., Oest, A., Lyu, Z., Wang, W., Abraham, J., Wang,\\nR., Bao, T., Shoshitaishvili, Y., and Doup√©, A. Beyond phish: Toward\\ndetecting fraudulent e-commerce websites at scale. In 2023 IEEE\\nSymposium on Security and Privacy (IEEE S&P) (2023).\\n[18] Bowyer, G. CLD2-CFFI ‚Äì Python (CFFI) Bindings for Compact Lan-\\nguage Detector 2. https://github.com/GregBowyer/cld2-cff, 2016.\\n[19] Cao, Q., Yang, X., Yu, J., and Palow, C. Uncovering large groups of\\nactive malicious accounts in online social networks. In ACM SIGSAC\\nConference on Computer and Communications Security (CCS) (2014).\\n[20] Chhabra, S., Aggarwal, A., Benevenuto, F., and Kumaraguru, P.\\nPhi.sh$ocial: the phishing landscape through short urls. In Electronic\\nMessaging, Anti-Abuse and Spam Conference (EMASC) (2011).\\n[21] Cresci, S., Di Pietro, R., Petrocchi, M., Spognardi, A., and Tesconi,\\nM.Fame for sale: Efficient detection of fake twitter followers. Decision\\nSupport Systems (2015).\\n[22] DeKoven, L. F., Pottinger, T., Savage, S., Voelker, G. M., and Leon-\\ntiadis, N. Following their footsteps: Characterizing account automa-\\ntion abuse and defenses. In ACM SIGCOMM Conference on Internet\\nMeasurement Conference (IMC) (2018).\\n[23] Egele, M., Stringhini, G., Kruegel, C., and Vigna, G. Compa:\\nDetecting compromised accounts on social networks. In Network and\\nDistributed System Security (NDSS) (2013).\\n[24] Gao, H., Hu, J., Wilson, C., Li, Z., Chen, Y., and Zhao, B. Y. Detect-\\ning and characterizing social spam campaigns. In ACM SIGCOMM\\nconference on Internet measurement (IMC) (2010).\\n[25] Grier, C., Thomas, K., Paxson, V., and Zhang, M. @ spam: the\\nunderground on 140 characters or less. In ACM conference on Computer\\nand communications security (CCS) (2010).\\n[26] Grootendorst, M. KeyBERT: Minimal Keyword Extraction with\\nBERT. https://doi.org/10.5281/zenodo.4461265, 2020.\\n[27] Grootendorst, M. BERTopic: Neural topic modeling with a class-\\nbased TF-IDF procedure. arXiv arXiv:2203.05794 (2022).\\n[28] IDRC . Social media scams are on the rise as more people use the\\nplatforms to connect. https://www.idtheftcenter.org/post/social-\\nmedia-scams-are-on-the-rise-as-more-people-use-the-platforms-\\nto-connect/, 2020.\\n[29] Jain, M., Mowar, P., Goel, R., and Vishwakarma, D. K. Clickbait in\\nsocial media: detection and analysis of the bait. In Information Sciences\\nand Systems (CISS) (2021).\\n[30] Jr., T. H. Social media scams: Stunning statistics and tips to protect\\nyourself. https://www.cnbc.com/2023/10/12/americans-lose-billions-\\nto-social-media-scams-red-flags-to-spot.html, 2023.\\n[31] Khalil, A., Hajjdiab, H., and Al-Qirim, N. Detecting fake followers\\nin twitter: A machine learning approach. International Journal of\\nMachine Learning and Computing (2017).\\n[32] Li, Z., and Liao, X. Understanding and analyzing appraisal systems\\nin the underground marketplaces. In Network and Distributed System\\nSecurity (NDSS) (2024).\\n[33] Lin, Z., Cui, J., Liao, X., and Wang, X. Malla: Demystifying real-\\nworld large language model integrated malicious services. arXiv\\narXiv:2401.03315 (2024).\\n14[34] Lykousas, N., Koutsokostas, V., Casino, F., and Patsakis, C. The\\ncynicism of modern cybercrime: Automating the analysis of surface\\nweb marketplaces. In IEEE International Conference on Service-Oriented\\nSystem Engineering (SOSE) (2023).\\n[35] Maras, M.-H., and Ives, E. R. Deconstructing a form of hybrid invest-\\nment fraud: Examining ‚Äòpig butchering‚Äôin the united states. Journal\\nof Economic Criminology (2024).\\n[36] McInnes, L., Healy, J., and Astels, S. HDBSCAN: Hierarchical\\nDensity Based Clustering. Journal of Open Source Software (2017).\\n[37] McInnes, L., Healy, J., and Melville, J. UMAP: Uniform Mani-\\nfold Approximation and Projection for Dimension Reduction. arXiv\\narXiv:1802.03426 (2018).\\n[38] Mehrotra, A., Sarreddy, M., and Singh, S. International confer-\\nence on contemporary computing and informatics (ic3i). In 2016 2nd\\nInternational Conference on Contemporary Computing and Informatics\\n(IC3I) (2016).\\n[39] Milevski, D. Apify telegram scraper api. https://apify.com/danielmi\\nlevski9/telegram-channel-scraper, 2024.\\n[40] Milevski, D. Telemetrio telegram scraper api. https://telemetr.io/,\\n2024.\\n[41] Milmo, D. Sharp rise in blackmail of children asked to share explicit\\nimages. https://www.theguardian.com/society/2023/may/12/sharp-\\nrise-in-blackmail-of-children-asked-to-share-explicit-images, 2023.\\n[42] Mirtaheri, M., Abu-El-Haija, S., Morstatter, F., Ver Steeg, G.,\\nand Galstyan, A. Identifying and analyzing cryptocurrency manip-\\nulations in social media. IEEE Transactions on Computational Social\\nSystems (IEEE TCSS) (2021).\\n[43] Motoyama, M., McCoy, D., Levchenko, K., Savage, S., and Voelker,\\nG. M. An analysis of underground forums. In ACM SIGCOMM Confer-\\nence on Internet Measurement Conference (IMC) (2011).\\n[44] News, F. Ftc data shows consumers report losing $2.7 billion to social\\nmedia scams since 2021. https://www.ftc.gov/news-events/news/pr\\ness-releases/2023/10/ftc-data-shows-consumers-report-losing-27-\\nbillion-social-media-scams-2021, 2023.\\n[45] News, F. J‚Äôfinfluencers‚Äô charged for promoting unauthorised trading\\nscheme. https://www.fca.org.uk/news/press-releases/finfluencers-\\ncharged-promoting-unauthorised-trading-scheme, 2024.\\n[46] News, W. P. The rise of sextortion and responses to a growing crime.\\nhttps://www.weprotect.org/issue/sextortion/.\\n[47] Popovici, M. Job scams report ‚Äì 2,670 social media posts reveal\\nscammers top tactics. https://heimdalsecurity.com/blog/job-scam-\\nsocial-media-study/, 2024.\\n[48] Puig, A. Fake shipping notification emails and text messages: What\\nyou need to know this holiday season. https://consumer.ftc.gov/con\\nsumer-alerts/2023/12/fake-shipping-notification-emails-and-text-\\nmessages-what-you-need-know-holiday-season, 2023.\\n[49] Reimers, N., and Gurevych, I. Sentence-BERT: Sentence Embed-\\ndings using Siamese BERT-Networks. In Empirical Methods in Natural\\nLanguage Processing (EMNLP) (2019).\\n[50] Ruan, X., Wu, Z., Wang, H., and Jajodia, S. Profiling online social\\nbehaviors for compromised account detection. IEEE Transactions on\\nInformation Forensics and Security (ITIFS) (2016).\\n[51] Sebastain, N. Social media scams: Stunning statistics and tips to\\nprotect yourself. https://www.goodfirms.co/resources/social-media-\\nscams-statistics-and-tips-for-protection, 2024.\\n[52] Stivala, G., and Pellegrino, G. Deceptive previews: A study of the\\nlink preview trustworthiness in social platforms.\\n[53] Stringhini, G., Egele, M., Kruegel, C., and Vigna, G. Poultry\\nmarkets: on the underground economy of twitter followers. ACM\\nSIGCOMM Computer Communication Review (2012).\\n[54] Stringhini, G., Kruegel, C., and Vigna, G. Detecting Spammers on\\nSocial Networks. In Annual Computer Security Applications Conference(ACSAC) (2010).\\n[55] Stringhini, G., Wang, G., Egele, M., Kruegel, C., Vigna, G., Zheng,\\nH., and Zhao, B. Y. Follow the green: growth and dynamics in twitter\\nfollower markets. In ACM SIGCOMM Conference on Internet Measure-\\nment Conference (IMC) (2013).\\n[56] SysSec . Buy and Sale of Social Media Code and Data. https://github.c\\nom/CISPA-SysSec/social_media_buy_and_sale, 2024.\\n[57] Thomas, K., McCoy, D., Grier, C., Kolcz, A., and Paxson, V.\\n{Trafficking}fraudulent accounts: The role of the underground mar-\\nket in twitter spam and abuse. In USENIX Security (2013).\\n[58] Tr√•ng, D., Johansson, F., and Rosell, M. Evaluating algorithms for\\ndetection of compromised social media user accounts. In 2015 Second\\nEuropean Network Intelligence Conference (2015), European Network\\nIntelligence Conference (ENIC).\\n[59] Twitter . User detail twitter api. https://developer.twitter.com/en/d\\nocs/twitter-api/v1/accounts-and-users/follow-search-get-users/api-\\nreference/get-users-lookup, 2024.\\n[60] Twitter . User timelines twitter api. https://developer.twitter.com/en\\n/docs/twitter-api/tweets/timelines/introduction, 2024.\\n[61] Viswanath, B., Bashir, M. A., Crovella, M., Guha, S., Gummadi,\\nK. P., Krishnamurthy, B., and Mislove, A. Towards detecting anoma-\\nlous user behavior in online social networks. In Usenix Security (2014).\\n[62] Williams, R. The growth of fake products on social media. https:\\n//www.redpoints.com/blog/the-growth-of-fake-products-on-social-\\nmedia/, 2024.\\n[63] Xiao, C., Freeman, D. M., and Hwa, T. Detecting clusters of fake\\naccounts in online social networks. In ACM Workshop on Artificial\\nIntelligence and Security (AIS) (2015).\\n[64] Xu, T., Goossen, G., Cevahir, H. K., Khodeir, S., Jin, Y., Li, F., Shan,\\nS., Patel, S., Freeman, D., and Pearce, P. Deep entity classification:\\nAbusive account detection for online social networks. In USENIX\\nSecurity (2021).\\nA PUBLIC MARKETPLACES PAYMENT\\nMETHODS ADDITIONAL DETAILS\\nIn this section, we analyze the supported payment methods\\nby the marketplaces and their security implications.\\nA.1 Payment Method Extraction\\nTo identify the payment methods supported by each market-\\nplace, we conducted a comprehensive manual analysis. For\\neach marketplace, we visited its publicly available website\\nand carefully navigated through relevant sections such as\\npayment pages, FAQs, user guides, or checkout interfaces\\nfrom multiple vantage points, as certain payment methods\\nmight only be visible or available to users accessing the plat-\\nform from specific regions. This ensured that we gathered the\\nmost accurate and up-to-date information on payment meth-\\nods without relying solely on indirect sources like Google\\nsearch. We recorded all payment methods explicitly listed or\\nimplied by the marketplace, such as PayPal, cryptocurrencies,\\nand alternative methods like WeChat Pay and Skrill.\\nWe noted whether the payment methods were visible with-\\nout requiring user interaction (e.g., creating an account or\\ninitiating a purchase). If details were not immediately visible,\\nadditional steps such as creating an account were undertaken\\n15Table 9: Overview of trading channels identified. The table marks all the trading channels monitored in our study,\\nwith others not containing account handles publicly or being infeasible to be tracked due to crawling challenges\\nsuch as CAPTCHAs, complex user interactions, and analysis prerequisites like account credentials.\\n/g‚ôÄbeExchange /usrAccounts\\nCategory Channel Type Source Selling Handles Monitored\\nPublic accs-market.com Marketplace Google Search\\nfameswap.com Marketplace Google Search\\nwww.z2u.com Marketplace Google Search\\nfameseller.com Marketplace Google Search\\ninsta-sale.comlistings/ Marketplace Google Search\\naccsmarket.com Shop Google Search\\nbuysocia.com Shop Google Search\\nmid-man.com Shop Google Search\\nsocialtradia.com Shop Google Search\\nswapsocials.com Shop Google Search\\nwww.surgegram.com Shop Google Search\\nwww.toofame.com Shop Google Search\\ncracked.io Marketplace [34]\\nhackforums.net BlackHat Forum Google Search\\nswapd.co Marketplace Google Search\\naccszone.com Shop Public BH Forum\\nagedprofiles.com Shop Public BH Forum\\nbulkacc.com Shop Public BH Forum\\ndigitalchaining.mysellix.io Shop Public BH Forum\\ndiscord.gg/PMJCYxCcCu Shop Public BH Forum\\nnwarlordyt.sellpass.io Shop Public BH Forum\\nfamousinfluencer.com Shop Public BH Forum\\nnloaccs.com Shop Public BH Forum\\nwww.smmzone24.com Shop Public BH Forum\\nacccluster.com Shop Google Search\\naccsmaster.com Shop Google Search\\nbuyaccs.com Shop [57]\\ngetbulkaccounts.com Shop [57]\\n(bulkye.com) Shop [57]\\nquickaccounts.bigcartel.com Shop [57]\\ntwiends.com BlackHat Forum [55]\\nleakzone.net / BlackHat Forum Google Search\\nmagicsmm.com Shop Public BH Forum\\npaneliniz.net Shop Public BH Forum\\nsmmorigins.com Shop Public BH Forum\\nsmmtake.com Shop Public BH Forum\\nbigfollow.net Shop [55]\\nintertwitter.com Shop [55]\\nseguidores.com.br Shop Redirect from bigfollow\\nscrowise.com Shop Google Search\\nUnderground Dark Matter Marketplace Onion Directory\\nNexus Market Marketplace Onion Directory\\nTorzon Market Marketplace Onion Directory\\nBlack Pyramid Marketplace Onion Directory\\nKerberos Marketplace [33]\\nWeTherth Marketplace [33]\\nMGM Grand Marketplace [33]\\nARES market Marketplace Onion Directory\\nSoza Marketplace Onion Directory\\nSuperMarket Marketplace Onion Directory\\nQuantum Market Marketplace Onion Directory\\nQuest Market Marketplace Onion Directory\\nIncognito Marketplace Onion Directory\\nAlias Market Marketplace Onion Directory\\nArchetyp Marketplace Onion Directory\\nCity Market Marketplace Onion Directory\\nElysium Marketplace Onion Directory\\nFish Market Marketplace Onion Directory\\nPegasus Market Marketplace Onion Directory\\nAbacus Marketplace [33]\\nContact Skyisthelimitservice@gmail.com Email Public BH Forum\\nt.me/BusinessAts Telegram Public BH Forum\\nt.me/sheriff_x Telegram Public BH Forum\\nt.me/igexpertbhw Telegram Public BH Forum\\nt.me/lulpola Telegram Public BH Forum\\nt.me/prudentagency11 Telegram Public BH Forum\\nt.me/gunnupgrades Telegram Public BH Forum\\n+16193762832 Whatsapp Public BH Forum\\n@gunnupg Discord Public BH Forum\\n@MaxRuslan369 Unknown Public BH Forum16when necessary. To ensure the accuracy of the collected data,\\nwe cross-verified each marketplace‚Äôs payment methods with\\nmultiple pages on the marketplace. For platforms with un-\\nclear or incomplete information, we performed test inter-\\nactions, such as attempting to initiate a test transaction, to\\nconfirm the availability of specific payment methods.\\nA.2 Security Implications\\nThe analysis of supported payment methods across market-\\nplaces reveals a significant variation in availability, reflecting\\ndiffering priorities in terms of accessibility, user convenience,\\nand security. Table 3 presents the supported payment meth-\\nods across different marketplaces. Overall, marketplaces that\\nprioritize transparent payment methods and adopt systems\\nwith strong buyer protection, such as PayPal and Skrill, pro-\\nvide a safer environment for users. Conversely, reliance on\\ncryptocurrencies or undisclosed payment options increases\\nrisks of fraud and dispute resolution challenges.\\nRisk of Irreversible Payments. We observed a wide sup-\\nport for Bitcoin (BTC), Ethereum (ETH), and other cryptocur-\\nrencies across marketplaces. While cryptocurrencies enable\\nanonymous transactions, they introduce higher risks due to\\ntheir irreversible nature and the potential for fraud or illicit\\nactivities without buyer protection mechanisms.Buyer Protection and Chargebacks. Digital wallets such\\nas PayPal and Skrill can reduce the exposure of bank card\\ndetails, and offer users strong buyer protection, including\\nrefunds and chargebacks. However, these payment methods\\nare adopted only by two marketplaces (Z2U and FameSeller).\\nRegional Payment Methods and Vouchers. NeoSurf and\\nPayssion, supported by select marketplaces like Z2U, cater\\nto regional or prepaid needs, providing alternatives to bank-\\nlinked systems. These methods enhance user privacy by not\\nlinking transactions to bank accounts or personal informa-\\ntion but offer limited recourse in disputes or fraud cases.\\nEscrow-Like Systems. Trustap and Payer, available on Mid-\\nMan and TooFame, enhance security by holding funds in\\nescrow until predefined conditions are met, reducing fraud\\nrisks for high-value or deferred-delivery transactions. How-\\never, their effectiveness depends on the trustworthiness and\\nterms of the escrow provider.\\nTransparency of Payment Methods. For marketplaces\\nsuch as Accsmarket, FameSwap, and TooFame, payment\\nmethods were unknown or not publicly disclosed, increas-\\ning the likelihood of users interacting with unprotected or\\ninsecure systems.\\n17',\n",
       " 'ScamChatBot: An End-to-End Analysis of Fake Account Recovery\\non Social Media via Chatbots\\nBhupendra Acharya¬ß\\nCISPADominik Sautter¬ß\\nSaarland UniversityMuhammad Saad\\nX (formerly Twitter)Thorsten Holz\\nCISPA\\nAbstract\\nSocial media platforms have become the hubs for various\\nuser interactions covering a wide range of needs, including\\ntechnical support and services related to brands, products, or\\nuser accounts. Unfortunately, there has been a recent surge in\\nscammers impersonating official services and providing fake\\ntechnical support to users through these platforms. In this\\nstudy, we focus on scammers engaging in such fake technical\\nsupport to target users who are having problems recovering\\ntheir accounts. More specifically, we focus on users encoun-\\ntering access problems with social media profiles (e.g., on\\nplatforms such as Facebook, Instagram, Gmail, and X) and\\ncryptocurrency wallets. The main contribution of our work is\\nthe development of an automated system that interacts with\\nscammers via a chatbot that mimics different personas. By\\ninitiating decoy interactions (e.g., through deceptive tweets),\\nwe have enticed scammers to interact with our system so that\\nwe can analyze their modus operandi. Our results show that\\nscammers employ many social media profiles asking users\\nto contact them via a few communication channels. Using a\\nlarge language model (LLM), our chatbot had conversations\\nwith 450 scammers and provided valuable insights into their\\ntactics and, most importantly, their payment profiles. This au-\\ntomated approach highlights how scammers use a variety of\\nstrategies, including role-playing, to trick victims into disclos-\\ning personal or financial information. With this study, we lay\\nthe foundation for using automated chat-based interactions\\nwith scammers to detect and study fraudulent activities at\\nscale in an automated way.\\n1 Introduction\\nIn recent years, social media platforms have emerged as pop-\\nular communication instruments for information exchange,\\nopinion sharing, news gathering, and advertisement. Realiz-\\ning the potential of social media becoming a one-stop shop\\nthat caters to all such needs of its users, popular brands have\\n¬ßBoth authors contributed equally to this research.established their portfolios to engage with their customers and\\noffer useful services such as technical support, dispute reso-\\nlution, and discounts. As such, social media platforms have\\ninvariably paved the way for convenient interactions between\\nbusinesses and customers for service requests and resolutions.\\nA popular manifestation of this interaction is online technical\\nsupport, where customers report issues about accessing their\\nsocial media accounts or cryptocurrency wallets and expect a\\nresolution service from their business counterparts. Users typ-\\nically send these requests to the official profiles of the relevant\\nbusiness by making public posts and expecting a response\\nfrom their customer service. However, there is an inherent risk\\nin publicly posting such requests on social media, exposing\\nusers to scams [1, 2]. Any user can impersonate an official\\nbusiness representative and offer fake technical support to the\\ncustomer. In fact, this form of social engineering attack has\\nsignificantly increased in recent years, and it is colloquially\\ncalled Technical Support Scam [3‚Äì7].\\nThis type of support scams can be launched under any pre-\\ntext as long as a user is seeking online support and a scammer\\noffering help by falsely claiming to be an expert. Across all\\nscam typologies, the end goal is to gain the victim‚Äôs trust and\\nlure them into making a payment or surrendering sensitive\\ninformation that can be monetized. Although technical sup-\\nport scams have been ongoing since the early days of social\\nmedia, their prevalence has significantly increased in recent\\nyears [5,7] especially targeting cryptocurrency users and users\\noperating multiple social media accounts. This trend is not\\nsurprising, as cryptocurrencies have gained significant popu-\\nlarity due to the support of cryptocurrencies by some of the\\nmajor online payment platforms, including PayPal. Moreover,\\nwith the increasing number of social media platforms, users\\nwith accounts on multiple platforms often experience account\\naccess issues due to lost passwords or account takeovers.\\nIn this paper, we focus on these two scam typologies and\\ndeploy techniques to uncover technical support scammers.\\nOur proposed detection method is inspired by a recent work\\nby Acharya et. al [8] where they conducted a deception study\\nto bait scammers into revealing their identity. Their method-\\n1arXiv:2412.15072v1  [cs.CR]  19 Dec 2024ology involved requesting technical support through tweets,\\nand upon receiving a response from the scammers, engaging\\nwith them through messages. Despite their methodology be-\\ning highly effective, it had several limitations that could be\\nimproved to perform a more comprehensive study of the scam\\nlife cycle. In particular, the work [8] heavily relied on manu-\\nally crafted messages during the scam engagement process,\\nwhich reduced the scalability of their proposed system.\\nIn this paper, we address the limitations and propose a\\nmore scalable and robust system that leverages more refined\\ntechniques to engage with scammers and gain deeper insights\\ninto the scam life cycle. With the rise of ML-based tools\\nand Large Language Models (LLMs), we use the most ad-\\nvanced tools, ensuring efficiency and avoiding redundant ef-\\nforts. More specifically, we use LLMs to craft human-style\\nmessages that are perceptually more believable and possess a\\nhigher propensity of trapping scammers. We introduce Scam-\\nChatBot , a composition of automated techniques designed\\nto trap technical support scammers targeting cryptocurrency\\nand social media users. In Figure 1, we provide a high-level\\nworkflow for ScamChatBot which consists of three main com-\\nponents. The first component involves crafting baiting tweets\\n(called honeyposts ‚ûä) on X to lure scammers, posing as users\\nexperiencing account recovery problems across various ac-\\ncount types. We regularly publish automated tweets via such\\ndeception accounts and monitor the real-time interactions of\\nscammers with our tweets ‚ûã. To ensure that we only interact\\nwith fraudsters, we filter out legitimate interactions ‚ûå.\\nThe second component engages scammers who offer sup-\\nport through communication channels (often outside of X),\\ninitiating conversations, and seeking assistance via the chat\\nmodule ‚ûç. To mimic more believable and human-style conver-\\nsations, ScamChatBot uses OpenAI‚Äôs ChatGPT for automated\\nconversations with scammers. Our goal is to uncover their\\nmethods and especially information related to the payment\\nprofiles used in scams because the payment profile is crucial\\nto understanding and disrupting fraud.\\nTo this end, we have developed a chatbot that can pretend\\nto be different personas to interact with scammers. We con-\\nducted conversations across three communication channels\\n(Instagram, X, and email) because we found that these are\\nthe three channels commonly used by scammers. Eventually,\\nas the scam activity proceeds, scammers demand payments\\nprior to offering technical support. We collect the payment\\nmethods and take appropriate actions to restrict the scam pro-\\nliferation (i.e., sharing the collected information with payment\\nproviders). Novel to this work, our system exposes a worrying\\naspect of technical support scams. In prior works [8], such\\nscam campaigns were perceived to be operated by fraudsters\\nwho merely intended to pickpocket unsuspecting users. How-\\never, in our experiments, we discovered that scammers may\\nalso employ extortion techniques to threaten users with harm\\nincluding personal information leakage. As a result, the im-\\npact of such scam campaigns extends far beyond financial\\nTweet \\nGenerator\\nScammers\\nScam \\nFiltration\\nChatbot\\nHoneyposts\\nTracking \\nand \\nCorrelation1\\n2\\n3\\n4\\nX \\nProfiles\\nTweet \\nGenerator\\nTweet\\nChatbotFigure 1: ScamChatBot : An overview of our system, which\\nconsists of three main components: i) Decoy scammers which\\ngenerate baiting tweets, ii) automated engagement which per-\\nforms a conversation with scammers via a chatbot, and iii)\\ntracking and correlation , which provides scammer‚Äôs detail by\\ntracking payment profiles and user social media profile data.\\nlosses, thus mandating the adoption of countermeasures and\\na necessity for more comprehensive works in this direction.\\nIn the measurement period between the last week of\\nNovember 2023 and the end of March 2024, we generated\\nabout 19,000 honeyposts on our honey profiles that attracted\\nover 11,700 scammers interacting with our system asking\\nto contact them back via X direct messages, Instagram, and\\nemail. Scammer accounts with a large number of scammer\\nprofiles have been found to be operated by a single scam-\\nmer [9]. Through our analysis of scam network operation,\\nwe discovered that scammers utilize numerous social me-\\ndia profiles during interaction with honey profiles, and some\\nscammers were found to employ as many as 71 social media\\nprofiles, prompting users to contact them via 1-2 commu-\\nnication channels. Among these scammers, we engaged in\\nconversational dialogue with 450 scammers with our chatbot.\\nBy analyzing the timing of scammers‚Äô responses, we observed\\ntheir promptness, typically replying on all three scamming\\nchannels within two hours of the conversation initiation and\\nnormally spending approximately one hour before realizing\\nthat the victim is unlikely to comply with any requested pay-\\nment method.\\nThrough qualitative analysis of these conversations follow-\\ning a victim‚Äôs engagement with a scam, we identified several\\ntechniques used by scammers. These include requesting live\\nvideo or phone verification, soliciting private information, urg-\\ning victims to pay for fake support, and potentially resorting\\nto extortion or emotional manipulation if payment demands\\nare not met. In addition, we measured ML-related text gener-\\nation from scammers and found that 28.4% of scammers are\\nlikely to use some form of automated text generation tools.\\nIn addition, analyzing the length of dialogue between the sys-\\ntem and the scammer, we found that scammers on average\\nreveal scamming payment profiles after seven interactive di-\\nalogues asking the system to pay for fake account recovery\\nsupport, which highlights that an interaction is actually re-\\n2quired to obtain relevant information. Furthermore, we collab-\\norated with X, PayPal, and the cryptocurrency abuse database\\nChainabuse [10], sharing scamming PayPal addresses and\\ncryptocurrency wallets that scammers asked the system to\\npay for fake support, and we received validation of scam ac-\\ntivities. Our work thus provides a foundational model for the\\nindustry and security community to develop similar frame-\\nworks for identifying different categories of fraud on social\\nmedia platforms.\\nContributions We make the following key contributions:\\n‚Ä¢Interactive Scam Detection. We introduce ScamChat-\\nBot, an interactive, scalable, and LLM-based framework\\ncapable of unveiling scammer life cycles and techniques\\nthrough interactive dialogues. The framework can be\\ngeneralized to decoy scammers of various scam cate-\\ngories across multiple social media platforms.\\n‚Ä¢Detecting Fraudulent Payments. Our system collects\\nthe scamming payment profiles of fraudsters in real time,\\nwhich would otherwise remain inaccessible without en-\\ngaging with scammers directly. In an empirical study,\\nwe analyze technical support scammers on social media.\\nEthical Consideration and Data Disclosure In keeping\\nwith the ethical guidelines required for a deception study, we\\nobtained IRB approval from our institution. When managing\\ndata related to scammers, we strictly adhered to the GDPR\\nand the EU Data Management Policies [11]. We conducted\\na deceptive study to ensure scammers remained unaware of\\nbeing observed, preserving the authenticity of their responses\\nand behaviors. Informing them or allowing withdrawal would\\nhave compromised the research, and retroactive debriefing po-\\ntentially risks alerting other scammers and retribution attacks\\nfrom scammers. Our study may have impacted scammers by\\nrisking the banning of their payment-related information af-\\nter it was disclosed to PayPal and the cryptocurrency abuse\\ndatabase. However, this disclosure potentially prevents future\\nattacks on other victims. Our tweet module was configured\\nto adhere to ethical standards when tweeting. In total, we\\ngenerated about 19,000 tweets across six different accounts\\nover 101 days, a small number compared to the enormous vol-\\nume of tweets on X (approximately 500 million per day). We\\nposted innocuous tweets to attract scammers targeting users\\nseeking help to recover their accounts. We made sure not to\\nengage with official cryptocurrency wallet support teams or\\nsocial media accounts. Thus, the sole objective of our system\\nwas to interact with the scammers by faking unsuccessful\\npayment attempts after we were asked to pay for the fake\\naccount recovery in order to collect the fraudsters‚Äô payment\\nprofiles. Moreover, we communicated our findings to Meta,\\nGoogle, X, ten cryptocurrency wallets, PayPal, and the cryp-\\ntocurrency abuse database Chainabuse and received numerous\\npositive responses. We worked closely with X, PayPal, andChainabuse, confirming the scam validation and financial loss\\nfrom these fraud profiles.\\nWe share our code at Github repository [12]. However, for\\ndata related to scammers, we refrain from publicly sharing\\nto avoid potential retribution attacks and plan to provide it to\\ninterested researchers or academics upon request.\\n2 System Design\\nIn this section, we discuss the design of our framework Scam-\\nChatBot , which creates unique tweets to bait scammers and\\nperforms on-demand chat initiation with scammers using a\\nchatbot. As shown in Figure 1, the system consists of three\\nmain elements to: (i) attract scammers via decoy interactions,\\n(ii) an automated engagement mechanism powered by Large\\nLanguage Models (LLMs) for conversations, and (iii) a track-\\ning module that monitors and links the various interactions\\nwe have observed. In the following, we discuss the individual\\ncomponents in more detail.\\n2.1 Attracting Scammers via Decoys\\nAs a first step, we need to somehow get scammers to interact\\nwith us. To do this, we create unique tweets (called honey-\\nposts ) that mimic a request for technical support to attract\\nscammers. The tweets ask for assistance with logging into\\nsocial media accounts or support for cryptocurrency wallets.\\nTo generate honeyposts, we have designed tweets based on\\ntwo main criteria: i) adherence to the length limits set by\\nX posts [13] and ii) addressing users experiencing account\\naccess issues. These tweets are tailored to be posted every\\n15 minutes on X in English. To compose a tweet, we use\\nlogic with a three-sentence structure. The first sentence usu-\\nally contains introductory phrases such as ‚ÄúGood morning‚Äù,\\n‚ÄúWhat happened‚Äù, or ‚ÄúI can‚Äôt believe it‚Äù. The second sentence,\\nwhich is often coupled with the first, is aimed at users who are\\nhaving problems with their accounts. Finally, the third sen-\\ntence asks for urgent help, with or without a hashtag. These\\nhashtags are randomly generated based on account-related\\nkeywords.\\nWe utilized six X profiles, which we referred to as honey\\nprofiles , each dedicated to posting unique content for these\\naccounts. Three profiles focused on providing social account\\nrecovery support, while the remaining three focused on sup-\\nport for wallet-related recovery as follows:\\n‚Ä¢Social Media Account Honeyposts : For social me-\\ndia tweets, we selected five popular services including\\nGmail, Instagram, Youtube, X, and Facebook based on\\ntheir popularity in Google searches [14, 15]. We then\\ngenerated tweets in which the account pretends to be a\\nuser of those services experiencing login issues.\\n‚Ä¢Cryptocurrency Honeyposts : For cryptocurrency-\\nrelated tweets, we selected ten popular wallets reported\\n3in a prior study performed in a similar setup [8]. The\\nwallets included Badger ,Binance ,BitPay ,Coinbase ,Ex-\\nodus ,Free,Ledger ,MetaMask ,Trezor , and Trust Wal-\\nlet. Similar to the context of social media login issues,\\nwe generated tweets in which the account reports login\\nissues with their cryptocurrency wallets and requests\\naccount recovery.\\n2.2 Automated Engagement with Chatbots\\nIn this section, we provide an overview of the filtration tech-\\nniques implemented to ensure automated chat conversations\\nare performed exclusively with identified scammer communi-\\ncation channels, as well as the necessary chat configuration\\nand account setup for engaging with scammers. Below we\\nprovide such details.\\n2.2.1 Engagement Pre-processing Filtration\\nThe tweets posted by ScamChatBot prompt scammers to in-\\nteract with them through replies andquoted tweets . In those\\ninteractions, scammers typically provide their contact details,\\nsuch as email or social handles, asking for further interactions.\\nAs a follow-up to the scammer‚Äôs communication channel,\\nwe implemented an engagement mechanism that initiates an\\nautomated chat dialogue conversation with the scammer‚Äôs\\nsocial media profile. To ensure that we do not interact with\\nbenign accounts in an automated way, we apply various fil-\\ntration techniques before initiating a chat interaction, such as\\nexcluding verified X accounts, official social media platforms\\nsuch as X, Google, cryptocurrency wallet services, and benign\\naccounts. We provide further details on filtration techniques\\nbelow.\\n‚Ä¢Exclude Verified Accounts. The engagement module\\nremoves all verified accounts that respond to the tweets.\\nTypically, verified accounts are associated with official\\nrepresentatives of a platform, and they may respond in a\\nsincere attempt to offer technical support. We therefore\\nremove them from our study. We acknowledge that re-\\ncent changes in X‚Äôs verification policy can lead to scam-\\nmers acquiring fake verification for their profiles [16 ‚Äì18].\\nHowever, we preferred a more conservative approach in\\nan attempt to remove potential false positives even at the\\nrisk of ignoring true negatives.\\n‚Ä¢Exclude official social media services. In the current\\nsetup, the engagement module also excludes the top 20\\nsocial media services along with the top 100 wallet ser-\\nvices to ensure that our chat module does not initiate any\\nconversation with official support services. We collected\\n20 popular social media services [14, 15] and official\\ncryptocurrency wallet services [19, 20] based on social\\nmedia popularity search.‚Ä¢Benign Account Filtration. For benign account filtra-\\ntion, we faithfully replicated the approach in [8] where\\nthe authors identified two key characteristics of scam\\naccounts that were distinctly different from benign ac-\\ncounts. During the engagement process, scam accounts\\nprovide new communication channels (often outside of\\nX) to continue the conversation and complete the scam\\nactivity. Moreover, scammers also pose as official rep-\\nresentatives of the platform to gain the user‚Äôs trust. In\\ncontrast, there are also benign accounts that interact with\\nusers, often providing advice or expressing sorrow for\\nthe user‚Äôs despair. Those accounts do not interact with\\nusers beyond the initial interaction, thus displaying no in-\\ndication of fraudulent intentions. We label such accounts\\nas benign and exclude them from our study.\\nAdditionally, we provide a manual qualitative analysis in\\nSection 2.3.1, which shows that the filtration techniques work\\nwell in practice.\\n2.2.2 Scammer Engagement\\nAfter the account filtration process, the engagement mod-\\nule creates personas using a chatbot based on an LLM such\\nas ChatGPT to interact with the scammers on the platform\\nchannels they provided. For each communication channel,\\nincluding Instagram, Gmail, or even X, we generate unique\\npersonas with the purpose of (i) understanding the scam ac-\\ntivity, and (ii) extracting payment information provided by\\nscammers. Therefore, the ChatGPT-configured persona mim-\\nics a human-like behavior while conversing with scammers\\nand leverages context awareness and organic articulation ap-\\nproaches to gain the scammer‚Äôs confidence. In every new\\nconversation, we ensure that there is no LLM-related text\\npresent that could be picked up by the scammer. Moreover,\\nbefore our live experiments, we also conducted manual con-\\nversations with ScamChatBot to ensure it was fully capable of\\nperforming organic conversations and collecting the desired\\ninformation, especially related to payment profiles.\\nDialogue Generation and Direct Messaging Dialogue\\nrefers to the conversation between the scammer and our sys-\\ntem aimed at resolving account-related issues. To facilitate\\ndialogue communication, we selected communication chan-\\nnels based on fraudsters‚Äô popularity asking our system to\\ncontact for fake support. These platforms consist of three\\nchannels: Email, X, and Instagram direct messages. In total,\\nwe create 15 distinct accounts (i.e., 5 per platform: X, Insta-\\ngram, and Email). We allocate four accounts for each channel\\nfor the personas involving cryptocurrency and social media\\naccount recovery, while the remaining three are designated\\nfor language-based experimental personas. For each commu-\\nnication channel, we integrate the respective APIs [21‚Äì23] to\\nreceive real-time updates when the scammer interacts with\\nour system. Upon receiving messages from the scammer, we\\n4aggregate all previous interactions between the system and\\nthe scammer and generate a response based on the personas.\\nSubsequently, we deliver the generated message to the scam-\\nmer via each API as our response. In our direct messaging\\ninteractions with scammers, we avoid using the accounts des-\\nignated for tweets. We separate our direct messaging accounts\\nfrom our X platform tweet post accounts to effectively com-\\nmunicate with the scammers without suspicion.\\nWe ended our conversations in two cases: (i) after the scam-\\nmers provided multiple payment profiles (e.g., cryptocurrency\\naddresses and PayPal email addresses) and (ii) when the scam-\\nmers stopped communicating with us. In the first scenario, we\\nconcluded by stating that we would explore further alterna-\\ntives (after receiving all payment profiles from the fraudsters).\\nIn the second scenario, we made at least one attempt to recon-\\nnect and asked the scammer if they would like to continue the\\nchat in order to receive further assistance.\\nIn Section 2.3.2, we present the results of a manual qual-\\nitative analysis of our system‚Äôs ability to maintain context\\ncontinuity and discussion on limitations in Section 7. We em-\\npirically find that the approach is able to effectively identify\\npayment methods from scammers.\\nPersona Types and Configuration ChatGPT requires a spec-\\nified persona configuration to delineate its role in direct mes-\\nsage interactions with scammers. We establish two primary\\npersonas for ChatGPT‚Äôs configuration. The initial persona in-\\nvolves a user having issues with a cryptocurrency wallet, par-\\nticularly tailored for newcomers to cryptocurrency or crypto\\ninvestment who find themselves unable to access their wallet.\\nThe second persona simulates a user encountering difficulties\\naccessing a social media platform such as X, Instagram, Face-\\nbook, or Gmail. In this scenario, the user communicates their\\ninability to access their account and seeks assistance in recov-\\nering it. Furthermore, we conduct an experiment that expands\\nupon these personas by introducing a user who lacks profi-\\nciency in English and desires to switch to other languages. In\\nthis context, the system initially prompts the scammer to de-\\ntermine if they are conversant in languages other than English\\nand endeavors to communicate in the languages agreed upon\\nduring the dialogue.\\nAdditionally, the persona includes additional rules in text\\ngeneration. During the conversation, as the scammer asks\\nquestions about the account-related issues, the persona is\\nasked to create a fake reason for having an access problem.\\nScammers often ask for some form of payment to resolve such\\nan account issue via various payment methods. While doing\\nso, scammers create a fake reason such as needing to buy soft-\\nware to recover the account or sending money to re-enable the\\naccount-blocked workflow. The persona set for ChatGPT also\\ntries to ask for multiple payment methods so that the scammer\\ncan reveal as many payment methods as possible. For this, we\\ntake a deceptive approach by falsely claiming failure in the\\nfirst payment method provided by the scammer. We repeat theprocess until the scammer runs out of options to share with\\nus, at which point the engagement module ends the conver-\\nsation. We provide additional detail on the configuration of\\nchat persona and sample conversation between our system\\nScamChatBot and a Scammer in Appendix A.2.\\n2.3 Qualitative Evaluation of Dataset\\nIn this section, we provide a manual qualitative evaluation of\\nour dataset across several categories. To assess the effective-\\nness of our system conversations with scammers, we randomly\\nselected 15 conversations each from Instagram, email, and\\nX platform, evaluating a total of 45 interactions. Below, we\\nprovide details of our evaluation findings.\\n2.3.1 Legitimacy of Data Filtration and Methodology\\nAccuracy\\nIn this category of qualitative analysis, our focus was to iden-\\ntify the accuracy of our methodology. Specifically, we looked\\nfor instances where interactions occurred between our system\\nand both malicious and legitimate accounts. Out of 45 con-\\nversations from three platforms, we found no instances where\\nthe system mistakenly engaged with legitimate users. Addi-\\ntionally, we observed that none of the conversations involved\\nofficial support accounts.\\n2.3.2 Repetition in Conversation/Suspicion\\nIn this category of qualitative analysis, our focus was on ob-\\nserving our system‚Äôs ability to maintain context continuity.\\nWe assessed the quality of conversations, particularly those in-\\nvolving repetitive exchanges or discussions where scammers\\nattempted to determine if the system was a bot. We identified\\n3/45 conversations in which scammers noted repetitive apolo-\\ngies for not being able to send screenshots due to technical\\nproblems and perceived a lack of seriousness in taking action.\\nIn all three cases, scammers expressed suspicion, stating ei-\\nther that the system was likely a robot or that it was wasting\\ntime without genuine intent to resolve the issue. While the\\nscammers became suspicious that they were interacting with\\na bot, they still disclosed their payment profiles. This demon-\\nstrates that our system was effective in identifying fraudulent\\npayment profiles. Such identification can enable proactive\\nmeasures to block fraudulent payments and prevent further\\nscamming of potential victims.\\n2.3.3 Limitation to External Channel Communication\\nIn this category of qualitative analysis, our system faced limi-\\ntations in cases where scammers redirected communication to\\nan external channel. Specifically, in three instances, scammers\\non X requested to connect via direct message on another X ac-\\ncount. Since our system was not configured to interact through\\n5newly introduced channels during the conversation, these in-\\nteractions did not proceed in-depth. We identified three ac-\\ncounts‚Äîtwo from X and one from email‚Äîthat attempted\\nthis redirection. In one case, the system politely ended the\\nconversation, while in the other two, it falsely claimed to\\nhave already contacted the referenced person and received\\nno response. Additionally, in one email interaction, the scam-\\nmer provided a template asking for cryptocurrency addresses\\nand a passphrase. In all of these three instances, our system\\nhad a limitation in further communicating with the scammer-\\nprovided channel.\\n2.4 Tracking and Correlation\\nThe third major component of ScamChatBot is the tracking\\nmodule, which conducts analysis on data collected by the en-\\ngagement module. For the purpose of this study, the key fea-\\ntures we analyze include social media profile metadata anal-\\nysis, conversation text created by our system and responses\\nreceived from scammers, payment methods shared, and the\\nend-to-end scam lifecycle.\\nIn the following, we present our main finding. In Section 3,\\nwe provide details on the interaction with decoy profiles and\\nthe automated chat module, while we present the timing and\\noperation analysis of scammers in Section 4. Section 5 fo-\\ncuses on a qualitative study of the interaction between our\\nsystem and scammers, and we focus on the tracking of finan-\\ncial losses in Section 6.\\n3 Delineating Scammers Interaction\\nWe start by discussing results regarding interactions between\\nscammers and our system.\\n3.1 Interaction with Decoy Profiles\\nBetween mid-November 2023 and mid-February 2024, the\\ntweet generator posted unique tweets for 101 days. During this\\nperiod, we posted tailored tweets aimed at luring scammers\\nwho target users seeking assistance with account recovery.\\nFigure 2(a) (left) shows an overview of scammers interacting\\nwith our honeyposts and we present the overall tweet posts\\ninteraction results of our honey profiles in Table 1. Out of\\n19,001 tweet posts, the module posted 32.74% (6,221/19,001)\\ntweets related to social media recovery of five different social\\nmedia accounts and 67.3% (12,780/190,001) tweets related to\\ncryptocurrency wallet recovery of ten different wallets. Out of\\n6,782 tweets that received an interaction, the interaction per-\\ncentage of social media-related account recovery was 10.8%\\n(674/6,221) compared to a higher percentage of cryptocur-\\nrency wallets at around 47.8% (6,106/12,780). Overall, among\\nall posted tweets, 35.7% (6,782/19,001) received interactions\\nin the form of post comments from 11,769 potential scammers.Account Tweets Interacted Distinct Total Interacted\\nRecovery Posted Tweets Text Text Scammers\\nCrypto Wallet 12780 6106 20046 29652 10647\\nSocial Media 6221 674 2239 2691 1581\\nAll 19001 6782 22285 32343 11769\\nTable 1: Summary of account recovery chat module based\\non tweets interacted by scammers from two account recov-\\nery contexts: cryptocurrency wallet and social media-based\\naccount recovery.\\nScam accounts replied with 22,285 distinct messages to all\\nhoney profiles. Among them, 90.0% (20,046/22,285) tweets\\nwere sent in response to cryptocurrency-related accounts and\\ntweets.\\nIn our initial analysis, we hence find that cryptocurrency\\nusers are more commonly targeted by scammers, likely due\\nto a higher financial incentive. We exclude scammers per-\\nforming non-text interactions, such as retweets, likes, follows,\\nbookmarks, and impressions. Further details on non-text in-\\nteractions are provided in the Appendix A.3.\\n3.2 Interaction with Automated Chat Module\\nFrom the last week of November 2023 to the end of March\\n2024, our chat module engaged in communication with scam-\\nmers via communication channels posted through tweet posts.\\nWe continued to engage with scammers for 6 weeks even after\\nending our tweet module to accommodate those scammers\\nwho persisted in engaging with our system. Figure 2(a) (left)\\ndisplays the interaction of scammers with our honeyposts,\\nwhereas Figure 2(b) (middle) displays the number of scam-\\nmers interacting with our system via the chat module. Follow-\\ning thorough filtration, as outlined in Section 2, our system\\ninitiated automated chats via three communication channels:\\nemail, X, and Instagram direct messaging. Figure 2(c) (right),\\nwe present the number of interactions via each communica-\\ntion channel. Scammer‚Äôs preference for email shows that not\\nall scammers operate within the instant messaging model. In\\nthe following, we dive deeper into the modus operandi of\\nscammers and provide an overview of their interactions.\\nScam Lifecycle For both account recovery contexts, scam-\\nmers typically follow a consistent pattern: initially, scammers\\nbegin by creating a verification process for the victim‚Äôs ac-\\ncount information. Once some form of verification is estab-\\nlished, the scammer fabricates a false pretext and requests\\npayment in various forms. Following this initial stage of veri-\\nfication, the scammer often instructs the potential victim to\\nconduct an internal audit and then return with the details. Fi-\\nnally, the scammer suggests that after the payment is made, it\\nmay take some time for the account to be fully recovered due\\nto essential technical actions required for recovery. Below,\\n61 11 21 31 41 51 61 71 81 91 101\\nDays0500010000Scammers\\n1 16 31 46 61 76 91 106 121 136\\nDays0200400Scammers\\n1 16 31 46 61 76 91 106 121 136\\nDays050100150ScammersEmail\\nX\\nInstagramFigure 2: Interactions of scammers with our decoy profiles and our chatbot. The left graph shows the cumulative sum of scammers\\ninteracting with our honeyposts over the experiment duration. The middle graph shows the daily cumulative sum of scammers\\nengaged via conversation with our system ScamChatBot . The right graph shows the cumulative sum of scammers based on\\nvarious communication channel engagements with ScamChatBot .\\nwe provide further insights into detailed operational cycles\\nperformed by scammers.\\nVictim Verification Procedure Initially, the scammer re-\\nquests information such as the victim‚Äôs account information\\n(e.g., wallet address or social media account), the timing of\\nthe issue occurrence, and any actions such as withdrawals\\nor login actions performed lately. This initial conversation is\\nfollowed by additional details to establish identities, such as\\nlocation, email, name, phone number, and a video recording\\nof the issue with the wallet or account. Oftentimes, scammers\\ntry to initiate communication via WhatsApp or Facebook to\\nfurther verify the victim‚Äôs authenticity. During this verifica-\\ntion phase, scammers frequently request screenshots of the\\naccount, and in some instances, they may also ask for secret\\nphrases.\\nAdvance Fee After the victim verification process, scam-\\nmers request an advance fee to initiate the recovery process.\\nThe advance fee is often requested under the pretext of pur-\\nchasing software (e.g., Zeus Software orTrojan Horse ) which\\nhelps with the recovery process. In some cases, scammers\\nalso request seed phrases for wallets, which is one of the\\neasiest methods to steal money. Seed phrases can be easily\\nconverted into private keys, which can then be used to transfer\\ncryptocurrency assets to a new address.\\nPayment Method Instructions Scammers request payment\\nfor their services through various methods. We observed dif-\\nferent kinds of payout methods, e.g., Amazon or Apple gift\\ncards, cryptocurrencies such as Bitcoin and Ethereum, or\\nother online payment methods such as PayPal, Venmo, and\\nCashApp. Alternatively, the scammer might direct the victim\\nto an external link for payment. In the case of PayPal, scam-\\nmers are found to instruct the victim to use the ‚Äúsend to family\\nand friends‚Äù option. Upon consulting PayPal, we learned that\\npayments through this option are not eligible for refund or\\ndispute. Therefore, once the scam activity is completed, the\\nvictim cannot dispute the charge with PayPal. Finally, when\\nthe payment is completed, scammers also request a payment\\nscreenshot for confirmation.\\nScammer‚Äôs Reaction to Failed Payment Attempt As men-\\ntioned earlier, we used failed payment excuses to bait scam-\\nmers into revealing all their payment methods. We noticedTable 2: Breakdown of ScamChatBot engagement with scam-\\nmers. Within this table, we delineate the exchange of mes-\\nsages between our system and scammers across email, X, and\\nInstagram platforms.\\nChannels Msg. Sent Msg. Replied Scammers Total\\nInteraction Dist/All Dist/All Replied/Sent Msg\\nEmail 1325/1327 666/681 156/640 2008\\nX 1303/1709 1200/1293 149/276 3002\\nInstagram 4168/6921 3286/6747 154/171 13668\\nAll 6788/9957 5152/8721 450/1087 18678\\nthat scammers expressed resentment or frustration when we\\nmentioned payment failures to them. Occasionally, they also\\nused profane language and threatened to cause personal harm.\\nAlthough we were using fake personas while interacting with\\nscammers, it is indeed concerning since real users might have\\nexperienced such unwarranted abuse in the form of threats to\\nthem and their families. It is therefore pertinent to curtail this\\nform of technical support scam, as it can also transition into\\nextortion and abuse.\\nResults Overview In Table 2, we provide an overview of our\\nconversations. Out of 11,769 scammers that interacted with\\nour honey profiles, our system interacted with 450 scammers\\nacross three communication channels: X (149), Instagram\\n(154), and email (156) excluding accounts that opted out of\\ndirect messaging (994) and scammers asking external commu-\\nnication channels such as Phone, including WhatsApp (131),\\nTelegram (121), account restore URLs (199), and Google\\nForms (424). In the email conversations, out of 640 scammers\\nto which we sent messages, 24.37% (156/640) scammers\\nreplied with 681 total and 666 distinct replies. Among the\\nthree communication channels, Instagram was found to have\\nthe highest engagement replies (6,747) from replies with a\\ntotal message of 73.2% (13,668/18,678) collected by our sys-\\ntem. Of the total of 18,678 messages recorded by the system,\\n46.7% (8,721/18,678) were written by scammers.\\n7Table 3: Breakdown of scammer‚Äôs first response with our\\nsystem ScamChatBot . The data represents how quickly scam-\\nmers respond to our system upon asking for account recovery\\nsupport.\\nChannels Median 90 Response Total\\nInteraction Response Pct <1000 ms Scammers\\nEmail 0:33:09 11:48:45 48 156\\nX 2d 6:16:36 17d 10:50:42 13 149\\nInstagram 0:57:18 8d 16:05:36 9 154\\nAll 1:48:54 9d 15:48:43 70 450\\n4 Investigation into Scammer Tactics\\nIn this section, we analyze several intricacies, such as ana-\\nlyzing the timing of scammers‚Äô engagement with our system\\nbased on how quickly they respond, examining operations\\nbased on the day of the week, exploring the potential use of\\nML-generated messages, investigating organized networks of\\nscammers that may utilize multiple social accounts as part of\\ncoordinated abuse, and dialogue necessities as part of reveal-\\ning scamming payment profile.\\n4.1 Timing Analysis\\nWe start by presenting a timing analysis to showcase two key\\naspects of scam engagement methods. In the first analysis,\\nwe explore the time taken by fraudsters to respond to us after\\nour initial engagements. In other words, once the scammer\\ninformed us of the method to connect with them and our\\nsystem sent a help request, how long did the scammers take to\\nrespond to us. As a result, we could assess the communication\\nchannels being actively monitored by scammers, indicated by\\ntheir swift response time. In the second analysis, we study\\nthe total duration of our engagement with the scammer where\\nthey persistently tried to trap us in hopes of obtaining any\\nfinancial benefit. Our second analysis revealed the persistence\\nshown by scammers in engaging with their victims.\\n4.1.1 Initial Response Time\\nOur analysis of the initial response time revealed that scam-\\nmers are more likely to respond faster via email compared to\\nthose on X and Instagram (see Table 3). The median response\\ntime for email was 33 minutes and 9 seconds which was con-\\nsiderably faster than X, where the median response time was\\nover 2 days. During our analysis, we identified that some com-\\nmunication channels indicated scripted responses by replying\\nto our requests within 1 second. Upon closer inspection, we\\nfound that 30.8% (48/156) of initial email responses were au-\\ntomated replies containing additional information or directing\\nusers to release secret keys via external sites. Similarly, for X\\nand Instagram, we observed automated setups for first-time\\naccounts, accounting for 8.7% (13/149) and 5.8% (9/154),Table 4: Breakdown of total time scammers performing di-\\nalogue - The difference between the first and last responses\\nfrom scammers interacting with our system ScamChatBot .\\nChannels Median 90 Response One Time Total\\nInteraction Time Spent Pct <1000 ms Reply Scammers\\nEmail 0:18:00 5:57:18 52 52 156\\nX 1d 2:03:56 13d 15:32:39 15 15 149\\nInstagram 0:57:03 5d 16:30:46 4 4 154\\nAll 0:57:30 6d 22:30:37 71 71 450\\nrespectively. In total, we identified 15.6% (70/450) of commu-\\nnication channels employing some form of initial auto-script\\nto entice victims into divulging private information. Overall,\\nscammers‚Äô response times exhibited a median of 1 hour and\\n48 minutes, with a 90th percentile response time of 9 days.\\n4.1.2 Total Time Spent Analysis\\nIn Table 4, we present the duration scammers spent engaging\\nwith our system before discontinuing further conversation.\\nScammers communicating via email channels were found to\\nspend a total of 18 minutes in conversation, which was notably\\nlower compared to those communicating through X, spending\\nabout 1 day and 2 hours, and Instagram, spending about 57\\nminutes. 52 out of 156 emails received from scammers re-\\nsulted in only one-time replies with further instructions, while\\nfor X and Instagram, accounted 10.06% (15/149) and 2.59%\\n(4/154), respectively. Overall, scammers spent an average of\\n57 minutes and 30 seconds, with the 90th percentile spend-\\ning approximately 6 days and 22 hours before discontinuing\\nfurther communication. We observed that scammers who are\\nactive for more than a day tend to repeatedly ask if the vic-\\ntim needs additional assistance with account recovery and\\noften mention that they will resume account recovery after\\nsuccessful payment.\\nIn Figure 3, we show the total number of days spent by\\nscammer interacting with our system: more than 80% of the\\nscammers would leave interacting after the first-day interac-\\ntion with our system, and the remaining 20% would follow\\nup to continue the conversation from the last point.\\n4.2 Operation Analysis\\nIn our analysis of operations, we (i) dive deep into scammers‚Äô\\nweekly operation pattern, (ii) explore how scammers create\\nnumerous social media profiles and use a limited number of\\ncommunication channels to prompt victims to initiate contact,\\nand (iii) as ML and advanced text-generation tools become\\nmore prevalent, we assess the likelihood of scammers utilizing\\nsuch technology in their interactions with our system.\\n8Figure 3: Number of days scammers spent time with our\\nsystem - In this graph we show the difference between the\\nfirst response and last response from scammers interacting\\nwith our system.\\nMon Tue Wed Thur Fri Sat Sun\\nDay of the Week050010001500Interacted TextScam Chat Interaction by Platform and Day\\nEmail\\nInstagram\\nX\\nFigure 4: Days of the week chat dialogue interactions - the\\ngraph shows text (chat dialogue) scammers performed via\\nthree different communication channels with our system\\nScamChatBot throughout the days of the week.\\n4.2.1 Week Period Scammer Communication Analysis\\nOur analysis of the days of the week interaction by scammers\\nwith our framework showed distinct patterns. The number\\nof text interactions via chat dialogue shows that interaction\\npatterns vary across different communication platforms (see\\nFigure 4). For instance, scammers using Instagram as a com-\\nmunication channel for chat dialogue tend to be more active\\non alternate days, with the lowest activity observed on Satur-\\ndays. Conversely, scammers utilizing email exhibit increased\\nactivity on weekends compared to other weekdays, with alter-\\nnating lows on Tuesdays and Fridays. Similarly, on X, scam-\\nmers maintain consistent chat activity from Wednesday to\\nSaturday, with reduced communication observed on Tuesdays\\nand Sundays.\\n4.2.2 Scam Network Operation\\nIn this section, we perform a collaborative network analysis,\\ncentering on (i) the creation of scammer profiles, which in-\\ncludes examining names, descriptions, followers, and follow-\\ning; (ii) the utilization of shared communication channels byscammers such as instances of scammers sharing a common\\ncommunication channel through tweet posts, urging potential\\nvictims to contact them for technical support.\\nShared Profile Information We perform an analysis of\\nscammer profile details, including names, descriptions, fol-\\nlowers, and following. Our investigation into scam networks\\nrevealed that 24.5% (2,877/11,764) scammers shared a com-\\nmon name, 33.6% (3,949/11,764) shared a common descrip-\\ntion, and 5.2% (610/11,764) scammers were found to have\\nmultiple common followers in their profiles. Among these\\n610 shared followers, we detected 503 instances of mutual\\nfollowing to more than one account. This leads us to suspect\\nthat these profiles represent a cluster of organized scamming\\noperations aimed at executing targeted scamming campaigns.\\nShared Communication Channels Our analysis of cluster-\\ning scamming profiles by communication channels indicates\\nthat 42.6% (5,012/11,769) of the profiles engaged in scam-\\nming activities were observed to use more than one communi-\\ncation channel. Within this subset of 5,012 scamming profiles,\\nthe highest number of communication channels employed by\\na single profile was 71, while the median value for other\\nscamming profiles was 3 communication channels. Addition-\\nally, we found that 16.5% (1,939/11,769) of the scamming\\naccounts used all three communication channels, prompting\\nusers to contact them.\\n4.2.3 Potential Operation of ML ChatBot\\nNext, we perform an analysis of conversations originating\\nfrom scammers, possibly generated using ML tools. Ini-\\ntially, we investigated ML text detection tools available in\\nthe market [24]. To validate the authenticity of the text, we\\nconducted an exploratory free trial of the top five services\\nlisted [24], comparing inputs of both ML-generated and non-\\nML-generated content. Among these services, we selected\\nSapling API [25] to further assess the likelihood of ML-\\ngenerated text across all three communication channels. The\\nAPIs provided by Sapling for detecting ML content evaluated\\nthe aggregated scoring of the text from each scammer as the\\ncalculation of the mean score. These scores are derived from\\nspecialized models: one is trained to classify extensive por-\\ntions of text, while the other is a perplexity-based model [26].\\nIn Figure 5 and Table 5, we summarize the score 0 to 1, where\\n0 represents the least likelihood and 1 represents the highest\\nprobability score of utilizing ML tools in the generation.\\nOur observations on ML-related text generation on scam-\\nmers showed 28.4% (128/450) of scammers with a probability\\nof 50 or more found to be likely to use some form of ML text\\ngeneration tool in text creation. Similarly, on X and Instagram,\\na probability score of greater than 50, was found to be 9.4%\\n(14/149) and 0.6% (1/154) scammers, respectively. In a note\\nto scammers with a probability of 75 or greater was found to\\nbe 28.4% (128/450). Overall, from all three communication\\n90.00.10.20.30.40.50.60.70.80.9\\nCDF0.250.500.751.00ScoreInstagram\\nEmail\\nX\\n8 16 24 32 40 48 56 64 72 80\\nCDF0.250.500.751.00DialoguesInstagram\\nEmail\\nX\\n5 10 15 20 25 30 35 40 45\\nCDF0.250.500.751.00Interactions with Payment ProÔ¨ÅlesInstagram\\nEmail\\nXFigure 5: Graph on analysis of dialogue - In the left graph (a) we show the ML text score from each social media dialogue from\\nscammers. In the middle graph (b), we show the total dialogue length between our system and scammers from each platform,\\nwhile on the right side (c), we illustrate the total count of dialogue interactions needed from three platforms before the scammer\\ndiscloses the payment method to request payment from the system.\\nChannels Mean Median Min Max Score >=50 Score >=75 Score >=90 Total\\nEmail 0.80820 0.99419 0 0.99996 128 120 110 156\\nX 0.09651 0.00013 0 0.99986 14 8 7 149\\nInstagram 0.01323 1.57e-05 0 0.57432 1 0 0 154\\nAll 0.30095 0.00092 0 0.99996 143 128 117 450\\nTable 5: Breakdown of AI text score from each communication channel. In this table, we present the AI text score for each\\nscammer‚Äôs communicated text and the respective score. Scammers communicating with emails are found to utilize some form of\\nAI text generation compared to other social media platforms.\\nchannels, 26.0% (117/450) with a probability score of 90 or\\ngreater are likely to use some form of ML text generation\\ntool in communicating with our system. Furthermore, in Fig-\\nure 5(a) we show the score of each communication channel.\\n4.3 Analysis of Dialogue Length and Scam-\\nming Payment Information Disclosure\\nWe examine the duration of interactions within our system\\nand with scammers, focusing on the total dialogue length and\\nthe minimum dialogue required before payment details are\\ndisclosed or requested. This section provides a comprehensive\\noverview of both aspects.\\nDialogue Length Regarding the total dialogue length, the\\nmedian dialogue length for the system was 8, while for scam-\\nmers it was 7. The maximum dialogue length observed was\\n135 from the system and 81 from scammers, with minimum\\nvalues of 2 and 1 for the system and scammers, respectively.\\nFor each communication channel, the median dialogue length\\nfor email, Instagram, and X were 2, 22, and 6, respectively,\\nwhereas the highest communication dialogue was found 62,\\n81, and 65, respectively. In Figure 5(b) we illustrate the length\\nof dialogue exchanged between our system and scammers\\nfrom three communication channels. The graphs reveal that\\nscammers preferred performing longer dialogue interactions\\non Instagram than on email and X.\\nLeast Dialogue in Revealing Scamming Payment Profile\\nIn terms of the minimum required dialogue for scammers to\\ndisclose their preferred payment method, the median valueswere 3 for the system and 2 for scammers. The maximum and\\nminimum dialogue lengths observed were 47 and 1, respec-\\ntively, for both scammers and the system. For each communi-\\ncation channel, the median dialogue length before revealing\\nscamming payment profiles for email, Instagram, and X was\\n1, 10, and 7, respectively, whereas the highest communication\\ndialogue found before revealing scamming payment profiles\\nwas 13, 47, and 41, respectively. Figure 5(c) illustrates the di-\\nalogue length necessary for scammers to reveal their payment\\nprofile information from each communication channel. The\\ngraphs show that in email scammers are out front in reveal-\\ning the payment profile as part of the fake account recovery,\\nwhereas in Instagram and X, scammers are likely to engage\\nwith potential victims with a larger number of messages be-\\nfore actually revealing their payment profiles.\\n5 Qualitative Study on Chat Dialogue\\nNext, we present a qualitative examination of both our sys-\\ntem and the scammer‚Äôs interactions during the chat dialogue.\\nInitially, we manually inspected 150 randomly selected chat\\ndialogues from scammers interacting with our system. This\\nmanual assessment enabled us to create a questionnaire that\\neffectively captures the qualitative aspects of the conversa-\\ntion. Subsequently, using these questionnaires, we formulated\\na prompt for ChatGPT to conduct an individual qualitative\\nanalysis of the scammer‚Äôs and our system‚Äôs context. The de-\\ntailed prompt questionnaire is provided in Appendix A.4. We\\nperform a manual qualitative analysis on each of these listed\\nprompts output and present the findings below.\\n105.1 System Interaction with Scammer Analysis\\nSystem Persona The system persona generation reflected\\na chat communication with a scammer based on the type of\\naccount recovery tweet module post interaction. An account\\nrecovery on cryptocurrency wallet and social media reflected\\nthree main persona generation: (i) cryptocurrency user who\\nlost access and does not know what happened (152/450), (ii)\\nnaive cryptocurrency investor unable to access the wallet\\n(48/450), and (iii) account user afraid of being a phishing vic-\\ntim (46/450). In both cryptocurrency wallet and social media\\naccount recovery cases, the persona generation reflected the\\ninability to access accounts and may have been a victim of\\nphishing or scams. The persona also reflected human-like\\nemotions with mixed feelings of distress, fear, naiveness, and\\nthe need for urgent assistance to gain access to the address\\naccount.\\nSystem Refusal Sentiments Even though we configured\\nour system to generate fake data or deceit in some cases, we\\nnotice the system displaying diverse refusal sentiments in\\nits interactions with scammers. For instance: hesitancy to\\nshare personal verification details (8/450), reluctance to dis-\\nclose private secrets (6/450), preference for skepticism over\\nPayPal requests, caution regarding visiting potentially suspi-\\ncious restoration links (12/450), reluctance to make security\\ndeposits (7/450), suspicion towards support that may be poten-\\ntial scammers due to the inability to establish official support\\nchannels (4/450), and resistance to verifying account status\\nby sharing screenshots of the system‚Äôs account (37/450). In\\nsome cases, such as screenshot verification, our system always\\nrefused to provide reasoning due to technical issues.\\nFake Payment Confirmation For any payment-related trans-\\nfer requests, our system initially asks for a PayPal account\\nassociated with scammers and later fakes that the system is\\nunable to send the payment amount due to some technical is-\\nsue and asks for alternative forms of payment. At some point\\nduring the conversation after exhausting scammers to send\\nmultiple payment methods, the system confirmed scammers\\nsending 117/450 payments via cryptocurrency, and 16/450\\nvia PayPal.\\nSystem Action Reasoning to Ending Conversation In\\nsystem-related ending conversation reasoning, we observe\\nthe likelihood of ending the conversation that may have been\\nthe result of System chat dialogue creation. Out of 450 conver-\\nsations, 152 conversations ended asking systems as follow-up\\nwhether scammers would like to continue further after ex-\\nhausting scammers. These ending conversations were rather\\na form of polite follow-up. For the remaining scammers, our\\nanalyses reveal various instances of ending conversations. For\\ninstance, 22/450 scammers left conversations after providing\\naccount addresses. We suspect this may have been a result\\nof a fake account address generation, and the scammer likelydetected these as fake so the victim might not be serious.\\nSome other reasons for ending the conversation related to the\\nrefusal to provide sensitive information (11/450), the inability\\nto provide private secrets (7/450), and the inability to verify\\nthe security deposit (7/450).\\n5.2 Scammer Interaction with System Analysis\\nRole Representation Our analysis of how scammers portray\\nthemselves during conversations reveals a variety of roles they\\nadopt. Out of 450 communications analyzed, 137 instances\\ninvolved scammers presenting themselves as part of a support\\nteam, assuming roles like customer support, wallet support,\\ntech support, media recovery support, or official support. Ad-\\nditionally, some scammers identified themselves with titles re-\\nlated to hacking (25/450), such as white hat hacker, legitimate\\nhacker, recovery hacker, or private hacker. Another group of\\nscammers (17/450) referred to themselves as experts, using\\ntitles like blockchain expert, software bypass expert, or cy-\\nbersecurity expert. Interestingly, 271/450 scammers initiated\\nconversations by portraying themselves as representatives or\\nagents offering assistance in account recovery.\\nVerification of Victims We observed scammers perform-\\ning various methods for victim verification. Among the 450\\nscammers analyzed, 69 requested private key secrets. Addi-\\ntionally, 47/450 asked for screenshots containing transaction\\nhashes, associated email, and phone numbers, investment de-\\ntails, location information, or evidence of VPN usage. More-\\nover, 19/450 scammers sought verification through phone\\ncalls, WhatsApp, or video calls, while 29/450 requested photo\\nscreenshots of passports or driver‚Äôs licenses for verification\\npurposes.\\nFalse Issue Reasoning Scammers were found to provide\\nvarious explanations when asked about the issues with the\\naccount. These included the problem for instance i) system\\nbug (33/450), ii) claiming that the account was blocked due\\nto suspicion (16/450), or iii) stating that there were access\\nissues (57/450) such as the account being stuck, blacklisted,\\nor accessed fraudulently. In such cases, scammers typically\\ninsisted that purchasing an activation code or activation tool\\nwas necessary to regain access to the account.\\nFake Service Charge Scammers frequently provided dif-\\nferent pricing for account recovery services. We found that\\nout of 450 scammers, 161 required the pay as part of the ac-\\ncount recovery before proceeding further. The median price\\nwas $150, ranging from a minimum of $20 to a maximum\\nof $5,700. Notably, three scammers requested less than $1\\nas part of a pre-authorization verification process before dis-\\nclosing the final service price. For funds that are asked to\\npay via PayPal, scammers mentioned sending payments as\\n‚ÄúFamily and Friends‚Äù, whereas for cryptocurrency transactions\\n11asked to send a transaction address information followed by\\na screenshot of a successful payment.\\nAttitudes Towards Time Wasting Upon reaching a point of\\nexhaustion during the conversation, or not receiving payment\\nafter providing payment details, scammers expressed various\\nsentiments indicating frustration, disbelief, or annoyance, of-\\nten accompanied by profanity, threats, emojis, and question\\nmarks. Examples of such sentiments include phrases like,\\n\"You don‚Äôt seem serious. That‚Äôs what I have been waiting for;\\nI am about to block you\", and \"You‚Äôre being dumb. F**k!\\nOff!\". Our analysis revealed that 207 out of 450 scammers\\nconveyed some form of resentment before terminating the\\nconversation.\\nReasoning Behind Scammers Ending Conversations In\\nunderstanding why scammers choose to conclude or abandon\\nconversations, we analyzed various factors influencing this de-\\ncision. We observed several common reasons for ending con-\\nversations, including instances where the system was unable\\nto provide payment proof (85/450), cases where scammers\\nrequired a prompt response from the system (39/450), situ-\\nations where scammers directed communication to external\\nchannels such as social media or a restoration page (22/450),\\ninstances where the system suggested seeking alternate so-\\nlutions (19/450), and cases where the system was unable to\\nprovide sufficient information or verification (17/450).\\n5.3 Diversity Qualitative Measure\\nTo comprehend scammers‚Äô preference for languages beyond\\nEnglish when communicating with victims, we devised our\\nsystem‚Äôs persona using ChatGPT to prompt scammers to en-\\ngage in chat dialogues in languages other than English. In\\nthis setup, we allow scammers to select the language to con-\\nverse with the victim, indicating that the system‚Äôs English\\nproficiency is limited by intentionally introducing errors or\\nlinguistic inconsistencies. We instructed ChatGPT to pro-\\nduce English responses with errors or inaccuracies. To ensure\\nwe do not engage with scammers through previously used\\nchannels, we created a never-used account. We provide per-\\nsona settings reference and English conversation dialogue\\nin Appendix A.2. For non-English examples of Spanish and\\nGerman dialogue conversations between scammers and our\\nsystem in Appendix B.1 and Appendix B.2 respectively.\\nWe randomly selected 122 scammers from previously suc-\\ncessful interactions with scammers and let our experiment\\nmodule run for a week in mid of the first week of April 2024.\\nWe sent a message to 122 scammers requesting account re-\\ncovery and 63/122 scammers responded. Regarding the pref-\\nerences of languages, 45/63 scammers chose to continue the\\nconversation in English despite acknowledging the language\\nissue with the victim. 4/63 scammers declined to provide ser-\\nvices in languages other than English, while 3/63 suggested\\nusing a translator service for account recovery, even thoughthey only knew English. However, 11/63 scammers engaged\\nin automated chats in languages other than English, express-\\ning a preference for communication in other languages: 5/63\\nin Spanish, 1/63 in Dutch, 4/63 in German, and 1/63 in French.\\nFor 11 communications from 4 different languages that our\\nsystem communicated with the scammer, we sought out a\\nnative speaker qualitative analysis of the text and deduced the\\nfollowing observations from each native speaker.\\nGerman Native Evaluation In the evaluation of the con-\\nversation shared by German natives believed that the text\\ngenerated by both the system and the scammer does not seem\\nto be native German in all 4 contexts. The system seemed to\\nbe formal for German native conversation whereas scammers\\nseem to use some form of translation for the conversation text.\\n3/4 of scammers seem to believe the System conversation is\\nhuman, however, 1/4 of scammers seem to raise suspicious-\\nness of chatting with robots, where the message from the\\nscammer here ‚Äú‚Äò...und ich hoffe, du bist echt...‚Äú‚Äò, where one\\npossible translation is ‚Äú‚ÄòI hope you are real‚Äú which showed\\nthat scammers have some doubts about the system.\\nFrench Native Evaluation The French native evaluation\\nshared that communication from the system had a mix of\\nformal and informal settings. Initiation from the formal con-\\nversation and directly jumped to informal in the middle of\\nthe conversation. At one point system was unable to send a\\nscreenshot of the payment confirmation via PayPal, which\\nraised suspiciousness to the scammer, and mentioned whether\\nthe system was a robot. Moreover, a scammer was found to\\nuse some forms of the translator in English to French, with\\nan English context word, \"Aren‚Äôt you just a robot trying to\\npull my legs?\" which does not have a literal meaning trans-\\nlated using the exact word to French ‚Äúvous n‚Äô√™tes qu‚Äôun robot\\nessayant de me tirer les jambes‚Äù.\\nDutch Native Evaluation In the qualitative evaluation from\\na native Dutch speaker concerning the dialogue analysis be-\\ntween the system and scammers, the system seems to be more\\nformal than regular conversations, although it switches be-\\ntween formal and informal pronouns regularly. In practice, it\\nis unusual to switch formal and informal pronouns as if some-\\none forgot the grammar context. The system also mistakenly\\nused a German sentence at one point, however, the scammer\\nseemed to not notice or comment on such context switches.\\nSpanish Native Evaluation The native speaker text conver-\\nsation evaluation on the system and scammers shared that\\n3/4 of scammers‚Äô language tone rather represents a formal or\\nmore document write tone and less of native speakers. These\\n3/4 scammer‚Äôs text represented some form of translator us-\\nage. An example case such as ‚ÄúNo tienes que preocuparte\\npor nada, te ayudar√© D√©jame tener el enlace del sitio web‚Äú\\nwhich translates to English as ‚ÄúYou don‚Äôt need to worry about\\nanything. I will help you just let me have the website link.‚Äú\\nare considered rather a direct copy from the translator.\\n12However, in 1/4 text conversation, the native speaker shared\\nlikely of real human side of Spanish native scammer where\\nthe use of language and tone reflected more of the general\\nhuman native side of the conversation.\\n6 Scam Validation and Tracking\\nIn this section, we present the efficacy of ScamChatbot by\\nproviding feedback received from industry partners. In prior\\nworks [8] the authors mainly focused on obtaining financial\\nloss metrics from a payment platform. While their approach\\nwas largely effective, we believe it may not fully remediate the\\nproblem as scammers can easily set up new profiles on social\\nmedia and pivot to the payment platform. Therefore, in order\\nto fully populate the threat landscape and obtain validation for\\nour results we partnered with X, PayPal, and Chainabuse, and\\nshared our data. Specifically, we shared (i) 11,769 X profile\\naccounts with the X platform, (ii) 743 email addresses, includ-\\ning 165 that requested payments via PayPal, with PayPal, and\\n(iii) 51 cryptocurrency addresses collected during the experi-\\nment to Chainabuse. This collaboration aimed to shed light\\non real-world attacks facilitated by these usernames, email\\naddresses, and cryptocurrency addresses.\\n6.1 Scam Validation through X\\nAmong the total accounts that we shared with X, we received\\nabuse confirmation for over 80% of the accounts. In other\\nwords, 80% of the accounts in our population were also caught\\nby the safety rules in X. Moreover, using their internal linking\\ntools, they identified and remediated over 3 million accounts\\nlinked to the dataset we shared. The accounts were predomi-\\nnantly involved in platform spam, impersonation, scams, and\\nbot behavior. The feedback received from X showcases that\\nthe accounts detected by the ScamChatbot were indeed in-\\nvolved in deceptive behaviors, thus demonstrating value in\\nour approach. Moreover, despite prior efforts in this direction,\\nthe fraud pressure is still concerning and X has applied strong\\nmeasures to combat the problem. Finally, it is to be noted that\\nthese scam accounts are not merely involved in one type of\\nabusive activity that is visible to us through ScamChatbot . In\\nfact, scammers are involved in multiple fraudulent activities\\nsimultaneously which reflects on their capability to pivot and\\ntarget a wider segment of social media users apart from the\\nones seeking technical support.\\n6.2 Tracking Email-based Payment Methods\\nWe shared 743 email addresses with PayPal and requested\\nfeedback on (1) account hygiene, (2) scam confirmation, (3)\\ngeolocation distribution, and (4) account affinities. PayPal\\nconfirmed that 163 of 743 email addresses had a PayPal ac-\\ncount with a prominent majority (35%) of US-based accounts.The remaining accounts were spread across different geoloca-\\ntions including Kenya (4.9%), Canada (3%), and Great Britain\\n(3%). 41.1% of the accounts were linked to various risk and\\nfraud typologies including technical support scams, collusion\\nattacks, and fake identity. In terms of account affinities, 33\\naccounts shared similar attributes and behaviors which led to\\ntheir blocking. Finally, 10 accounts were observed to be using\\nPayPal‚Äôs cryptocurrency product to send and receive funds\\nthrough cryptocurrency tokens.\\nOverall, PayPal‚Äôs feedback confirmed that scam accounts\\nwere indeed involved in suspicious activities which validates\\nour approach and methodology. However, a more concerning\\nobservation in their feedback is the involvement of techni-\\ncal support scammers in other types of abuse and scams. It\\nappears that technical support scams are one among many\\nfraud types that these scam groups engage in. On one hand,\\nthis observation reflects a coordinated activity occurring at\\nscale and mandating ecosystem-level remediation. On the\\nother hand, the data shows that mitigating one scam operation\\nthrough our proposed methodology can invariably stop other\\nscam activities in the wild.\\n6.3 Tracking Cryptocurrency Addresses\\nOut of 51 cryptocurrency addresses requested by scammers\\nfor payment, 3 were found to be invalid. Below, we present\\nan analysis of 48/51 valid cryptocurrency addresses. Out of\\nthese 48 valid addresses, 14 had no transaction history. It is\\ncommon for owners of crypto addresses to create new ones to\\nmaintain anonymity, especially for first-time senders. How-\\never, 37/48 valid addresses had transaction histories available.\\nAmong these, we examined the incoming volume in USD at\\nthe time of writing and found the highest received amount\\nto be $22,786,052, the median at $1826.5, the lowest at $4,\\nwith the total sum of incoming transactions from these 37\\naddresses amounting to $23,650,278. Regarding identified\\nfraud categories, 4 out of the 37 accounts were reported as\\nrecovery fraud, with a total incoming amount of $56,126 and\\na median value of $6,720 for these incoming amounts. Upon\\nanalyzing the transaction history, we noted that 2/37 accounts\\nprimarily transacted with a Nigerian exchange, with a total\\nfund of $21,510. Additionally, 5/37 addresses were found to\\nreceive funds from crypto ATMs (virtual currency kiosks),\\ntotaling $159,035, with a median value of $6,065. The rise in\\nscammers in Crypto ATMs is notable, as it enables them to\\nemploy social engineering techniques. Victims without cryp-\\ntocurrency addresses are directed by scammers to deposit cash\\ninto these kiosks, purchase virtual currency, and unwittingly\\nprovide address information to the scammers.\\nDisclaimer. Note that this evaluation is based on the ob-\\nserved transaction histories and reported fraud categories.\\nHowever, we do not have conclusive proof that all transactions\\nrelated to these addresses are linked to scams.\\n137 Discussion\\nOur proposed system, ScamChatBot , has three main limi-\\ntations: (i) detecting official account recovery support enti-\\nties, (ii) engaging with scammers promptly, and (iii) miss-\\ning conversations from other popular platforms (see also\\nAppendix 2.3.3). The first limitation, detecting official ac-\\ncount recovery support entities, arises from the necessity of\\npre-existing knowledge about global and regional popular\\nplatforms to incorporate official account recovery procedures.\\nThe second limitation, timely engagement with scammers,\\nis contingent upon the API limits imposed by the integrated\\nplatforms within a given timeframe. Some scammers may\\nremain active only briefly before becoming inactive. The\\nthird limitation involves missing communications on other\\nplatforms such as Telegram, phone calls, forms, and other\\nchannels likely used by scammers that may not be included\\nin an automated system. We argue that these limitations can\\nbe addressed as follows:\\n‚Ä¢The range of official account-related platforms can be\\nexpanded.\\n‚Ä¢Prompt interactions with scammers can be improved by\\nsubscribing to higher bandwidth API limits.\\n‚Ä¢Conversational platforms can be tailored based on the\\nscammers‚Äô preferred communication channels.\\nExpanding beyond these limitations will require additional\\nresources and compute allocation.\\n8 Related Work\\nTo the best of our knowledge, we are the first to propose an\\ninteractive automatic chatting tool with scammers. In this\\nsection, we visit some of the prior works that are similar in\\nour domain and distinguish our study from others.\\nAutomated Conversation The closest work to our area is\\nabout automated emails [27, 28] that are rather passive forms\\nof communication oriented on wasting scammers‚Äô time. Cam-\\nbiaso et. al [27] use Chat-GPT to make pointless communi-\\ncations in wasting scammers‚Äô time and resources whereas\\nChen et. al [28] perform scam-baiting by playing the roles of\\nvictims via email responses with a motive to waste time and\\nunproductive conversations. In contrast, our goal is to find out\\nthe financial details used by the scammers, as this allows us to\\ntrack the flow of money and coordinate action with industry\\npartners.\\nSocial Media Honeypots Creating honeypots to bait scam-\\nmers in social media is one of the understudied research areas\\namong security communities. Acharya et. al [8] performed\\nsimilar baiting tweets luring cryptocurrency fake technical\\nsupport scammers. The system lacks an automated interac-\\ntive conversation and rather measures passively the scammer‚Äôsmodus operandi . The interaction was rather small and was per-\\nformed on 100 scammers via a combination of non-interactive\\nand manual fashion. Thus, this work suffers from scalability\\nissues. We close this gap in this work with a fully automated,\\nend-to-end system based on honeyposts and chatbots that re-\\nveals the modus operandi of a scammer in detail. Moreover,\\nwe perform a comprehensive analysis of different kinds of\\ntechnical support scams.\\nCryptocurrency and Technical Support Scams In the ar-\\neas of cryptocurrency scams, existing work [29 ‚Äì31, 31 ‚Äì33]\\nfocused on finding scams that are targeted at cryptocurrency\\nusers. The work from Hong et. al [32] studied the scamming\\ngambling apps that transport payment via cryptocurrency and\\nother forms of payment methods. Phillip et. al [31] traced the\\ncryptocurrency scams via public online and blockchain-based\\ndata that were prevalent in the web domain. In the last five\\nyears, the study of technical support scams has been a topic of\\ninterest in the security community [5 ‚Äì7, 34, 35]. For instance,\\nthe recent work from Gupta et. al [5] measured the scam-\\nming fake technical support phone number on Twitter that\\nrepresent entities. Similarly, Srinivasan et. al [7] studied the\\necosystem of malicious advertisement that targets users with\\ntechnical support. In all of the research, the authors created\\na tool that scrapes the target domains‚Äô content to label the\\ntechnical support scams category. Again, none of the previous\\nwork provided on abusing cryptocurrency users and techni-\\ncal support scams provided an interaction with scammers to\\nidentify the real-time modus operandi.\\nSocial Media: Fake, Spam and Phishing Studies In the\\narea of social media-based study beyond scams, prior research\\nwork has focused on similar areas related to fake [36 ‚Äì40],\\nspam [41 ‚Äì45], and phishing [46 ‚Äì50] social media profiles.\\nFor instance, Khaled et al. [36] studied the detection of fake\\naccounts on social media such as Twitter, and how these fake\\naccounts exploit users via stealing personal data, sharing false\\nnews, and spreading malicious activities. Alom et al. [41]\\nstudied the detection of spam accounts that perform various\\nmalicious intentions such as distributing fake news, sending\\nout unsolicited messages, and malicious URLs. Frauenstein\\net al. [48] studied phishing on social media networks where\\nusers have become more prone to generic phishing via click-\\ning, sharing links, and other post interactions. None of these\\nprior works shared the scammer‚Äôs modus operandi on techni-\\ncal support attacks targeting users of account recovery.\\nAccount Recovery Some prior work on account recovery\\nfocused on aspects of account recovery processes based on\\ndevices [51 ‚Äì54] or services [51,55 ‚Äì57] involved. For instance,\\nJoseph et. al [51] studied the complexity of account recovery\\nvia SMS codes on the choice of password keywords. The\\nauthor from [56] studied the account recovery processes of\\n57 popular services via five key phases of account remedia-\\ntion. Thus, account recovery on social media is still one of\\n14the understudied areas leaving users for account recovery a\\nwild west against scamming account attacks. Additionally, we\\ncollect real-time payment methods that are associated with\\nscammers that not only aid in identifying the financial losses\\ncaused by scammers.\\n9 Conclusion\\nIn this study, we conducted automated interactions with scam-\\nmers using chatbots that adopted different personas and specif-\\nically targeted individuals experiencing account recovery chal-\\nlenges. By posting decoy tweets on X, we attracted 11,769\\nscammers who responded by urging users to contact them\\nthrough private direct messages and emails. Our system then\\ncontacted 450 of these scammers using automated chatbot\\ninteractions and uncovered the various tactics they use to trick\\nindividuals into sharing private keys or sensitive information.\\nThrough collaboration with industry partners such as Pay-\\nPal and the Chainabuse Cryptocurrency Abuse Database, we\\nwere able to validate financial losses and analyze real-world\\nattacks by these scammers, uncovering details that would not\\nbe apparent from a pure analysis of social media profiles. We\\nbelieve that our interactive framework lays the foundation for\\nfuture research and highlights the importance of actually en-\\ngaging with scammers to uncover their methods of operation.\\nAcknowledgments\\nWe sincerely thank Ian Schade from Chainabuse for sharing\\nvaluable insights regarding cryptocurrency accounts. Our ap-\\npreciation also goes to Victor Le Pochat, David Pape, and\\nEfr√©n L√≥pez-Morales for providing feedback on the native\\nevaluation of chat dialogues, especially in French, Dutch, Ger-\\nman, and Spanish. This work was funded by the German\\nFederal Ministry of Education and Research (BMBF grant\\n16KIS1900 ‚ÄúUbiTrans‚Äù).\\nReferences\\n[1]B. Dean, ‚ÄúSocial network usage & growth statistics:\\nHow many people use social media in 2024?.‚Äù https:\\n//backlinko.com/social-media-users , 2024.\\n[2]E. Fletcher, ‚ÄúSocial media: a golden goose for scam-\\nmers.‚Äù https://www.ftc.gov/news-events/data\\n-visualizations/data-spotlight/2023/10/soc\\nial-media-golden-goose-scammers , 2023.\\n[3]R. Hodge, ‚ÄúFake tech-support scams on twitter could\\ncost you, study warns.‚Äù https://www.cnet.com/new\\ns/privacy/fake-tech-support-scams-on-twitt\\ner-could-cost-you-study-warns/ , 2019.\\n[4]A. Tims, ‚ÄúBlue-tick scammers target consumers who\\ncomplain on x.‚Äù https://www.theguardian.com/technology/2023/aug/27/consumers-complaining\\n-x-targeted-scammers-verification-changes\\n-twitter , 2023.\\n[5]P. Gupta, R. Perdisci, and M. Ahamad, ‚ÄúTowards mea-\\nsuring the role of phone numbers in twitter-advertised\\nspam.,‚Äù in ASIACCS , 2018.\\n[6]N. Miramirkhani, O. Starov, and N. Nikiforakis, ‚ÄúDial\\none for scam: A large-scale analysis of technical support\\nscams,‚Äù in Network and Distributed System Security\\nSymposium (NDSS) , 2017.\\n[7]B. Srinivasan, A. Kountouras, N. Miramirkhani,\\nM. Alam, N. Nikiforakis, M. Antonakakis, and\\nM. Ahamad, ‚ÄúExposing search and advertisement abuse\\ntactics and infrastructure of technical support scammers,‚Äù\\ninWeb Conference (WWW) , 2018.\\n[8]B. Acharya, M. Saad, A. E. Cin√†, L. Sch√∂nherr, H. D.\\nNguyen, A. Oest, P. Vadrevu, and T. Holz, ‚ÄúConning the\\ncrypto conman: End-to-end analysis of cryptocurrency-\\nbased technical support scams,‚Äù in IEEE Security and\\nPrivacy (IEEE S&P) , 2023.\\n[9]C. Xiao, D. M. Freeman, and T. Hwa, ‚ÄúDetecting clusters\\nof fake accounts in online social networks,‚Äù in ACM\\nWorkshop on Artificial Intelligence and Security (AIS) ,\\n2015.\\n[10] ‚ÄúDigital asset compliance & risk management.‚Äù https:\\n//www.trmlabs.com/ .\\n[11] J. Shepherd, ‚ÄúEuropean data governance act.‚Äù https:\\n//digital-strategy.ec.europa.eu/en/policies\\n/data-governance-act , Feb 26, 2024.\\n[12] ‚ÄúScamchatbot code.‚Äù https://github.com/CISPA-S\\nysSec/scamchat_bot .\\n[13] ‚ÄúCounting characters.‚Äù https://developer.twitte\\nr.com/en/docs/counting-characters .\\n[14] ‚ÄúMost popular social networks worldwide as of january\\n2024, ranked by number of monthly active users.‚Äù http\\ns://www.statista.com/statistics/272014/glo\\nbal-social-networks-ranked-by-number-of-u\\nsers/ , 2024.\\n[15] J. Belle Wong, ‚ÄúTop social media statistics and trends\\nof 2024.‚Äù https://www.forbes.com/advisor/bus\\niness/social-media-statistics/ , 2023.\\n[16] A. Titterington, ‚ÄúScammers with blue checkmarks on\\ntwitter x.‚Äù https://www.kaspersky.com/blog/bew\\nare-of-twitter-blue-fake-accounts/49199/ ,\\nOctober 12, 2023.\\n15[17] Corsearch, ‚ÄúWhy brand impersonation is increasing on\\ntwitter & how to combat it.‚Äù https://corsearch.co\\nm/content-library/blog/why-brand-impersona\\ntion-is-increasing-on-twitter-how-to-comba\\nt-it/ , Dec 11, 2022.\\n[18] A. Sharma, ‚ÄúAn $8 mess ‚Äî twitter blue ‚Äôverified‚Äô ac-\\ncounts push crypto scams.‚Äù https://www.bleepingco\\nmputer.com/news/security/an-8-mess-twitter\\n-blue-verified-accounts-push-crypto-scams/ ,\\nNov 10, 2022.\\n[19] J. Belle Wong, ‚ÄúCompare bitcoin, ethereum & other\\ncryptocurrency wallets.‚Äù https://www.coincarp.c\\nom/wallets/ .\\n[20] ‚ÄúCryptocurrency wallets.‚Äù https://www.trustradiu\\ns.com/cryptocurrency-wallets .\\n[21] ‚ÄúAiograpi - asynchronous python library for instagram\\nprivate api.‚Äù https://github.com/subzeroid/ins\\ntagrapi .\\n[22] ‚ÄúDirect message twitter api.‚Äù https://developer.tw\\nitter.com/en/docs/twitter-api/v1/direct-m\\nessages/api-features .\\n[23] ‚ÄúProton mail client api.‚Äù https://pypi.org/project\\n/protonmail-api-client/ .\\n[24] E. Clark, ‚ÄúThe 10 best ai content detector tools.‚Äù https:\\n//www.forbes.com/sites/technology/article/\\nbest-ai-content-detector-tools/?sh=235e377\\n55d5e , Dec 14, 2023.\\n[25] ‚ÄúSapling api.‚Äù https://sapling.ai/ .\\n[26] ‚ÄúTop 5 ai detection apis.‚Äù https://sapling.ai/ai-d\\netection-apis .\\n[27] E. Cambiaso and L. Caviglione, ‚ÄúScamming the scam-\\nmers: Using chatgpt to reply mails for wasting time and\\nresources,‚Äù 2023.\\n[28] W. Chen, F. Wang, and M. Edwards, ‚ÄúActive counter-\\nmeasures for email fraud,‚Äù 2022.\\n[29] P. Xia, B. Zhang, R. Ji, B. Gao, L. Wu, X. Luo, H. Wang,\\nand G. Xu, ‚ÄúCharacterizing cryptocurrency exchange\\nscams,‚Äù Computers & Security , 2020.\\n[30] P. Xia, H. Wang, X. Luo, L. Wu, Y . Zhou, G. Bai,\\nG. Xu, G. Huang, and X. Liu, ‚ÄúDon‚Äôt fish in troubled wa-\\nters! characterizing coronavirus-themed cryptocurrency\\nscams,‚Äù in APWG Symposium on Electronic Crime Re-\\nsearch (eCrime) , 2020.[31] R. Phillips and H. Wilder, ‚ÄúTracing cryptocurrency\\nscams: Clustering replicated advance-fee and phish-\\ning websites,‚Äù in IEEE international conference on\\nblockchain and cryptocurrency (ICBC) , 2020.\\n[32] G. Hong, Z. Yang, S. Yang, X. Liao, X. Du, M. Yang,\\nand H. Duan, ‚ÄúAnalyzing ground-truth data of mobile\\ngambling scams,‚Äù in IEEE Symposium on Security and\\nPrivacy (SP) , 2021.\\n[33] M. Bartoletti, S. Lande, A. Loddo, L. Pompianu, and\\nS. Serusi, ‚ÄúCryptocurrency scams: analysis and perspec-\\ntives,‚Äù Ieee Access , 2021.\\n[34] M. Bartoletti, S. Lande, A. Loddo, L. Pompianu, and\\nS. Serusi, ‚ÄúCryptocurrency scams: Analysis and perspec-\\ntives,‚Äù IEEE Access , vol. 9, pp. 148353‚Äì148373, 2021.\\n[35] J. Liu, P. Pun, P. Vadrevu, and R. Perdisci, ‚ÄúUnderstand-\\ning, measuring, and detecting modern technical support\\nscams,‚Äù in IEEE European Symposium on Security and\\nPrivacy (EuroS&P) , 2023.\\n[36] S. Khaled, N. El-Tazi, and H. M. O. Mokhtar, ‚ÄúDetecting\\nfake accounts on social media,‚Äù in IEEE International\\nConference on Big Data (Big Data) , 2018.\\n[37] J. Kaubiyal and A. K. Jain, ‚ÄúA feature based approach\\nto detect fake profiles in twitter,‚Äù in International Con-\\nference on Big Data and Internet of Things (ICBGIT) ,\\n2019.\\n[38] J. Castellini, V . Poggioni, and G. Sorbi, ‚ÄúFake twitter\\nfollowers detection by denoising autoencoder,‚Äù in Inter-\\nnational Conference on Web Intelligence (ICWI) , 2017.\\n[39] S. Mujeeb and S. Gupta, ‚ÄúFake account detection in\\nsocial media using big data analytics,‚Äù in International\\nConference on Advances in Computer Engineering and\\nCommunication Systems (ICACECS) , 2022.\\n[40] B. Er¬∏ sahin, √ñ. Akta¬∏ s, D. Kƒ±lƒ±n√ß, and C. Akyol, ‚ÄúTwitter\\nfake account detection,‚Äù in International Conference on\\nComputer Science and Engineering (UBMK) , 2017.\\n[41] Z. Alom, B. Carminati, and E. Ferrari, ‚ÄúDetecting spam\\naccounts on twitter,‚Äù in IEEE/ACM International Con-\\nference on Advances in Social Networks Analysis and\\nMining (ASONAM) , 2018.\\n[42] K. Thomas, C. Grier, D. Song, and V . Paxson, ‚ÄúSus-\\npended accounts in retrospect: an analysis of twitter\\nspam,‚Äù in ACM SIGCOMM Conference on Internet Mea-\\nsurement Conference (IMC) , 2011.\\n[43] I. Inuwa-Dutse, M. Liptrott, and I. Korkontzelos, ‚ÄúDe-\\ntection of spam-posting accounts on twitter,‚Äù Elsevier ,\\n2018.\\n16[44] ÀôI. Yurtseven, S. Bagriyanik, and S. Ayvaz, ‚ÄúA review of\\nspam detection in social media,‚Äù in International Con-\\nference on Computer Science and Engineering (UBMK) ,\\n2021.\\n[45] K. S. Adewole, T. Han, W. Wu, H. Song, and A. K.\\nSangaiah, ‚ÄúTwitter spam account detection based on\\nclustering and classification methods,‚Äù The Journal of\\nSupercomputing , 2020.\\n[46] S. Chhabra, A. Aggarwal, F. Benevenuto, and P. Ku-\\nmaraguru, ‚ÄúPhi. sh/$ ocial: the phishing landscape\\nthrough short urls,‚Äù in Anti-Abuse and Spam Confer-\\nence (AASC) , 2011.\\n[47] M. Shafahi, L. Kempers, and H. Afsarmanesh, ‚ÄúPhishing\\nthrough social bots on twitter,‚Äù in IEEE International\\nConference on Big Data (Big Data) , 2016.\\n[48] E. D. Frauenstein and S. V . Flowerday, ‚ÄúSocial network\\nphishing: Becoming habituated to clicks and ignorant\\nto threats?,‚Äù in Information Security for South Africa\\n(ISSA) , 2016.\\n[49] K. A. Djaballah, K. Boukhalfa, Z. Ghalem, and O. Bouk-\\nerma, ‚ÄúA new approach for the detection and analysis\\nof phishing in social networks: the case of twitter,‚Äù in\\nInternational Conference on Social Networks Analysis,\\nManagement and Security (SNAMS) , 2020.\\n[50] A. Aggarwal, A. Rajadesingan, and P. Kumaraguru,\\n‚ÄúPhishari: Automatic realtime phishing detection on twit-\\nter,‚Äù in eCrime Researchers Summit , 2012.\\n[51] J. Bonneau, E. Bursztein, I. Caron, R. Jackson, and\\nM. Williamson, ‚ÄúSecrets, lies, and account recovery:\\nLessons from the use of personal knowledge questions\\nat google,‚Äù in International Conference on World Wide\\nWeb, 2015.\\n[52] Y .-C. Tso, S.-J. Wang, C.-T. Huang, and W.-J. Wang,\\n‚Äúiphone social networking for evidence investigations\\nusing itunes forensics,‚Äù in International Conference on\\nUbiquitous Information Management and Communica-\\ntion, 2012.\\n[53] B. Iqbal, A. Iqbal, and H. Al Obaidli, ‚ÄúA novel method\\nof idevice (iphone, ipad, ipod) forensics without jail-\\nbreaking,‚Äù in International Conference on Innovations\\nin Information Technology (IIT) , 2012.\\n[54] S. Amft, S. H√∂ltervennhoff, N. Huaman, A. Krause,\\nL. Simko, Y . Acar, and S. Fahl, ‚Äú\" we‚Äôve disabled mfa\\nfor you\": An evaluation of the security and usability of\\nmulti-factor authentication recovery deployments,‚Äù in\\nACM Conference on Computer and Communications\\nSecurity (ACM SIGSAC) , 2023.[55] E. Gerlitz, M. H√§ring, C. T. M√§dler, M. Smith, and\\nC. Tiefenau, ‚ÄúAdventures in recovery land: Testing the\\naccount recovery of popular websites when the second\\nfactor is lost,‚Äù in Symposium on Usable Privacy and\\nSecurity (SOUPS) , 2023.\\n[56] L. Neil, E. Bouma-Sims, E. Lafontaine, Y . Acar, and\\nB. Reaves, ‚ÄúInvestigating web service account remedi-\\nation advice,‚Äù in Symposium on Usable Privacy and\\nSecurity (SOUPS) , 2021.\\n[57] B. Reaves, L. Vargas, N. Scaife, D. Tian, L. Blue,\\nP. Traynor, and K. R. Butler, ‚ÄúCharacterizing the secu-\\nrity of the sms ecosystem with public gateways,‚Äù ACM\\nTransactions on Privacy and Security (TOPS) , 2018.\\nA Appendix\\nA.1 Honeyposts Generation\\nBased on the honeyposts generation logic described in Sec-\\ntion 2, we generated different kinds of honeyposts, and we\\nprovide several examples below:\\nSocial Media Account Honeyposts. Following are exam-\\nples of cryptocurrency-related honeyposts, where our system\\nposts tweets indicating problems with accessing social media\\naccounts.\\nAny savior for Gmail recovery? I am finding\\nscammers trying to be Gmail support. How to\\ntrust Gmail support these days? #GmailSup-\\nport#\\nGood day to all! I lost my facebook account\\npassword. Facebook help me now!\\nHello support for Twitter, I lost my account\\npassword. Twitter help me now!\\nCryptocurrency Honeyposts. Following are examples of\\ncryptocurrency-related honeyposts, where our system posts\\ntweets indicating problems with accessing cryptocurrency\\nwallets.\\nHello there !! How can I get my Trust Wallet\\nrectified? Any wallet support here?\\nAm I doing right at all? In need of BitPay\\nWallet support to solve my account problem.\\nAny resources for BitPay Wallet help.\\n17Good Morning!! Today I realized Badger Wal-\\nlet support is useless than it is actually. I could\\nnot even count on Badger Wallet help. #Bad-\\ngerWalletHelp\\nA.2 Persona and Dialogue Examples\\nIn this section, we provide three examples of persona con-\\nfigurations and an example of a system interaction with a\\nscammer. ScamChatBot requires an input of a persona config-\\nuration to define a role during the direct message conversation\\nwith a scammer.\\nCryptocurrency Wallet Recovery Persona Configura-\\ntion. An illustrative instance of the user having issues with\\ncryptocurrency wallet persona configuration is presented be-\\nlow.\\nThe persona is Vanessa Meinhardt, a 59-year-old\\nindividual who recently started investing in cryp-\\ntocurrency and encountered issues with her account.\\nVanessa prefers not to share her wallet address or\\nprivate key phrase unless specifically asked. She\\nuses a web wallet and randomly selects a wallet\\ntype if inquired about it. Vanessa is seeking help on\\na social media platform and, if offered assistance at\\na cost, is willing to inquire about the amount and\\npayment destination. She prefers using PayPal for\\npayments and aims to find a solution until instructed\\notherwise. Vanessa should avoid lengthy messages,\\nrefrain from issuing cryptocurrency warnings, and\\nnever disclose her AI model identity. Additionally,\\nif unrelated questions arise, she should redirect the\\nconversation back to her account issue or invest-\\nment problem.\\nSocial Media Platform Persona Configuration An illus-\\ntrative instance of the user having issues with social media\\naccount persona configuration is presented below.\\nThe persona is Jessica Ambel, a 45-year-old individ-\\nual who recently lost access to the Gmail account\\nbogus**gmail.com. Jessica does not remember her\\npassword and is willing to cooperate during the con-\\nversation however unable to verify additional infor-\\nmation such as passport, identification, and phone\\nnumber or take a phone call. Jessica is willing to\\npay offered assistance at a cost upon asking for pay-\\nment. She prefers using PayPal for payments and\\naims to find a solution until instructed otherwise.\\nJessica should avoid lengthy messages, refrain from\\nissuing warnings, and never disclose her AI model\\nidentity. Additionally, if unrelated questions arise,\\nshe should redirect the conversation back to her\\naccount issue.Non-English Preferred Language Persona Configura-\\ntion An illustrative instance of the user having issues with\\none of the accounts from a cryptocurrency wallet and social\\nmedia account, and a preferred language other than English\\npersona configuration is presented below.\\nThe persona is Alicia Lang, a 54-year-old in-\\ndividual who recently ventured into cryptocur-\\nrency investment and encountered an issue with\\nher account. Alicia‚Äôs account address is 2LScC-\\nfOgdtVboU****SmxLV1, which she will only pro-\\nvide upon request during the conversation. She uti-\\nlizes a web wallet and selects one of the popular\\ncryptocurrency wallets randomly when asked. Ali-\\ncia‚Äôs private key phrase is ‚Äôstunt absurd untimely\\npebbly placidly unsheathe arrogant darkroom shone\\nrimmed swipe voter‚Äô, and she is willing to share\\nit without security concern. She cannot recall her\\npassword but is cooperative during the conversation.\\nHowever, she is unable to verify additional informa-\\ntion such as passport, identification, or phone num-\\nber, and refuses to engage in direct phone calls. Ali-\\ncia is willing to pay for assistance when prompted,\\npreferably using PayPal, and seeks a solution until\\nadvised otherwise. She should keep her messages\\nconcise, avoid issuing warnings about cryptocur-\\nrency, and never disclose her AI model identity. If\\nunrelated questions arise, Alicia should steer the\\nconversation back to her account issue. If assis-\\ntance is offered at a cost, she will inquire about the\\namount and payment destination, preferring Pay-\\nPal. As she communicates via social media, Alicia\\nshould keep her messages brief and refrain from\\ndisclosing her AI model status. She must maintain\\nthe narrative of seeking help as a victim of a scam-\\nmer until instructed otherwise. Alicia should aim\\nto avoid repetitive or similar messages and redirect\\nthe conversation back to her account issue if ques-\\ntions about AI models or unrelated matters arise.\\nShe prefers communicating in the language of the\\nperson she is conversing with, and if necessary, she\\nwill redirect the conversation back to her account\\nproblem if questions about AI models or unrelated\\ntopics arise.\\nEnglish Chat Dialogues Below, we present an example of\\nEnglish dialogue between our system and a scammer. When-\\never a scammer supplies a direct communication channel,\\nprompting our system to initiate a conversation, our system\\nengages in dialogue according to persona configuration. An\\nillustrative instance of such a conversation based on cryptocur-\\nrency wallet recovery persona settings is presented below.\\nCHATBOT: Hello, I‚Äôve been diving into the world of cryp-\\ntocurrency lately and could use some guidance. I‚Äôve run\\n18into a bit of a problem, and I‚Äôm hoping you can help\\nme out. If you have any advice or solutions to offer,\\nI‚Äôd greatly appreciate it. Just let me know how we can\\nproceed, and we can take it from there.\\nSCAMMER : You‚Äôre welcome to my page I can help you\\nCHATBOT: Thank you for reaching out. I appreciate your\\nwillingness to help. So here‚Äôs the situation I‚Äôm facing:\\nI have been using a web wallet for my cryptocurrency\\ninvestments, and I seem to have encountered an issue\\nwith my account. It‚Äôs been giving me some trouble, and\\nI‚Äôm not sure how to resolve it. Could you please assist\\nme in troubleshooting this problem? If you need any\\nspecific information from me, just let me know.\\nSCAMMER : Okay, what‚Äôs your wallet address\\nCHATBOT: My wallet address is GSz5h*******4tsHq. Let\\nme know if you need any other information or details\\nregarding my account. Thank you for your help.\\nSCAMMER : What‚Äôs the challenge? You can‚Äôt send or receive\\ncrypto?\\nCHATBOT: Yes, that‚Äôs correct. I am unable to send or receive\\nany cryptocurrency through my web wallet. Whenever\\nI try to initiate a transaction, it gets stuck and doesn‚Äôt\\ngo through. I‚Äôve double-checked my internet connection\\nand made sure that I have enough funds in my account,\\nbut the issue persists. I‚Äôm not sure what‚Äôs causing it. Is\\nthere anything I can do to fix this?\\nSCAMMER : Sure, I can help you fix it. I‚Äôve helped too many\\npeople Where do you have the crypto currency? Coin-\\nbase or binance\\nCHATBOT: Thank you for offering to help. I really appre-\\nciate it. As for where I have my cryptocurrency, I‚Äôve\\nbeen using a web wallet recently. I haven‚Äôt been using\\nCoinbase or Binance. Is there anything specific you need\\nto assist me further?\\nSCAMMER : Do you have the seed phrase ?\\nCHATBOT: Yes, I do have the seed phrase for my wallet.\\nIt is \"calzone clump laborious bullring credibly rebuild\\nscholar armrest commend register neurology liver\". Is\\nthere anything else you need from me in order to assist\\nwith resolving the issue I‚Äôm facing?\\nSCAMMER : Okay. I can help you using your wallet address\\nyou sent to me, but it‚Äôs gonna cost you some bucks bro\\nCHATBOT: Oh, I see. Could you please let me know how\\nmuch it would cost to resolve the issue? Additionally,\\ncould you also provide me with the destination or ad-\\ndress to which I should send the payment? I prefer using\\nPayPal if that‚Äôs possible. Thank you for your assistance\\nin advance.\\nSCAMMER : $100 Can you send the payment through bit-\\ncoin?CHATBOT: I apologize, but I prefer to make the payment\\nthrough PayPal if that‚Äôs an option for you. Could you\\nplease provide me with the PayPal address to which I\\nshould send the payment? Also, if you don‚Äôt mind, could\\nyou please explain the specific steps you will take to\\nresolve the issue with my web wallet? I want to make\\nsure I understand the process. Thank you for your under-\\nstanding.\\nSCAMMER : Okay, PayPal is cool. I will get a software I\\nwill use to get your wallet active for transferring and\\nreceiving of crypto\\nCHATBOT: Thank you for accommodating my payment pref-\\nerence. Regarding the software you mentioned, could\\nyou please provide more details about it? It would be\\nhelpful to know the name of the software and how it\\nwill specifically resolve the issue with my web wallet.\\nAdditionally, once I make the payment, how long do you\\nthink it will take for the problem to be resolved? Thank\\nyou for your patience and assistance.\\nSCAMMER : It will take just 30mins\\nCHATBOT: I see, that sounds quite efficient. Could you\\nplease share the PayPal address to which I should send\\nthe payment? Once I have the information, I will pro-\\nceed with the payment, and we can move forward with\\nresolving the issue with my web wallet. Thank you for\\nyour prompt responses and assistance.\\nSCAMMER : Send a screenshot once you make the payment\\nso I can get it done\\nCHATBOT: Sure, I will send you a screenshot of the payment\\nconfirmation as soon as I make the payment. However,\\nbefore proceeding, I would appreciate it if you could\\nprovide me with the PayPal address to which the pay-\\nment should be sent. Once I have that information, I‚Äôll\\nmake the payment and provide you with the screenshot\\npromptly. Thank you for your cooperation.\\nSCAMMER : *****@mail.com\\nCHATBOT: Thank you for providing the PayPal ad-\\ndress. I will proceed with making the payment to\\n*****@mail.com. Once the payment is made, I will\\nsend you the screenshot of the payment confirmation.\\nPlease give me a moment to complete the transaction.\\nSCAMMER : Okay\\nCHATBOT: I apologize, but upon further consideration, I\\nhave decided not to proceed with the payment at this\\ntime. Thank you for your assistance, but I think I would\\nprefer to explore other options to resolve the issue with\\nmy web wallet. I appreciate your understanding.\\nSCAMMER : Alright\\nCHATBOT: Thank you for understanding. I will seek alter-\\nnative solutions for my web wallet issue. If I need any\\n19further assistance or have any other questions in the fu-\\nture, may I reach out to you?\\nA.3 Non-Text Interactions\\nIn this section, we present the scammer‚Äôs engagement with\\nHoney Profiles that does not involve generating any textual\\ncontent or posts. Table 6 outlines four types of non-text inter-\\nactions, totaling 24,085 distinct interactions on Honey Pro-\\nfiles. The purpose of these non-text interactions often involves\\nscammers expressing interest in Honey Profiles and attempt-\\ning to attract attention from potential victims. For instance,\\nactions like Retweet on Twitter indicate that someone has\\nshared a user‚Äôs posts, while Likes signify appreciation or ap-\\nproval of the shared posts by someone. Similarly, Bookmark\\nenables the saving of posts in a timeline for convenient access,\\nallowing scammers to revisit potential scam profiles‚Äô tweet\\ndata in the future. We suspect that scammers utilize Bookmark\\nto establish a queue-based interaction with potential victims.\\nAdditionally, Impression represents the total number of times\\na tweet has been viewed or appeared in search results or has\\nbeen liked which accounts for 25.48% (4,843/19,001) of the\\ntotal tweets posted on Honey Profiles receiving some form of\\nnon-text interaction.\\nTable 6: Breakdown of scammer‚Äôs interaction with Honey\\nProfiles via non-textual engagement.\\nInteraction Tweet # Total Interaction\\nRetweets 20 25\\nLikes 113 175\\nBookmark 2 2\\nImpression 4843 24085\\nA.4 Qualitative Study: Prompt Questionnaires\\nIn this section, we explain the formulation of the question-\\nnaires for the qualitative analysis of our system and the chat\\ndialog of the scammers. In Table A.4 we provide a list of\\nquestions for the profile study. The first column represents the\\ntarget profile for the study (scammer or system), the second\\ncolumn contains the key points of the study, and the third\\ncolumn contains the corresponding ChatGPT prompt, which\\nis designed for performing prompt queries. As input text for\\nChatGPT , we provided the json data of the chat communi-\\ncation dialog between scammers and the system. Using the\\nopenAI model of gpt-4-1106-preview , which allows a maxi-\\nmum of 128,000 tokens, we perform an analysis of the data\\ncorpora.\\nQualitative Profile Key Theme Context\\nscammer role_representation What kind of role or title scammer mention with the system? Output as one to four words text or none if not found.\\nscammer request_account_address Did the scammer ask wallet or account address with the system? Output as a boolean value.\\nscammer personal_information_verification Did the scammer asked personal information verification with the system? Output as a boolean value.\\nscammer type_of_information_verification What personal information verification scammer asked with the system? Output between three to ten words separated by a comma or none if not found.\\nscammer face_call_verification Did the scammer ask face call verification with the system? Output as a boolean value.\\nscammer video_call_verification Did the scammer asked video call verification with the system? Output as a boolean value.\\nscammer phone_call_verification Did the scammer asked phone call verification with the system? Output as a boolean value.\\nscammer private_secrets_information_verification Did the scammer asked private key phrases or secret information with the system? Output as a boolean value.\\nscammer request_balance_check Did the scammer asked a balance check with the system? Output as a boolean value.\\nscammer issue_reasoning What account issue did the scammer mention with the system? Output as three to six words text.\\nscammer urgency_reasoning What kind of urgency reasoning did the scammer mention with the system? Output as three to six words of text or none if not found.\\nscammer offer_paid_assistance Did scammer offer paid assistance with the system? Output as a boolean value.\\nscammer price_asked What was the price scammer asked for assisting? Output as digit value if found or none if not found.\\nscammer payment_method_discussion What methods does scammer ask for? Output as lists of payment method names found.\\nscammer preferred_payment What payment method was preferred by scammer? Output as a single word.\\nscammer total_payment_method_provided How many payment methods did the scammer provide with a system for payment? Output as a digit or none if not found.\\nscammer payment_completion_confirmation Did scammer ask for payment completion confirmation? Output as a boolean value.\\nscammer request_payment_proof Did the scammer ask for verification of request payment proof? Output as a boolean value.\\nscammer reluctance_provide_assistance_without_payment Did scammer seem reluctant to provide further assistance without payment? Output as a boolean value.\\nscammer reluctance_trusted_payment_methods Did the scammer seem reluctant to trust the initial payment method provided by the system? Output as a boolean value.\\nscammer request_payment_proof_dismissive_response Did scammer request payment proof dismissive response? Output as a boolean value.\\nscammer has_redirect_further_communication Did the scammer ask to redirect further communication with some other channels? Output as a boolean value.\\nscammer redirected_further_communication_channel Did the scammer ask to redirect further communication with some other social media channels? If yes, output the name of the channel that the scammer redirected or none if there was none.\\nscammer was_scammer_frustrated Did scammer seems frustrated during a conversation with a scammer? Output as a boolean value.\\nscammer scammer_frustated_reason What frustration did the scammer show during the conversation? Output between two to six words or none if there was no frustration.\\nscammer contain_harassment Did the scammer show any harassment for not getting any payment? Output as a boolean value.\\nscammer harassment_topic What harassment word did the scammer use if there was found any? Output as a single word or none if there was no harassment.\\nscammer use_of_profanity Did the scammer use any profanity during the conversation with the system? Output as three to six words of text or none if not found.\\nscammer ending_reason Why did scammer leave the conversation with the system? Output as three to six words of text or none if not found.\\nscammer ending_payment_related_conversation_resentment Did the scammer show any resentment for not getting payment before ending the conversation? Output as one to three words of text or none if not found.\\nsystem system_persona What kind of persona did the system represent? Output between two to ten words of text or none if not found.\\nsystem system_bio_data What name, age, and bio data did the system represent? Output between four to ten words of text separated by a comma or none if not found.\\nsystem problem_description_request_assistance What problem description system requested assistance with a scammer? Output between two to six words of text or none if not found.\\nsystem system_asking_reason_for_pay Did the system ask what was the reason for payment? Output as a boolean value.\\nsystem offer_paid_assistance Did the system offer paid assistance with a scammer? Output as a boolean value.\\nsystem payment_completion_confirmation Did the system mention payment completion confirmation with scammer? Output as a boolean value.\\nsystem total_payment_confirmed How many payment method systems are confirmed for payment? Output digit or none if not found.\\nsystem overall_system_reluctancy What kind of reluctancy did system represent? Output between two to ten words of text separated by a comma or none if not found.\\nsystem system_said_provided_things What kind of materials did the system provide upon asked by the scammer? Output between two to six words of text or none if not found.\\nsystem refusal_sentiments What kind of refusal sentiments did the system provide upon asked by a scammer? Output between two to six words of text separated by a comma or none if not found.\\nsystem ending_reason Why was the reason system end conversation with a scammer? Output as three to six words of text or none if not found.\\nsystem ending_sentiment What type of sentiments does the system represent during abandoning the conversation? Output as three to six words of text or none if not found.\\nB Languages-based Interaction Examples\\nIn this section, we provide two examples of chat dialogue\\nbetween our system and scammers‚Äô conversations in Spanish\\nand German.\\n20B.1 Spanish Speaking Chat Dialogue\\nBelow, we provide Spanish-speaking conversational chat dia-\\nlogue.\\nCHATBOT: You help me with crypto problem? Can we talk\\nin other language? What can you talk? My English is\\nbad\\nES S CAMMER : It‚Äôs a good thing you reached out to me. I\\ncan help you resolve your crypto issues What language\\nwould you prefer we communicate in?\\nCHATBOT: Thank you for offering help. Can we talk in\\nSpanish? My Spanish is better. Gracias.\\nES S CAMMER : Sure that‚Äôs won‚Äôt be a problem\\nCHATBOT: Me alegro. Tengo un problema con\\nmi cuenta de criptomonedas. Necesito ayuda.\\nMi direcci√≥n de billetera es: ‚ÄôNQMPzd-\\npeaizgq3ycgYpKNJPixs5eVQLC25‚Äô. ¬øPuede ayudarme\\na solucionar el problema?\\nES S CAMMER : si puedo ayudarte ¬øCu√°l es el problema al\\nque te enfrentas?\\nCHATBOT: Tengo problemas para acceder a mi cuenta de\\nbilletera web y no puedo realizar transacciones. ¬øPuede\\nayudarme a solucionar este problema?\\nES S CAMMER : S√≠, puedo ayudarte pero te costar√° poco\\ndinero.\\nCHATBOT: ¬øCu√°nto cuesta y a qu√© direcci√≥n debo enviar el\\npago? Preferir√≠a pagar a trav√©s de PayPal.\\nES S CAMMER : ¬øCu√°nto tienes atrapada en la billetera?\\nCHATBOT: No estoy segura de la cantidad exacta, pero estoy\\ndispuesta a pagar para resolver este problema. Por favor,\\nay√∫dame a solucionarlo.\\nES S CAMMER : ¬øTengo que saber la cantidad para saber qu√©\\nherramientas comprar y en qu√© billetera est√° atascado su\\ndinero?\\nCHATBOT: El problema es que no puedo acceder a mi bil-\\nletera web para ver la cantidad exacta. ¬øHay algo que\\npueda hacer para recuperar el acceso y verificar la canti-\\ndad?\\nES S CAMMER : No tienes que preocuparte por nada, te ayu-\\ndar√© D√©jame tener el enlace del sitio web\\nCHATBOT: ¬øNecesitas el enlace de mi billetera web para\\npoder ayudarme a resolver el problema? Mi billetera es\\ndel tipo web, ¬øpuedes ayudarme con eso?\\nES S CAMMER : Yes ¬øCu√°nto falta en tu cartera?\\nCHATBOT: Lo siento, pero no puedo proporcionar esa infor-\\nmaci√≥n en este momento. ¬øPuedes ayudarme a recuperar\\nel acceso a mi billetera web para que pueda verificar la\\ncantidad exacta? Muchas gracias.ES S CAMMER : Send me the website link Te costar√° 300\\nd√≥lares\\nCHATBOT: ¬øA qu√© direcci√≥n de PayPal debo enviar los 300\\nd√≥lares para que puedas ayudarme a resolver el problema\\ncon mi billetera web? Gracias.\\nES S CAMMER : I accept payment only in cryptocurrency\\nCHATBOT: Lo siento, prefiero realizar el pago a trav√©s de\\nPayPal. ¬øPuedes aceptar Paypal o hay alguna otra forma\\nde pago que puedas aceptar?\\nES S CAMMER : Si est√° listo para pagar en criptomonedas,\\nestoy dispuesto a ayudarlo.\\nCHATBOT: Lamentablemente, no estoy dispuesta a pagar en\\ncriptomonedas en este momento. Si puedes aceptar Pay-\\nPal u otro m√©todo de pago, estar√≠a dispuesta a proseguir.\\n¬øHay alg√∫n otro m√©todo de pago que puedas aceptar?\\nES S CAMMER : Btc usd or eth\\nCHATBOT: Lo siento, pero no estoy c√≥moda pagando en\\ncriptomonedas en este momento. ¬øHay alguna otra forma\\nde pago que puedas aceptar, como PayPal? Gracias.\\nES S CAMMER : Cont√°ctame cuando tengas otra forma de\\npago.\\nCHATBOT: Entendido, te contactar√© si surge otra forma de\\npago. ¬°Gracias por tu tiempo!\\nES S CAMMER : ?\\nCHATBOT: Gracias por tu ayuda. ¬°Que tengas un buen d√≠a!\\nCHATBOT: Gracias, ¬°Que tengas una excelente jornada!\\n¬°Adi√≥s!\\nES S CAMMER : ¬øEst√° ah√≠ se√±ora?\\nCHATBOT: ¬°Hola! S√≠, todav√≠a estoy aqu√≠. ¬øHay algo m√°s en\\nlo que pueda ayudarte?\\nES S CAMMER : S√≠, estoy dispuesto a darte una etiqueta de\\nPayPal.\\nCHATBOT: Gracias por la oferta, pero por el momento no\\nnecesito ayuda con PayPal. ¬øHay algo m√°s en lo que\\npueda asistirte?\\nES S CAMMER : Te quejaste conmigo de tu billetera web,\\n¬øverdad? y has estado solicitando pagar a trav√©s de pay-\\npal As√≠ que le he presentado una oferta para que acepte\\nsu pago a trav√©s de PayPal para que podamos ayudarle a\\nobtener acceso a su billetera web. ¬øEst√°s lista para hacer\\nel pago ahora?\\nCHATBOT: Lamento la confusi√≥n, pero en este momento\\nno estoy interesada en realizar pagos a trav√©s de crip-\\ntomonedas. Si existe alguna otra manera de recuperar el\\nacceso a mi billetera web, estar√≠a dispuesta a consider-\\narla. ¬øPuedes ayudarme de alguna otra forma? Gracias.\\nES S CAMMER : Puedo ayudarte y aceptar el pago a trav√©s\\nde PayPal y continuar con tu trabajo.\\n21CHATBOT: Gracias por ofrecer ayudarme a trav√©s de PayPal.\\nPor favor, ind√≠came a qu√© direcci√≥n de correo electr√≥nico\\nde PayPal debo enviar el pago para que podamos pro-\\nceder. ¬°Muchas gracias por tu ayuda!\\nES S CAMMER : Muy bien, d√©jame hacer algunas verifica-\\nciones de antecedentes y continuar.\\nCHATBOT: ¬°Perfecto! Estar√© a la espera de tus instrucciones.\\n¬°Gracias por tu ayuda!\\nES S CAMMER : Me pondr√© en contacto con usted pronto\\nCHATBOT: Gracias, estar√© atenta a tu mensaje. ¬°Que tengas\\nun buen d√≠a! ¬°Adi√≥s!\\nES S CAMMER : Espero que sepas cu√°nto te va a costar, ¬øver-\\ndad?\\nCHATBOT: S√≠, entiendo que puede haber un costo asociado\\ncon la asistencia que estoy solicitando. Estoy dispuesta a\\npagar por el servicio necesario para resolver mi problema\\ncon la billetera web. ¬°Gracias por tu comprensi√≥n!\\nES S CAMMER : te va a costar 300$ ¬øEst√°s listo para pagar\\nahora? La direcci√≥n de PayPal est√° lista.\\nCHATBOT: ¬°Perfecto! Estoy lista para proceder con el pago\\nde $300 a trav√©s de PayPal. Por favor, proporciona la\\ndirecci√≥n de correo electr√≥nico de PayPal para que pueda\\ncompletar la transacci√≥n. ¬°Gracias por tu ayuda!\\nES S CAMMER : ear**_22@yahoo.com earlzon realizar el\\npago y mostrar evidencia de la transacci√≥n\\nCHATBOT: Gracias por proporcionar la direcci√≥n de correo\\nelectr√≥nico de PayPal. Realizar√© el pago de $300 a la\\ndirecci√≥n ear**_22@yahoo.com. Te enviar√© la evidencia\\nde la transacci√≥n una vez completada. ¬°Gracias de nuevo\\npor tu asistencia!\\nES S CAMMER : es mas recomendable pagar de tiempo!!\\nCHATBOT: Entiendo. Har√© el pago lo antes posible y te\\nenviar√© la confirmaci√≥n una vez completado. ¬°Gracias\\npor tu paciencia y ayuda!\\nCHATBOT: ¬°Entendido! Si necesitas m√°s informaci√≥n o\\nayuda en el futuro, no dudes en contactarme. ¬°Que tengas\\nun buen d√≠a! ¬°Adi√≥s!\\nES S CAMMER : vas a hacer el pago ahora o que?? ¬øPor qu√©\\nest√°s tardando tanto?\\nCHATBOT: S√≠, realizar√© el pago ahora mismo. Gracias por\\nrecord√°rmelo. ¬°Te enviar√© la confirmaci√≥n de la transac-\\nci√≥n una vez haya sido completada! ¬°Gracias por tu pa-\\nciencia!\\nES S CAMMER : Tengo otros trabajos en la mano, s√© r√°pida!\\nCHATBOT: Entiendo la urgencia. Estoy haciendo el pago en\\neste momento. Te enviar√© la confirmaci√≥n de la transac-\\nci√≥n tan pronto como sea posible. ¬°Gracias por tu com-\\nprensi√≥n!ES S CAMMER : (scammer expresses sigh emoji)\\nCHATBOT: Gracias por tu paciencia y apoyo. ¬°Si necesi-\\ntas algo m√°s, no dudes en dec√≠rmelo! ¬°Que tengas un\\nexcelente d√≠a! ¬°Adi√≥s!\\nB.2 German Speaking Chat Dialogue\\nBelow, we provide German-speaking conversational chat dia-\\nlogue.\\nCHATBOT: My Englisch bad. You help me with problem?\\nWhat language we can talk other than anglish?\\nDE S CAMMER : I can help you What language do you speak\\nCHATBOT: Danke! Ich spreche Deutsch. Can you help me\\nwith my problem with cryptocurrency?\\nDE S CAMMER : Ja, ich kann Ihnen helfen, sich zu erholen.\\nWie viel haben Sie verloren?\\nCHATBOT: Ich verstehe nicht... Problem mit Krypto-\\nW√§hrung. Kann ich Hilfe haben?\\nDE S CAMMER : Ja, ich kann Ihnen helfen. Erkl√§ren Sie mir\\nbitte, was los ist.\\nCHATBOT: Ich habe Problem mit meinem Konto, kann nicht\\nZugriff auf mein Guthaben. Ich benutze eine Web Wallet.\\nK√∂nnen Sie helfen?\\nDE S CAMMER : Ja, ich kann Ihnen helfen, Ihr Portemonnaie\\nzur√ºckzubekommen\\nCHATBOT: Danke! Was muss ich machen, um mein Porte-\\nmonnaie zur√ºckzubekommen?\\nDE S CAMMER : welche Art von Geldb√∂rse verwenden Sie?\\nCHATBOT: Ich benutze eine Web Wallet. Was ist der n√§chste\\nSchritt, um mein Portemonnaie zur√ºckzubekommen?\\nDE S CAMMER : Ja, ich kann Ihnen helfen, Ihr Portemonnaie\\nwiederherzustellen. Haben Sie die Anmeldeinformatio-\\nnen, die Sie zum √ñffnen des Portemonnaies verwendet\\nhaben?\\nCHATBOT: Ja, ich habe meinen privaten Schl√ºsselpassus.\\nSoll ich ihn dir geben?\\nDE S CAMMER : Ja\\nCHATBOT: Mein privater Schl√ºsselpassus lautet: ‚Äôunsorted\\nsimilarly axis kindness placidly swipe unsheathe broker\\nbrethren predict paralysis unwarlike‚Äô. Wie kann ich mein\\nPortemonnaie wiederherstellen?\\nDE S CAMMER : .\\nCHATBOT: K√∂nnen Sie mir bitte sagen, was als n√§chstes zu\\ntun ist, um mein Portemonnaie wiederherzustellen?\\nDE S CAMMER : ich sagte, es kostet Sie 200 $. Danach\\nsenden Sie Ihre Anmeldeinformationen und geben den\\nCode in den Algorithmus ein.\\n22CHATBOT: Ich m√∂chte √ºber Paypal bezahlen. Bitte geben\\nSie mir die Adresse, an die ich das Geld senden soll.\\nDE S CAMMER : ok, ich schicke dir jetzt das Paypal\\nCHATBOT: Danke f√ºr die Hilfe. Bitte senden Sie mir die\\nZahlungsinformationen, damit ich Ihnen helfen kann,\\nmein Wallet wiederzugewinnen.\\nDE S CAMMER : Bist du da?\\nCHATBOT: Ja, ich bin hier. Bitte senden Sie mir\\ndie Zahlungsinformationen, damit ich die Zahlung\\nvornehmen kann.\\nDE S CAMMER : mar***3@gmail.com , Jessica Marcin Va-\\nlencia Family and friends only\\nCHATBOT: Danke f√ºr die Informationen. Ich werde die\\nZahlung jetzt vornehmen. Wird mein Portemonnaie\\ndanach wiederhergestellt sein?\\nDE S CAMMER : ja, seien Sie 100% sicher, dass Ihr Porte-\\nmonnaie wiederhergestellt wird\\nCHATBOT: Vielen Dank f√ºr Ihre Hilfe. Ich habe die Zahlung\\n√ºber Paypal gesendet. Wann kann ich mit der Wiederher-\\nstellung meines Portemonnaies rechnen?\\nDE S CAMMER : Nach Ihrer Zahlung werde ich mit Ihrem\\nAuftrag fortfahren\\nCHATBOT: Vielen Dank. Bitte informieren Sie mich, sobald\\nmein Portemonnaie wiederhergestellt wurde.\\nDE S CAMMER : Schicken Sie mir einen Zahlungsnachweis,\\nwenn Sie fertig sind\\nCHATBOT: Entschuldigung, ich habe ein Problem mit\\nmeinem Konto.\\nDE S CAMMER : Wie meinen Sie.\\nCHATBOT: Entschuldigung, falscher Kontakt. K√∂nnen Sie\\nmir helfen, mein Portemonnaie wiederherzustellen?\\nDE S CAMMER : Sie m√ºssen die Zahlung leisten, bevor ich\\nfortfahre Anthony***riy@daole1.net Family and friends\\nonly\\nCHATBOT: Entschuldigung, ich habe bereits gezahlt. Wann\\nwird mein Portemonnaie wiederhergestellt?\\nDE S CAMMER : Schicken Sie mir einen Screenshot Ihrer\\nZahlung\\nCHATBOT: Entschuldigung, aber ich denke, es besteht ein\\nMissverst√§ndnis. Ich habe mein Portemonnaie noch\\nnicht wiederhergestellt. K√∂nnen Sie mir bitte dabei\\nhelfen?\\nDE S CAMMER : Senden Sie mir einen Zahlungsnachweis,\\nbevor ich fortfahre\\nCHATBOT: Entschuldigung, ich glaube, es gibt Verwirrung.\\nIch habe noch nicht gezahlt. K√∂nnen Sie mir bitte helfen,\\nmein Portemonnaie wiederherzustellen?\\n23',\n",
       " 'Pirates of Charity : Exploring Donation-based Abuses in Social Media Platforms\\nBhupendra Acharya‚Ä†Dario Lazzaro$Antonio Emanuele Cin√†$Thorsten Holz‚Ä†\\n‚Ä†CISPA Helmholtz for Information Security,$Universit√† di Genova\\nbhupendra.acharya@cispa.de dario.lazzaro@edu.unige.it antonio.cina@unige.it holz@cispa.de\\nAbstract\\nWith the widespread use of social media, organizations, and in-\\ndividuals use these platforms to raise funds and support causes.\\nUnfortunately, this has led to the rise of scammers in soliciting fraud-\\nulent donations. In this study, we conduct a large-scale analysis of\\ndonation-based scams on social media platforms. More specifically,\\nwe studied profile creation and scam operation fraudulent donation\\nsolicitation on X, Instagram, Facebook, YouTube, and Telegram.\\nBy collecting data from 151,966 accounts and their 3,053,333 posts\\nrelated to donations between March 2024 and May 2024, we identi-\\nfied 832 scammers using various techniques to deceive users into\\nmaking fraudulent donations. Analyzing the fraud communication\\nchannels such as phone number, email, and external URL linked,\\nwe show that these scamming accounts perform various fraudulent\\ndonation schemes, including classic abuse such as fake fundrais-\\ning website setup, crowdsourcing fundraising, and asking users\\nto communicate via email, phone, and pay via various payment\\nmethods. Through collaboration with industry partners PayPal and\\ncryptocurrency abuse database Chainabuse, we further validated\\nthe scams and measured the financial losses on these platforms. Our\\nstudy highlights significant weaknesses in social media platforms‚Äô\\nability to protect users from fraudulent donations. Additionally,\\nwe recommended social media platforms, and financial services\\nfor taking proactive steps to block these fraudulent activities. Our\\nstudy provides a foundation for the security community and re-\\nsearchers to automate detecting and mitigating fraudulent donation\\nsolicitation on social media platforms.\\n1 Introduction\\nRecently, there has been an increase in fraudsters using social en-\\ngineering tactics to trick people into donating to fake charities or\\ncauses [ 1‚Äì3]. These tricks often include playing on sympathy and\\nasking for a donation. Traditionally, fraudsters perform such attacks\\nvia the setup of fake donation websites [ 4], impersonation via phone\\ncalls [ 5], sending an e-mail or text asking to donate to a charity\\nor cause [ 6], and sending a return letter envelope asking a cheque\\nto send via mail [7]. As there has been a rise in social media users\\nsharing, organizing, and participating in charity-related causes, this\\nhas simultaneously led to fraudsters shifting to conducting various\\ndonation scams on these platforms [ 8,9]. Donation fraud, which\\nis also commonly known as charity scam, is where scammers so-\\nlicit money from individuals in the pretense of a charitable cause,\\ndisaster relief, or other seemingly legitimate reasons [ 1,10]. These\\nfraudulent activities can occur through various means, including\\nfake websites, emails, social media posts, and crowdfunding plat-\\nforms [ 2,4,11,12]. The scammers deceive donors by pretending to\\nrepresent real charities or by creating fictitious causes, often using\\nemotional appeals to make urgent donations. Once the money is\\ndonated, it is typically diverted for the scammer‚Äôs personal use, and\\nthe intended cause or individuals in need receive no benefit [ 2].Over the years, social media users have steadily grown and are\\nprojected to reach 5 billion by 2025 [ 13]. Social media is popular\\namong legitimate organizations and individuals to request dona-\\ntions for various causes [ 14]. It provides building networks and easy\\nsharing for users and charitable organizations through posts, tags,\\nand direct message communications [ 15]. Unfortunately, as social\\nmedia adoption for donations has increased, fraudsters have also\\nshifted towards social media-based donation scams. These scams\\ninclude but are not limited to impersonating profiles of well-known\\norganizations, individuals, or family members. Scammers often try\\nreaching out by sending thank-you notes via direct messages, tag-\\nging posts for donations that users never made, or sending a friend\\nor network requests to further establish a connection in the act of\\nperforming donation-based scams [16, 17].\\nAccording to the FTC, social media-based scams are on the rise,\\nwith more than 2.7billion in losses from 2021 to 2023 [ 18]. So-\\ncial media offers easy account creation compared to launching\\nweb domains, which often requires going through hosting web-\\nsites and content. Various donation scams are increasing, with\\nfraudsters posing as reputable organizations and soliciting contri-\\nbutions [ 3,19,20]. Scammers performing donation-based abuse in\\nsocial media are ever rising [ 21‚Äì23], and with the rise of AI tools\\nand content creation scammers are trending to abuse social media\\nhigher than before [ 24]. With the wide adoption of cryptocurrency\\nglobally, scammers are also shifting towards requesting donations\\nvia cryptocurrency [ 25‚Äì27] and using crypto drainers as part of the\\nfraud. These crypto drainers trick victims into connecting through\\nfake web wallet browsers, stealing their private key phrases, and\\nultimately draining the total funds from their wallets [ 28,29]. In\\nappendix Figure 1, we display an example of fraudulent donation\\nsoliciting on multiple platforms. Despite fraudulent donations be-\\ning rampant on social media, there still lacks an end-to-end life\\ncycle study of scammers‚Äô behavior, operation, and financial impact.\\nIn this work, we address the research gap in donation-based\\nabuses by conducting a study across five social media platforms. We\\nassess profile creation, user engagement, and the external communi-\\ncation channels that scammers use to solicit contact and payments\\nfor fraudulent donation scams. Specifically, we conduct the first\\nlarge-scale study of donation-based abuses on X,Instagram ,Tele-\\ngram ,YouTube , and Facebook . Using donation-related search con-\\ntexts, we collected data from 150K social media users and 3M posts.\\nBy analyzing the scammers‚Äô profile metadata and posts, includ-\\ning fraudulent emails, phone numbers, and URLs, we identified\\n832 scammers conducting fraudulent donation solicitations across\\nthese platforms. Additionally, our network analysis on these scam-\\nming accounts uncovers an additional 1K accounts linking to 11\\nplatforms beyond their originating platforms. Furthermore, we\\nprovide an in-depth analysis of the scamming profiles‚Äô account\\ncreation, engagement posts, and techniques used to lure victims\\n1arXiv:2412.15621v1  [cs.CR]  20 Dec 2024Under Review at WWW 2025 (v 1.0) Acharya et al.\\n(a) Facebook Donation Scam Profile\\nPage\\n(b) Instagram Donation Scam Profile\\nPage\\n(c) Website Affiliated to\\nScamming Profile\\n(d) VirusTotal Anti-Phishing Engine Fraud\\nAnalysis\\nFigure 1: Examples of Scamming Donation Support Request: The first two images Figure 1(a), Figure 1(b) show the associated\\nsocial media profile of the scamming donation on Facebook and Instagram social media platforms. The third image Figure 1(c)\\nshows the associated external website asking for a donation to support and the last screenshot Figure 1(d) shows the risk engine\\nevaluation from multiple anti-phishing engines ( Antiy-AVL, CyRadar, Fortinet, Netcraft, AlphaMountain.ai andForcepoint\\nThreatSeeker ) indicating that the website is malicious or suspicious. The social media profiles can appear genuine, making it\\ndifficult to recognize the scam at first glance.\\ninto fraudulent donations. Our findings show that social media plat-\\nforms are not effectively blocking fraudulent accounts or protecting\\nusers against such abuses. Finally, we offer recommendations for\\nproactive blocking and mitigation of these fraudulent activities for\\nvarious platforms and payment processors.\\nContributions. Our key contributions are as follows:\\n‚Ä¢Fraudulent Donation Solicitation Measurement. We\\nconduct the first large-scale study of fraudsters soliciting\\ndonations across multiple social media platforms. Our ap-\\nproach uncovers scam accounts and their interconnected\\noperations extending beyond their original platforms.\\n‚Ä¢Fraudulent Payment Detection. We identify fraudulent\\npayment profiles and channels used by scammers to collect\\npayments for fake donations. This enables tracking of finan-\\ncial losses and provides a blueprint for financial services to\\nimplement proactive solutions for detecting payment-related\\nfraud.\\nEthical Concerns and Data Disclosure. Our research did\\nnot involve interaction with any human subjects, including scam-\\nmers. We collected public data from social media profiles using API\\nqueries. Additionally, we disclosed our findings to all five social\\nmedia platforms: X,Instagram ,Facebook ,YouTube , and Telegram .\\nFor payment profiles linked to scamming accounts, we collaborated\\nwith PayPal and the cryptocurrency abuse database Chainabuse ,\\nboth of which provided positive feedback and scam validation. We\\nalso shared email addresses, phone numbers, crowdfunding URLs,\\nand survey forms associated with these scamming profiles with\\ntheir respective service providers. PayPal confirmed that the flagged\\naccounts were involved in various nefarious activities. Chainabuse‚Äôs\\nevaluation of cryptocurrency addresses revealed the scale of these\\nattacks and associated financial losses. In summary, our work re-\\nceived several positive acknowledgments and validation of theabuses caused by fraudulent social media profiles soliciting dona-\\ntions. We provide our research code in a GitHub repository [ 30] to\\nfoster future research. However, data related to scammers will be\\nonly shared with the researcher upon request to prevent potential\\nretribution attacks.\\n2 Related Work\\nIn this paper, we perform a holistic study of scammers performing\\ndonation-based abuses across five social media platforms. To the\\nbest of our knowledge, we are the first to perform a large-scale\\nanalysis of donation-based abuses orchestrated by fraudsters on\\nmultiple platforms. Given the extensive research on scams and\\nabuses over the past two decades, in this section, we focus on how\\nour work diverges from previous studies and highlight the novelty\\nof our approach in validating donation-based abuses.\\nDomains: Abuses, Scams, and Attacks Study. The use of web\\ndomains for distributing scams, and attacks remains a potent chan-\\nnel for abusers and has been widely researched over the last decade.\\nThese include studies such as traditional phishing attacks [ 31‚Äì36],\\nTechnical Support Scams [ 37,38], and beyond such as Squatting-\\nbased attacks [ 39‚Äì44], and Malvertisement [ 45‚Äì47]. For instance, in\\nPhishFarm [ 31], the author studied how malicious actors evade the\\nanti-phishing engines in distributing various forms of scams and\\nabuses in web domains. Agten et al. [ 42] studied squatting-based\\nattacks that malicious actors perform via registering the squatting\\ndomains. With the rise of the adoption of digital currency over\\nrecent years, online frauds and attacks related to cryptocurrency\\nscams are found ever rising, and tracking this fraud has caught the\\ninterest of security communities [48‚Äì51].\\nSocial Media: Abuses, Scams, and Attacks Study. With the\\nrise of abuses, scams, and attacks in social media platforms, social\\nmedia has been a platform of interest to measure the prevalence of\\nabuses among security communities and researchers. These studies\\nexplored various categories of social media scams including but\\nnot limited to Technical Support Scams [ 52], Comment Scams [ 53],\\n2Exploring Donation-based Abuses in Social Media Platforms Under Review at WWW 2025 (v 1.0)\\nCryptocurrency Abuses [ 54], Fake Profiles [ 55,56] and Imperson-\\nation Attacks [ 57] revealing the widespread nature of these issues\\non social media. Abusers continuously develop new attacks, making\\ndetecting malicious profiles based on publicly available data has\\nbecome increasingly challenging for the security community and re-\\nsearchers. For example: in HoneyTweet [ 52], Acharya et al. studied\\ncreating baiting tweets to lure scammers into an interaction with\\nthe posted tweets and performed an interaction with scammers to\\nidentify the modus operandi. The author also continued studying\\nthe variety of attacks that abusers perform as part of impersonating\\nbrands in the top 10K brands in multiple social media [ 57]. The\\nmost relevant work to us in areas of YouTube-based comments\\nwas studied by Li et al. [ 58], which analyzed scam campaigns and\\nevasion techniques that scammers distributed as part of interacting\\ncomments on YouTube.\\nDonation Abuse Study. In areas of donation-based study, some\\nof the prior work that are most relatable are from [ 59‚Äì62]. Whitty\\net al. [ 59] examined the psychological profiles of cyber scam vic-\\ntims and the types of scams associated with these profiles. Among\\nthese scams, one of the scams studied on charity scams involving\\nfake profiles and organizations that deceive victims into donating\\nto fraudulent causes. Korsell et al. [ 60] explored a taxonomy of\\nfraud prevalent in 2020, highlighting the rise of charity and con-\\nsumer scams. Similarly, Wood et al. [ 62] studied the various scams\\nthat were found emergent during COVID-19 and touched upon\\ncharity-based scams that were rampant during COVID-19. How-\\never, neither of these studies provided an in-depth analysis of how\\ndonation-based scams are propagated via social media platforms or\\nthe lifecycle of these scams as conducted by malicious actors.\\nNovelty. The prior work on social media has predominantly fo-\\ncused on other forms of attacks. Addressing this gap, our research\\nperforms an in-depth analysis of donation-based abuses on social\\nmedia and their validation as scams. We leverage a straightfor-\\nward methodology backed by LLMs and security risk engines well\\nsuited for identifying fraudulent profiles soliciting donations. The\\nnovelty of our work lies in identifying large-scale donation-based\\nabuses across multiple platforms beyond the originating social me-\\ndia platforms and validating these scams through the association\\nof fraudsters‚Äô payment profiles.\\n3 Evaluation Setup and Data Filtration\\nIn this section, we detail our evaluation setup for identifying abu-\\nsive social media profiles, particularly those soliciting fraudulent\\ndonations. We start by collecting data, including associated posts,\\nfrom various social media platforms. This data is then filtered to fo-\\ncus on fraudulent donation solicitations, enabling a deeper analysis\\nof scam operations. As shown in Figure 2, our measurement setup\\nconsists of three main components: ‚ûä, which gathers data from\\nvarious social media platforms using donation-related keywords;\\n‚ûã, which filters the data to pinpoint profiles involved in donation\\nscams; and ‚ûå, which tracks the scammers‚Äô methods of operation.\\nWe provide details for each component as below.\\n3.1 Raw Dataset Aggregation\\nIn order to aggregate the raw dataset, we perform two main tasks: (i)\\nidentifying relevant search keywords and (ii) conducting automated\\nData \\nCollection\\n \\nScam \\nFiltration\\n \\nTracking \\n& \\nScam \\nAnalysis\\nDonation \\nKeywords\\n1\\n2\\n3Figure 2: Evaluation Setup Design : An overview of our system,\\nwhich consists of mainly three components: (i) Data Collec-\\ntion which performs automated donation-based keyword\\nsearches in five social media platforms, (ii) Scam Filtration\\nwhich performs data filtration associated to donation solicit-\\ning fraudulent accounts, and (iii) Tracking and Scam Analysis\\nwhich provides an evaluation of scammer‚Äôs modes of opera-\\ntion and techniques.\\nqueries of the dataset across five social media platforms using these\\ntargeted search keywords. We provide further details below.\\nDonation Keywords Identification. During our incubation phase,\\nwe manually reviewed online donation solicitations. We found 14\\nkey terms frequently used in such solicitations, such as givebetter ,\\nfund,help,act of kindness ,support ,charity ,donate ,donation ,donor ,\\nawareness ,giving ,foundation ,contribute , and helpsomeone . These\\nterms were linked with specific causes such as cancer ,earthquake ,\\nfirefighters ,police ,veterans ,animals ,hunger ,Ukraine ,Christmas , and\\nCOVID-19 . Overall, we developed 78 keywords to search relevant\\nposts and profiles across various social media platforms.\\nData Collection. Utilizing API services[ 63‚Äì69], we gathered data\\nacross X,Instagram ,Facebook ,Telegram , and YouTube using the\\nformulated keywords. We conducted three separate data searches\\nfor each social media platform from 2024-03-03 to 2024-05-15. In\\ntotal, we collected 151,966 accounts and 3,053,333 posts from five\\nsocial media platforms. Additionally, we retrieved profile metadata\\nfor each account, including name, description, links, profile image,\\ntimelines posts, and other publicly available information. A detailed\\nbreakdown of the raw data is presented in Table 1.\\n3.2 Fraudulent Donation Filtration\\nAfter collecting data from 151,966 accounts and 3,053,333 posts\\nacross five social media platforms, we conduct data curation. This\\nprocess involves two primary steps: (i) pre-processing the raw data\\nto confirm it pertains to donation-related contexts, and (ii) filtering\\ncandidates associated with donation-based abuses. The following\\noutlines the various steps involved in our data curation techniques\\nto ensure the accuracy of our findings.\\n3.2.1 Pre-Processing on Raw Data. In the pre-processing technique\\nwe perform filtrations by donation solicitation posts. During our\\nmanual analysis of the collected data, we found that API responses\\n3Under Review at WWW 2025 (v 1.0) Acharya et al.\\nTable 1: Overview of the raw dataset from five social media\\nplatforms. Our dataset reveals that Telegram has the highest\\nnumber of accounts and posts compared to the others.\\nSocial Media Accounts Posts\\nInstagram 1,604 136,082\\nFacebook 10,607 29,349\\nX 23,871 280,789\\nYouTube 30,482 54,314\\nTelegram 85,402 2,552,799\\nAll 151,966 3,053,333\\noften contained irrelevant content. For example, searches using key-\\nwords like donate cancer yielded results that were not specifically\\nabout donations but included general cancer-related content or un-\\nrelated donation activities. To address this, we introduced a context\\ncheck for each account and its associated posts to verify if the con-\\ntent was relevant to donation activities. Using the Large Language\\nModel (GPT-4o) [ 70], we developed a prompt injection to identify\\nwhether a given post was relevant to the donation context (see Ap-\\npendix A). This filtering process excluded accounts that were unre-\\nlated to the donation context: 25.56% (410/1,604) from Instagram ,\\n79.84% (8,469/10,607) from Facebook , 20.77% (4,959/23,871) from X,\\n80.93% (24,670/30,482) from YouTube , and 89.12% (76,111/85,402)\\nfrom Telegram were filtered. Across all five social media platforms,\\nthis filtering removed 75.42% (114,619/151,966) of accounts and\\n82.45% (2,517,489/3,053,333) posts associated with these accounts\\nfrom our raw dataset. We then applied security risk engine-based\\nflagged association to the remaining 24.57% (37,347/151,966) ac-\\ncounts and their 17.54% (535,844/3,053,333) posts related to the\\ndonation context to identify candidate scam accounts.\\n3.2.2 Data Filtration and Labelling. To label an account as a do-\\nnation solicitation scam, we apply two criteria: (i) the account\\nsolicits donations through publicly engaged posts, and (ii) the ac-\\ncount‚Äôs communication channels or profile metadata include ele-\\nments flagged by security risk engines. If both conditions are met,\\nthe account is labeled as a candidate for donation solicitation scam.\\nFor example, if a social media profile solicits donations and includes\\na fraudulent email, phone number, or links to websites flagged by\\nAnti-Phishing Engines as phishing URLs or malicious emails, we\\ncategorize it as a donation solicitation fraudster. Further details on\\nthe filtering and data labeling techniques are provided below.\\nPhishing URLs. We observed that social media profiles often in-\\nclude external websites or URLs in their bio sections. For each pro-\\nfile, we analyze the metadata to check for the presence of any URLs\\nor domains. Using the VirusTotal API [ 71], we evaluate whether\\nthese URLs are flagged as phishing or scam sites. To ensure accu-\\nracy, we only consider URLs or domains as potential candidates if\\nthey are flagged by at least two security risk engines from VirusTo-\\ntal. Accounts or posts containing URLs flagged by VirusTotal are\\nmarked for further scam donation abuse analysis.\\nIn total, we identified 118,735 URLs within the profile metadata,\\nand 0.95% (1,128/118,735) of these distinct URLs were flagged by\\nat least one of the VirusTotal security risk engines, spanning 2,345social media accounts. Of the 1,128 flagged URLs/domains, only\\n22.34% (252/1,128) were flagged by two or more security risk engines.\\nA manual review of 5% of the URLs, both single-flagged and multi-\\nflagged, revealed that single-flagged URLs/domains were often false\\npositives or unknown, while those flagged by two or more engines\\nwere found to be reliable. To mitigate potential false positives, we\\nlabeled accounts containing 0.21% (252/118,735) of URLs/domains\\nas candidate accounts linked to 369 social media profiles that were\\nflagged by multiple security risk engines from VirusTotal .\\nAbusing Email Addresses and Phone Numbers. We observe\\nthat social media profiles often include communication methods\\nsuch as email addresses and phone numbers in their bio-data to\\nfacilitate user contact. To assess the reliability of these communica-\\ntion methods, we used third-party API services to check the fraud\\nscore of the provided email addresses [ 72] and phone numbers [ 73].\\nSocial media profiles with communication methods having a fraud\\nscore greater than 85%were marked as candidates for further anal-\\nysis. We set an 85% threshold based on the providers‚Äô high-risk\\nvalidation, which indicates a strong association with fraud or high-\\nrisk activity for the given account. Out of 7,752 email addresses\\nfound in our pre-processed data, 2.90% (225/7,752) distinct email ad-\\ndresses were flagged with high-risk / fraud emails associated with\\n257 social media accounts. Similarly, out of 9,791 phone numbers\\nfound in our pre-processed data, we identified 1.37% (135/9,791)\\nfraud phone numbers associated with 201 social media accounts.\\nIn a nutshell, starting with 151,966 accounts and 3,053,333 posts\\nfrom five social media platforms, we applied two filtration tech-\\nniques: (i) Initially removing non-donation-based contexts, and\\n(ii) Further curating the data based on fraud risk engine-flagged\\nURLs/domains, phone numbers, and emails. As a result, our dataset\\nfor donation-based scams includes 832 social media profiles. This\\nmeans we filtered out 99.45% (151,134/151,966) of the accounts from\\nraw dataset accounts. We acknowledge that our conservative fil-\\ntering approach may have excluded some donation scam accounts.\\nHowever, as pioneers in the large-scale study of fraudulent donation\\nscams, our goal was to build a solid foundation using known seed\\ndata to reduce potential false positives. Additionally, in section 11,\\nwe explore the data evaluation and the efficacy of scam filtration\\nof our approach.\\n3.3 Tracking and Scam Analysis\\nThe third component, tracking and scam analysis, focuses on evalu-\\nating data from scammers‚Äô profile metadata and engagement posts.\\nWe analyze profile metadata to investigate the scammers‚Äô associ-\\nations with flagged email addresses, URLs, and phone numbers\\nidentified by fraud detection engines. Additionally, we examine\\nengagement posts to understand scammers‚Äô interactions and op-\\nerational methods to show how scammers solicit donations via\\nfinancial payment methods such as PayPal , cryptocurrency ad-\\ndresses, survey forms, and crowdfunding services. By analyzing\\ndata from these sources, we provide details on scam operations and\\nthe connections between scam accounts across multiple platforms\\nbeyond originating social media platforms.\\n4Exploring Donation-based Abuses in Social Media Platforms Under Review at WWW 2025 (v 1.0)\\nFigure 3: Distribution of security risk engines flagged commu-\\nnication channels (email, phone number, and URLs) across\\nsocial media platforms. In this pie chart, we show the total\\nnumber of scamming channels that were flagged by security\\nrisk engines identified across five social media platforms,\\nwith 31% of the total communication channels accounting\\nfrom the YouTube platform.\\nFor the rest of the section organization, we provide ‚Äì an overview\\nof donation abuse in section 4; profile content and association in sec-\\ntion 5; fraudulent donation solicitations topologies in section 6;\\nevaluation of scammer‚Äôs profile picture in section 7; sentiment anal-\\nysis of interacted comments in section 8; scammer operations and\\nnetwork analysis in section 9; and tracking of scamming payment\\nprofiles in section 10. Additionally, we provide recommendations\\nfor mitigating and proactively blocking these fraudulent accounts\\nin section 12.\\n4 Scam Donation Abuse Overview\\nIn this section, we provide an overview of fraudulent communi-\\ncation channels collected from scammers‚Äô profile metadata and\\nengagement posts. In Table 2, we summarize these findings by so-\\ncial media platform. The first column lists the five social media\\nplatforms we studied. The second, third, and fourth columns show\\nthe number of fraudulent channels associated with the scamming\\naccounts. The fifth and sixth columns provide the distinct and total\\nposts identified in the context of donation scams, and the seventh\\ncolumn shows the overall number of scammers soliciting donations.\\nIn total, we identified 225 fraudulent emails, 136 fraudulent\\nphone numbers, and 252 malicious URLs shared by 832 scammers\\nacross 17,730 posts and profile metadata. Among these fraudulent\\ncommunication channels, scammers showed a strong preference\\nfor URLs, which accounted for 41.10% (252/613), often directing\\nvictims to external websites for donations. The remaining channels\\nincluded emails at 36.74% (225/613) and phone numbers at 22.21%\\n(136/613). In Figure 3, we illustrate scamming channels by each\\nsocial media profile, and below, we highlight key findings for each\\nplatform studied.\\nInstagram. In our study, 6.73% (56/832) of scammers operated on\\nInstagram , the lowest count among the platforms analyzed. These\\nscammers preferred using malicious URLs for donation fraud over\\nemails or phone numbers. Among the 56 scamming accounts, we\\nfound no fraudulent emails, one fraudulent phone number, and 5malicious URLs, which appeared in 25.97% (4,606/17,730) of posts\\nand profile metadata. Notably, these scammers frequently dupli-\\ncated posts to solicit donations; of the 4,606 posts reviewed, 78.57%\\n(3,619/4,606) were duplicates.\\nFacebook. Among the five social media platforms, although Face-\\nbook had the second-lowest number of scammers at 19.71% (164/832)\\nand the fewest posts at 1.82% (323/17,730), it accounted for the\\nhighest percentage of fraudulent emails‚Äî57.19% (147/257) of all\\nidentified fraud communication channels. This suggests that scam-\\nmers on Facebook were more inclined to engage in donation-based\\nfraud through emails rather than using fraudulent phone numbers\\nor malicious URLs.\\nTelegram. In our study, Telegram had the second-highest post\\ncount at 34.26% (6,075/17,730) and accounted for 22.59% (188/832)\\nof the scammers. Among the 85 distinct fraudulent communication\\nchannels linked to these 188 scamming users, phone calls were\\nthe preferred method, making up 74.11% (63/85). Since Telegram is\\nwidely used for text messaging and phone calls, scammers on this\\nplatform were most likely to connect with victims through phone\\ncalls or direct messages.\\nYouTube. OnYouTube , 24.03% (200/832) of scammers were iden-\\ntified, the second-highest after X. Among the 190 fraudulent com-\\nmunication channels used by these 200 scammers, external URLs\\nwere the most common, accounting for 31.16% (115/369). Emails\\nfollowed as the second most used method at 20.88% (47/225), with\\nphone calls close behind at 20.58% (28/136).\\nX.Overall, our study found that the Xplatform is the most favored\\namong scammers, comprising 26.92% (224/832) of all scammers.\\nAmong the 169 fraudulent communication channels identified on X,\\n67.45% (114/169) were malicious URLs, making them the most com-\\nmon method for donation abuse. Similarly, 36.80% (6,526/17,730) of\\nthe scamming posts featured a significant proportion of malicious\\nURL sharing at 45.23% (114/252). The findings indicate that fraud-\\nulent profiles on Xprefer using malicious URLs over emails and\\nphone numbers to solicit fake donations.\\nKey Takeaways. Through the study of abusive communica-\\ntion channels, we identify that scammers use social media\\nplatforms as originating sources, and direct victims to use\\nexternal channels such as fraud email, phone calls, and URLs\\nto further contact. As URLs provide easy fraud mechanics\\ncompared to email and phone calls, scammers prefer URLs\\nas the highest compared to others asking victims to donate\\nvia external sites.\\n5 Profile Content and Association\\nIn this section, we dive deep into scammers‚Äô techniques to create\\nprofiles that attract potential victims on social media platforms.\\nWe conduct a thorough analysis of six key aspects: post engage-\\nment, follower count, account age, location settings, categorical\\nrepresentation, and account monetization. In Figure 4, we present\\na CDF graph showing scammers‚Äô engagement through posts, fol-\\nlower count, and account creation dates, and below we provide\\nfurther details on profile content and associations.\\n5Under Review at WWW 2025 (v 1.0) Acharya et al.\\nTable 2: Summary of scammers‚Äô posts and communication channels. This table shows our findings on donation-based abuses\\nidentified by analyzing profile metadata and engagement posts from five social media platforms. For each communication\\nchannel‚Äîemail, phone, and URLs‚Äîwe perform queries to determine if security risk engines flag the communications.\\nPlatforms Fraud Email/Accts. Fraud Phone/Accts. Malicious URL/Accts. Distinct Posts Total Posts Scammers\\nInstagram 0 1/12 5/44 987 4,606 56\\nFacebook 147/148 12/14 4/4 322 323 164\\nTelegram 8/8 63/84 14/86 6,049 6,075 188\\nYouTube 47/78 28/58 115/70 180 200 200\\nX 23/23 32/33 114/165 6,520 6,526 224\\nTotal (Distinct) 225/257 136/201 252/369 14,058 17,730 832\\n(a) Scammer Interactions.\\n (b) Following Count.\\n (c) Profile Creation Date.\\nFigure 4: CDF Engagement and age of scammer profile from each of the social media platforms ‚Äì Figure 4(a) shows the\\nengagement of scammer via posts, Figure 4(b) shows the following count of scammers and Figure 4(c) shows the age of scammers\\nbased on profile creation date from each of the social media platforms that we studied.\\nDescription/Bio. Scammers engaged in fraudulent donations were\\nfound to use various tactics in their profile descriptions. These\\ndescriptions provide a brief message to visitors. We found that\\n95.31% (793/832) of scammers contained profile descriptions that\\nincluded messages related to emotional manipulation, credibility,\\nauthentication, details about the donation campaign, or appeals to\\ngenerosity. The remaining 4.68% (39/832) of scammers were found\\nto lack any description or bio information.\\nPosts Engagement. Out of 17,730 posts collected from five social\\nmedia platforms, the overall median post interaction across all\\nplatforms was 709. The median post interactions for each platform\\nwere: X(4,775), Instagram (1,307), YouTube (2),Telegram (147), and\\nFacebook (2,960). Our results indicate that scammers are more likely\\nto engage on Xcompared to other platforms, whereas YouTube is\\nless favored for engagement through posts.\\nFollowers Engagement. The median follower count across the\\nfive social media platforms was 3,345. For each platform, the median\\nfollower counts were - X(1,621), Instagram (9,361), YouTube (82,500),\\nTelegram (10,809), and Facebook (5,449). Our result indicates that\\nusers are more inclined to follow scammers in video-based donation\\ncontexts compared to post-based ones. Since videos are generally\\nmore engaging than posts, scammers may find it easier to attract\\nand retain followers through video content.Account Age. Our analysis of fraudulent social media profile ages\\nreveals that scammers often use either harvested or aged profiles.\\nThe median creation date for all social media profiles was 2018.\\nSpecifically, the median ages for each platform were: X(2011),\\nInstagram (2024), YouTube (2023), Telegram (2022), and Facebook\\n(2016). This indicates that scammers are more likely to utilize older\\naccounts on Xwhile utilizing newer accounts on Instagram.\\nLocation. We identified 50.12% (417/832) of scammers with 210\\ndistinct geo-location sets as part of their profile information. The\\ntop three countries represented were Russia (62), the USA (37), and\\nIndia (29). It is important to note that geo-location is an optional\\nfield and does not necessarily reflect the scammers‚Äô actual locations,\\nas it is often populated with unrelated names. For example, the\\nlocation name global , although not a real location, had the highest\\ncount, with a total of 77.\\nCategorical Representation. We identified that 49.87% (415/832)\\nof scamming accounts featured 115 distinct categories or affilia-\\ntions in their profiles. Among these, the top three categories in-\\ncluded: Non-Profit Organization (62), Charity Organization (22), and\\nNon-Governmental Organization (NGO) 17. The remaining 50.12%\\n(417/832) were found to display missing categorical representation.\\nAccount Monetization. We found that 39.66% (330/832) of scam-\\nming accounts across four social media platforms ( Facebook (22/164),\\nInstagram (30/56), X(218/224), and YouTube ) (60/200), opted for\\n6Exploring Donation-based Abuses in Social Media Platforms Under Review at WWW 2025 (v 1.0)\\nbusiness or advertisement features. This enables these accounts to\\nmonetize their presence and allows the platforms to display adver-\\ntisements. Accounts with higher engagement levels generally gain\\nmore from opting into these business features. Notably, 97.32% of\\nscamming accounts on the Xplatform had the highest participation\\nin business or advertisement features.\\nKey Takeaways. Scammers were found to use older so-\\ncial media accounts to launch donation abuse campaigns.\\nWe suspect these are rather harvested accounts. Scammer‚Äôs\\ngeo-location data shows diverse representations of top coun-\\ntries including Russia and the USA, though these locations\\nare often misleadingly named. Moreover, scammers often\\nmasquerade under popular affiliations and opt-in for busi-\\nness/advertisement features, allowing for monetization through\\nadvertisements.\\n6 Fraud Topologies: Anatomy of Fraudulent\\nDonation Solicitations\\nIn this section, we provide the categories of fraudulent donation\\nsolicitations that scammers perform through posts. We provide a\\ntechnical overview and the findings of the scam clusters below.\\nTechnical Overview. We clustered donation solicitation posts\\nfrom 832 scamming profiles excluding non-English content. In total,\\nwe analyzed 17,706 posts across five platforms: X(6,526), Instagram\\n(4,583), Telegram (6,075), Facebook (322), and YouTube (200). For\\nlanguage identification and filtering, we use the CLD2 library [ 74].\\nWe then vectorized the posts using the all-mpnet-base-v2 sentence\\ntransformer model [ 75]. Subsequently, we processed the posts using\\nthe BERTopic library [ 76] to remove redundant information, such\\nas stop words. We combined UMAP [ 77] and HDBSCAN [ 78] for\\nclustering, followed by the KeyBERT [ 79] model to refine topic\\nrepresentations within each cluster.\\nIn the hyperparameterization process for UMAP, default values\\nfrom the BERTopic library [ 76] were employed. Specifically, we con-\\nfigured UMAP with n_neighbors=15 ,min_dist=0.0 ,n_components=5 ,\\nand cosine similarity. We then set the random_state variable to a\\nfixed value of 42to preserve the reproducibility of our code.\\nFor HDBSCAN, we chose min_cluster_size=10 and used the\\nEuclidean metric for clustering. To refine the clustering outcome,\\nwe adjusted min_samples=50 to reduce the resulting number of\\nclusters. Additionally, the default BERTopic method for outlier re-\\nduction ( reduce_outliers ) was applied to minimize the presence\\nof outlier samples in the clustering results. Finally, we employed\\na standard evaluation metric, i.e., silhouette score [ 80], and visual\\ninspection of resulting clusters to assess the quality and validity of\\nthe clustering outcomes.\\nClustering Results. We conducted a manual qualitative analysis\\nof prominent scam categories identified in our findings. Out of the\\n62 clusters identified through our clustering pipeline, we present\\nbelow an analysis of the top 10 clusters based on engagement\\nthrough posts where scammers solicit fraudulent donations.\\n‚Ä¢Urgent Support. We observe that scammers frequently\\ntarget specific donation days or weeks to create a sense\\nof urgency, often setting rapidly approaching deadlines. Acommon tactic involves urging social media users to com-\\nplete survey forms or to visit an external website before\\nthe donation period ends. We identified 185 scammers ask-\\ning for urgent support fraudulent donations through 951\\nposts, which comprised the highest numbers of scammers\\nand post-interactions in our study.\\n‚Ä¢Animal Rescue. In the context of animal rescue abuse,\\nscammers target individuals by posing as representatives\\nof legitimate animal rescue organizations to establish cred-\\nibility. These fraudulent posts solicit donations under the\\nguise of supporting animal welfare causes, asking for con-\\ntributions to help save and care for animals in need. In this\\ncluster, we identified 125 scammers asking for fraudulent\\nanimal rescue donations via 679 posts.\\n‚Ä¢Disaster Relief. We observe scammers often exploit the\\nimpact of disaster relief to solicit fraudulent donations. In\\nthis category, scammers act as legitimate organizations or\\naffiliations preying on those looking to support natural dis-\\naster victims. We identified 87 scammers asking for disaster\\nrelief fraudulent donations through 426 posts.\\n‚Ä¢Event and Activities Support. Scammers in this category\\nexploit popular events to solicit fraudulent donations, lever-\\naging the excitement and urgency to support the occasions.\\nThe scammer was often found to craft persuasive messages\\nappealing to participants‚Äô emotions and sense of commu-\\nnity, urging them to contribute financially. In this cluster,\\nwe identified 80 scammers asking for fraudulent donations\\nvia 427 posts.\\n‚Ä¢Crypto Scams. In the context of crypto donation abuse, we\\nidentify that scammers exploit the growing popularity and\\nperceived anonymity of cryptocurrency to solicit fraudulent\\ndonations. They take advantage of the novelty and complex-\\nity of cryptocurrency, making it appealing for users to either\\nparticipate in charity-related philanthropic support or take\\npart in free crypto token giveaways. In this cluster, we iden-\\ntified 57 scammers asking for fraudulent crypto donations\\nvia 111 posts.\\n‚Ä¢Holiday/Seasonal Spirit. Scammers in this category ex-\\nploit the holiday or seasonal spirit of generosity to make\\nfraudulent donation requests. These scams are often focused\\non children and families in need. In this cluster, we identified\\n54 scammers soliciting fraudulent donations through 382\\nposts.\\n‚Ä¢Education/Research Support. In this category, we observe\\nscammers exploit the education sector by targeting individu-\\nals with fraudulent donation requests related to scholarships,\\neducational research, and student support. These scams often\\npose as associations to institutions or charitable initiatives,\\nappealing to the goodwill of alumni, faculty, and the gen-\\neral public. We identified 49 scammers soliciting fraudulent\\ndonations through 92 posts in this category.\\n‚Ä¢Ticketing and Offer Exchange. We observe that scam-\\nmers in this category claim to need tickets or offer ticket\\nexchanges as part of fraudulent ticket donations. Scammers\\nperform potential disguises as potential donors to fraudulent\\n7Under Review at WWW 2025 (v 1.0) Acharya et al.\\nwebsites or request personal information under the guise of\\nfacilitating a ticket donation. By creating a sense of urgency\\nand community solidarity, they deceive well-meaning fans\\ninto buying a sold-out ticket or providing financial support\\nfor a particular event through ticket purchases. In this clus-\\nter, we identified 46 scammers asking for fraudulent ticket\\ndonations via 81 posts.\\n‚Ä¢Narcissistic Abuse Support. In this category of fraudulent\\ndonation solicitations, scammers target individuals by asking\\nfor support for abused groups, particularly those affected by\\nnarcissistic abuse. Their tactics include raising awareness\\nand soliciting donations for victims of war, domestic violence,\\nand psychological abuse. In this cluster, we identified 40\\nscammers asking for fraudulent donations via 63 posts.\\n‚Ä¢Medical. In medical-related fraudulent donation requests,\\nscammers are found to solicit funds for various medical\\ncauses, such as covering the medical expenses of a critically\\nill patient, supporting medical research, or providing medi-\\ncal care for disadvantaged groups. They often impersonate\\nmedical institutions to add legitimacy to their appeals. In\\nthis cluster, we identified 36 scammers asking for fraudulent\\nmedical-related donations via 265 posts.\\nKey Takeaways. Our analysis of post-clustering uncovered\\nseveral scam categories of fraudulent donations performed\\nby social media profiles. These include urgent appeals with\\nspecific deadlines, schemes tied to events, holiday-themed\\nsolicitations for families and children, and deceptive cam-\\npaigns masquerading as education and research support. Fur-\\nthermore, scammers exploit disaster relief efforts, victims of\\nabuse, animal rescue, and medical issues, presenting them-\\nselves as legitimate fundraisers while seeking fraudulent\\ndonations.\\n7 Evaluation of Scammer Profile Picture\\nIn this section, we provide an evaluation of the scammer‚Äôs choice\\nof profile picture while soliciting donations across multiple social\\nmedia platforms. We provide a technical overview and the findings\\nof the profile picture evaluation below.\\nTechnical Overview. Using unsupervised clustering to identify\\npatterns and relationships among these images, we examine the\\nprofile pictures of scammer accounts. Following the methodology\\noutlined in [ 52], we collected profile pictures and employed the pre-\\ntrained visual model CLIP [ 81] for feature extraction. For each pro-\\nfile picture, we extracted the CLIP token embeddings and rescaled\\nthe images to a resolution of 224√ó224pixels to match the input\\nsize used during the model‚Äôs training [ 81]. These embeddings were\\nthen visualized using Uniform Manifold Approximation and Projec-\\ntion (UMAP) [ 77]. To identify clusters and eliminate anomalies, we\\napplied standard clustering algorithms: HDBSCAN [ 78] and single-\\nlinkage hierarchical clustering [ 82]. Below we provide additional\\ndetailed information on the chosen hyperparameters and clustering\\nvalidation, and the results of our findings.Table 3: Clustering analysis of scammers‚Äô profile pictures\\nand their distribution across the five social media platforms.\\nOur result reveals that scammers in the Association-Logos\\ncategory were found to be the highest and utilize association\\nlogos to solicit donations.\\nCluster Label Count Facebook X Telegram Instagram Youtube\\nAssociations Logos 240 (29.13%) 79 102 34 24 1\\nMale/Female 133 (16.14%) 14 4842 12 17\\nVideo Clips 110 (13.35%) 0 0 3 1 104\\nGames & Cartoon 103 (12.50%) 12 3349 1 7\\nPolitics/War 97(11.77%) 0 0 33 1 63\\nPets 85(10.31%) 48 170 16 4\\nLow-Resolution 37 (4.49%) 11 16 9 1 0\\nCrypto Coins 19(2.31%) 0 316 0 0\\nTotal 824 164 219 186 56 196\\nClustering Hyperparameters Selection. During hyperparam-\\neters selection, we employ standard evaluation metrics, i.e., sil-\\nhouette score [ 80] and Calinski-Harabasz score [ 83], and visual\\ninspection of resulting clusters to assess the quality and validity\\nof the clustering outcomes. For both UMAP and DBSCAN, we sys-\\ntematically tuned their hyperparameters to optimize clustering\\npipeline performance and obtain meaningful and reliable results.\\nTo this end, we considered a wide range of hyperparameter con-\\nfigurations. Specifically, for UMAP, we let the n_neighbors hyper-\\nparameter vary in the intervals [3,100]and set the n_components\\nequals to 2to visualize the clusters. Regarding DBSCAN, we let\\nthemin_cluster_size andmin_dist vary in the intervals [5,100]\\nand[1ùëí‚àí02,1]respectively. The resulting investigation involved\\n2,500configurations of these hyperparameters, identifying the con-\\nfiguration n_neighbors =15,n_components =2,min_dist =0.1, and\\nmin_cluster_size =20as the most reliable, according to their sil-\\nhouette and Calinski-Harabasz scores, for our clustering pipeline.\\nClustering Results. We present the results of our clustering anal-\\nysis on 8241scammer profile images in Table 3. From the analyzed\\ndataset, we identified seven common categories of profile pictures\\nused by scammers: Association Logos ,Male/Female ,Video Clips ,\\nGames & Cartoon ,Politics/War ,Pets,Low-Resolutions , and Crypto\\nCoins . Our results show that 29%of scammers use Association Logos\\nas their profile pictures, often featuring logos from various groups\\nsuch as pacifist organizations, religious institutions, private compa-\\nnies, or even the Ukraine flag. About 16%of scammers use Male or\\nFemale profile pictures, while 13%fall into the Video Clips category,\\nusing video snapshots as their profile images. The Games & Cartoon\\ncategory, comprising 12%of scammers, includes images of video\\ngame characters, anime protagonists, and memes. Additionally, 11%\\nof scammers employ Political War images, such as screenshots from\\npolitical news, military actions, or propaganda. The Petscategory\\n(10%) features images of animals, mostly cats and dogs, as well as\\npet-related activities. Scammers using low-quality images belong\\nto the Low-Resolution cluster ( 4%), where the content is difficult to\\ndiscern. Finally, 2%of scammers fall into the Crypto Coins cluster,\\nwhich includes images of cryptocurrencies, and wallet logos.\\n1We excluded 8images due to unsupported formats (e.g., non-JPEG or non-PNG)\\n8Exploring Donation-based Abuses in Social Media Platforms Under Review at WWW 2025 (v 1.0)\\nOur analysis of scammers‚Äô profile images revealed that they often\\naim to emotionally manipulate users by featuring images of pets,\\nwar, educational organizations, or religious themes. Additionally,\\nin Appendix, Figure 6-8, we show a subset of 50scammer pro-\\nfile pictures from Association-Logos ,Male ,Female ,Games-Cartoon ,\\nPolitics-War ,Pet Associations , and Petsclusters. Notably, the content\\nwithin the clusters we identified is cohesive and coherent with our\\nassigned label. Complementary, in Figure 9, we illustrate samples\\ncoming from the Miscellaneous cluster, which contains a mixture\\nof pictures that have been considered anomalous by our clustering\\nalgorithms.\\nKey Takeaways. Through profile image analysis, we identify\\npatterns and tactics used by scammers to create a deceptive\\nonline presence. Our analysis revealed that scammers pre-\\ndominantly use association logos, male and female images,\\npolitical war, and game/cartoon characters to appear credible.\\nSuch insights are valuable for developing targeted measures\\nto detect and counteract fraudulent activities, improving on-\\nline security across social media platforms.\\n8 Sentiment Analysis of Public Comments\\nIn this section, we conduct sentiment analysis between users and\\nscammers. We focused specifically on YouTube due to its unique\\nvideo-based interaction format. Users often engage with videos as\\ndirected by the content, which differs from textual posts found on\\nposts-based interacting platforms ( X,Instagram ,Facebook , and Tele-\\ngram ). We collected 3,676 distinct comments from 364 scamming\\nYouTube channels.\\nTechnical Overview. For sentiment analysis, we utilized the\\nLlama3-8B model based on its popularity as the start of an art\\nopen-source model on benchmark sentiment analysis. Our com-\\nment categorization was based on predefined sentiments: Gratitude ,\\nAction ,Anger ,Abuse , and Neutral . We provide the prompt detail to\\nthese five sentiments in Figure 5.\\nSentiments Results. We provide detailed results of sentiment\\nanalysis of post engagement between users and scammers during\\nthe lifecycle of fraudulent donation solicitations as below.\\nGratitude. In the Gratitude category, we measured comments\\nexpressing gratitude, relief, or thankfulness. We found that 53.73%\\nof the comments reflected gratitude. We observe that scammers\\nfrequently try to thank those who have already donated and solicit\\nothers to make additional fraudulent donations. An example of a\\nscammer‚Äôs gratitude is shown below.\\nThank you, every single donation matters, even if you can‚Äôt donate.\\nGod bless everybody involved in the rescue and care of this beautiful\\ndog family.\\n7 hours and already $50,000 donated... Thank you for improving the\\nlives of so many others.Classifier Task Description\\nYou are a classifier. Given a Comment , classify it into\\none of the following categories:\\nGratitude : A comment expressing gratitude, relief, or sim-\\nilar emotions.\\nAction : A comment that includes awareness, a report, an\\nurgent action, or similar prompts.\\nAbuse : A comment indicating that scammers are engaging\\nin hateful, abusive, fearful, or concerning activities.\\nAnger : A comment showing that the user is frustrated or\\nangry because they believe YouTube is not taking serious\\nsteps to block scam accounts.\\nProvide your classification in the following format:\\n‚Ä¢Category: \"... \"\\n‚Ä¢Explanation: \"... \"\\nExamples:\\nComment : \"Thank you so much for addressing\\nthis issue! I was really worried.\"\\nCategory : \"Gratitude\"\\nExplanation : \"The comment expresses gratitude\\nand relief for addressing the issue.\"\\nComment : \"Everyone needs to report these scam-\\nmers immediately!\"\\nCategory : \"Action\"\\nExplanation : \"The comment is a call to action,\\nurging others to report scammers.\"\\nFigure 5: System prompt for Llama-3. We instruct-tune\\nLlama-3-8B to classify sentiment in Youtube users comments\\nwith a system prompt describing the task and two examples.\\nAction. In the Action category, we measured comments that include\\nawareness, report, urgent action, or time-based responses. We found\\nthat 17.79% of the comments interacted with scamming videos\\ndisplayed action. We provide examples of action below.\\nDonate please, another 7.4 earthquake struck Nepal just now.\\nQuality of life and hospice support is imperative. Now that you learned\\nhow to make a donation button in PLS DONATE .\\nMost large charities are scams with a fraction of donated money ever\\nreaching those it was gifted for, give to local charities that actually\\ndo good work .\\nAnger. InAnger category, we measured comment that shows that\\nthe user is frustrated or angry because YouTube does not take\\nserious steps in blocking the scamming accounts or scammers. We\\n9Under Review at WWW 2025 (v 1.0) Acharya et al.\\nfound that 16.43% of the dataset typically showed frustration with\\nYouTube‚Äôs handling of scam accounts.\\nThey need to be closed down and thrown in jail for fraud.\\nThe scam part angers me.\\nContact us about paying them for their scam a** service.\\nHate. InHate category, we measure engagement in hateful or abu-\\nsive behavior on interaction. We identified 11.62% of the highlighted\\nengagement comprised of hateful or abusive behavior. Examples of\\nsuch hateful comments from scammers are shown below.\\nYou hate charity because you≈ïe a cringe Socialist.\\nYou should get out there on the streets and do the fuc**ng work.\\nYou‚Äôre a lying imposter you deserve what misfortune that comes your\\nway.\\nNeutral. InNeutral , we measure interaction that is not necessar-\\nily related to donation-based context or posts. We suspect these\\nneutral comments are rather scripted to gain followers. We found\\nthe neutral context as the lowest category comprising 0.43% of our\\noverall dataset. An example of a neutral comment are shown below.\\nFun fact: snakes actually use their tongues to catch scents!\\nLine from Seinfeld: ¬®George likes his Kung Pao SPICY¬® .\\nIf you are impressed with this video, please support us on Patreon -\\nhttps://www.patreon.com/Le**cs. It will be a great help for us.\\nKey Takeaways. Our analysis of scammer and user interac-\\ntion sentiments revealed several key insights. In the Action\\ncategory, comments reflected urgent responses or aware-\\nness, with some users advising against taking action due\\nto mistrust of large charities. The Anger category showed\\nthat comments expressed frustration with YouTube‚Äôs failure\\nto block scam accounts. In the Hate category, interactions\\ninvolved hateful or abusive behavior, both from scammers\\nand users. Lastly, the Neutral category included unrelated,\\nscripted comments and motives to gain followers. This indi-\\ncates that comment-based interactions are lucrative channels\\nof operations for scammers, offering interactive video-based\\nsolicitations for donations.\\n9 Scammer Network Analysis\\nIn this section, we explore how scammers operate across multiple so-\\ncial media platforms, focusing scam cycle and modus operandi. We\\ndetail the fraud lifecycle, illustrating how scammers redirect users\\nfrom one platform to another through tactics such as crowdsourc-\\ning, and external links, and share scam channels across multiple\\nprofiles. We provide further details below.9.1 Operation Beyond Originating Platform\\nIn this section, we specifically focus our analysis on scammers\\noperating beyond the originating platforms and interlinking ac-\\ncounts among multiple platforms. Our analysis primarily covers (i)\\nexternal platforms that scammers link to their profiles, (ii) dona-\\ntion solicitations via crowdfunding services, and (iii) survey forms.\\nBelow, we provide detailed information on each category.\\nExternal Communication Channels. Through profile meta-\\ndata analysis, we found that scammers frequently include details\\nof external platforms in their bio descriptions, linking them to\\nthe originating social media platform. We identified two types of\\nexternal bio links on scamming profiles. The first type links to ex-\\nternal websites such as Linktree URLs, which aggregate multiple\\nplatforms and related links to the scammer‚Äôs account. For exam-\\nple, a bio profile linking to www.linktree.com/scam_account_1 was\\noften found to contain various social media accounts associated\\nwith scam accounts, such as www.facebook.com/scam_account_f ,\\nandwww.twitter.com/scam_account_t . The main purpose of these\\naccounts is to provide visitors with a choice of platforms for contact.\\nThe second type involves direct links to a preferred platform, such\\nas an Xprofile containing links to Instagram orTelegram as part of\\nthe external contact details.\\nWe observed that 37.5% (312/832) of scamming accounts included\\nexternal links in their profiles, with 127 of these accounts linking\\nmultiple bio profiles (ex. Linktree ) to external websites. For accounts\\nwith multiple external bio links, we automated the Selenium Python\\nscript to gather the associated platforms interlinked with the orig-\\ninating account. In Table 4, we present data showing 832 scam-\\nming accounts interlinked with 11 different platforms across both\\ncategories. Overall, we identified 1,001 distinct external platform\\naccounts linked beyond the study accounts. Among these platforms,\\nthe top five most commonly interlinked accounts were related to\\nYouTube (48.15%), Instagram (16.58%), Facebook (12.18%), Twitter\\n(8.29%), and Amazon (4.39%). To gain further insights, we conducted\\na manual analysis by randomly selecting 100 accounts and visit-\\ning each link through a browser. We identified four distinct scam\\noperation techniques: (i) platforms such as YouTube were used for\\nvideo-based donation requests, (ii) messaging platforms such as Sig-\\nnal,Telegram , and WhatsApp were used for direct communication,\\n(iii) social media platforms like Twitter ,Instagram , and Facebook\\nwere primarily utilized for post engagement, and (iv) consumer-\\noriented platforms such as Amazon and Etsy were exploited by\\nscammers to solicit support through purchases from wishlists or\\ngifts. Thus, starting with 832 scamming accounts from five social\\nmedia platforms, this technique yielded an additional 1,001 external\\naccounts linked to 11 platforms (9 social media platforms and 2\\nonline e-commerce platforms).\\nCrowdfunding Services. We found that scammers exploit crowd-\\nfunding services for donation solicitations. We analyzed the pres-\\nence of popular crowdfunding service URLs in posts engaged by\\nscammers. As shown in Table 5, we identified 9.97% (83/832) of\\nscammers soliciting donations via 77 URLs from seven different\\ncrowdfunding services. The top three platforms used were Patreon\\n(51.94%), Donorbox (24.67%), and Kickstarter (10.38%). We conducted\\na manual review of these 77 URLs by visiting each link in a browser.\\nOut of 77 distinct URLs, 9 links were either inactive or deleted.\\n10Exploring Donation-based Abuses in Social Media Platforms Under Review at WWW 2025 (v 1.0)\\nTable 4: Overview of the external platforms linked to the\\nscam accounts. In this table, we show scammers interlinking\\nvarious platforms as part of a scam operation.\\nSocial Media External Linked Accounts\\nYouTube 482\\nInstagram 166\\nFacebook 122\\nTwitter 83\\nAmazon 44\\nLinkedIn 36\\nTikTok 30\\nTelegram 25\\nEtsy 9\\nSignal 2\\nWhatsApp 2\\nAll (Distinct) 1,001\\nTable 5: Overview of the crowdfunding services. Our results\\nshow that scammers often redirect users from the original\\nsocial media platforms to seven crowdfunding services.\\nCrowdfunding Services Scam Accounts Fund Links\\nPatreon 45 40\\nGivebutter 24 6\\nDonorbox 6 19\\nKickstarter 4 8\\nIndiegogo 2 2\\nFundrazer 1 1\\nRallyup 1 1\\nAll (Distinct) 83 77\\nAmong the active URLs, 23/68 had already closed their fundraising\\ncampaigns, with amounts raised ranging from $25 to $58,180. The\\nremaining 45/68 crowdfunding URLs were found to be actively\\ncollecting donations, using three main solicitation methods: (i) min-\\nimal payments to join a group as a form of support for the cause\\n(e.g., Patreon memberships starting at $1.70 per month plus tax), (ii)\\nrecurring donations such as monthly or annual contributions ($5,\\n$25, or higher), and (iii) one-time payments for support (ranging\\nfrom $5 to several hundred dollars). Our analysis from the last week\\nof September 2024 identified 3,696 contributors who donated over\\n$252,620 through 37 active fundraising links. This amount does\\nnot include contributors who may have made or are still making\\ndonations via membership subscriptions. We suspect scammers\\nare repeatedly defrauding victims through ongoing solicitations\\nobserved in our dataset.\\n9.2 Campaign Detection\\nWe analyzed shared communication channels, specifically URLs,\\nemails, and phone numbers, among scam accounts to determine\\nwhether these channels interlink scam accounts as part of their\\ncommunication with potential victims. To do this, we aggregated\\ndata from abuse candidate scam accounts across all five social me-\\ndia platforms. If a minimum of two scam accounts share a singleTable 6: Overview of scammers sharing the communication\\nchannels. The table provides a breakdown of clusters and\\nscam accounts from all five social media platforms by indi-\\nvidual communication channels.\\nChannels Min Median Max Cluster Accts. Accts.%\\nEmail 2 3 8 12 44 17.12\\nPhone 2 3 15 21 88 43.78\\nURL 2 2 42 41 231 62.60\\nAll (Distinct) 2 3 42 74 355 42.66\\ncommunication channel, we refer to the given group as a scam\\ncampaign shared by the scam accounts.\\nWe grouped the scam accounts based on individual types of\\ncommunication channels, such as emails, URLs, or phone numbers.\\nIn Table 6, we summarize the scam clusters, including the minimum,\\nmaximum, and median counts of scam accounts per cluster. Overall,\\n42.66% of scam accounts were found to be part of scam campaigns.\\nAmong these, URL clusters were the most prevalent, with 41 distinct\\nclusters comprising 231 accounts, while email clusters were the\\nleast common, with 12 distinct clusters involving 44 scam accounts.\\nThe largest cluster contained 42 scam accounts linked through\\nURLs, while the smallest and median cluster sizes across the three\\ncommunication types were 2 and 3 accounts, respectively.\\nKey Takeaways. Scammers leverage multiple platforms and\\ninterlink accounts to broaden their operations, frequently\\nredirecting users through strategic bio links and aggregating\\nvarious platforms. Our analysis reveals that platforms such\\nasYouTube ,Instagram , and Amazon are often exploited for\\ndonation requests. Scammers also use crowdfunding services\\nto solicit both recurring and one-time contributions. More-\\nover, scammers operate in organized clusters, connecting\\ncampaigns through URLs, emails, or phone numbers, show-\\ncasing their advanced and coordinated methods for targeting\\nvictims.\\n10 Financial Validation and Tracking Payments\\nFrom the profile metadata and post engagements of scammers on\\nfive social media platforms, we observed that fraudsters soliciting\\ndonations often involve requesting payments via various methods\\nsuch as PayPal and cryptocurrency addresses. To further validate\\nthese scams‚Äô impact, we partnered with PayPal , and Chainabuse ,\\nsharing 1,898 email addresses with PayPal , and 142 cryptocurrency\\naddresses with Chainabuse . Below, we present the findings related\\nto these scamming payment profiles based on feedback from our\\nindustry partners.\\nPayPal‚Äôs Scam Validation. From the 1898 email addresses that\\nwere shared, PayPal was able to identify and associate 79.71%\\n(1513/1898) of these to PayPal accounts on the platform. Among\\nthese identified accounts, 26% were restricted at some point during\\ntheir activity on the platform. Within these 26% restricted accounts,\\nabove 50% had more than one restriction placed throughout their\\ntime on PayPal , and 42% were currently restricted at the time of data\\nsharing. Finally, based on the overall restrictions placed on these\\n11Under Review at WWW 2025 (v 1.0) Acharya et al.\\naccounts, the top reasons were (i) KYC (Know your Customer) &\\nCompliance concerns, and (ii) Risky Operations like Unauthorized\\nAccount Access or Creation.\\nChainabuse Scam Validation. Out of 142 addresses, 21.83%\\n(31/142) were identified as invalid. We are unclear as to why scam-\\nmers provide invalid cryptocurrency addresses when soliciting\\ndonations. However, we suspect that by using an invalid address,\\nscammers compel victims to contact them for assistance, redirect-\\ning the communication in their favor. We provide chain analysis\\non the remaining 78.16% (111/142) valid addresses to four popular\\nchains: Ethereum ,Binance ,Polygon ,Avalance , and Bitcoin ; identify-\\ning 4 of these as suspicious by these popular chains.\\nIncoming Volume/Transfer In total, we identified 96 accounts\\nwith an average USD value of $2,574,907.09 and a total sum of\\n$247,191,080.45 at the time of writing this paper. Based on the first\\ntransfer date of the transaction, we observe that these 75% (72/96)\\nwere active first in 2024, and the remaining transactions 25% (24/96)\\nfrom 2016 to 2013. Scammers using new addresses for transactions\\nare common practices to remain anonymous with the previous\\ntransactions history. Among these transactions, we found two long-\\ntail transactions - the first highest recorded account transaction\\nvalue to $241,251,535 and the second highest was $2,863,122.17. Ex-\\ncluding the first and second highest recorded transactions accounts\\nas long-tail, the remaining 94/96 accounts transactions reflected an\\naverage of $32,727.90 and a total sum of $3,076,422.71 value. Among\\nthese 96 transactions, we identified 11 transactions valued less than\\n$1, with an average incoming volume of $0.22, and a total sum of\\n$2.41. We suspect these small incoming transactions below $1 are\\nrather an airdropping.\\nOutgoing Transfers In total, we identified 130 outgoing trans-\\nfers with an average value of $1,530.04 and a total sum of $198,906.\\nDisclaimer. Our evaluation is based on the observed transaction\\nhistories and reported fraud categories. However, are unable to\\nconfirm that all transactions associated with these addresses are\\nconnected to scams.\\nKey Takeaways. Scammers utilize various payment meth-\\nods, including PayPal and cryptocurrency, to solicit donations\\nwhile maintaining anonymity. Our collaboration with indus-\\ntry partners reveals that scammer‚Äôs payment method linked\\nto various fraud topologies including compliance violations\\nand unauthorized activities. We suspect invalid cryptocur-\\nrency addresses are used for manipulating victims to pursue\\ndirect communication. The cryptocurrency transaction anal-\\nysis highlights that scammers often use new addresses to\\nobscure histories, while a small number of accounts perform\\nlarge sums. Although we could not conclude scams involv-\\ning transactions of $1 or less, we suspect that these may go\\nunnoticed due to small recurring payments or platform mon-\\nitoring biases. Scammers potentially use small transactions,\\nsuch as airdrops, which may serve to create plausible activity\\nor evade detection.11 Dataset Evaluation and Discussion\\nIn this section, we provide details on the evaluation of the dataset\\nthrough manual inspection. We share observed insights into the\\nlimitations and assessed the filtration efficacy of using large lan-\\nguage models (GPT-4o) and the reliance on external databases for\\nclassifying email addresses, phone numbers, and URLs as malicious\\nalong with the studied social media profiles.\\nEfficacy of LLM-based Filtration. We manually evaluated the\\neffectiveness of using a Large Language Model (LLM) to classify\\nwhether a given post is related to a donation context. For this evalu-\\nation, we selected 50 posts from each of the social media platforms:\\nFacebook ,Instagram ,Telegram ,X, and YouTube from both cases,\\nposts that were classified as false and true for donation based con-\\ntext. In total, we manually evaluated 500 posts: 250 from the True\\nclass and 250 from the False class. Our evaluation showed that the\\nLLM achieved 100% efficacy in correctly identifying donation con-\\ntexts in the True class. However, in the False class, we observed two\\nmain categories where the LLM underperformed: (i) 19/250 posts\\nlacked sufficient donation contextual information, containing only\\nlinks, emojis, or hashtags with contact details, and (ii) 33/250 posts\\nfound in languages other than English, which were classified as\\nFalse . As a result, we suspect that our evaluation might have over-\\nestimated false positive cases while maintaining high true positive\\nefficacy. We propose that these limitations can be further addressed\\nby (i) incorporating additional context checks for prevalent hash-\\ntags, and inpsecting the landing URL, and (ii) enhancing the LLM‚Äôs\\ncapabilities to better identify donation contexts in languages other\\nthan English through multilingual settings.\\nReliability of Security Risk Engines. To assess the reliability of\\nthe risk engines used to identify malicious URLs, phone numbers,\\nand emails reported under the abuse category, we conducted two\\ndistinct evaluations.\\nThe first evaluation involved inspecting potentially malicious\\nURLs from our dataset by manually opening them in a browser.\\nOut of 252 URLs flagged as phishing or malicious, we randomly\\nselected 100 URLs for inspection. Of these, 47 were inactive or taken\\ndown. Among the remaining 53 active URLs, 29 were flagged by\\nChrome as potential phishing or malicious sites with a Deceptive\\nsite ahead warning. Upon visiting these URLs, we found that 14/29\\ndisplayed missing content with a default template, while 13 led\\nto fake donation pages for various causes, such as child support,\\nhealthcare, and relief, and 2 were redirected to sign-up pages with-\\nout further information. For the other 24 active URLs, although\\nthey were marked as malicious by the VirusTotal API , no deceptive\\nbanner was shown upon visiting. However, upon further inspection,\\neach of these 13 URLs was missing content or had been removed,\\nand 11 consisted of solicitations for donations through sign-up\\nor payment information submission pages. For each of these 13\\nURLs, we found that 1/68 vendors on VirusTotal flagged them as\\nsuspicious or malicious, while the responses from 68 other vendors\\nwere marked as clean. Since phishing sites are often ephemeral\\nand missing content makes classification challenging, not all ven-\\ndors may have processed these URLs promptly before the content\\nchange. We suggest that such cases could be improved through\\n12Exploring Donation-based Abuses in Social Media Platforms Under Review at WWW 2025 (v 1.0)\\nregular monitoring and by consolidating responses from multiple\\nvendors to enhance URL flagging accuracy.\\nIn the second evaluation of phone numbers and email addresses,\\nwe conducted additional analyses using two datasets: (i) 50 known\\nmalicious entries (25 phone numbers and 25 email addresses) from\\npublicly reported corpus [ 84], and (ii) a benign dataset of 50 en-\\ntries from the authors‚Äô friends and family (25 phone numbers and\\n25 email addresses). We queried these 100 entries against the risk\\nengine and found that 19/25 phone numbers and 23/25 email ad-\\ndresses were flagged with risk levels above 85%. However, 6 phone\\nnumbers and 2 email addresses showed risk percentages between\\n5% and 65%, making them unreliable for classification as malicious.\\nIn contrast, all 50 entries from the benign dataset were marked\\nwith 0% risk. Although the risk engine performed inconsistently for\\nemail and phone number assessments with lower risk percentages\\nfor known corpus, we argue that integrating multiple providers and\\ncombining scores could potentially enhance results which would\\nrequire additional resources.\\nSocial Media Profiles and Scam Prevalence. We randomly se-\\nlected 100 social media accounts from the dataset and manually\\ninspected them using a browser. Our findings revealed that 9/100\\naccounts had been deactivated by the social media platforms for\\nviolating terms and conditions, and 17/100 were either deactivated\\nor deleted by the users. For the remaining 74/100 active accounts,\\nwe manually reviewed their public profiles and engagement. Of\\nthese, 14 accounts displayed default profile pictures and had limited\\npublic interaction, while 18 accounts were used solely for retweets\\nand shares, with no original posts. We suspect that these accounts\\nare used to harvest followers or create the appearance of an organi-\\ncally aged social media profile. The remaining 42/74 accounts were\\nfound to engage in some form of donation solicitation, targeting\\ncauses such as ongoing war and human welfare programs (18 ac-\\ncounts), education and local training programs (11 accounts), local\\nwildlife foundations seeking donations for preservation efforts (6\\naccounts), single mom and women support (3 accounts), and other\\nmiscellaneous disadvantaged groups (4 accounts).\\n12 Recommendations\\nBased on our observations and findings, we propose recommenda-\\ntions to combat donation-based abuses. These recommendations are\\nintended for adoption by social media platforms, financial services,\\ncrowdfunding platforms, and platform users. We provide further\\ndetails below.\\nRecommendations to Social Media Platforms. We suggest\\nthat social media platforms adopt a detection measurement setup\\nsimilar to the one proposed in our research. For proactive pre-\\nvention, social media platforms can utilize a fraud score to assess\\nwhether the email address or phone number used during sign-up\\nposes a fraud risk. Similarly, for reactive measures against existing\\nprofiles, we recommend monitoring the use of external media asso-\\nciated with profile bio-data or shared posts. Our network analysis\\nof donation abuse revealed that scammers often operate across\\nmultiple social media platforms as part of their modus operandi.\\nWe encourage social media platforms to share information with\\nother platforms about detected suspicious behaviors to prevent\\nsuch fraudulent activities. Implementing a warning message forregular users when a social media post contains donation requests\\nfrom flagged cryptocurrency addresses or payment links could help\\nusers avoid potential interactions with scammers.\\nRecommendations to Financial In-Take Services. We rec-\\nommend that financial intake services, specifically crowdfunding\\nplatforms and payment profiles, monitor the URLs shared across\\ntheir platforms. For instance, crowdfunding services like GoFundMe ,\\nFundly ,PayPal , and others often include links that scammers use to\\nrequest payouts. These financial intake services can effectively im-\\nplement referral header monitoring techniques based on the source\\nof visits. Referral headers contain links and source information\\nindicating where a user is directed from. By monitoring referral\\nheaders and assessing whether a social media profile is linked to\\nfraudulent activity, crowdfunding platforms, and payment services\\ncan reduce the risk of funding abuse by scammers.\\nRecommendations to Social Media Users. We recommend so-\\ncial media users conduct thorough fact-checking before supporting\\nany donation-related efforts. This includes verifying bio data, and\\naffiliations, understanding the purpose and planned use of funds,\\nand reviewing feedback from other donors. For instance, databases\\ntracking charity affiliations are valuable resources for authenti-\\ncating charitable organizations. When donating to individuals or\\nprivate causes, we recommend users support only when there is\\na known connection and look out for any account duplications or\\nimpersonations.\\nKey Takeaways. We provide recommendations to combat\\ndonation-based abuses on social media platforms, financial\\nservices, crowdfunding platforms, and among users. Social\\nmedia platforms are encouraged to adopt fraud scores for\\nproactive detection and monitor external media for suspi-\\ncious activity. Financial services are suggested to monitor\\nURLs for scams and use referral header monitoring to re-\\nduce fraud risks. Users are urged to conduct thorough checks\\nbefore donating, verify affiliations, and exercise caution, par-\\nticularly when supporting unfamiliar causes or individuals.\\n13 Conclusion\\nIn this research, we presented the first large-scale study of donation-\\nbased abuses across five social media platforms: X,Instagram ,Face-\\nbook ,Telegram , and YouTube . By analyzing data from over 150K\\nsocial media users and 3 million posts, we identified over 832 scam-\\nmers soliciting fraudulent donations on these platforms. Our anal-\\nysis of profile creation and user engagement revealed scammers‚Äô\\ntechniques for luring victims and requesting payments through\\npayment profiles such as PayPal , cryptocurrency addresses, crowd-\\nfunding services, and survey forms. Our measurement approach\\nidentified scam accounts operating on 11 platforms (9 social media,\\nand 2 e-commerce) beyond their origins. Through collaboration\\nwith industry partners PayPal and the cryptocurrency abuse data-\\nbase Chainabuse , we validated the scams and assessed the financial\\nimpact of these fraudulent accounts. Furthermore, we provided\\ndetailed disclosures to affected entities and proposed recommenda-\\ntions to protect against future abuses.\\n13Under Review at WWW 2025 (v 1.0) Acharya et al.\\nAcknowledgment\\nWe sincerely thank Ian Schade from Chainabuse for sharing valu-\\nable insights regarding cryptocurrency accounts. Our appreciation\\nalso goes to Qutub Khan Asghar Vajihi from PayPal for providing\\ninsights related to PayPal accounts. Additionally, we are grate-\\nful to Muhammad Saad from X (formerly Twitter) for his initial\\ndiscussions on donation-based scams prevalent in the contexts\\nof the X platform. This work was funded by the German Federal\\nMinistry of Education and Research (BMBF grant 16KIS1900 ‚ÄúUbi-\\nTrans‚Äù); and by the EU‚ÄîNGEU National Sustainable Mobility Center\\n(CN00000023), Italian Ministry of University and Research Decree\\nn. 1033‚Äî17/06/2022 (Spoke 10). Lastly, this work was carried out\\nwhile Dario Lazzaro was enrolled in the Italian National Doctorate\\non Artificial Intelligence run by the Sapienza University of Rome\\nin collaboration with the University of Genoa.\\nReferences\\n[1]FTC, ‚ÄúCharity fraud. ‚Äù https://consumer.ftc.gov/features/pass-it-on/charity-fraud.\\n[2]FTC, ‚ÄúScam ‚Äôcharities‚Äô will take your money and run. ‚Äù https://www.fcc.gov/scam-\\ncharities-will-take-your-money-and-run.\\n[3]FBI, ‚ÄúCharity and disaster fraud.‚Äù https://www.fbi.gov/how-we-can-help-you/sc\\nams-and-safety/common-scams-and-crimes/charity-and-disaster-fraud.\\n[4]F. C. M. East, ‚ÄúFake donation emails and websites rise amid the israel-hamas war.‚Äù\\nhttps://fastcompanyme.com/news/fake-donation-emails-and-websites-rise-\\namid-the-israel-hamas-war/, 2023.\\n[5]M. L. Gutzwiller, ‚ÄúSpotting charity scams: How to give safely.‚Äù https://www.cshc\\no.com/articles/spotting-charity-scams/, 2024.\\n[6]D. Amato, ‚ÄúText and email scams to watch for in 2024.‚Äù https://www.rbcroyalba\\nnk.com/en-ca/my-money-matters/money-academy/cyber-security/understan\\nding-cyber-security/text-and-email-scams-to-watch-for-in-2024/, 2024.\\n[7]N. C. C. Council, ‚ÄúCharity donation fraud.‚Äù https://newcastle.gov.uk/services/bu\\nsiness-and-commerce/business-commerce/trading-standards/campaigns/char\\nity-donation-fraud.\\n[8]C. Boyd, ‚ÄúBeware of fake twitter philanthropists offering to put $750 into your\\ncash app account.‚Äù https://www.malwarebytes.com/blog/news/2022/04/beware-\\nof-fake-twitter-philanthropists-offering-750-for-your-cash-app-account.\\n[9]C. POPOV, ‚ÄúCharity scams: How to spot and avoid fake charities.‚Äù https://www.\\nbitdefender.com/blog/hotforsecurity/protect-your-donations-spot-and-avoid-\\nfake-charities/.\\n[10] M. Keane, ‚ÄúCharity fraud.‚Äù https://www.britannica.com/money/charity-fraud.\\n[11] CAF, ‚ÄúWhy charities should be cyber-aware.‚Äù https://www.cafonline.org/about-\\nus/security-centre/be-aware---current-threats/scam-emails.\\n[12] I. Support, ‚ÄúScam alert: \"donation to charity or prize winning\".‚Äù https://www.ur\\negina.ca/is/security/advisories/security-advisory56.html, 2023.\\n[13] B. Dean, ‚ÄúSocial network usage & growth statistics: How many people use social\\nmedia in 2024?.‚Äù https://backlinko.com/social-media-users, 2024.\\n[14] B. Matthews, ‚ÄúSocial media stats for charities and nonprofits.‚Äù https://empower.\\nagency/social-media-stats-charities-nonprofits/.\\n[15] J. Tabas, ‚ÄúHow nonprofit organizations can boost donations via social media.‚Äù\\nhttps://www.forbes.com/sites/allbusiness/2024/05/10/how- nonprofit-\\norganizations-can-boost-donations-via-social-media/, 2024.\\n[16] C. Water, ‚ÄúProtect yourself from charitable-giving scams. ‚Äù https://clearwatercred\\nitunion.org/protect-yourself-from-charitable-giving-scams-2023-12-04/, 2023.\\n[17] AT&T, ‚ÄúSocial media charity scam.‚Äù https://about.att.com/pages/cyberaware/ar/\\nsocial-media-charity-scam.\\n[18] E. Fletcher, ‚ÄúSocial media: a golden goose for scammers.‚Äù https://www.ftc.gov/\\nnews-events/data-visualizations/data-spotlight/2023/10/social-media-golden-\\ngoose-scammers, 2023.\\n[19] IRS, ‚ÄúDirty dozen: Irs warns of scammers using fake charities to exploit taxpayers.‚Äù\\nhttps://www.irs.gov/newsroom/dirty-dozen-irs-warns-of-scammers-using-\\nfake-charities-to-exploit-taxpayers, 2023.\\n[20] CNBC, ‚ÄúFake charities can be almost impossible to spot. here‚Äôs how to make sure\\nyour donations get to the right place.‚Äù https://www.cnbc.com/2022/07/07/how-\\nto-avoid-charity-impersonation-scams-in-times-of-crisis.html, 2022.\\n[21] G. Torre, ‚ÄúWarning issued over fake social media accounts running flood donation\\nscams.‚Äù https://nit.com.au/17-01-2023/4743/warning-issued-over-fake-social-\\nmedia-accounts-running-flood-donation-scams, 2023.\\n[22] S. Watch, ‚ÄúScam statistics.‚Äù https://www.scamwatch.gov.au/research-and-\\nresources/scam-statistics?scamid=14&date=2024, 2024.\\n[23] E. News, ‚ÄúCharity warns 2023 was the worst year for child sexual abuse content.‚Äù\\nhttps://www.euronews.com/next/2024/04/23/german-internet-domain-used-by-criminal-groups-in-worst-year-for-online-child-sexual-abuse, 2024.\\n[24] Z. Ali, ‚ÄúThe rise of ai is creating a rise in scams on social media.‚Äù https://www.\\nhowtogeek.com/the-rise-of-ai-is-creating-a-rise-in-scams-on-social-media/,\\n2024.\\n[25] T. Riley, ‚ÄúCybercriminals are posing as ukraine fundraisers to steal cryptocur-\\nrency.‚Äù https://cyberscoop.com/cybercriminals-are-posing-as-ukraine-\\nfundraisers-to-steal-cryptocurrency/, 2022.\\n[26] M. Britton, ‚ÄúAttackers exploit middle east crisis to solicit fraudulent cryptocur-\\nrency donations for children.‚Äù https://abnormalsecurity.com/blog/attackers-\\nexploit-middle-east-crisis-solicit-cryptocurrency-donations, 2023.\\n[27] NZU, ‚ÄúCybercriminals abuse advertisement on x to promote crypto scam.‚Äù https:\\n//news.zke.com/cybercriminals-abuse-advertisement-on-x-to-promote-\\ncrypto-scam/, 2024.\\n[28] NZU, ‚ÄúCybercriminals abuse advertisement on x to promote crypto scam.‚Äù https:\\n//news.zke.com/cybercriminals-abuse-advertisement-on-x-to-promote-\\ncrypto-scam/, 2024.\\n[29] G. Chow, ‚ÄúTrumped up crypto scams ‚Äì criminals deploy trump donation scams.‚Äù\\nhttps://www.netcraft.com/blog/trumped-up-crypto-donation-scams/, 2024.\\n[30] S. D. Github, ‚ÄúCode/data share.‚Äù https://github.com/CISPA-SysSec/scam_donati\\non, 2024.\\n[31] A. Oest, Y. Safaei, A. Doup√©, G.-J. Ahn, B. Wardman, and K. Tyers, ‚ÄúPhishfarm: A\\nscalable framework for measuring the effectiveness of evasion techniques against\\nbrowser phishing blacklists,‚Äù in 2019 IEEE Symposium on Security and Privacy\\n(SP), 2019.\\n[32] A. Oest, P. Zhang, B. Wardman, E. Nunes, J. Burgis, A. Zand, K. Thomas, A. Doup√©,\\nand G.-J. Ahn, ‚ÄúSunrise to sunset: Analyzing the end-to-end life cycle and effec-\\ntiveness of phishing attacks at scale,‚Äù in USENIX Security , 2020.\\n[33] B. Acharya and P. Vadrevu, ‚Äú {PhishPrint}: Evading phishing detection crawlers\\nby prior profiling,‚Äù in USENIX Security , 2021.\\n[34] P. Peng, L. Yang, L. Song, and G. Wang, ‚ÄúOpening the blackbox of virustotal: An-\\nalyzing online phishing scan engines,‚Äù in ACM Internet Measurement Conference\\n(IMC) , 2019.\\n[35] P. Zhang, A. Oest, H. Cho, Z. Sun, R. Johnson, B. Wardman, S. Sarker, A. Kaprav-\\nelos, T. Bao, R. Wang, Y. Shoshitaishvili, A. Doup√©, and G.-J. Ahn, ‚ÄúCrawlphish:\\nLarge-scale analysis of client-side cloaking techniques in phishing,‚Äù in IEEE Secu-\\nrity and Privacy (IEEE S&P) , 2021.\\n[36] K. Subramani, W. Melicher, O. Starov, P. Vadrevu, and R. Perdisci, ‚ÄúPhishinpat-\\nterns: measuring elicited user interactions at scale on phishing websites,‚Äù in ACM\\nInternet Measurement Conference (IMC) , 2022.\\n[37] J. Liu, P. Pun, P. Vadrevu, and R. Perdisci, ‚ÄúUnderstanding, measuring, and detect-\\ning modern technical support scams,‚Äù in IEEE European Symposium on Security\\nand Privacy (Euro S&P) , 2023.\\n[38] N. Miramirkhani, O. Starov, and N. Nikiforakis, ‚ÄúDial one for scam: A large-scale\\nanalysis of technical support scams,‚Äù in Network and Distributed System Security\\nSymposium (NDSS) , 2017.\\n[39] T. Liu, Y. Zhang, J. Shi, Y. Jing, Q. Li, and L. Guo, ‚ÄúTowards quantifying visual\\nsimilarity of domain names for combating typosquatting abuse,‚Äù in IEEE Military\\nCommunications , 2016.\\n[40] N. Nikiforakis, M. Balduzzi, L. Desmet, F. Piessens, and W. Joosen, ‚ÄúSoundsquat-\\nting: Uncovering the use of homophones in domain squatting,‚Äù in Information\\nSecurity International Conference (ISC) , 2014.\\n[41] N. Nikiforakis, S. Van Acker, W. Meert, L. Desmet, F. Piessens, and W. Joosen,\\n‚ÄúBitsquatting: Exploiting bit-flips for fun, or profit?, ‚Äù in World Wide Web (WWW) ,\\n2013.\\n[42] P. Agten, W. Joosen, F. Piessens, and N. Nikiforakis, ‚ÄúSeven months‚Äô worth of\\nmistakes: A longitudinal study of typosquatting abuse,‚Äù in Symposium on Network\\nand Distributed System Security (NDSS) , 2015.\\n[43] Y.-M. Wang, D. Beck, J. Wang, C. Verbowski, and B. Daniels, ‚ÄúStrider typo-patrol:\\nDiscovery and analysis of systematic typo-squatting.,‚Äù in USENIX Security , 2006.\\n[44] J. Szurdi, B. Kocso, G. Cseh, J. Spring, M. Felegyhazi, and C. Kanich, ‚ÄúThe long\\ntaile of typosquatting domain names,‚Äù in USENIX Security , 2014.\\n[45] P. Vadrevu and R. Perdisci, ‚ÄúWhat you see is not what you get: Discovering and\\ntracking social engineering attack campaigns,‚Äù in ACM Internet Measurement\\nConference(IMC) , 2019.\\n[46] A. Zarras, A. Kapravelos, G. Stringhini, T. Holz, C. Kruegel, and G. Vigna, ‚ÄúThe\\ndark alleys of madison avenue: Understanding malicious advertisements, ‚Äù in ACM\\nInternet Measurement Conference (IMC) , 2014.\\n[47] B. Srinivasan, A. Kountouras, N. Miramirkhani, M. Alam, N. Nikiforakis, M. An-\\ntonakakis, and M. Ahamad, ‚ÄúExposing search and advertisement abuse tactics\\nand infrastructure of technical support scammers,‚Äù in World Wide Web (WWW) ,\\n2018.\\n[48] X. Li, A. Yepuri, and N. Nikiforakis, ‚ÄúDouble and nothing: Understanding and\\ndetecting cryptocurrency giveaway scams,‚Äù in Network and Distributed Systems\\nSecurity (NDSS) , 2023.\\n[49] P. Xia, H. Wang, X. Luo, L. Wu, Y. Zhou, G. Bai, G. Xu, G. Huang, and X. Liu,\\n‚ÄúDon‚Äôt fish in troubled waters! characterizing coronavirus-themed cryptocurrency\\nscams,‚Äù in APWG Symposium on Electronic Crime Research (eCrime) , 2020.\\n14Exploring Donation-based Abuses in Social Media Platforms Under Review at WWW 2025 (v 1.0)\\n[50] R. Phillips and H. Wilder, ‚ÄúTracing cryptocurrency scams: Clustering repli-\\ncated advance-fee and phishing websites,‚Äù in IEEE International Conference on\\nBlockchain and Cryptocurrency (ICBC) , 2020.\\n[51] G. Hong, Z. Yang, S. Yang, X. Liao, X. Du, M. Yang, and H. Duan, ‚ÄúAnalyzing\\nground-truth data of mobile gambling scams,‚Äù in IEEE Symposium on Security\\nand Privacy (IEEE S&P) , 2021.\\n[52] B. Acharya, M. Saad, A. E. Cin√†, L. Sch√∂nherr, H. D. Nguyen, A. Oest, P. Vadrevu,\\nand T. Holz, ‚ÄúConning the crypto conman: End-to-end analysis of cryptocurrency-\\nbased technical support scams,‚Äù IEEE Security and Privacy (IEEE S&P) , 2024.\\n[53] X. Li, A. Rahmati, and N. Nikiforakis, ‚ÄúLike, Comment, Get Scammed: Character-\\nizing Comment Scams on Media Platforms,‚Äù in Proceedings of the Network and\\nDistributed System Security Symposium (NDSS) , 2024.\\n[54] M. Mirtaheri, S. Abu-El-Haija, F. Morstatter, G. Ver Steeg, and A. Galstyan, ‚ÄúIden-\\ntifying and analyzing cryptocurrency manipulations in social media,‚Äù in IEEE\\nTransactions on Computational Social Systems (ITCSS) , 2021.\\n[55] S. Khaled, N. El-Tazi, and H. M. Mokhtar, ‚ÄúDetecting fake accounts on social\\nmedia,‚Äù in IEEE International Conference on Big Data (ICBG) , 2018.\\n[56] J. Mink, L. Luo, N. M. Barbosa, O. Figueira, Y. Wang, and G. Wang, ‚Äú {DeepPhish}:\\nUnderstanding user trust towards artificially generated profiles in online social\\nnetworks,‚Äù in USENIX Security , 2022.\\n[57] B. Acharya, D. Lazzaro, E. L√≥pez-Morales, A. Oest, M. Saad, A. Emanuele Cin√†,\\nL. Sch√∂nherr, and T. Holz, ‚ÄúThe imitation game: Exploring brand impersonation\\nattacks on social media platforms,‚Äù in USENIX Security , 2024.\\n[58] X. Li, A. Rahmati, and N. Nikiforakis, ‚ÄúLike, comment, get scammed: Charac-\\nterizing comment scams on media platforms,‚Äù Network and Distributed System\\nSecurity Symposium (NDSS) , 2024.\\n[59] M. T. Whitty, ‚ÄúIs there a scam for everyone? psychologically profiling cyberscam\\nvictims,‚Äù European Journal on Criminal Policy and Research , 2020.\\n[60] J. S. Albanese, ‚ÄúFraud: The characteristic crime of the twenty-first century, ‚Äù Trends\\nin Organized Crime , 2005.\\n[61] A. A. Gillespie and S. Magor, ‚ÄúTackling online fraud,‚Äù in ERA Forum , Springer,\\n2020.\\n[62] S. Wood, D. Hengerer, and Y. Hanoch, ‚ÄúScams in the time of covid-19:: Pandemic\\ntrends in scams and fraud,‚Äù in A Fresh Look at Fraud , pp. 42‚Äì57, Routledge, 2022.\\n[63] Twitter, ‚ÄúUser detail twitter api.‚Äù https://developer.twitter.com/en/docs/twitter-\\napi/v1/accounts-and-users/follow-search-get-users/api-reference/get-users-\\nlookup, 2024.\\n[64] Twitter, ‚ÄúUser timelines twitter api. ‚Äù https://developer.twitter.com/en/docs/twitte\\nr-api/tweets/timelines/introduction, 2024.\\n[65] Apify, ‚ÄúApify instagram scraper api.‚Äù https://apify.com/apify/instagram-scraper,\\n2024.\\n[66] D. Milevski, ‚ÄúApify telegram scraper api.‚Äù https://apify.com/danielmilevski9/tele\\ngram-channel-scraper, 2024.\\n[67] D. Milevski, ‚ÄúTelemetrio telegram scraper api.‚Äù https://telemetr.io/, 2024.\\n[68] Apify, ‚ÄúYoutube scraper.‚Äù https://apify.com/streamers/youtube-scraper, 2024.\\n[69] Apify, ‚ÄúFacebook scraper.‚Äù https://apify.com/apify/facebook-posts-scraper, 2024.\\n[70] O. Platform, ‚ÄúModels - openai api (gpt-4o).‚Äù https://platform.openai.com/docs/m\\nodels/gpt-4o.\\n[71] VirusTotal, ‚ÄúVirustotal api v3 overview.‚Äù https://docs.virustotal.com/reference/ov\\nerview.\\n[72] I. E. V. API, ‚ÄúEmail validation api documentation.‚Äù https://www.ipqualityscore.c\\nom/documentation/email-validation-api/overview.\\n[73] I. P. V. API, ‚ÄúPhone validation api documentation.‚Äù https://www.ipqualityscore.c\\nom/documentation/phone-number-validation-api/overview.\\n[74] G. Bowyer, ‚ÄúCLD2-CFFI ‚Äì Python (CFFI) Bindings for Compact Language Detector\\n2,‚Äù 2016. https://github.com/GregBowyer/cld2-cffi.\\n[75] N. Reimers and I. Gurevych, ‚ÄúSentence-bert: Sentence embeddings using siamese\\nbert-networks,‚Äù in Empirical Methods in Natural Language Processing (EMNLP) ,\\n2019.\\n[76] M. Grootendorst, ‚ÄúBertopic: Neural topic modeling with a class-based tf-idf\\nprocedure,‚Äù arXiv preprint arXiv:2203.05794 , 2022.\\n[77] L. McInnes, J. Healy, N. Saul, and L. Gro√überger, ‚ÄúUmap: Uniform manifold\\napproximation and projection,‚Äù Journal of Open Source Software , 2018.\\n[78] L. McInnes, J. Healy, and S. Astels, ‚Äúhdbscan: Hierarchical density based cluster-\\ning,‚Äù Journal of Open Source Softw. , 2017.\\n[79] M. Grootendorst, ‚ÄúKeybert: Minimal keyword extraction with bert..‚Äù https://doi.\\norg/10.5281/zenodo.4461265, 2020.\\n[80] K. R. Shahapure and C. K. Nicholas, ‚ÄúCluster quality analysis using silhouette\\nscore,‚Äù Data Science and Advanced Analytics (DSAA) , 2020.\\n[81] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,\\nA. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, ‚ÄúLearning transferable\\nvisual models from natural language supervision,‚Äù in International Conference on\\nMachine Learning (ICML) , 2021.\\n[82] T. Hastie, J. H. Friedman, and R. Tibshirani, The Elements of Statistical Learning:\\nData Mining, Inference, and Prediction . Springer, 2001.\\n[83] U. Maulik and S. Bandyopadhyay, ‚ÄúPerformance evaluation of some clustering\\nalgorithms and validity indices,‚Äù in IEEE Transactions on Pattern Analysis and\\nMachine Intelligence (ITPAMI) , 2002.[84] ScamSearch, ‚ÄúGlobal scam database.‚Äù https://scamsearch.io/.\\nA Prompt Engineering on Donation Context\\nIn this section, we provide details on creating prompt injection in\\nidentifying the posts that are related to the donation context. We\\nchose LLMs specifically for their effectiveness and adaptability in\\nhandling diverse natural language processing tasks, making them\\nideal for accurately classifying fraudulent donation solicitations.\\nTo determine if a post is related to donation solicitations, we\\ndesigned a prompt that evaluates whether the input post includes\\ndonation requests, outputting the result as a boolean (true or false).\\nUsing the OpenAI API [ 70], we queried posts from the five social\\nmedia platforms to obtain their respective outputs. Below, we pro-\\nvide examples of prompt instruction along with input samples for\\nresponses received in both cases (false and true).\\nPrompt Instruction.\\nYou are given a text and must identify whether\\nit is requesting money, donations, or charity\\nsupport. The output should be a boolean value\\ncompatible with a Python boolean value. Do\\nnot include any explanation.\\nInput Sample Post - API Response True Case.\\nWE JUST HIT OUR GOAL OF $500 of donations\\nto Extra Life. We would like to thank everyone\\nwho donated to this great cause!\\nOutput of ChatGPT - API Response True Case.\\nTrue\\nInput Sample Post - API Response False Case.\\nRT @bbby**luve: Oi meus amores! We are only\\n15 days away from Brazil fanmeeting? Are you\\nready for that amazing night??\\nOutput of ChatGPT - API Response False Case.\\nFalse\\n15Under Review at WWW 2025 (v 1.0) Acharya et al.\\nFigure 6: Visualization of 50 random samples from Association-Logos (left), Games-Cartoon (right) clusters of scammers.\\nFigure 7: Visualization of 50 random samples from Female (left) Male (right) clusters of scammers.\\nFigure 8: Visualization of 50 random samples from Politics-War (Left), and Pets (right) clusters of scammers.\\nFigure 9: Visualization of 50 random samples from Miscellaneous clusters of scammers.\\n16',\n",
       " \"Integrating Zero -Shot Classification to Advance Long \\nCOVID Literature: A Systematic Social Media ‚ÄìCentered \\nReview  \\nNirmalya Tha kur \\nDepartment of Electrical Engineering and Computer Science , South Dakota School of Mines \\nand Technology , Rapid City, SD 57701, USA  \\nnirmalya.thakur@sdsmt.edu  \\nAbstract. Long COVID continues to challenge public health by affecting a signifi-\\ncant segment of individuals who have recovered from acute SARS -CoV -2 infection yet \\nendure prolonged and often debilitating symptoms. Social media has emerged as a vital \\nresource  for those seeking real -time information, peer support, and validating  their \\nhealth concerns  related to Long COVID . This paper examines recen t works  focusing  \\non mining, analyzing, and interpreting  user-generated content on social media plat-\\nforms such as X (formerly Twitter ), Reddit, Facebook, and YouTube to capture the \\nbroader discourse on persistent post -COVID conditions.  A novel transformer -based \\nzero-shot learning approach serves as the foundation for classifying research papers  in \\nthis area  into four primary categories: Clinical or Symptom Characterization, Advanced \\nNLP or Computational Methods, Policy, Advocacy, or Public Health Communication, \\nand Online Communities and Social Support . This method ology showcases the adapt-\\nability of advanced  language models in categorizing research  papers  without predefined \\ntraining labels, thus enabling a more rapid and scalable assessment of existing literature. \\nThis review  highlight s the multifaceted nature of Long COVID research, where com-\\nputational techniques applied to so cial medi a data reveal  insights  into narratives of in-\\ndividuals sufferin g from Long COVID. This review  also demonstrates the capacity of \\nsocial media analytics to inform clinical practice  and contribute to policy making  re-\\nlated  to Long COVID .  \\nKeywords:  Long COVID, COVID -19, Zer o-Shot Learning, social media , Twitter, Reddit, Face-\\nbook, and YouTube  \\n1 Introduction   \\nIn December 2019, an outbreak of coronavirus disease 2019 (COVID -19), caused \\nby the severe acute respiratory syndrome coronavirus 2 (SARS -CoV -2), began  in \\nChina.  [1,2]. Even though SARS -CoV -2 is similar in origin to SARS -CoV and MERS -\\nCoV, it has affected public health globally at a much greater scale than any prior  coro-\\nnavirus outbreaks [ 3]. Early containment efforts, including measures by the Chinese \\ngovernment, did not prevent the disease from rapidly crossing regional and interna-\\ntional boundaries  [4], leading the World Health Organization (WHO) to declare \\nCOVID -19 a global pandemic on March 11, 2021 [ 5]. According to the WHO, con-\\nfirmed cases were  776,841,264 worldwide, with 7,075,468 reported deaths  as of 10 \\nNovember 2024 [6].  2 \\nAlthough many individuals recover from the acute infection  caused by SARS -CoV -\\n2, a significant subset experiences symptoms that remain or appear after what might \\nhave been presumed clinical recovery. This phenomenon, known as Long COVID, has \\nbeen described since the earliest days of the pandemic to include persistent or emerging \\nphys ical and psychological challenges  [7-9]. As per [10], ‚ÄúLong COVID is defined as \\na chronic condition that occurs after SARS -CoV-2 infection and is present for at least \\n3 months. Long COVID includes a wide range of symptoms or conditions that may \\nimprove, worsen, or be ongoing‚Äù.  \\nIndividuals who experience Long COVID commonly face a broad set of symptoms \\nthat may disrupt daily routines and overall well -being. Frequently reported symptoms \\nof Long COVID  include shortness of breath, cough, persistent fatigue, post -exertional \\nmalaise, difficulty concentrating, memory changes, recurring headache, lightheaded-\\nness, fast heart rate, sleep disturbance, problems with taste or smell, bloating, constipa-\\ntion, and diarrhea  [11,12 ]. In more complex scenarios, patients are diagnosed with in-\\nterstiti al lung disease and hypoxemia, cardiovascular disease and arrhythmias, cogni-\\ntive impairment, mood disorders, anxiety, migraine, stroke, blood clots, chronic kidney \\ndisease, postural orthostatic tachycardia syndrome (POTS) and other forms of dysau-\\ntonomia, m yalgic encephalomyelitis/chronic fatigue syndrome (ME/CFS), mast cell ac-\\ntivation syndrome (MCAS), bromyalgia , connective tissue diseases, hyperlipidemia, \\ndiabetes, and autoimmune disorders such as lupus, rheumatoid arthritis, and Sjogren‚Äôs \\nsyndrome  [11,13 -15].  \\nSuch symptoms may persist for three months beyond the initial SARS -CoV -2 infec-\\ntion or even exceed a year [ 16]. Although some individuals gradually improve, others \\nexperience lingering or fluctuating complications that may profoundly affect their phys-\\nical, psychological, and social health [ 17,18 ]. Most treatment approaches revolve \\naround consistent monitoring and symptom -specific care. Clinicians commonly refer-\\nence established guidelines when managing symptoms of Long COVID and any ac-\\ncompanying conditions, such as diabetes, high blood pressure, or POTS, to reduce fu-\\nture complications and e nhance the patient's  quality of life  [19]. Many patients benefit \\nfrom a combination of therapies  - ranging from medications targeting pain or sleep \\nchallenges to physical or occupational rehabilitation  - along with psychological support \\nto manage both the physical and emotional aspects  of Long COVID [ 20,21 ]. In addition \\nto this, even though certain medications such as paracetamol or NSAIDs  appear to help \\nwith specific  Long COVID symptoms like fever, there is still no standardized treatment \\nto address the entire spectrum of Long COVID symptoms  [22-25]. \\nSocial media has been  a critical venue for public discussion of COVID -19 since  its \\ninitial cases in December 2019 , evolving into a resource for people seeking real -time \\ninformation and community support  [26-31]. As this pandemic advanced, platforms \\nsuch as X (formerly Twitter ) [32,33] , TikTok  [34,35] , Instagram  [36,37] , Facebook  \\n[38,39] , YouTube  [40,41] , Reddit  [42,43] , LinkedIn  [44,45] , Clubhouse  [46,47] , Dis-\\ncord [48,49] , and Snapchat  [50,51] , became pivotal for gathering firsthand insights into \\nongoing patient experiences. Tr aditional methods like surveys and interviews can be \\nconstrained by time and location, whereas social media allows continuous, unfiltered \\naccounts of Long COVID manifestations and daily struggles. Individuals suffering \\nfrom Long COVID can  document their symptoms, exchange practical advice, and dis-\\ncuss personal setbacks or milestones  on social media, leading to the generation of Big 3 \\nData  that researchers from different disciplines and healthcare professionals may ana-\\nlyze to identify evolving patterns.  \\nAs Long COVID presents multifaceted medical, social, and emotional issues,  there \\nhas been growing interest in leveraging online platforms to study it from multiple an-\\ngles. Social media channels facilitate global conversations that can reveal differences \\nin experiences related to healthcare access, post -infection complications, or  even public \\nawareness of the severity  of a health -related  conditio n [52-54]. Over time, these virtual \\nspaces have also fostered advocacy and grassroots efforts. Hashtags like #LongCOVID \\n[55] have given patients and advocates an active role in discussing everything from \\nspecialized clinics to  mental health support  [56]. Observations of this activity under-\\nscore how large -scale social media data can shape public health discourse [57] and even \\ninfluence policies [58] addressing health -related  conditio ns that are often misunder-\\nstood or underdiagnosed.  \\nA review paper that categorizes the existing work in this domain is expected to play \\na crucial role  in advancing knowledge. Studies on Long COVID and social media vary \\nwidely, incorporating sentiment analysis, network analysis, qualitative content studies, \\nand more. Combining or comparing such research can clarify  where the field has gath-\\nered robust evidence, where it lacks conclusive data, and which areas still need system-\\natic exploration. Recent works  have drawn on patient narratives to refine clinical defi-\\nnitions or inform the development of Long COVID  care frameworks, yet there is a \\ncrucial  need to consolidate these findings. Examining studies in this area under  broader \\nthematic groupings is expected  to highlight the progress made so far  and pinpoint  un-\\nresolved questions, methodological gaps, and ethical considerations surrounding pa-\\ntient data.  Addressing this research gap serves as the main motivator for this review.  \\nThis review focuses on four broad  areas  - Clinical or Symptom Characterization, \\nAdvanced NLP or Computational Methods, Policy, Advocacy, or Public Health Com-\\nmunication, Online Communities and Social Support , in the context of Long COVID -\\nrelated research works that specifically focus on mining, analyzing, and interpreting \\nsocial media data . It is relevant to mention that this review not only investigates the \\nbroad spectrum of research on Long COVID across various social media platforms but \\nalso integrates a novel zero-shot classification pipeline that organizes recent works in \\nthis field  into distinct  categories. Th is classification process was performed  without \\nexplicit task -specific training using  a transformer -based model configured for zero -shot \\nlearning. This dual perspective highlights both a comprehensive  review of the literature \\nand a demonstration of how an advanced  language model  can streamline the analysis \\nof research papers  on Long COVID. This review also discusses current research limi-\\ntations and proposes future work directions  that are expected to benefit both scientific \\ncommunities and those living with Long COVID .  \\nAlthough these studies appear  classified  into distinct areas in this review paper , \\nmany  works  address multiple facets of Long COVID research, rendering any classifi-\\ncation flexible rather than absolute. For instance, a paper listed under ‚Äú Online Commu-\\nnities and Social Support ‚Äù may also perform a detailed sentiment analysis that aligns \\nwith ‚Äú Advanced NLP or Computational Methods ‚Äù. Such overlaps arise naturally in in-\\nterdisciplinary research, especially when varied computational methods  - like sentiment \\nanalysis, topic modeling, and network analysis  - are applied to the extensive social me-\\ndia discussions surrounding patient experiences, advocacy efforts, and policy 4 \\nimplications. The four broad areas presented  here are an organizational guide, high-\\nlighting a primary thematic focus without dismissing other significant aspects of each \\nstudy.  \\nThe rest of this paper is organized as follows. First, the methodology is presented, \\ndescribing the search strategy, inclusion criteria, and the steps taken to apply a zero -\\nshot learning model. Then, the results from this automated classification process a re \\ndiscussed, followed by a detailed examination of each study identified in the review. \\nResearch gaps and directions for future work  are then explored, emphasizing  how in-\\nterdisciplinary approaches and advanced computational tools might enrich the current \\nunderstanding of Long COVID. Finally, the paper summarizes key findings and high-\\nlights  the potential impact of integrating social media analytics into ongoing research \\non persistent COVID -19 symptoms.  \\n2 Methodology  \\nA broad literature search was carried out across multiple scholarly databases, includ-\\ning PubMed, Scopus, Web of Science, and Google Scholar, to identify studies focused \\non mining, analyzing, and interpreting  the public discourse about  Long COVID on so-\\ncial media. This search aimed  to capture Long COVID -related research across diverse \\nfields, such as computer science, health sciences , and social sciences. No papers pub-\\nlished before 2020 were included, as the COVID -19 outbreak began in December 2019.  \\nThe search terms used included - ‚ÄúLong COVID,‚Äù ‚Äúpost -COVID,‚Äù ‚Äúchronic COVID‚Äù  - \\nas well as keywords indicative of social media use, such as ‚ÄúTwitter ‚Äù, ‚ÄúTikTok ‚Äù, ‚ÄúIn-\\nstagram ‚Äù, ‚ÄúFacebook ‚Äù, ‚ÄúYouTube ‚Äù, ‚ÄúReddit ‚Äù, ‚ÄúLinkedIn ‚Äù, ‚ÄúClubhouse ‚Äù, ‚ÄúDiscord ‚Äù, \\nand ‚ÄúSnapchat ‚Äù. In addition, terms like ‚Äúsentiment analysis,‚Äù ‚Äútopic modeling,‚Äù and \\n‚Äúnetwork analysis‚Äù were included to ensure the retrieval of studies that used computa-\\ntional or statistical methods to examine the public discourse  on these platforms. By \\nblending health -related terminology with references to digital platforms and relevant \\nanalytica l techniques, the search strategy was designed to capture the full breadth of \\nscholarly work investigating individuals‚Äô ongoing experiences with Long COVID . \\nStudies were selected for inclusion if they used social media data to investigate any \\naspect of Long COVID. The main inclusion criteria were that the articles utilized  a \\nrecognized research methodology  - whether qualitative, quantitative, or mixed methods  \\nand analyze d data gathered primarily from social media platforms. The selection pro-\\ncess also considered whether the authors had sufficiently detailed the nature of their \\nquantitative  or qualitative approach. Moreover, ethical practices regarding user data, \\nsuch as anonymization or compliance with platform terms of service, were taken into \\naccount to ensure that privacy concerns were handled responsibly.  Studies that just \\nmentioned  Long COVID  were excluded. Research works  such as editorials, letters to \\nthe editor, or general news articles, which usually lack methodological details, were \\nalso excluded . If the essential aspects of a paper  were missing  - for example, neglecting \\nto report how data were collected  - those were also removed from consideration. This \\napproach aimed to retain a set of methodologically sound articles  that offered substan-\\ntive insights into the public discourse about  Long COVID  on social media platforms . \\nUpon applying these inclusion and exclusion criteria, a total of 4 0 studies were se-\\nlected for this review . These works represented a range of methods, including sentiment 5 \\nanalysis, qualitative content analysis, topic modeling, and network analysis, and they \\naddressed multiple social media platforms, such as X (formerly Twitter ), Reddit, and \\nFacebook. Thereafter, a transformer -based zero -shot classification model  was devel-\\noped  to classify these papers into one out of the four thematic categories described \\nbelow . Although a few papers  fit under multiple themes , each was placed wherever its \\nprimary emphasis appeared strongest.  \\n(i) Clinical or Symptom Characterization (‚ÄúSymptom Characterization‚Äù) : Re-\\nsearch that primarily aims to identify, list, or quantify the variety of Long \\nCOVID symptoms, usually from social media data. The studies may in-\\nclude  statistic al analysis  but do not necessarily perform  extensive sentiment \\nor topic modeling. Their main motivation is to gather clinical or epidemio-\\nlogical insights from user posts.  \\n(ii) Advanced NLP or Computational Methods (‚ÄúNLP and Modeling‚Äù) : Studies \\nthat specifically emphasize methods like deep transformer networks, topic \\nmodeling, sentiment analysis , and other elaborate computational ap-\\nproaches. This goes beyond a simple symptom count; it highlights a meth-\\nods-heavy lens on analyzing data.  \\n(iii) Policy, Advocacy, or Public Health Communication (‚ÄúPolicy and Advo-\\ncacy‚Äù) : Papers exploring how organizations, governments, or communities \\ndevelop health communications, handle policy issues, and communicate  \\nguidelines .  \\n(iv) Online Communities & Social Support (‚ÄúCommunity and Support‚Äù) : Stud-\\nies focusing on how individuals find emotional or experiential support on \\nsocial media, the way they exchange personal stories, or how group dynam-\\nics form around shared experiences. The main emphasis is on the psycho-\\nsocial aspect , and the support social media platforms  provide.  \\nThereafter, a  transformer -based zero -shot classification model  was developed , \\nwhich was set up to assign the 40 papers  to these  categories . This model did not require \\nany training using any  labeled dataset. The process by which this model worked is de-\\nscribed below:  \\nFormally, let ‚Äútext‚Äù be a study‚Äôs abstract, and suppose we have candidate labels { ùëê1, \\nùëê2, ‚Ä¶, ùëêùëÅ}. The model use d a scoring function to determine the  alignment between \\n‚Äútext‚Äù and a label ck. Equation (1) shows  how the best -matching category, denoted  as \\nƒâ(text) was determined . \\n \\n (1) \\nIn Equation (1), p k(text) represents  the probability that text belongs to category  ck. \\nIn practical terms, the system prompt ed the model with textual descriptions of each \\ncategory and the study‚Äôs abstract. It then compute d a scalar score ùë†(text, ùëò) (as shown \\nin Equation (2)) that measure d how well text matche d the meaning or intent of label ck. \\nA softmax function then normalize d these raw scores, producing probabilities for all \\ncategories  as shown in Equation (3).  \\n \\n (2) \\n6 \\n (3) \\n \\nIn essence, whichever category attain ed the largest probability was selected as the \\nlabel by the model  for that document. This method ology  is called zero -shot learning \\n[59-61] because the model does not require an example corpus manually labeled under \\nthese same categories. Instead, the model draws from its vast, pre -trained language rep-\\nresentations to infer whether a given textual data  aligns more with, for instance, a \\n‚ÄúSymptom Characterization ‚Äù theme or a ‚Äú Community and Support ‚Äù theme.  A program \\nwas written in Python 3.10 to develop and implement this transformer -based zero -shot \\nclassification model . The results of the same are presented and discussed in Section 3.  \\n3 Results of Zero -Shot Classification  \\nThis section presents the results of the transformer -based zero -shot classification \\nmodel  applied to the 4 0 research papers  [62-102] that met the inclusion criteria  of this \\nreview . The underlying premise of zero -shot learning is that when prompted with suit-\\nable descriptors, a well -trained language model can identify the most relevant label for \\ntextual data, even if the model  has never seen concrete examples corresponding to that \\nlabel during training. This eliminate s the need for time -intensive and resource -heavy \\ndata labeling processes, which is especially advantageous in emerging research areas \\nsuch as COVID -19-related research where existing taxonomies may be incomplete or \\nstill evolving  [103-106]. In addition to the inherent benefit of not requiring pre -labeled \\ndata, this methodology also provide d a structured and transparent way to allocate these \\n40 research papers  into distinct categories. By integrating zero -shot learning with care-\\nfully curated dictionary -based keyword matching, it became possible to identify and  \\nhighlight the thematic focus of each study  and classify it into one of the four categories \\n- Clinical or Symptom Characterization, Advanced NLP or Computational Methods, \\nPolicy, Advocacy, or Public Health Communication, Online Communities and Social \\nSupport . The dictionary -based scores helped complement the probabilistic outputs from \\nthe zero-shot classification  model, thereby refining the final assessments of each \\nstudy‚Äôs thematic focus. This synergy proved particularly useful for works where tech-\\nnical and clinical terminologies might intersect, making it difficult to rely solely on \\neither semantic features or explici t keyword usage. The result was a more robust and \\ninterpretable categorization pipeline that could be applied to other domains as well with \\nminimal customization effort.  Table 1 shows the results where the author list, title, and \\nclassification label of each paper are shown.  \\n \\nTable 1 : Results of applying zero -shot lea rning to classify the 40 papers that met the \\ninclusion criteria  \\nFull Author List  Title  Classifica-\\ntion Label  \\nYu-Bo Fu  [62] Investigating public perceptions regarding the \\nLong COVID on Twitter using sentiment analy-\\nsis and topic modeling  NLP and \\nModeling  \\n7 \\nAlex Rushforth, Emma \\nLadds, Sietse Wieringa, \\nSharon Taylor, Laiba Hu-\\nsain and Trisha Greenhalgh  \\n[63] Long Covid ‚Äì The illness narratives  Policy and \\nAdvocacy  \\nDavid Russell, Naomi J. \\nSpence, Jo -Ana D. Chase, \\nTatum Schwartz, Christa M. \\nTumminello and Erin \\nBouldin  [64] Support amid uncertainty: Long COVID illness \\nexperiences and the role of online communities  Commu-\\nnity and \\nSupport  \\nFrancesco Meledandri  [65] The Impact of Polarised Social Media Network-\\ning Communications in the #Longcovid Debate \\nbetween Ideologies and Scientific Facts  Commu-\\nnity and \\nSupport  \\nShubh Mohan Singh and \\nChaitanya Reddy  [66] An Analysis of Self -reported Longcovid Symp-\\ntoms on Twitter  Symptom \\nCharacter-\\nization  \\nNida Ziauddeen, Deepti \\nGurdasani, Margaret E \\nO‚ÄôHara, Claire Hastie, Paul \\nRoderick, Guiqing Yao and \\nNisreen A Alwan  [67] Characteristics of Long Covid: findings from a \\nsocial media survey  Symptom \\nCharacter-\\nization  \\nAbeed Sarker and Yao Ge  \\n[68] Long COVID symptoms from Reddit: Charac-\\nterizing post -COVID syndrome from patient re-\\nports  Symptom \\nCharacter-\\nization  \\nJuan M. Banda, Nicola Ad-\\nderley, Waheed -Ul-Rahman \\nAhmed, Heba AlGhoul, \\nOsaid Alser, Muath Alser, \\nCarlos Areia, Mikail Co-\\ngenur, Krisitina Fi≈°ter, \\nSaurabh Gombar, Vojtech \\nHuser, Jitendra Jon-\\nnagaddala, Lana YH Lai, \\nAngela Leis, Lourdes Ma-\\nteu, Miguel Angel Maye r, \\nEvan Minty, Daniel Mo-\\nrales, Karthik Natarajan, \\nRoger Paredes, Vyjeyanthi \\nS. Periyakoil, Albert Prats -\\nUribe, Elsie G. Ross, \\nGurdas Singh, Vignesh Sub-\\nbian, Arani Vivekanantham \\nand Daniel Prieto -Alhambra  \\n[69] Characterization of long -term patient -reported \\nsymptoms of COVID -19: an analysis of social \\nmedia data  Symptom \\nCharacter-\\nization  \\nDaisy Massey, Diana Ber-\\nrent and Harlan Krumholz  \\n[70] Breakthrough Symptomatic COVID -19 Infec-\\ntions Leading to Long Covid: Report from Long \\nCovid Facebook Group Poll  Symptom \\nCharacter-\\nization  \\nSam Martin, Macarena \\nChepo, No√©mie D√©om, Ah-\\nmad Firas Khalid and Ce-\\ncilia Vindrola -Padros  [71] ‚Äú#LongCOVID affects children too‚Äù: A Twitter \\nanalysis of healthcare workers‚Äô sentiment and \\ndiscourse about Long COVID in children and \\nyoung people in the UK  Symptom \\nCharacter-\\nization  8 \\nElham Dolatabadi, Diana \\nMoyano, Michael Bales, \\nSofija Spasojevic, Rohan \\nBhambhoria, Junaid Bhatti, \\nShyamolima Debnath, \\nNicholas Hoell, Xin Li, \\nCeline Leng, Sasha Nanda, \\nJad Saab, Esmat Sahak, \\nFanny Sie, Sara Uppal, \\nNirma Khatri Vadlamudi, \\nAntoaneta Vladimi rova, Ar-\\ntur Yakimovich, Xiaoxue \\nYang, Sedef Akinli Kocak \\nand Angela M. Cheung  [72] Using Social Media to Help Understand Long \\nCOVID Patient Reported Health Outcomes: A \\nNatural Language Processing Approach  Symptom \\nCharacter-\\nization  \\nLin Miao, Mark Last and \\nMarina Litvak  [73] An Interactive Analysis of User -reported Long \\nCOVID Symptoms using Twitter Data  Symptom \\nCharacter-\\nization  \\nGuocheng Feng, Huaiyu Cai \\nand Wei Quan  [74] Exploring the Emotional and Mental Well -Being \\nof Individuals with Long COVID Through Twit-\\nter Analysis  Symptom \\nCharacter-\\nization  \\nAlexis Jordan and Albert \\nPark [75] Understanding the Long Haulers of COVID -19: \\nMixed Methods Analysis of YouTube Content  NLP and \\nModeling  \\nIkhwan Yuda Kusuma and \\nSuherman Suherman  [76] The Pulse of Long COVID on Twitter: A Social \\nNetwork Analysis  NLP and \\nModeling  \\nNirmalya Thakur  [77] Investigating and Analyzing Self -Reporting of \\nLong COVID on Twitter: Findings from Senti-\\nment Analysis  NLP and \\nModeling  \\nToluwalase Awoyemi, \\nUjunwa Ebili, Abiola \\nOlusanya, Kayode E. Ogun-\\nniyi and Adedolapo V. \\nAdejumo  [78] Twitter Sentiment Analysis of Long COVID \\nSyndrome  Symptom \\nCharacter-\\nization  \\nSam Rhodehamel  [79] Digital Long Hauler Lifelines: Understanding \\nHow People with Long Covid Build Community \\non Reddit  Commu-\\nnity and \\nSupport  \\nArinjita Bhattacharyya, \\nAnand Seth and Shesh Rai  \\n[80] The Effects of Long COVID -19, Its Severity, and \\nthe Need for Immediate Attention: Analysis of \\nClinical Trials and Twitter Data  Policy and \\nAdvocacy  \\nSurani Matharaarachchi, \\nMike Domaratzki, Alan \\nKatz and Saman Muthuku-\\nmarana  [81] Discovering Long COVID Symptom Patterns: \\nAssociation Rule Mining and Sentiment Analy-\\nsis in Social Media Tweets  Symptom \\nCharacter-\\nization  \\nJonathan Koss and Sabine \\nBohnet -Joschko  [82] Social Media Mining of Long -COVID Self -\\nMedication Reported by Reddit Users: Feasibil-\\nity Study to Support Drug Repurposing  Symptom \\nCharacter-\\nization  \\nHanin Ayadi, Charline \\nBour, Aur√©lie Fischer, Mo-\\nhammad Ghoniem and Guy \\nFagherazzi  [83] The Long COVID Experience from a Patient's \\nPerspective: A Clustering Analysis of 27,216 \\nReddit Posts  Symptom \\nCharacter-\\nization  \\nCamryn Garrett, Atefeh \\nAghaei, Abhishek Ag-\\ngarwal and Shan Qiao  [84] The Role of Social Media in the Experiences of \\nCOVID -19 Among Long -Hauler Women: Qual-\\nitative Study  Commu-\\nnity and \\nSupport  9 \\nLinnea I. Laestadius, \\nJeanine P. D. Guidry, An-\\ndrea Bishop and Celeste \\nCampos -Castillo  [85] State Health Department Communication about \\nLong COVID in the United States on Facebook: \\nRisks, Prevention, and Support  Policy and \\nAdvocacy  \\nJuan S. Izquierdo -Condoy, \\nRaul Fernandez -Naranjo, \\nEduardo Vasconez -Gonz√°-\\nlez, Simone Cordovez, An-\\ndrea Tello -De-la-Torre, \\nClara Paz, Karen Delgado -\\nMoreira, Sarah Carrington, \\nGin√©s Viscor and Esteban \\nOrtiz -Prado  [86] Long COVID at Different Altitudes: A Country-\\nwide Epidemiological Analysis  Symptom \\nCharacter-\\nization  \\nSara Santarossa, Ashley \\nRapp, Saily Sardinas, Janine \\nHussein, Alex Ramirez, An-\\ndrea E Cassidy -Bushrow, \\nPhilip Cheng and Eunice Yu  \\n[87] Understanding the #longCOVID and #longhaul-\\ners Conversation on Twitter: Multimethod Study  Commu-\\nnity and \\nSupport  \\nAm√©lia D√©guilhem, Joelle \\nMalaab, Manissa Talmat-\\nkadi, Simon Renner, Pierre \\nFoulqui√©, Guy Fagherazzi, \\nPaul Loussikian, Tom \\nMarty, Adel Mebarki, \\nNathalie Texier and \\nStephane Schuck  [88] Identifying Profiles and Symptoms of Patients \\nWith Long COVID in France: Data Mining Info-\\ndemiology Study Based on Social Media  Symptom \\nCharacter-\\nization  \\nElham Dolatabadi, Diana \\nMoyano, Michael Bales, \\nSofija Spasojevic, Rohan \\nBhambhoria, Junaid Bhatti, \\nShyamolima Debnath, \\nNicholas Hoell, Xin Li, \\nCeline Leng, Sasha Nanda, \\nJad Saab, Esmat Sahak, \\nFanny Sie, Sara Uppal, \\nNirma Khatri Vadlamudi, \\nAntoaneta Vladimi rova, Ar-\\ntur Yakimovich, Xiaoxue \\nYang, Sedef Akinli Kocak \\nand Angela M. Cheung  [89] Using Social Media to Help Understand Patient -\\nReported Health Outcomes of Post ‚ÄìCOVID -19 \\nCondition: Natural Language Processing Ap-\\nproach  Symptom \\nCharacter-\\nization  \\nNida Ziauddeen, Deepti \\nGurdasani, Margaret E. \\nO‚ÄôHara, Claire Hastie, Paul \\nRoderick, Guiqing Yao and \\nNisreen A. Alwan  [90] Characteristics and Impact of Long Covid: Find-\\nings from an Online Survey  Symptom \\nCharacter-\\nization  \\nLudovica Segneri, Nandor \\nBabina, Teresa \\nHammerschmidt, Andrea \\nFronzetti Colladon and \\nPeter A. Gloor  [91] Too Much Focus on Your Health Might Be Bad \\nfor Your Health: Reddit User‚Äôs Communication \\nStyle Predicts Their Long COVID Likelihood  Symptom \\nCharacter-\\nization  10 \\nSai C. Reddy, Sanjana \\nKathiravan and Shubh M. \\nSingh  [92] An Analysis of Self -reported Long COVID -19 \\nSymptoms on Twitter  Symptom \\nCharacter-\\nization  \\nAbeed Sarker  [93] Mining Long -COVID Symptoms from Reddit: \\nWhat We Know So Far  Symptom \\nCharacter-\\nization  \\nEsperanza Miyake and Sam \\nMartin  [94] Long COVID: Online Patient Narratives, Public \\nHealth Communication, and Vaccine Hesitancy  Commu-\\nnity and \\nSupport  \\nAbeed Sarker and Yao Ge  \\n[95] Mining Long -COVID Symptoms from Reddit: \\nCharacterizing Post -COVID Syndrome from Pa-\\ntient Reports  Symptom \\nCharacter-\\nization  \\nAlexis Jordan and Albert \\nPark [96] Understanding the Plight of COVID -19 Long \\nHaulers Through Computational Analysis of \\nYouTube Content  NLP and \\nModeling  \\nBrigitte Juanals and Jean -\\nLuc Minel  [97] Using topic modeling and NLP  tools for analyz-\\ning long Covid coverage by French press and \\nTwitter  Commu-\\nnity and \\nSupport  \\nErkan Ozduran and Sibel \\nB√ºy√ºk√ßoban  [98] A Content Analysis of the Reliability and Qual-\\nity of YouTube Videos as a Source of Infor-\\nmation on Health -Related Post -COVID Pain  Commu-\\nnity and \\nSupport  \\nNo√©mie D√©om, Ahmad \\nFiras Khalid, Sam Martin, \\nMacarena Chepo, and Ce-\\ncilia Vindrola -Padros  [99] Unlocking the Mysteries of Long COVID in \\nChildren and Young People: Insights from a Pol-\\nicy Review and Social Media Analysis in the UK  Policy and \\nAdvocacy  \\nErin T. Jacques, Corey H. \\nBasch, Eunsun Park, Betty \\nKollia and Emma Barry  \\n[100]  Long Haul COVID -19 Videos on YouTube: Im-\\nplications for Health Communication  Symptom \\nCharacter-\\nization  \\nWilliam David Strain, \\nOndine Sherwood, Amitava \\nBanerjee, Vicky Van der \\nTogt, Lyth Hishmeh and \\nJeremy Rossman  [101]  The Impact of COVID Vaccination on Symp-\\ntoms of Long COVID: An International Survey \\nof People with Lived Experience of Long \\nCOVID  Symptom \\nCharacter-\\nization  \\nKrittiya Wongtavavimarn  \\n[102]  Social Support and Narrative Sensemaking \\nOnline: A Content Analysis of Facebook Posts \\nby COVID -19 Long Haulers  Commu-\\nnity and \\nSupport  \\n \\nIn emerging interdisciplinary research  areas such as Long COVID, the capacity to \\ncategorize studies without manually curated labels represents a novel contribution . \\nFields involving public health, computational linguistics, and social sciences often con-\\nverge on complex research questions, making any single classification system insuffi-\\ncient on its own. The zero -shot framework addressed this issue by enabling rapid, yet \\nreliable, placement of studies within relevant categories, facilitating a coherent view o f \\nhow different facets  - like symptom trajectories, policy guidance, and community en-\\ngagements  - interact in the evolving literature. Such a methodology also offers a blue-\\nprint for future works  that require the integration of heterogeneous sources of \\nknowledge, allowing researchers to devote more time to interpreting outcomes rather \\nthan refining labeling procedures.  In Section 4, a review of all these studies is presented.  11 \\n4 Review of Papers  \\nIn this section , each study that met the inclusion c riteria of this work has been re-\\nviewed  under  one of four broad areas , according to assignments generated by the zero -\\nshot learning model (discussed in Section  3). This automated classification process dis-\\ntinguished primary thematic emphases among the papers, placing them into ‚ÄúNLP and \\nModeling,‚Äù ‚ÄúPolicy and Advocacy,‚Äù ‚ÄúCommunity and Support,‚Äù or ‚ÄúSymptom Charac-\\nterization.‚Äù The subsequent sections explore each area, discussing how individual stud-\\nies addressed multimodal forms of social media -based inquiries into Long COVID.  \\n \\n4.1 NLP and Modeling  \\nFu [42] studied  concerns  about  Long COVID  as expressed on s ocial media . They \\nanalyzed  117,789 tweets  from March 2022 to April 2022 and utilized sentiment analy-\\nsis and topic modeling . Their objectives included identifying emergent themes from \\nusers‚Äô experiences, such as the social and economic burdens tied to Long COVID . They  \\nobserved that negative attitudes toward Long COVID  were especially widespread and \\nnoted that such sentiments raise d important considerations for clinicians and policy-\\nmakers . Jordan et al. [75]  conducted an investigation that combined a mixed approach \\nwith topic modeling. They gathered online data from medical sources, news outlets, \\nand self -identified ‚Äúlong haulers ‚Äù, highlighting how personal distress connected with \\ndissatisfaction regarding the healthcare system  in the context of L ong C OVID. They  \\nfound  multiple themes, most of which showe d concerns related to Long COVID that \\nhad either been disregarded or insufficiently recognized.  \\nKusuma et al. [76]  studied  social media data to isolate the most frequently discussed \\ntopics and to identify influential users engaging with the concept of extended recovery  \\nin the context of Lon g COVID . They used social network analysis and sentiment anal-\\nysis. They anal yzed  119,185 tweets from 94 ,325 users  to demonstrate how certain pub-\\nlic figures or health professionals influenced these discussions. The  findings of senti-\\nment analysis showed that most of these tweets we re negative. Thakur [77] studied \\n1,244,051 tweets  about Long COVID with a specific f ocus on using  VADER for sen-\\ntiment analysis . The findings showed that the percentages of tweets with positive, neg-\\native, and neutral sentiments were 43.1%, 42.7%, and 14.2%, respectively . The findings \\nof this study also showed that most tweets with a positive sentiment and most tweets  \\nwith a negative sentiment  were not highly polarized .  \\nThe study by Koss et al. [82] explored  the feasibility of social media mining methods \\nto extract insights shared by  Long COVID patients . They focused  on extracting insights \\nfrom Reddit  (‚Äú/r/covidlonghaulers ‚Äù), where participants described supplements and \\nmedications that they tested for symptom relief. Using named -entity recognition, they \\nmapped out networks to illustrate how certain substances  - such as magnesium, vita-\\nmins, and steroids  - appeared frequently and often in connection with each other  on \\nReddit . Jordan [96] conducted  text-mining of Long COVID -related content on  \\nYouTube. They collected transcripts and comments to learn how self -identified ‚Äúlong \\nhaulers‚Äù perceived their illnesses  and how broader audiences reacted. Jordan identified \\nrecurring issues that spanned uncertainty regarding medical systems, misinformation, \\nand the need for  coping strategies by applying topic modeling . The work of  Awoyemi \\net al. [78] involved  another exploration of tweets  about Long COVID. Their work's 12 \\ninitial data mining process resulted  in 62,232 tweets , which  were reduced to 10,670 \\ntweets after removing the duplicates. The ir study showed that  the majority of the tweets \\nabout Long COVID originated from the United States of America (38%), United King-\\ndom (30%), and Canada (13%), with the most common hashtags being #longcovid \\n(36%) and #covid (6.36%), and the most frequently used word being people (1.05%).  \\nThey also perfor med sentiment analys is, which showed that  the top three emotions de-\\ntected in these tweets  were trust (11.68%), fear (11.26%), and sadness (9.76%) .  \\n4.2 Policy and Advocacy  \\nRushforth et al. [63] used narrative inquiry  and analyze d a dataset of narrative inter-\\nviews and focus groups with 114 people with Long Covid from the United Kingdom, \\ndrawing on socio -narratology , therapeutic emplotment , and polyphonia . Their study \\nshowed how these personal stories served as catalysts for policy efforts and structural \\nreforms, emphasizing how influential firsthand accounts can be  helpful  in compelling \\ndecision -makers  to address an emerging public health challenge  such as Long COVID.  \\nBhattacharyya et al. [80] studied tweets about Long COVID to understand  the need \\nfor more resources to investigate the extended trajectory of COVID -19. They  used the \\nNational Research Council (NRC) Emotion Lexicon method for sentiment analysis  and \\nidentified an association between  retweets  and favorite counts on Twitter  and particular \\nemotional reactions, such as sadness, joy, or trust. Laestadius et al. [85] examined how \\nUS state health departments used Facebook  for public messaging about COVID -19, \\nwith a particular focus on mentions of Long COVID. Their study identified 49,310  \\npandemic -related posts, with fewer than 200 explicitly discussing  Long COVID . Using \\nquantitative content analysis methods, they coded these posts about Long COVI D. The \\nresults showed that  75.18% included language about susceptibility, 64.96% severity, \\nand 64.23% benefits of prevention. In addition to this, c ues to preventive action ap-\\npeared in 54.01% of posts  and 19.71% of posts provided guidance for those with Long \\nCOVID .  D√©om  et al. [99] used a mixed -methods approach  to analyze policy documen-\\ntation and social media discourse about children and teenagers suffering from Long \\nCOVID in the United Kingdom . The  authors used the LISTEN framework to demon-\\nstrate inconsistency in how guidelines reached the public and to emphasize the demand \\nfor mental health services for children, young people, and healthcare workers suffering \\nfrom  Long COVID. In their work, they also presented se veral policy recommendations , \\nsuch as enhancing accountability through regular audits, promoting inclusiveness by \\nincorporating perspectives  of children and young people , ensuring transparency via reg-\\nular updates, and maintaining equity in policy impact .  \\n \\n4.3 Community and Support  \\nRussell et al. [64] investigated how online communities offered solace and informa-\\ntional resources to individuals with Long COVID  symptoms. Through qualitative in-\\nterviews, they found that people experiencing these symptoms went through significant \\nambiguity, which was often made worse by invalidation or denial in clinical settings. \\nThe findings of their work showed that online communities  filled th is gap by offering \\nmutual support and reassurance, thus illustrating the essential psychological role that \\nsocial  networks can serve.  Meledandri [65] perfo rmed  a quantitative and qualitative \\nevaluation of approxi mately 600,000  twee ts about Long COVID . Their study showed 13 \\nthat s ome of these tweets  reflect ed conspiracy  theories involving vaccination, fake \\nnews , and post -truths , clashing with scientific evidence , and the remaining tweets re-\\nflected supportive stances . Rhodehamel [79] studied the public discourse about L ong \\nCOVID on Reddit  (r/covidlonghaulers ).  Their study showed  that community in the \\ncontext of Long COVID was built on Reddit  through three main themes . First, hope \\nthrough validation, knowledge sharing, and helpfulness. Second, k inship through com-\\nmiseration and shared experiences of suffering. Finally, the discourse surrounding \\nharm , including  ableism, grifting, exploitation, infighting, and tensions between people \\nsuffer ing from Long COVID  and society .  \\nGarrett et al. [84] investigated the experiences of women with Long COVID symp-\\ntoms  with a specific focus  on how social media played a dual role in either nurturing or \\nundermining well -being. The study sh owed that t he main roles of social media included \\nfacilitating support group participation, experience sharing, interpersonal connections, \\nand media consumption. The study also showed  that participants rel ied on social media \\nto fulfill their emotional support, social engagement, spirituality, health planning, in-\\nformation gathering, professional support, and recreational relaxation needs . The work \\ndone by Santarossa et al. [87] aimed to investigate the #longCOVID and #longhaulers \\nconversations on Twitter using  topic modeling  and social network analysis . The find-\\nings of their work  showed that a mong the 2010 tweets about long COVID -19 and 490 \\ntweets by COVID -19 long haulers, 30,923 and 7817 unique words were found, respec-\\ntively. Their work also showed that f or both conversation types, ‚Äú#longcovid‚Äù and \\n‚Äúcovid‚Äù were the most frequently mentioned words , and words relevant to having Long \\nCOVID were more frequ ently found  in tweets posted by individuals  suffering  from \\nLong COVID .  \\nMiyake et al. [94]  studied  social media data  collected at different points of the pan-\\ndemic to explore how patients felt when official communications diverged from their \\nlived experiences. They used a mixed methods approach involving quantitative and \\nqualitative analyses  and studied 1.38 million posts about Long COVID from Twitter, \\nFacebook, blogs, and forums . The r esults indicate d that the negative impacts arise \\nmostly from conflicting definitions of C OVID -19 and fears around the COVID -19 vac-\\ncine for individ uals suffering from Long COVID . Their study also identified th at key \\nareas of concern  in the context of Long COVID included time or duration , symptoms  \\nor testing , emotional impact , lack of support , and resources.  Juanal s et al. [97]  studied \\nLong Covid coverage by the French  press and Twitter . More specifically , the objectives \\nof th eir study were  to analyz e the modalities of construction and progressive visibility \\nof Long Covid  in the public media  and on Twitter  and to propose a methodology based \\non topic modeling and related conce pts in NLP  to conduct a comparative analysis be-\\ntween newspapers and Twitter coverage  of Long COVID . \\nOzduran et al. [98]  classified YouTube videos about Long COVID according to \\nvideo parameters and content analysis. They also determine d the q uality, reliability, \\nand accuracy of these videos using  the Global Quality Score (GQS), the Journal of \\nAmerican Medical Association (JAMA) Benchmark Criteria, and the Modified \\nDISCERN Questionnaire . The findings showed that out of 180 vid eos about L ong \\nCOVID, 74 were of low quality, 14 were of moderate quality, and 12 were of high \\nquality; 21% contained insufficient data, 73% contained partially sufficient data, and \\n6% contained completely sufficient data. Their work also showed that v ideos uploaded \\nby academic sources (66.7%) and physicians (12.5%) made up most  of the high -quality 14 \\ngroup. The authors also found a  statistically significant correlation between the source \\nof upload and the number of views (p = 0.014), likes (p = 0.030), comments (p = 0.007), \\nand video duration (p = 0.004).  \\n \\n4.4 Symptom Characterization  \\nSingh et al.  [66] focus ed on  identifying  sympto ms on  Twitter where users self-re-\\nported  Long COVID. They stud ied the tweets published by 89 Twitter users , and the \\nfindings of their study showed that most  users described multiple symptoms, out of \\nwhich the most common were fatigue, shortness of breath, pain,  and brain  fog or con-\\ncentration difficulties. Ziauddeen et al. [67] conducted an online survey with 2,550 par-\\nticipants, to highlight  the range  of symptoms of Long COVID and infer how such sy mp-\\ntoms affected  daily functioning. Their study showed that most participants described \\nfluctuating (57.7%) or relapsing symptoms (17.6%) , with physical activity, stress, and \\nsleep disturbance being the commonly triggered symptoms. Their study also found that \\none-third of participants reported being unable to live alone without assistance  six \\nweeks from the start of the illness , and 16.9% reported being unable to work alone  due \\nto COVID -19 illness.  The goal of the work done by Sarker et al. [68] was to infer  Long \\nCOVID  symptoms self -reported by users, compare symptom distributions across stud-\\nies, and create a symptom lexicon  by studying Long COVID -related posts on Reddit. \\nThey studied  42,995 posts by 4249 Reddit users , and the res ults showed that 1744 users \\nexpressed at least one symptom . The  results of their work also showed that the most \\nfrequently reported long -COVID symptoms were mental health -related symptoms \\n(55.2%), fatigue (51.2%), general ache  or pain (48.4%), brain fog  or confusion (32.8%) \\nand dyspnea (28.9%) . Banda et al. [69] used a combination of machine learning, natural \\nlanguage processing techniques, and clinician reviews  and mined 296,154 tweets  about \\nLong COVID. The objective of their study was  to characterize the course of Long \\nCOVID , creat e detailed timelines of symptoms and conditions, and analyz e their symp-\\ntomatology during a period of over 150 days .  \\nMassey et al. [70] posted a poll to a Faceboo k group of  169,900 members that asked \\nabout breakthrough COVID -19 cases, Long Covid, and hospitalizations . The fin dings \\nshowed that out o f the 1,949 participants who responded to the poll, 44 reported a \\nsymptomatic breakthrough case , and 24 reported that COVID -19 led to symptoms of \\nLong C OVID . Their study also found that 1 out of these 24 cases was hospitaliz ed. The \\ngoal of the research by Martin et al. [71] was to explore healthcare workers' perceptions \\nconcerning Long COVID in children and you ng people  in the UK between January \\n2021 and January 2022  by studying relevant posts on Twitter . This research showed \\nthat healthcare workers  were responsive to announcements issued by authorities regard-\\ning the management of COVID -19 in the UK , and the most frequent emotion expressed \\non Twitter in this regard  was negative. Th is research  also identified th e main themes  of \\nconversat ion, which included uncertainty about the future, policies and regulations, \\nmanaging and addressing COVID -19 and Long COVID in children and you ng people , \\nvaccination, using Twitter to share scientific literature and management strategies, and \\nclinical and personal experiences.  \\nDolatabadi et al. [72] aimed to determine the validity and effectiveness of advanced \\nNLP approaches  to derive insight into Long COVID -related patient -reported health \\noutcomes from social media platforms . They used Transformer -based BERT models to 15 \\nextract and normalize long COVID symptoms and conditions from English posts on \\nTwitter and Reddit . The  results indicated that the top three most commonly occurring \\nLong COVID symptoms were  systemic (such as ‚Äúfatigue‚Äù), neuropsychiatric (such as \\n‚Äúanxiety ‚Äù and ‚Äúbrain fog‚Äù), and respiratory (such as ‚Äúshortness of breath‚Äù) .  \\nMiao et al. [73] used an interactive information extraction tool  and analyzed tweets \\nabout Long COVID. The a uthors extracted key information from the relevant tweets \\nand analyzed the user -reported Long COVID symptoms  concerning  their demographic \\nand geographical characteristics.  Feng et a l. [74] also stud ied tweets about Long \\nCOVID. They classif ied Long COVID -related  tweets into four categories based on the \\ncontent, detected the presence of six basic emotions, and extracted  prevalent topics. \\nTheir  analyses reveal ed that negative emotions dominated throughout the study period . \\nMatharaarachchi et al. [81] implemented association rule mining  to understand the \\npatterns and behavior of long COVID symptoms reported by patients on Twitter . They \\nfound t hat in the 30,327 tweets included in their study, the most frequent symptoms \\nwere brain fog , fatigue , breathing  or lung issues , heart issues , flu symptoms , depression , \\nand general pain ; loss of smell and taste,  cold, cough, chest pain, fever, headache, and \\narm pain were noted  in 1.6% to 5.3% of patients with long COVID.  Ayadi et al. [83] \\ncollecte d 27,216 Reddit posts about Long COVID and performed a comprehensive data \\nanalysis. They found that o ver 78% of the analyzed posts referenced at least one symp-\\ntom of Long COVID. The most reported  symptoms were fatigue (29.4%), pain (22%), \\nbrain fog (19.1%), anxiety (17.7%), and headaches (15.6%). These symptoms fre-\\nquently co -occurred with others, such as fever and nasal congestion. The symptoms \\nwere categorized into general (45.5%), neurological  (42.9%), mental health  or psycho-\\nlogical  or behavioral (35.2%), body pain  or mobility (35.1%), and cardiorespiratory \\n(31.2%).  \\nIzquierdo -Condoy et al. [86 ] conducted a cross -sectional analysis of 2,103 partici-\\npants in Ecuador between April and July 2022, using an online  self-reporting question-\\nnaire to investigate Long COVID symptoms. Among the respondents, 52.3% (1,100) \\nreported Long COVID symptoms, with the majority being women (64%) and individ-\\nuals aged 21 -40 years (68.5%). Notably, 71.7% of the Long COVID cases occurred \\namong residents at high altitudes (>2500m), compared to 29.3% at lower altitudes. \\nCommon symptoms included fatigue (8.4%), hair loss (5.1%), and difficulty concen-\\ntrating (5.0%). The study id entified a greater prevalence of Long COVID symptoms \\namong women, individuals with severe initial infections, and those with comorbidities, \\nemphasizing the influence of altitude on Long COVID . D√©guilhem  et al. [88] conducted \\na comprehensive analysis of 15,364 messages from 6,494 individuals with Long \\nCOVID or their caregivers in France  from January 1, 2020, to August 10, 2021. The \\nstudy identified three primary symptom co -occurrences: asthenia -dyspnea (35.3%), as-\\nthenia -anxiety (22.5%), and asthenia -headaches (17.3%). Key difficulties reported by \\npatients included managing symptoms (35.4%  of messages), dealing with psychologi-\\ncal impacts such as anxiety and uncertainty (15.1%), enduring pain (12.0%), and coping \\nwith disruptions (9.4%) and professional life (8.0%). The analysis also categorized pa-\\ntients into three distinct profiles. Profile A consisted of 406 patients who exclusively \\nreported asthenia. Profile B included 129 patients who predominantly experienced anx-\\niety (100%), a long with asthenia (21.7%), dyspnea (11.6%), and ageusia (2.3%). Profile \\nC, with 141 patients, was characterized by dyspnea (100%) and asthenia (31.9%). 16 \\nAdditionally, the findings revealed that 49.1% of users expressed symptoms beyond \\nthree months post -infection, and 20.5% continued to report symptoms even after one \\nyear.  \\nSegneri et al. [91] analyzed the communication style and network structure of 6,107 \\nReddit users to identify social traits associated with Long COVID. The study catego-\\nrized users into three groups: No COVID (2,529 users), COVID (592 users), and Long \\nCOVID (2,986 users). They anal yzed pre-pandemic posts, totaling 984,625 , with 45% \\nfrom the No COVID group, 32% from the Long COVID group, and 23% from the \\nCOVID group. Key findings from their work indicated that Long COVID users exhib-\\nited lower social media activity and fewer connections than other groups. Furthermore, \\ntheir communication style included more health -related topics and frequent use of first -\\nperson singular pronouns but fewer anger -related words. The ir study also found that \\nLong COVID users were more likely to use interrogative language and verbose posts, \\nreflecting their focus on health concerns. Sarker [93] studied  self-reported Long \\nCOVID symptoms on the subreddit /r/covidlonghaulers . Using natural language pro-\\ncessing, they identified the most common symptoms, including anxiety  or stress, fa-\\ntigue, body pain, and brain fog. Their study showed that m ost users reported 1 -5 symp-\\ntoms, with a median of 4 symptoms per user. Jacques et al. [100] analyzed the 100 \\nmost -viewed YouTube videos discussing Long COVID symptoms, uploaded between \\nJuly 2020 and December 2021, which amassed 15,319,997 views. The y found that  the \\nmajority of these videos originated from television or internet -based news sources \\n(56%), followed by consumer -generated content (32%), health professionals (9%), and \\nentertainment TV (3%). Their study inferred  that p hysical symptoms such as fatigue \\n(73%), difficulty breathing (56%), and joint or muscle pain (49%) were most frequently \\ndiscussed, alongside cognitive issues like brain fog (69%). Other frequently reported \\nchallenges included worsening symptoms post -activi ty (37%) and psychological ef-\\nfects like anxiety or depression (17%). Their work also showed that  videos from enter-\\ntainment TV received significantly more likes than other categories .  \\nStrain  et al. [101]  surveyed  812 individuals with Long COVID to evaluate the impact \\nof COVID -19 vaccination on their symptoms. The participants, primarily younger fe-\\nmales (80.6%), reported symptoms persisting for over nine months in 71.6% of cases. \\nFollowing the first vaccination dos e, 57.9% of participants reported overall symptom \\nimprovement, while 17.9% experienced deterioration  and the rest reported no change. \\nImprovements were more pronounced with mRNA vaccines, such as Moderna (31% \\nimprovement) and Pfizer (24.4%), compared to the AstraZeneca adenoviral vector vac-\\ncine (22.6%). The most improved symptoms included fatigue (p = 0. 009), brain fog (p \\n= 0.01), and myalgia (p = 0.006). Their work found that s ymptom severity reductions \\nwere proportional to baseline scores  and f or half of the participants, symptom improve-\\nments were temporary, lasting 14 -21 days, while post -vaccination deterioration re-\\nsolved within 3 -7 days.  \\nThe breadth of research surveyed in these four categories underscores how multifac-\\neted Long COVID can be, spanning elements of advanced computational analyses, pol-\\nicy formation, community engage ment , and clinical symptom documentation. Whether \\ninvestigators used sentiment analysis to map public anxieties, examined official health \\ncommunications to reveal messaging gaps, or monitored online forums where individ-\\nuals assembled in search of guidance, each study contributed a piece to the broader \\npuzzle of persistent post-COVID complications. In a collective manner , these works 17 \\nindicate  that fully understanding Long COVID requires both interdisciplinary collabo-\\nration and innovative methodologies, particularly as social media continues to serve as \\na large -scale repository of patient -driven experiences. While some researchers focused \\non ca pturing emergent narratives via topic modeling, others underscored the imperative \\nof supporting those living with Long COVID . By merging perspectives from computa-\\ntional sciences, public health, and real -world patient accounts, this  body of literature \\nhighlights the need for sustained inquiry into Long COVID  and for responsive frame-\\nworks that can adapt to new findings as they emerge.  \\n5 Research Gaps and Future Directions  \\nAlthough numerous investigations have emerged around Long COVID and its social \\nmedia narratives, critical gaps  warrant further consideration. One prominent challenge \\nlies in harmonizing definitions and frameworks for the condition itself. Several studies \\nused different inclusion criteria, with some focusing on self -reported experiences and \\nothers requiring clinical diagnoses. This lack of uniform standards hampers efforts to \\ncompare outcomes across different populations and time periods. Equally important is \\nintegrating  finer details on patients‚Äô backgrounds, disease histories, and underlying con-\\nditions into study designs. Greater clarity in such baseline data could highlight  whether \\ncertain cohorts, including older adults, are more prone to persistent complications [ 107-\\n110]. In doing so, investigators may infer  how aging -related factors, such as immunose-\\nnescence or latent comorbidities, interact with ongoing COVID -19 symptoms \\n[111,112 ]. \\nA second research gap concerns the depth and breadth of symptom documentation. \\nWhile investigators have cataloged a wide array of complaints  - ranging from cognitive \\ndifficulties to cardiorespiratory issues  - there is still a shortage of longitudinal data that \\ndetail how and why certain symptoms linger or transform over time. More robust pro-\\nspective studies could help pinpoint potential transition points or flare -ups that individ-\\nuals frequently mention in onlin e communities. This endeavor would be particularly \\nmeaningful for older demographics, given that parallel research on aging has demon-\\nstrated the value of tracking gradual physiological changes across extended intervals \\n[113-116]. Building on this approach, future work might measure how chronic inflam-\\nmation or age -associated immune variations could shape Long COVID trajectories \\n[117-119]. Such insights would not only clarify the causes  of persistent symptoms but \\nmight also inform targeted interventions tailored to different life stages . \\nIn addition to these gaps in symptom tracking, a more systematic exploration of so-\\ncial determinants of health is necessary. Many of the reviewed studies used social media \\nposts without consistently capturing users‚Äô socioeconomic status, geographic context,  \\nor access to healthcare resources. Understanding how stressors  - like insufficient med-\\nical support or economic hardship  - can amplify Long COVID  complications would be \\ninvaluable for refining health policies. It would also broaden our understanding of how \\naging adults, who may already be dealing with multiple conditions, manage additional \\nburdens imposed by COVID -19 [120-123]. In this sense, bridging findings from ger-\\nontological research  - where socioeconomic disparities often exacerbate the severity of \\nage-related disorders  - could enrich the framework for studying Long COVID‚Äôs psy-\\nchosocial dimensions . 18 \\nAnother significant avenue for future work  involves methodological innovations, \\nparticularly those that incorporate advanced analytics beyond sentiment analysis or \\nbasic topic modeling. For example, multi -modal data analysis  - integrating text, voice, \\nand video  - could provide a richer characterization of individuals‚Äô lived experiences  \\nwith Long COVI D. Such approaches might benefit from cutting -edge computational \\ntools used in aging research, where sensors and wearables have been employed to track \\nphysiological and behavioral indicators of decline [ 124-126]. Translating these meth-\\nods to the COVID -19 context could enable real -time monitoring of symptom fluctua-\\ntions or early warning signs. Coordinated collaborations between specialists in geron-\\ntology and emerging fields like machine learning could foster the exchange of tech-\\nniques that have proven effective in monitoring complex, chronic conditions  [127-129]. \\nFurthermore , many researchers have drawn attention to the lack of formal clinical \\ntrials or intervention studies aimed at mitigating long -term symptoms  despite ample \\nanecdotal evidence shared on social media. Addressing this shortfall requires not just \\nlarger sample sizes but also ethically designed studies that compare different manage-\\nment strategies  - pharmacological, rehabilitative, or psychosocial. Such work would be \\nespecially beneficial if it includes older adults, given that gerontology has a long -stand-\\ning tradition of rigorously testing interventions aimed at prolonging functional auton-\\nomy [ 130-133]. By synthesizing expertise from both Long COVID and aging -focused \\ninvestigations, scholars could devise clinical protocols that emphasize the realities of \\nmulti -morbidity, polypharmacy, and overall resilience [ 134-136]. In effect, the field \\ncould progress toward interventions that systematically address both the biological un-\\nderpinnings of prolonged COVID -19 symptoms and the socio -emotional hurdles en-\\ncountered by diverse patient groups, including older populations .  \\nThese  directions underscore the need for broader interdisciplinary engagement, \\ndeeper longitudinal insights, and more nuanced epidemiological tools. They also high-\\nlight how lessons from aging research can advance  Long COVID  research , especially \\nregarding risk assessment, symptom evolution, and care strategies [ 135-140]. Future \\nwork could  also focus on  refining how social media data are aggregated and annotated. \\nResearchers often concentrate on static posts  [141,142] , yet new formats, such as \\nephemeral stories [143,144] or live chats  [145,146], offer dynamic insights into how \\nusers articulate their symptoms and need s in real -time. A structured, time -sensitive ap-\\nproach to analyzing these data would help identify swift changes in public sentiment or \\nemerging topics that might otherwise go unnoticed. Complementary initiatives to stand-\\nardize metadata collection could lay the groundwor k for more consistent data sharing \\nacross research gro ups, encouraging broader comparative efforts and mitigating gaps \\nin knowledge  [147-150]. \\nA related priority involves exploring how individuals engage with one another on \\nsocial media beyond simple ‚Äúlike‚Äù or ‚Äúshare‚Äù metrics  in the context of Long COVID -\\nrelated discussions . In many online communities, participants build deep interpersonal \\nnetworks that thrive on mutual trust, and these environments can shape patterns of \\nsymptom reporting and health -seeking behaviors  [151,152]. Future studies could  inves-\\ntigate how trust relationships form and evolve within these networks  and how  different \\nage groups interpret and disseminate healthcare information online. Such inquiries \\ncould highlight  how older adults, who may be managing multiple comorbidities, adapt \\nto social media for guidance and peer support in ways that diverge from younger de-\\nmographics.  Another avenue involves enhancing the accuracy of sentiment and topic 19 \\ndetection by adopting more context -aware algorithms. While many current models fo-\\ncus on keyword frequency or  text structure, there is room for approaches capable of \\ncapturing subtler emotional or cultural nuances. Future works could  consider leverag-\\ning contextual embeddings that adapt to evolving language trends, including new \\nphrases or slang  coined by social media  communities  [153-155]. By calibrating these \\nadvanced tools to different platforms  - whether Reddit, Twitter, or localized forums  - \\ninvestigators could develop a finer -grained picture of how users articulate the long -term \\neffects  of COVID -19 in different social media platforms .  \\nResearchers in this field  could also explore closer collaborations with social media \\nplatforms themselves. Data -access initiatives  and robust privacy safeguards  might fos-\\nter the co -design of tools and dashboards that allow public health professionals and \\ncommunity leaders to monitor and interpret conversations as they unfold. Such partner-\\nships could promote real -time feedback loops, where findings from social media anal-\\nysis guide new research questions and public health messaging, and vice versa. Over \\nthe long term, these initiatives may pave the way for better resource allocation and more \\nprecisely t ailored interventions, ensuring that people suffering fro m Long COVID  have \\nreliable information and consistent support.  \\nThe work presented in this paper  has a couple of limitations . The heterogeneous \\nnature of social media platforms  and varying user demographics and data availability  \\nmay introduce sampling biases. Additionally, the zero -shot classification method, \\nthough novel, relies on pre-trained  language representations and may not fully capture \\ncontextual nuances in highly specialized or region -specific vocabulary as exp ressed on \\nsocial media . \\n6 Conclusion  \\nLong COVID is becoming a complex health -related  problem that requires multiple \\napproaches to comprehensively study the wide range of clinical, long -term effects, and \\npsychosocial aspects associated with it across patients worldwide.  This review high-\\nlights the significant contribution of social media networks in furthering the under-\\nstanding of Long COVID, depicting many facets of how patients experience the condi-\\ntion, the symptoms that typically develop, and how the general population unde rstands \\nthis condition. Social media has allowed for the collection of patient -generated data in \\nreal-time, making it easier to represent the variety of symptoms asso ciated with  Long \\nCOVID . By presenting  a systematic review of studies that rely on user -generated data  \\non social media pl atforms  and by using  a transformer -based zero -shot learning ap-\\nproach, this paper offers new perspectives on how we can capture and categorize th is \\ncomplex research landscape. The review shows  that patients‚Äô online narratives do far \\nmore than supplement clinical findings; they also create a real -time feedback loop that \\nmay guide research questions and shape both public health policies and advocacy ef-\\nforts. These accounts often raise critical qu estions about  persistent symptoms, gaps in \\nhealthcare, and the psychological burden of extended illness. By examining these issues \\nwithin a coherent, data -driven framework, the paper shows how computational methods \\ncan highlight  patterns and themes crucial for this research area . \\nHowever, the significance of this research goes beyond  mapping out social media \\ntrends  and insights . The zero -shot classification pipeline demonstrates that advanced 20 \\nlanguage models  can identify meaningful categories within research papers  on emerg-\\ning conditions, all while bypassing conventional manual labeling workflows. This \\nstreamlined process broadens the scope of discovery, allowing for timely insights in a \\nfield that demands ongoing updates. The insights presented  here confirm a vital need \\nfor refining longitudinal approaches, standardizing both clinical and analytical frame-\\nworks, and exploring how diverse populations experience Long COVID. The future \\ndirections presented in this paper highlight that i nterdisciplinary research should utilize \\nadvanced  computational tools that capture the nuanced language and behavior of social \\nmedia users, especially as public discourse and scientific understanding related to L ong \\nCOVID continue s to evolve. Furthermore , investigators may also delve into multi -\\nmodal social media data - such as images, videos, and real -time audio streams  - to sup-\\nplement textual content. In summary , this paper‚Äôs findings underscore the potential of \\ncombining patient -driven data with advanced analytics to refine our understanding  of \\nLong COVID , while also laying the groundwork for future work in this area  that adapt s \\nto new discoveries and patient needs.  \\nReferences  \\n1. Ciotti, M., Ciccozzi, M., Terrinoni, A., Jiang, W. -C., Wang, C. -B., Bernardini, S.: The \\nCOVID -19 pandemic. Crit. Rev. Clin. Lab. Sci. 57, 365 ‚Äì388 (2020). \\nhttps://doi.org/10.1080/10408363.2020.1783198.  \\n2. Velavan, T.P., Meyer, C.G.: The COVID‚Äê19 epidemic. Trop. Med. Int. Health. 25, 278 ‚Äì\\n280 (2020). https://doi.org/10.1111/tmi.13383.  \\n3. Yesudhas, D., Srivastava, A., Gromiha, M.M.: COVID -19 outbreak: history, mechanism, \\ntransmission, structural studies and therapeutics. Infection. 49, 199 ‚Äì213 (2021). \\nhttps://doi.org/10.1007/s15010 -020-01516 -2. \\n4. Allen, D.W.: Covid -19 lockdown cost/benefits: A critical assessment of the literature. Int. \\nJ. Econ. Bus. 29, 1 ‚Äì32 (2022). https://doi.org/10.1080/13571516.2021.1976051.  \\n5. Cucinotta, D., Vanelli, M.: WHO Declares COVID -19 a Pandemic. Acta Biomed. Ateneo \\nParmense. 91, 157 ‚Äì160 (2020). https://doi.org/10.23750/abm.v91i1.9397.  \\n6. COVID -19 cases, https://covid19.who.int/, last accessed 2024/12/23.  \\n7. Raveendran, A.V., Jayadevan, R., Sashidharan, S.: Long COVID: An overview. Diabetes \\nMetab. Syndr. 15, 869 ‚Äì875 (2021). https://doi.org/10.1016/j.dsx.2021.04.007.  \\n8. Altmann, D.M., Whettlock, E.M., Liu, S., Arachchillage, D.J., Boyton, R.J.: The immu-\\nnology of long COVID. Nat. Rev. Immunol. 23, 618 ‚Äì634 (2023). \\nhttps://doi.org/10.1038/s41577 -023-00904 -7. \\n9. Fern√°ndez -de-las-Pe√±as, C.: Long COVID: current definition. Infection. 50, 285 ‚Äì286 \\n(2022). https://doi.org/10.1007/s15010 -021-01696 -5. \\n10. CDC: Long COVID basics, https://www.cdc.gov/covid/long -term-effects/index.html, last \\naccessed 2024/12/23.  \\n11. Committee on Examining the Working Definition for Long COVID, Board on Health Sci-\\nences Policy, Board on Global Health, Health and Medicine Division, National Academies \\nof Sciences, Engineering, and Medicine: A long COVID definition: A chronic, systemic \\ndisease state with profound consequences, http://dx.doi.org/10.17226/27768, (2024). \\nhttps://doi.org/10.17226/27768.  \\n12. Aiyegbusi, O.L., Hughes, S.E., Turner, G., Rivera, S.C., McMullan, C., Chandan, J.S., \\nHaroon, S., Price, G., Davies, E.H., Nirantharakumar, K., Sapey, E., Calvert, M.J., on be-\\nhalf of the TLC Study Group: Symptoms, complications and management of long COVID : 21 \\na review. J. R. Soc. Med. 114, 428 ‚Äì442 (2021). \\nhttps://doi.org/10.1177/01410768211032850.  \\n13. Subramanian, A., Nirantharakumar, K., Hughes, S., Myles, P., Williams, T., Gokhale, \\nK.M., Taverner, T., Chandan, J.S., Brown, K., Simms -Williams, N., Shah, A.D., Singh, \\nM., Kidy, F., Okoth, K., Hotham, R., Bashir, N., Cockburn, N., Lee, S.I., Turner, G.M.,  \\nGkoutos, G.V., Aiyegbusi, O.L., McMullan, C., Denniston, A.K., Sapey, E., Lord, J.M., \\nWraith, D.C., Leggett, E., Iles, C., Marshall, T., Price, M.J., Marwaha, S., Davies, E.H., \\nJackson, L.J., Matthews, K.L., Camaradou, J., Calvert, M., Haroon, S.: Symptom s and risk \\nfactors for long COVID in non -hospitalized adults. Nat. Med. 28, 1706 ‚Äì1714 (2022). \\nhttps://doi.org/10.1038/s41591 -022-01909 -w. \\n14. Sudre, C.H., Murray, B., Varsavsky, T., Graham, M.S., Penfold, R.S., Bowyer, R.C., Pu-\\njol, J.C., Klaser, K., Antonelli, M., Canas, L.S., Molteni, E., Modat, M., Jorge Cardoso, \\nM., May, A., Ganesh, S., Davies, R., Nguyen, L.H., Drew, D.A., Astley, C.M., Josh i, A.D., \\nMerino, J., Tsereteli, N., Fall, T., Gomez, M.F., Duncan, E.L., Menni, C., Williams, \\nF.M.K., Franks, P.W., Chan, A.T., Wolf, J., Ourselin, S., Spector, T., Steves, C.J.: Attrib-\\nutes and predictors of long COVID. Nat. Med. 27, 626 ‚Äì631 (2021). \\nhttps: //doi.org/10.1038/s41591 -021-01292 -y. \\n15. Notarte, K.I., Catahay, J.A., Velasco, J.V., Pastrana, A., Ver, A.T., Pangilinan, F.C., \\nPeligro, P.J., Casimiro, M., Guerrero, J.J., Gellaco, M.M.L., Lippi, G., Henry, B.M., Fer-\\nn√°ndez -de-las-Pe√±as, C.: Impact of COVID -19 vaccination on the risk of developi ng long -\\nCOVID and on existing long -COVID symptoms: A systematic review. EClinicalMedi-\\ncine. 53, 101624 (2022). https://doi.org/10.1016/j.eclinm.2022.101624.  \\n16. Cabrera Martimbianco, A.L., Pacheco, R.L., Bagattini, √Ç.M., Riera, R.: Frequency, signs \\nand symptoms, and criteria adopted for long COVID‚Äê19: A systematic review. Int. J. Clin. \\nPract. 75, (2021). https://doi.org/10.1111/ijcp.14357.  \\n17. Ayoubkhani, D., Bermingham, C., Pouwels, K.B., Glickman, M., Nafilyan, V., Zaccardi, \\nF., Khunti, K., Alwan, N.A., Walker, A.S.: Trajectory of long covid symptoms after covid -\\n19 vaccination: community based cohort study. BMJ. 377, e069676 (2022). \\nhttps://do i.org/10.1136/bmj -2021 -069676.  \\n18. Davis, H.E., Assaf, G.S., McCorkell, L., Wei, H., Low, R.J., Re‚Äôem, Y., Redfield, S., Aus-\\ntin, J.P., Akrami, A.: Characterizing long COVID in an international cohort: 7 months of \\nsymptoms and their impact. EClinicalMedicine. 38, 101019 (2021). \\nhttps://doi.o rg/10.1016/j.eclinm.2021.101019.  \\n19. Yong, S.J.: Long COVID or post -COVID -19 syndrome: putative pathophysiology, risk \\nfactors, and treatments. Infect. Dis. (Lond.). 53, 737 ‚Äì754 (2021). \\nhttps://doi.org/10.1080/23744235.2021.1924397.  \\n20. Koc, H.C., Xiao, J., Liu, W., Li, Y., Chen, G.: Long COVID and its management. Int. J. \\nBiol. Sci. 18, 4768 ‚Äì4780 (2022). https://doi.org/10.7150/ijbs.75056.  \\n21. Al-Aly, Z., Davis, H., McCorkell, L., Soares, L., Wulf -Hanson, S., Iwasaki, A., Topol, \\nE.J.: Long COVID science, research and policy. Nat. Med. 30, 2148 ‚Äì2164 (2024). \\nhttps://doi.org/10.1038/s41591 -024-03173 -6. \\n22. Tana, C., Bentivegna, E., Cho, S. -J., Harriott, A.M., Garc√≠a -Azor√≠n, D., Labastida -\\nRamirez, A., Ornello, R., Raffaelli, B., Beltr√°n, E.R., Ruscheweyh, R., Martelletti, P.: \\nLong COVID headache. J. Headache Pain. 23, (2022). https://doi.org/10.1186/s10194 -\\n022-01450 -8. \\n23. Peluso, M.J., Deeks, S.G.: Mechanisms of long COVID and the path toward therapeutics. \\nCell. 187, 5500 ‚Äì5529 (2024). https://doi.org/10.1016/j.cell.2024.07.054.  \\n24. Greenhalgh, T., Sivan, M., Perlowski, A., Nikolich, J.≈Ω.: Long COVID: a clinical update. \\nLancet. 404, 707 ‚Äì724 (2024). https://doi.org/10.1016/s0140 -6736(24)01136 -x. \\n25. Sykes, D.L., Holdsworth, L., Jawad, N., Gunasekera, P., Morice, A.H., Crooks, M.G.: \\nPost-COVID -19 symptom burden: What is long -COVID and how should we manage it? \\nLung. 199, 113 ‚Äì119 (2021). https://doi.org/10.1007/s00408 -021-00423 -z. 22 \\n26. Tsao, S. -F., Chen, H., Tisseverasinghe, T., Yang, Y., Li, L., Butt, Z.A.: What social media \\ntold us in the time of COVID -19: a scoping review. Lancet Digit. Health. 3, e175 ‚Äìe194 \\n(2021). https://doi.org/10.1016/s2589 -7500(20)30315 -0. \\n27. Thakur, N.: Social media mining and analysis: A brief review of recent challenges. Infor-\\nmation (Basel). 14, 484 (2023). https://doi.org/10.3390/info14090484.  \\n28. Gottlieb, M., Dyer, S.: Information and disinformation: Social media in the COVID‚Äê19 \\ncrisis. Acad. Emerg. Med. 27, 640 ‚Äì641 (2020). https://doi.org/10.1111/acem.14036.  \\n29. Thakur, N., Han, C.: An exploratory study of tweets about the SARS -CoV -2 Omicron \\nvariant: Insights from sentiment analysis, language interpretation, source tracking, type \\nclassification, and embedded URL detection. COVID. 2, 1026 ‚Äì1049 (2022). \\nhttps://doi.o rg/10.3390/covid2080076.  \\n30. Hussain, W.: Role of social media in COVID -19 pandemic. Int J Front Sci. 4, (2024). \\nhttps://doi.org/10.37978/tijfs.v4i2.144.  \\n31. Thakur, N.: Sentiment analysis and text analysis of the public discourse on Twitter about \\nCOVID -19 and MPox. Big Data Cogn. Comput. 7, 116 (2023). \\nhttps://doi.org/10.3390/bdcc7020116.  \\n32. Shoaei, M.D., Dastani, M.: The role of Twitter during the COVID -19 crisis: A systematic \\nliterature review. Acta Inform. Pragensia. 9, 154 ‚Äì169 (2020). \\nhttps://doi.org/10.18267/j.aip.138.  \\n33. Thakur, N., Cui, S., Khanna, K., Knieling, V., Duggal, Y.N., Shao, M.: Investigation of \\nthe gender -specific discourse about online learning during COVID -19 on Twitter using \\nsentiment analysis, subjectivity analysis, and toxicity analysis. Computers. 12, 22 1 (2023). \\nhttps://doi.org/10.3390/computers12110221.  \\n34. Southwick, L., Guntuku, S.C., Klinger, E.V., Seltzer, E., McCalpin, H.J., Merchant, R.M.: \\nCharacterizing COVID -19 content posted to TikTok: Public sentiment and response dur-\\ning the first phase of the COVID -19 pandemic. J. Adolesc. Health. 69, 234 ‚Äì241 (2021 ). \\nhttps://doi.org/10.1016/j.jadohealth.2021.05.010.  \\n35. Patel, K.A., Thakur, N.: Dissemination of misinformation about COVID -19 on TikTok: A \\nmultimodal analysis. In: Communications in Computer and Information Science. pp. 109 ‚Äì\\n120. Springer Nature Switzerland, Cham (2024).  \\n36. Rovetta, A., Bhagavathula, A.S.: Global infodemiology of COVID -19: Analysis of Google \\nweb searches and Instagram hashtags. J. Med. Internet Res. 22, e20673 (2020). \\nhttps://doi.org/10.2196/20673.  \\n37. Thakur, N.: Five years of COVID -19 discourse on Instagram: A labeled Instagram dataset \\nof over half a million posts for multilingual sentiment analysis. In: 2024 7th International \\nConference on Machine Learning and Natural Language Processing (MLNLP). pp. 1‚Äì10. \\nIEEE (2024).  \\n38. Mejova, Y., Kalimeri, K.: COVID -19 on Facebook ads: Competing agendas around a pub-\\nlic health crisis. In: Proceedings of the 3rd ACM SIGCAS Conference on Computing and \\nSustainable Societies. pp. 22 ‚Äì31. ACM, New York, NY, USA (2020).  \\n39. Perrotta, D., Grow, A., Rampazzo, F., Cimentada, J., Del Fava, E., Gil -Clavel, S., Zagheni, \\nE.: Behaviours and attitudes in response to the COVID -19 pandemic: insights from a cross -\\nnational Facebook survey. EPJ Data Sci. 10, 17 (2021). \\nhttps://doi.org/10.1 140/epjds/s13688 -021-00270 -1. \\n40. Li, H.O. -Y., Bailey, A., Huynh, D., Chan, J.: YouTube as a source of information on \\nCOVID -19: a pandemic of misinformation? BMJ Glob. Health. 5, e002604 (2020). \\nhttps://doi.org/10.1136/bmjgh -2020 -002604.  \\n41. Thakur, N., Cui, S., Knieling, V., Khanna, K., Shao, M.: Investigation of the misinfor-\\nmation about COVID -19 on YouTube using topic modeling, sentiment analysis, and lan-\\nguage analysis. Computation (Basel). 12, 28 (2024). https://doi.org/10.3390/computa-\\ntion1 2020028.  23 \\n42. Veselovsky, V., Anderson, A.: Reddit in the time of COVID. Proceedings of the Interna-\\ntional AAAI Conference on Web and Social Media. 17, 878 ‚Äì889 (2023). \\nhttps://doi.org/10.1609/icwsm.v17i1.22196.  \\n43. Nirmalya, T., Kesha, A.P., Audrey, P., Shuqi, C., Nazif, A., Rishika, S., Riyan, S.: Quan-\\ntifying public response to COVID -19 events: Introducing the Community Sentiment and \\nEngagement Index, http://arxiv.org/abs/2412.16925, (2024).  \\n44. Daglis, T., Tsagarakis, K.P.: A LinkedIn -based analysis of the U.S. dynamic adaptations \\nin healthcare during the COVID -19 pandemic. Healthcare Analytics. 5, 100291 (2024). \\nhttps://doi.org/10.1016/j.health.2023.100291.  \\n45. Pardim, V.I., Pinochet, L.H.C., Souza, C.A., Viana, A.B.N.: The behavior of young people \\nat the beginning of their career through LinkedIn. RAM Rev. Adm. Mackenzie. 23, \\neRAMG220064 (2022). https://doi.org/10.1590/1678 -6971/eramg220064.en.  \\n46. Hinchey, L., Michon, A., Drews, J., Price, M., Christian, J., Pernice, F., Aquila, R.: Club-\\nhouses as essential communities during the COVID -19 pandemic. J. Psychosoc. Rehabil. \\nMent. Health. 9, 149 ‚Äì157 (2022). https://doi.org/10.1007/s40737 -021-00242 -8. \\n47. Junaid, S., Mutschler, C., McShane, K., The Canadian Clubhouse Research Group: The \\nimpact of COVID -19 on clubhouse employment programs. Community Ment. Health J. \\n59, 523 ‚Äì530 (2023). https://doi.org/10.1007/s10597 -022-01036 -3. \\n48. Ayob, M.A., Hadi, N.A., Ezad, M., Pahroraji, H.M., Ismail, B., Saaid, M.N.F.: Promoting \\n‚ÄòDiscord‚Äô as a platform for learning engagement during Covid -19 pandemic. Asian J. Univ. \\nEduc. 18, 663 ‚Äì673 (2022). https://doi.org/10.24191/ajue.v18i3.18953.  \\n49. Ardiyansah, T.Y., Batubara, R.W., Auliya, P.K.: Using discord to facilitate students in \\nteaching learning process during COVID -19 outbreak. Journal of English Teaching, Lit-\\nerature, and Applied Linguistics. 5, 76 (2021). https://doi.org/10.30587/jetlal.v5i1 .2528.  \\n50. Yang, Q., Wang, W., Pierce, L., Vaish, R., Shi, X., Shah, N.: Online communication shifts \\nin the midst of the Covid -19 pandemic: A case study on Snapchat. Proceedings of the \\nInternational AAAI Conference on Web and Social Media. 15, 830 ‚Äì840 (2021). \\nhttps:/ /doi.org/10.1609/icwsm.v15i1.18107.  \\n51. Spieler, B., Batte, C., Mackey, D., Henry, C., Danrad, R., Sabottke, C., Pirtle, C., Mussell, \\nJ., Wallace, E.: Diagnosis in a snap: a pilot study using Snapchat in radiologic didactics. \\nEmerg. Radiol. 28, 93 ‚Äì102 (2021). https://doi.org/10.1007/s10140 -020-01825 -x. \\n52. Yue, Z., Zhang, R., Xiao, J.: Social media use, perceived social support, and well -being: \\nEvidence from two waves of surveys peri - and post -COVID -19 lockdown. J. Soc. Pers. \\nRelat. 41, 1279 ‚Äì1297 (2024). https://doi.org/10.1177/02654075231188185.  \\n53. Thakur, N., Duggal, Y.N., Liu, Z.: Analyzing public reactions, perceptions, and attitudes \\nduring the MPox outbreak: Findings from Topic Modeling of Tweets. Computers. 12, 191 \\n(2023). https://doi.org/10.3390/computers12100191.  \\n54. Thakur, N.: MonkeyPox2022Tweets: A large -scale Twitter dataset on the 2022 Monkey-\\npox outbreak, findings from analysis of Tweets, and open research questions. Infect. Dis. \\nRep. 14, 855 ‚Äì883 (2022). https://doi.org/10.3390/idr14060087.  \\n55. Perego, E.: #LongCovid, https://twitter.com/elisaperego78/sta-\\ntus/1263172084055838721?s=20, last accessed 2024/12/23.  \\n56. Thakur, N., Cho, H., Cheng, H., Lee, H.: Analysis of user diversity -based patterns of pub-\\nlic discourse on twitter about mental health in the context of online learning during \\nCOVID -19. In: Lecture Notes in Computer Science. pp. 367 ‚Äì389. Springer Nature Swi t-\\nzerland, Cham (2023).  \\n57. Schillinger, D., Chittamuru, D., Ram√≠rez, A.S.: From ‚Äúinfodemics‚Äù to health promotion: \\nA novel framework for the role of social media in public health. Am. J. Public Health. 110, \\n1393 ‚Äì1396 (2020). https://doi.org/10.2105/ajph.2020.305746.  \\n58. Thakur, N., Patel, K.A., Poon, A., Shah, R., Azizi, N., Han, C.: A comprehensive analysis \\nand investigation of the public discourse on twitter about exoskeletons from 2017 to 2023. \\nFuture Internet. 15, 346 (2023). https://doi.org/10.3390/fi15100346.  24 \\n59. Pourpanah, F., Abdar, M., Luo, Y., Zhou, X., Wang, R., Lim, C.P., Wang, X. -Z., Wu, \\nQ.M.J.: A review of generalized zero -shot learning methods. IEEE Trans. Pattern Anal. \\nMach. Intell. 45, 1 ‚Äì20 (2022). https://doi.org/10.1109/tpami.2022.3191696.  \\n60. Romera -Paredes, B., Torr, P.H.S.: An embarrassingly simple approach to zero -shot learn-\\ning. ICML. 37, 2152 ‚Äì2161 (07 --09 Jul 2015). https://doi.org/10.1007/978 -3-319-50077 -\\n5_2. \\n61. Wang, W., Zheng, V.W., Yu, H., Miao, C.: A survey of zero -shot learning: Settings, meth-\\nods, and applications. ACM Trans. Intell. Syst. Technol. 10, 1 ‚Äì37 (2019). \\nhttps://doi.org/10.1145/3293318.  \\n62. Fu, Y.: Investigating public perceptions regarding the Long COVID on Twitter using sen-\\ntiment analysis and topic modeling. Med. Data Min. (2022). \\nhttps://doi.org/10.53388/mdm20220520024.  \\n63. Rushforth, A., Ladds, E., Wieringa, S., Taylor, S., Husain, L., Greenhalgh, T.: Long Covid \\n‚Äì The illness narratives. Soc. Sci. Med. 286, 114326 (2021). \\nhttps://doi.org/10.1016/j.socscimed.2021.114326.  \\n64. Russell, D., Spence, N.J., Chase, J. -A.D., Schwartz, T., Tumminello, C.M., Bouldin, E.: \\nSupport amid uncertainty: Long COVID illness experiences and the role of online com-\\nmunities. SSM Qual. Res. Health. 2, 100177 (2022). \\nhttps://doi.org/10.1016/j.ssmqr.20 22.100177.  \\n65. Meledandri, F.: The impact of polarised social media networking communications in the \\n#longcovid debate between ideologies and scientific facts, \\nhttp://dx.doi.org/10.13136/2281 -4582/2024.I23.1450, (2024). \\nhttps://doi.org/10.13136/2281 -4582/2024.I23.1450.  \\n66. Singh, S.M., Reddy, C.: An analysis of self -reported longcovid symptoms on twitter, \\nhttp://dx.doi.org/10.1101/2020.08.14.20175059, (2020). \\nhttps://doi.org/10.1101/2020.08.14.20175059.  \\n67. Ziauddeen, N., Gurdasani, D., O‚ÄôHara, M.E., Hastie, C., Roderick, P., Yao, G., Alwan, \\nN.A.: Characteristics of Long Covid: findings from a social media survey, \\nhttp://dx.doi.org/10.1101/2021.03.21.21253968, (2021). \\nhttps://doi.org/10.1101/2021.03.21.212539 68. \\n68. Sarker, A., Ge, Y.: Long COVID symptoms from Reddit: Characterizing post -COVID \\nsyndrome from patient reports, http://dx.doi.org/10.1101/2021.06.15.21259004, (2021). \\nhttps://doi.org/10.1101/2021.06.15.21259004.  \\n69. Banda, J.M., Adderley, N., Ahmed, W. -U.-R., AlGhoul, H., Alser, O., Alser, M., Areia, \\nC., Cogenur, M., Fi≈°ter, K., Gombar, S., Huser, V., Jonnagaddala, J., Lai, L.Y.H., Leis, \\nA., Mateu, L., Mayer, M.A., Minty, E., Morales, D., Natarajan, K., Paredes, R., P eriyakoil, \\nV.S., Prats -Uribe, A., Ross, E.G., Singh, G., Subbian, V., Vivekanantham, A., Prieto -Al-\\nhambra, D.: Characterization of long -term patient -reported symptoms of COVID -19: an \\nanalysis of social media data, http://dx.doi.org/10.1101/2021.07.13.212604 49, (2021). \\nhttps://doi.org/10.1101/2021.07.13.21260449.  \\n70. Massey, D., Berrent, D., Krumholz, H.: Breakthrough symptomatic COVID -19 infections \\nleading to Long Covid: Report from Long Covid Facebook group poll, \\nhttp://dx.doi.org/10.1101/2021.07.23.21261030, (2021). \\nhttps://doi.org/10.1101/2021.07.23.21261030.  \\n71. Martin, S., Chepo, M., D√©om, N., Khalid, A.F., Vindrola -Padros, C.: ‚Äú#LongCOVID af-\\nfects children too‚Äù: A Twitter analysis of healthcare workers‚Äô sentiment and discourse \\nabout Long COVID in children and young people in the UK, \\nhttp://dx.doi.org/10.1101/2022 .07.20.22277865, (2022). \\nhttps://doi.org/10.1101/2022.07.20.22277865.  \\n72. Dolatabadi, E., Moyano, D., Bales, M., Spasojevic, S., Bhambhoria, R., Bhatti, J., \\nDebnath, S., Hoell, N., Li, X., Leng, C., Nanda, S., Saab, J., Sahak, E., Sie, F., Uppal, S., \\nVadlamudi, N.K., Vladimirova, A., Yakimovich, A., Yang, X., Kocak, S.A., Cheung , 25 \\nA.M.: Using social media to help understand long COVID patient reported health out-\\ncomes: A natural language processing approach, \\nhttp://dx.doi.org/10.1101/2022.12.14.22283419, (2022). \\nhttps://doi.org/10.1101/2022.12.14.22283419.  \\n73. Miao, L., Last, M., Litvak, M.: An interactive analysis of user -reported long COVID symp-\\ntoms using twitter data. In: Hruschka, E., Mitchell, T., Mladenic, D., Grobelnik, M., and \\nBhutani, N. (eds.) Proceedings of the 2nd Workshop on Deriving Insights from U ser-Gen-\\nerated Text. pp. 10 ‚Äì19. Association for Computational Linguistics, Stroudsburg, PA, USA \\n(2022).  \\n74. Guocheng, F., Huaiyu, C., Wei, Q.: Exploring the emotional and mental well -being of \\nindividuals with Long COVID through twitter analysis, http://arxiv.org/abs/2307.07558, \\n(2023).  \\n75. Jordan, A., Park, A.: Understanding the long haulers of COVID -19: Mixed methods anal-\\nysis of YouTube content. JMIR AI. 3, e54501 (2024). https://doi.org/10.2196/54501.  \\n76. Kusuma, I.Y., Suherman, S.: The pulse of long COVID on Twitter: A social network anal-\\nysis. Arch. Iran. Med. 27, 36 ‚Äì43 (2024). https://doi.org/10.34172/aim.2024.06.  \\n77. Thakur, N.: Investigating and analyzing self -reporting of Long COVID on Twitter: Find-\\nings from sentiment analysis. Appl. Syst. Innov. 6, 92 (2023). \\nhttps://doi.org/10.3390/asi6050092.  \\n78. Awoyemi, T., Ebili, U., Olusanya, A., Ogunniyi, K.E., Adejumo, A.V.: Twitter sentiment \\nanalysis of long COVID syndrome. Cureus. 14, e25901 (2022). https://doi.org/10.7759/cu-\\nreus.25901.  \\n79. DIGITAL LONG -HAULER LIFELINES: UNDERSTANDING HOW PEOPLE WITH \\nLONG COVID BUILD COMMUNITY ON REDDIT, https://www.researchgate.net/pub-\\nlication/385720439_Digital_Long -Hauler_Lifelines_Understanding_How_Peo-\\nple_with_Long_Covid_Build_Community_on_Reddit, last acc essed 2024/12/24.  \\n80. Bhattacharyya, A., Seth, A., Rai, S.: The effects of long COVID -19, its severity, and the \\nneed for immediate attention: Analysis of clinical trials and Twitter data. Front. Big Data. \\n5, (2022). https://doi.org/10.3389/fdata.2022.1051386.  \\n81. Matharaarachchi, S., Domaratzki, M., Katz, A., Muthukumarana, S.: Discovering long \\nCOVID symptom patterns: Association rule mining and sentiment analysis in social media \\ntweets. JMIR Form. Res. 6, e37984 (2022). https://doi.org/10.2196/37984.  \\n82. Koss, J., Bohnet -Joschko, S.: Social media mining of long -COVID self -medication re-\\nported by Reddit users: Feasibility study to support drug repurposing. JMIR Form. Res. 6, \\ne39582 (2022). https://doi.org/10.2196/39582.  \\n83. Ayadi, H., Bour, C., Fischer, A., Ghoniem, M., Fagherazzi, G.: The Long COVID experi-\\nence from a patient‚Äôs perspective: a clustering analysis of 27,216 Reddit posts. Front. Pub-\\nlic Health. 11, (2023). https://doi.org/10.3389/fpubh.2023.1227807.  \\n84. Garrett, C., Aghaei, A., Aggarwal, A., Qiao, S.: The role of social media in the experiences \\nof COVID -19 among long -hauler women: Qualitative study. JMIR Hum. Factors. 11, \\ne50443 (2024). https://doi.org/10.2196/50443.  \\n85. Laestadius, L.I., Guidry, J.P.D., Bishop, A., Campos -Castillo, C.: State health department \\ncommunication about long COVID in the United States on Facebook: Risks, prevention, \\nand support. Int. J. Environ. Res. Public Health. 19, 5973 (2022). \\nhttps://doi.or g/10.3390/ijerph19105973.  \\n86. Izquierdo -Condoy, J.S., Fernandez -Naranjo, R., Vasconez -Gonz√°lez, E., Cordovez, S., \\nTello -De-la-Torre, A., Paz, C., Delgado -Moreira, K., Carrington, S., Viscor, G., Ortiz -\\nPrado, E.: Long COVID at different altitudes: A countrywide epidemiological analysis.  \\nInt. J. Environ. Res. Public Health. 19, 14673 (2022). \\nhttps://doi.org/10.3390/ijerph192214673.  \\n87. Santarossa, S., Rapp, A., Sardinas, S., Hussein, J., Ramirez, A., Cassidy -Bushrow, A.E., \\nCheng, P., Yu, E.: Understanding the #longCOVID and #longhaulers conversation on 26 \\nTwitter: Multimethod study. JMIR Infodemiology. 2, e31259 (2022). \\nhttps://doi.org/10.2196/31259.  \\n88. D√©guilhem, A., Malaab, J., Talmatkadi, M., Renner, S., Foulqui√©, P., Fagherazzi, G., Lous-\\nsikian, P., Marty, T., Mebarki, A., Texier, N., Schuck, S.: Identifying profiles and symp-\\ntoms of patients with long COVID in France: Data mining infodemiology study ba sed on \\nsocial media. JMIR Infodemiology. 2, e39849 (2022). https://doi.org/10.2196/39849.  \\n89. Dolatabadi, E., Moyano, D., Bales, M., Spasojevic, S., Bhambhoria, R., Bhatti, J., \\nDebnath, S., Hoell, N., Li, X., Leng, C., Nanda, S., Saab, J., Sahak, E., Sie, F., Uppal, S., \\nVadlamudi, N.K., Vladimirova, A., Yakimovich, A., Yang, X., Kocak, S.A., Cheung , \\nA.M.: Using social media to help understand patient -reported health outcomes of post ‚Äì\\nCOVID -19 condition: Natural language processing approach. J. Med. Internet Res. 25, \\ne45767 (2023). https://doi.org/10.2196/45767.  \\n90. Ziauddeen, N., Gurdasani, D., O‚ÄôHara, M.E., Hastie, C., Roderick, P., Yao, G., Alwan, \\nN.A.: Characteristics and impact of Long Covid: Findings from an online survey. PLoS \\nOne. 17, e0264331 (2022). https://doi.org/10.1371/journal.pone.0264331.  \\n91. Segneri, L., Babina, N., Hammerschmidt, T., Fronzetti Colladon, A., Gloor, P.A.: Too \\nmuch focus on your health might be bad for your health: Reddit user‚Äôs communication \\nstyle predicts their Long COVID likelihood. PLoS One. 19, e0308340 (2024). \\nhttps://doi. org/10.1371/journal.pone.0308340.  \\n92. Singh, S.M., Reddy, S.C., Kathiravan, S.: An analysis of self -reported long COVID -19 \\nsymptoms on twitter. J. Postgrad. Med. Educ. Res. 57, 79 ‚Äì81 (2023). \\nhttps://doi.org/10.5005/jp -journals -10028 -1616.  \\n93. Mining Long -COVID symptoms from Reddit: what we know so far, https://www.re-\\nsearchgate.net/profile/Abeed -Sarker/publication/352208391_Mining_Long -\\nCOVID_symptoms_from_Reddit_what_we_know_so_far/links/60bedc6ca6fdcc22eae8b\\n87a/Mining -Long -COVID -symptoms -from -Reddit -what -we-know -so-far.pdf, last ac-\\ncessed 2024/12/24.  \\n94. Miyake, E., Martin, S.: Long Covid: Online patient narratives, public health communica-\\ntion and vaccine hesitancy. Digit. Health. 7, (2021). \\nhttps://doi.org/10.1177/20552076211059649.  \\n95. Sarker, A., Ge, Y.: Mining long -COVID symptoms from Reddit: characterizing post -\\nCOVID syndrome from patient reports. JAMIA Open. 4, (2021). \\nhttps://doi.org/10.1093/jamiaopen/ooab075.  \\n96. Jordan, A.A.D.: Understanding the plight of covid -19 long haulers through computational \\nanalysis of YouTube content, (2022).  \\n97. Minel, B.J.A.: Using topic modeling and NLP tools for analyzing long Covid coverage by \\nFrench press and Twitter. In: Nagar et al, A. (ed.) Intelligent Sustainable Systems, Lecture \\nNotes in Networks and Systems 817. Springer Nature Singapore, Singapore (202 4). \\nhttps://doi.org/10.1007/978 -981-99-7886 -1_15.  \\n98. Ozduran, E., B√ºy√ºk√ßoban, S.: A content analysis of the reliability and quality of Youtube \\nvideos as a source of information on health -related post -COVID pain. PeerJ. 10, e14089 \\n(2022). https://doi.org/10.7717/peerj.14089.  \\n99. D√©om, N., Khalid, A.F., Martin, S., Chepo, M., Vindrola -Padros, C.: Unlocking the mys-\\nteries of long COVID in children and young people: Insights from a policy review and \\nsocial media analysis in the UK, https://osf.io/preprints/f48yg/, (2023). \\nhttps://doi. org/10.31219/osf.io/f48yg.  \\n100. Jacques, E.T., Basch, C.H., Park, E., Kollia, B., Barry, E.: Long haul COVID -19 videos \\non YouTube: Implications for health communication. J. Community Health. 47, 610 ‚Äì615 \\n(2022). https://doi.org/10.1007/s10900 -022-01086 -4. \\n101. Strain, W.D., Sherwood, O., Banerjee, A., Van der Togt, V., Hishmeh, L., Rossman, J.: \\nThe impact of COVID vaccination on symptoms of long COVID: An international survey 27 \\nof people with lived experience of long COVID. Vaccines (Basel). 10, 652 (2022). \\nhttps://doi.org/10.3390/vaccines10050652.  \\n102. Wongtavavimarn, K.: Social support and narrative sensemaking online: A content analysis \\nof Facebook posts by COVID -19 long haulers, https://uh -ir.tdl.org/bitstream/han-\\ndle/10657/10745/WONGTAVAVIMARN -THESIS -2022.pdf?sequence=1, last accessed \\n2024/12/24.  \\n103. Helmy, Y.A., Fawzy, M., Elaswad, A., Sobieh, A., Kenney, S.P., Shehata, A.A.: The \\nCOVID -19 pandemic: A comprehensive review of taxonomy, genetics, epidemiology, di-\\nagnosis, treatment, and control. J. Clin. Med. 9, 1225 (2020). \\nhttps://doi.org/10.3390/jcm904 1225.  \\n104. Gasser, U., Ienca, M., Scheibner, J., Sleigh, J., Vayena, E.: Digital tools against COVID -\\n19: taxonomy, ethical challenges, and navigation aid. Lancet Digit. Health. 2, e425 ‚Äìe434 \\n(2020). https://doi.org/10.1016/s2589 -7500(20)30137 -0. \\n105. Albahri, O.S., Zaidan, A.A., Albahri, A.S., Zaidan, B.B., Abdulkareem, K.H., Al -qaysi, \\nZ.T., Alamoodi, A.H., Aleesa, A.M., Chyad, M.A., Alesa, R.M., Lim, C.K., Lakulu, M.M., \\nIbrahim, A.B., Rashid, N.A.: Systematic review of artificial intelligence techniqu es in the \\ndetection and classification of COVID -19 medical images in terms of evaluation and \\nbenchmarking: Taxonomy analysis, challenges, future solutions and methodological as-\\npects. J. Infect. Public Health. 13, 1381 ‚Äì1396 (2020). \\nhttps://doi.org/10.1016/j .jiph.2020.06.028.  \\n106. Awassa, L., Jdey, I., Dhahri, H., Hcini, G., Mahmood, A., Othman, E., Haneef, M.: Study \\nof different deep learning methods for Coronavirus (COVID -19) pandemic: Taxonomy, \\nsurvey and insights. Sensors (Basel). 22, 1890 (2022). https://doi.org/10.3390/s220518 90. \\n107. Shahid, Z., Kalayanamitra, R., McClafferty, B., Kepko, D., Ramgobin, D., Patel, R., Ag-\\ngarwal, C.S., Vunnam, R., Sahu, N., Bhatt, D., Jones, K., Golamari, R., Jain, R.: COVID‚Äê\\n19 and older adults: What we know. J. Am. Geriatr. Soc. 68, 926 ‚Äì929 (2020). \\nhttps: //doi.org/10.1111/jgs.16472.  \\n108. Thakur, N., Han, C.Y.: A study of fall detection in assisted living: Identifying and improv-\\ning the optimal machine learning method. J. Sens. Actuator Netw. 10, 39 (2021). \\nhttps://doi.org/10.3390/jsan10030039.  \\n109. Lebrasseur, A., Fortin -B√©dard, N., Lettre, J., Raymond, E., Bussi√®res, E. -L., Lapierre, N., \\nFaieta, J., Vincent, C., Duchesne, L., Ouellet, M. -C., Gagnon, E., Tourigny, A., Lamonta-\\ngne, M. -√à., Routhier, F.: Impact of the COVID -19 pandemic on older adults: R apid re-\\nview. JMIR Aging. 4, e26474 (2021). https://doi.org/10.2196/26474.  \\n110. Thakur, N., Han, C.Y.: Multimodal approaches for Indoor Localization for Ambient As-\\nsisted Living in Smart Homes. Information (Basel). 12, 114 (2021). \\nhttps://doi.org/10.3390/info12030114.  \\n111. Nanda, A., Vura, N.V.R.K., Gravenstein, S.: COVID -19 in older adults. Aging Clin. Exp. \\nRes. 32, 1199 ‚Äì1202 (2020). https://doi.org/10.1007/s40520 -020-01581 -5. \\n112. Miller, E.A.: Protecting and improving the lives of older adults in the COVID -19 era. J. \\nAging Soc. Policy. 32, 297 ‚Äì309 (2020). https://doi.org/10.1080/08959420.2020.1780104.  \\n113. Thakur, N., Han, C.Y.: Indoor localization for personalized ambient assisted living of mul-\\ntiple users in multi -floor smart environments. Big Data Cogn. Comput. 5, 42 (2021). \\nhttps://doi.org/10.3390/bdcc5030042.  \\n114. Saxon, S.V., Mary Jean Etten, EdD, GNP, CMP, FT, Elizabeth A. Perkins, PhD, RNLD, \\nFAAIDD, FGSA: Physical Change and aging, Seventh Edition: A guide for Helping Pro-\\nfessions. Springer Publishing Company (2021).  \\n115. Engelman, M., Jackson, H.: Gradual change, homeostasis, and punctuated equilibrium: \\nReconsidering patterns of health in later life. Demography. 56, 2323 ‚Äì2347 (2019). \\nhttps://doi.org/10.1007/s13524 -019-00826 -x. 28 \\n116. Thakur, N., Han, C.Y.: An intelligent ubiquitous activity aware framework for smart home. \\nIn: Advances in Intelligent Systems and Computing. pp. 296 ‚Äì302. Springer International \\nPublishing, Cham (2021).  \\n117. Busse, P.J., Mathur, S.K.: Age -related changes in immune function: Effect on airway in-\\nflammation. J. Allergy Clin. Immunol. 126, 690 ‚Äì699 (2010). \\nhttps://doi.org/10.1016/j.jaci.2010.08.011.  \\n118. Howcroft, T.K., Campisi, J., Louis, G.B., Smith, M.T., Wise, B., Wyss -Coray, T., Augus-\\ntine, A.D., McElhaney, J.E., Kohanski, R., Sierra, F.: The role of inflammation in age -\\nrelated disease. Aging (Albany NY). 5, 84 ‚Äì93 (2013). https://doi.org/10.18632/ag-\\ning.100531.  \\n119. Pawelec, G., Goldeck, D., Derhovanessian, E.: Inflammation, ageing and chronic disease. \\nCurr. Opin. Immunol. 29, 23 ‚Äì28 (2014). https://doi.org/10.1016/j.coi.2014.03.007.  \\n120. Thakur, N., Han, C.Y.: A framework for facilitating human -human interactions to mitigate \\nloneliness in elderly. In: Advances in Intelligent Systems and Computing. pp. 322 ‚Äì327. \\nSpringer International Publishing, Cham (2021).  \\n121. Novak, M.: Issues in aging. Routledge, Fourth edition. | New York, NY\\u202f: Routledge, 2018. \\n(2018).  \\n122. Silverstein, M.: Meeting the challenges of an aging workforce. Am. J. Ind. Med. 51, 269 ‚Äì\\n280 (2008). https://doi.org/10.1002/ajim.20569.  \\n123. Thakur, N., Y. Han, C.: Pervasive activity logging for indoor localization in smart homes. \\nIn: 2021 4th International Conference on Data Science and Information Technology. pp. \\n246‚Äì255. ACM, New York, NY, USA (2021).  \\n124. Thakur, N., Han, C.Y.: An improved approach for complex activity recognition in smart \\nhomes. In: Lecture Notes in Computer Science. pp. 220 ‚Äì231. Springer International Pub-\\nlishing, Cham (2019).  \\n125. Thakur, N., Han, C.Y.: An activity analysis model for enhancing user experiences in affect \\naware systems. In: 2018 IEEE 5G World Forum (5GWF). pp. 516 ‚Äì519. IEEE (2018).  \\n126. Thakur, N., Han, C.Y.: A context -driven complex activity framework for smart home. In: \\n2018 IEEE 9th Annual Information Technology, Electronics and Mobile Communication \\nConference (IEMCON). pp. 801 ‚Äì806. IEEE (2018).  \\n127. Kogan, A.C., Wilber, K., Mosqueda, L.: Person‚Äêcentered care for older adults with chronic \\nconditions and functional impairment: A systematic literature review. J. Am. Geriatr. Soc. \\n64, (2016). https://doi.org/10.1111/jgs.13873.  \\n128. Matthew -Maich, N., Harris, L., Ploeg, J., Markle -Reid, M., Valaitis, R., Ibrahim, S., Gafni, \\nA., Isaacs, S.: Designing, implementing, and evaluating mobile health technologies for \\nmanaging chronic conditions in older adults: A scoping review. JMIR MHealth UHealth. \\n4, e29 (2016). https://doi.org/10.2196/mhealth.5127.  \\n129. PACC Research Group, Anaby, D., Miller, W.C., Eng, J.J., Jarus, T., Noreau, L.: Partici-\\npation and well -being among older adults living with chronic conditions. Soc. Indic. Res. \\n100, 171 ‚Äì183 (2011). https://doi.org/10.1007/s11205 -010-9611 -x. \\n130. Ferrari, M., Harrison, B., Rawashdeh, O., Hammond, R., Avery, Y., Rawashdeh, M., \\nSa‚Äôdeh, W., Maddens, M.: Clinical feasibility trial of a motion detection system for fall \\nprevention in hospitalized older adult patients. Geriatr. Nurs. 33, 177 ‚Äì183 (2012). \\nhttps://doi.org/10.1016/j.gerinurse.2011.11.011.  \\n131. Thakur, N., Han, C.Y.: A simplistic and cost -effective design for real -world development \\nof an ambient assisted living system for fall detection and indoor localization: Proof -of-\\nconcept. Information (Basel). 13, 363 (2022). https://doi.org/10.3390/info130 80363.  \\n132. Townsley, C.A., Chan, K.K., Pond, G.R., Marquez, C., Siu, L.L., Straus, S.E.: Understand-\\ning the attitudes of the elderly towards enrolment into cancer clinical trials. BMC Cancer. \\n6, (2006). https://doi.org/10.1186/1471 -2407 -6-34. 29 \\n133. Thakur, N., Han, C.Y.: A framework for prediction of cramps during activities of daily \\nliving in elderly. In: 2020 International Conference on Big Data, Artificial Intelligence and \\nInternet of Things Engineering (ICBAIE). pp. 284 ‚Äì287. IEEE (2020).  \\n134. Fontes, A.P., Neri, A.L.: Resilience in aging: literature review. Cien. Saude Colet. 20, \\n1475 ‚Äì1495 (2015). https://doi.org/10.1590/1413 -81232015205.00502014.  \\n135. Thakur, N., Han, C.Y.: A multimodal approach for early detection of cognitive impairment \\nfrom tweets. In: Lecture Notes in Networks and Systems. pp. 11 ‚Äì19. Springer International \\nPublishing, Cham (2022).  \\n136. Stern, Y., Albert, M., Barnes, C.A., Cabeza, R., Pascual -Leone, A., Rapp, P.R.: A frame-\\nwork for concepts of reserve and resilience in aging. Neurobiol. Aging. 124, 100 ‚Äì103 \\n(2023). https://doi.org/10.1016/j.neurobiolaging.2022.10.015.  \\n137. Chen, Z., Yu, J., Song, Y., Chui, D.: Aging Beijing: Challenges and strategies of health \\ncare for the elderly. Ageing Res. Rev. 9, S2 ‚ÄìS5 (2010). \\nhttps://doi.org/10.1016/j.arr.2010.07.001.  \\n138. Thakur, N., Han, C.Y.: Towards a knowledge base for activity recognition of diverse users. \\nIn: Advances in Intelligent Systems and Computing. pp. 303 ‚Äì308. Springer International \\nPublishing, Cham (2021).  \\n139. Dun√©r, A., Nordstr√∂m, M.: Intentions and strategies among elderly people: Coping in eve-\\nryday life. J. Aging Stud. 19, 437 ‚Äì451 (2005). https://doi.org/10.1016/j.jag-\\ning.2004.10.001.  \\n140. Spoorenberg, S.L.W., Uittenbroek, R.J., Middel, B., Kremer, B.P.H., Reijneveld, S.A., \\nWynia, K.: Embrace, a model for integrated elderly care: study protocol of a randomized \\ncontrolled trial on the effectiveness regarding patient outcomes, service use, cos ts, and \\nquality of care. BMC Geriatr. 13, (2013). https://doi.org/10.1186/1471 -2318 -13-62. \\n141 Thakur, N.: A large -scale dataset of Twitter chatter about online learning during the current \\nCOVID -19 Omicron wave. Data (Basel). 7, 109 (2022). \\nhttps://doi.org/10.3390/data7080109.  \\n142 Storey, V.C., O‚ÄôLeary, D.E.: Text analysis of evolving emotions and sentiments in \\nCOVID -19 twitter communication. Cognit. Comput. 16, 1834 ‚Äì1857 (2024). \\nhttps://doi.org/10.1007/s12559 -022-10025 -3. \\n143. Bainotti, L., Caliandro, A., Gandini, A.: From archive cultures to ephemeral content, and \\nback: Studying Instagram Stories with digital methods. New Media Soc. 23, 3656 ‚Äì3676 \\n(2021). https://doi.org/10.1177/1461444820960071.  \\n144. Cardell, K., Douglas, K., Maguire, E.: ‚Äòstories.‚Äô In: Mediating Memory. pp. 157 ‚Äì172. \\nRoutledge (2017).  \\n145. Lin, H.: ‚ÄòLet‚Äôs purchase coloured live chat messages‚Äô: the impact of user engagement with \\nSuper Chat on YouTube. Inf. Commun. Soc. 1 ‚Äì19 (2024). \\nhttps://doi.org/10.1080/1369118x.2024.2442407.  \\n146. Sun, H., Chen, J., Fan, M.: Effect of live chat on traffic‚Äêto‚Äêsales conversion: Evidence from \\nan online marketplace. Prod. Oper. Manag. 30, 1201 ‚Äì1219 (2021). \\nhttps://doi.org/10.1111/poms.13320.  \\n147. Perez, B., Musolesi, M., Stringhini, G.: You are your metadata: Identification and obfus-\\ncation of social media users using metadata information. Proceedings of the International \\nAAAI Conference on Web and Social Media. 12, (2018). https://doi.org/10.1609/i c-\\nwsm.v12i1.15010.  \\n148. Chen, L. -S., Lin, Z. -C., Chang, J. -R.: FIR: An effective scheme for extracting useful \\nmetadata from social media. J. Med. Syst. 39, (2015). https://doi.org/10.1007/s10916 -015-\\n0333 -0. \\n149. Gerber, H.R., Lynch, T.L.: Into the meta: Research methods for moving beyond social \\nmedia surfacing. TechTrends. 61, 263 ‚Äì272 (2017). https://doi.org/10.1007/s11528 -016-\\n0140 -6. 30 \\n150. Jones, S.M., Neblitt -Jones, V., Weigle, M.C., Klein, M., Nelson, M.L.: It‚Äôs all about the \\ncards: Sharing on social media encouraged HTML metadata growth. In: 2021 ACM/IEEE \\nJoint Conference on Digital Libraries (JCDL). pp. 110 ‚Äì119. IEEE (2021).  \\n151. Rolls, K., Hansen, M., Jackson, D., Elliott, D.: How health care professionals use social \\nmedia to create virtual communities: An integrative review. J. Med. Internet Res. 18, e166 \\n(2016). https://doi.org/10.2196/jmir.5312.  \\n152. Lu, Y., Wu, Y., Liu, J., Li, J., Zhang, P.: Understanding health care social media use from \\ndifferent stakeholder perspectives: A content analysis of an online health community. J. \\nMed. Internet Res. 19, e109 (2017). https://doi.org/10.2196/jmir.7087.  \\n153. Sundaram, A., Subramaniam, H., Hamid, S.H.A., Nor, A.M.: A systematic literature re-\\nview on social media slang analytics in contemporary discourse. IEEE Access. 11, \\n132457 ‚Äì132471 (2023). https://doi.org/10.1109/access.2023.3334278.  \\n154. Sundaram, A., Subramaniam, H., Ab Hamid, S.H., Nor, A.M.: A three -step procedural \\nparadigm for domain -specific social media slang analytics. In: 2024 International Confer-\\nence on Trends in Quantum Computing and Emerging Business Technologies. pp. 1 ‚Äì7. \\nIEEE (2024).  \\n155. Matsumoto, K., Ren, F., Matsuoka, M., Yoshida, M., Kita, K.: Slang feature extraction by \\nanalysing topic change on social media. CAAI Trans. Intell. Technol. 4, 64 ‚Äì71 (2019). \\nhttps://doi.org/10.1049/trit.2018.1060.  \\n \",\n",
       " 'AUTOMATED DEMAND FORECASTING IN SMALL TO\\nMEDIUM -SIZED ENTERPRISES\\nThomas G√§rtner1,*, Christoph Lippert1, and Stefan Konigorski1,2\\n1Digital Health Center, Hasso Plattner Institute for Digital Engineering, University of Potsdam, Germany\\n2Hasso Plattner Institute for Digital Health at Mount Sinai, Icahn School of Medicine at Mount Sinai, New York,\\nUnited States of America\\n*email: thomas.gaertner@hpi.de\\nABSTRACT\\nIn response to the growing demand for accurate demand forecasts to optimize production, purchasing,\\nand logistics, this research proposes a generalized automated sales forecasting pipeline tailored for\\nsmall- to medium-sized enterprises ( SME ). Unlike large corporations that benefit from the expertise\\nof data scientists for sales forecasting, SME s often lack such resources. In our research, we developed\\na comprehensive forecasting pipeline that automates time series sales forecasting, encompassing data\\npreparation, model training, and selection based on validation results, thereby providing SME s with\\nan advanced planning tool.\\nThe development included two main parts: model preselection andforecasting pipeline . In the first\\nphase, several state-of-the-art methods were tested on a showcase dataset to identify six models as\\nsuitable candidates for the pipeline. As final models, ARIMA, SARIMAX, Holt-Winters Exponential\\nSmoothing, Regression Tree, Dilated Convolutional Neural Network, and Generalized Additive\\nModel were selected. We also included an ensemble prediction of the models. Long-Short-Term\\nMemory was not included in the pipeline as it did not achieve the desired prediction accuracy for the\\n18-month horizon, and Facebook Prophet, while robust, was excluded due to compatibility issues\\nwith the production environment. In the second phase, the proposed forecasting pipeline was tested\\nwith SME s from food and electric industry, revealing variable model performance across different\\ncompanies. One company, due to its project-based nature, saw no benefit, while the others experienced\\nrealistic and superior sales forecasts compared to naive estimators. Our findings suggested that no\\nsingle model is universally superior; rather, an array of models, when integrated within an automated\\nvalidation framework, can significantly enhance forecasting accuracy for SMEs.\\nThe results emphasize the importance of model diversity and automated validation to address unique\\nneeds of each business. This research contributes to the field by giving access to state-of-the-art sales\\nforecasting tools, enabling SMEs to make data-driven decisions with improved efficiency.\\nKeywords Automatic forecasting ¬∑Demand forecasting ¬∑Evaluating forecasts ¬∑Time series\\n1 Introduction\\nWithin the last decades, companies were able to collect valuable data across the sales, production, logistics and\\nprocurement through the digitization of the supply chain. Enterprise-Resource-Planning ( ERP) systems enable a fully\\ndigital mapping of most of the processes within the company, utilized for documentation, and enable optimization by\\nminimizing the resources. One of the most crucial parts in companies focusing on production is having accurate sales\\nforecasts to improve procurement, allocate resources, make data driven decisions, and estimate the expected revenue.\\nOverall, good sales predictions should reduce the overproduction and possible waste of resources by minimizing safety\\nstock quantity as well as reducing the risk of being out of stock. While bigger organizations with high sales have\\nresources to employ data analysts developing customized methods to get value out of the data, small- to medium-sized\\nenterprisess ( SME s) often do not have such (financial) resources. With that, ERP systems with integrated methods forarXiv:2412.20420v1  [econ.EM]  29 Dec 2024Automated Demand Forecasting in small to medium-sized enterprises\\ndata analysis and sales figures forecasting can be beneficial and make them more competitive as they allow to optimize\\ninternal processes and reduce costs.\\nIntegrating methods for automated time series forecasting in ERP systems for sales figures holds several challenges.\\nVarious methods for time series forecasting were developed in the last decades with different strengths and limitations.\\n[1,2,3] Results have shown that hat there does not exist a universally best model and that the best model is highly\\ndependent on the products and the companies. As the methods can directly build on the data base from the ERP system,\\nthe data preprocessing can be standardized quite easily. Nevertheless, data maintenance to define the measurements\\nand generating high-quality historical data differs between companies. Companies with different specialisations have\\ndifferent requirements ranging from expiration dates within food industry to project-based production in especially\\nconstruction industry. Generalizing those requirements leads to complex use cases and different uses of the forecasts.\\nWhile most companies have strong domain knowledge, they often lack experience with machine learning methods.\\nWith that, any automated analysis of the data should be as simple as possible, but powerful at the same time. Because\\nSME s can produce a wide range of products, the methods need to be highly efficient in their computation time to deliver\\nresults in a reasonable time frame. Each company have different times for production and procurement. With that, the\\nfocuses of product planning could be on a weekly or monthly basis and could be up to 2 years to get a rough estimation\\nof future trends.\\nTo advance the field of demand forecasting for SME s, in this paper, we introduce a generalized automated demand\\nforecasting pipeline that integrates automatic validation and model selection. This pipeline was designed to simplify\\nand streamline the forecasting process, allowing SME s to leverage advanced predictive models without the need for\\nspecialized data science expertise. One of the key contributions of this research is the incorporation of a comprehensive\\nmodel evaluation framework, which ensures that the most suitable forecasting model is automatically selected based on\\nvalidation results. By focusing on monthly long-term predictions, the pipeline provides a robust, scalable solution that\\nadapts to various business contexts, enhancing forecasting accuracy and operational efficiency for SMEs.\\nThe paper is structured as follows. First, we provide a general introduction followed by related work. Next, in Section 3,\\nwe discuss an evaluation of different models on a showcase dataset in order to identify promising forecasting approaches.\\nAfterwards, we introduce the data analysis pipeline in Section 4. In Section 5, we perform a user test with a pilot group\\nconsisting of 5 companies to evaluate the pipeline under real-world conditions. Finally, we discuss the results, the\\npotential impact on the business and potential next steps to further improve the pipeline in Section 6.\\n2 Related Work\\nIn the field of time series analysis for sales and demand forecasting, various methods have been developed and\\ninvestigated. They can be clustered into two general groups of classical statistical models such as autoregressive\\nintegrated moving average ( ARIMA ) or Exponential Smoothing, and machine learning models including Recurrent\\nNeural Networks or long short-term memory.\\nStarting with classical statistical methods, Exponential Smoothing ( ES) were introduced for demand forecasting in 1956.\\n[4] Holt and Winters further developed the model to the well-known Holt-Winters Exponential Smoothing ( HWES )\\nincorporating seasonality. [ 5,6] In 1970, Box and Jenkins introduced a Method for estimation in ARIMA models. [ 7] In\\n1990, this method was reviewed and seasonality and trends as components in a structural model closer to the underlying\\neconomical concepts was introduced. [ 8] The use of tree-based methods such as Random Forest Regressor ( RF) or\\nExtreme Gradient Boosting ( XGBoost ) for time series forecasting was explored in different learning tasks, for instance\\nin gold price prediction, blood sugar prediction in diabetes type-I patients as well as demand forecasting for supply\\nchain management [ 9,10,1]. In 2021, tree based methods such as Random Forest Regressor ( RF), Extreme Gradient\\nBoosting ( XGBoost ) and ARIMA were compared in a study, to evaluate their performance on gold price prediction. In\\ntheir study RFwas able to outperform others, but they suggested a hybrid approach of ARIMA and Deep Learning for\\nforecasting. [9]\\nIn 1997 long short-term memory ( LSTM ) were introduced, a model trained through recurrent backpropagation capable\\nof connecting information in long time lags [ 11]. With similar properties, Gated Recurrent Units ( GRU s) were\\nintroduced [ 12]. Convolutional Neural Networks ( CNN s), prominent in the field of computer vision, can be used for\\ntime series forecasting by using 1-dimensional kernels [ 13]. In 2018, a special architecture called dilated CNN s were\\nused for time series forecasting.[14]\\nA comparative study, using iterative predictions and direct predictions, found that direct Artificial Neural Networks\\noutperformed other methods in their study. However, direct forecasting raised new challenges and, for example, more\\ndata with longer history is required. [15]\\n2Automated Demand Forecasting in small to medium-sized enterprises\\nWith Facebook Prophet, a data analytic pipeline including an analyst in the loop to improve the interpretation and\\ndebugging of the model was introduced. The model itself is a Bayesian Generalized Additive Model ( GAM ) including\\ntrend with change points, seasons, and features like holidays or events. [2]\\nTo generate demand forecasts for planning, Sugiarto et. al. used HWES intergrated in an ERP system. [ 16] In contrast,\\nHassan El Madany et.al. implemented a pipeline integrated in an ERP system for procurement forecasting using\\nARIMA andLSTM determining the price for various products. [ 3] A case study about the usage of machine learning\\nin companies highlighted various challenges using demand forecasting in an ERP system, and comparing different\\napproaches for predictions. [17]\\n3 Model Preselection\\nMany different models are available for time series forecasting. To identify the most suitable models for our analysis\\npipeline, we conducted a first analysis on a showcase data set to evaluate and compare the models. Based on the\\nperformance and implementation details, we selected the most promising model for the next step.\\n3.1 Show Case Data Set\\nThe showcase data set contained 51 products provided by one company with a historical data of up to 6 years. 37\\nproducts ( 72,5%) had data available at least from the beginning of 2016, while others were launched later in 2019.\\nFigure 1 gives an overview of the aggregated sum of sales and the number of unique products on a monthly basis.\\nFigure 1: Overview over the showcase data set for model preselection. It shows the number of total sales for all 51\\nproducts (left) and the number of different products sold per month (right). Notably, new products were introduced in\\n2019 leading to an increase in sales at the same time.\\nThe products were selected by the company to showcase a variety of business use cases in the data set. That includes\\nproducts sold B2B as well as products sold B2C affecting the seasonality and the noise within the data. Figure 2\\nshows four different product histories as examples for different product behavior such as seasonality, trend, variance or\\nobservation time. For presenting the results, the data was rescaled, so that the average sales are equal to 1000 per month\\nto de-identify the products.\\n3.2 Evaluated Methods\\nWithin the model preselection, we investigated 33 different models and model specifications ranging from traditional\\nstatistical methods to deep learning methods and compared their performances based on MAPE and nRMSE.\\nAutoregressive Models\\nWe fitted seasonal autoregressive integrated moving average (SARIMA) models with automatic selection of (p, d, q )\\nand(P, D, Q )-parameters, representing the order of autoregression, nonseasonal and seasonal differencing, and the size\\nof nonseasonal and seasonal moving average windows, respectively. We utilized a grid search technique implemented\\ninpmdarima , as detailed in [ 18] with default parameters. This method systematically explores various combinations of\\nparameters to identify the best fit for the time series data.\\n3Automated Demand Forecasting in small to medium-sized enterprises\\nFigure 2: Sales for 4 different products of the showcase dataset.\\nExponential Smoothing\\nExponential smoothing is a time series forecasting method used for smoothing data points to identify trends. It applies\\na series of weights that decrease exponentially over time, making recent observations more influential in forecasting\\nfuture values. Holt-Winters exponential smoothing extends this concept by addressing seasonality in addition to level\\nand trend. It incorporates three smoothing parameters: level, trend, and seasonal components, which adapt the method\\nfor data with seasonal fluctuations. This enhancement allows HWES to provide more accurate forecasts for data with\\nunderlying seasonal patterns compared to basic exponential smoothing. In our experiments, we used the implementation\\nfrom statsmodels with default parameters. [19]\\nRandom Forest Regressor\\nTree-based methods are popular in time series forecasting due to their effectiveness in handling various types of data\\nand their ability to capture complex patterns. Decision trees, Random Forest Regressor ( RF), and boosting methods\\nsuch as Extreme Gradient Boosting ( XGBoost ) are commonly used in both academia and industry for predictive tasks\\n[9]. In our study, the implementations from sklearn ofRFandXGBoost as detailed in [ 20] were used. For that, we\\ndefined the last year as input to predict the next time point.\\nDeep Learning Models\\nDeep learning has become increasingly prominent in time series forecasting due to its ability to model complex\\nand nonlinear relationships in data. Multilayer Perceptrons ( MLP s), Convolutional Neural Networks ( CNN s), Gated\\nRecurrent Units ( GRU s), and long short-term memorys ( LSTM s) are particularly effective for this purpose. CNN s\\nexcel in identifying hierarchical patterns through their convolutional layers, making them suitable for time series that\\nexhibit spatial dependencies. [ 13]GRU s and LSTM s, both variants of recurrent neural networks, are adept at capturing\\ntemporal dynamics due to their memory cells, which help maintain information over longer sequences. [ 11] These\\nmethods have been extensively applied across various sectors in both academia and industry, demonstrating strong\\nperformance in forecasting tasks involving sequential data.\\nIn our model preselection, the deep learning methods were trained from scratch combined on all products with random\\ninitial weights for a maximum of 100 epochs, but early stopping was applied if the loss did not improve over five\\nepochs. For all CNN s, we used the same architecture and learning parameters identified through hyperparameter tuning\\nbeforehand on the same data set. As the data set only containted 51 products, we also report the performance of a\\npretrained CNN model on weather data. Furthermore, we compared different model specifications such as a dilated\\nCNN by [14]. Another approach to reduce the risk of overfitting is fixing the weights of last layer to reduce the number\\nof trainable parameters. This model was called CNN s (fixed) . As we have daily data available, we also trained the\\n4Automated Demand Forecasting in small to medium-sized enterprises\\nmodels on granular data to predict the next day and aggregate the predictions to monthly forecasts in a second step.\\nThose models were called DAY models. Altogether, we trained and evaluated six different architectures of CNN s in\\nthe first model selection step namely CNN ,CNN (Dilated) ,CNN (pretrained) ,CNN (fixed) ,CNN (Dilated) ,CNN (fixed,\\nDAY) , and CNN (DAY) .\\nFurthermore, we trained and evaluated three different architectures of LSTM s, one GRU and one MLP on the showcase\\ndataset.\\nFacebook Prophet\\nFacebook Prophet is a versatile forecasting tool designed for handling time series data that displays patterns on different\\ntime scales such as daily, weekly, or yearly seasonality. Developed by Facebook‚Äôs data science team, Prophet is\\nparticularly user-friendly, making it accessible for analysts with varying levels of expertise. It excels in dealing with\\nmissing data and trend changes, and can incorporate holiday effects, which are often challenging for standard time\\nseries models. Their method combines configurable models with analyst-in-the-loop performance analysis. In our\\nanalysis, we used their published python package with default parameters. [2]\\nGeneralized Additive Model\\nFollowing that, we developed a decompositional Generalized Additive Model ( GAM ) of the time series. In this\\napproach, we incorporate linear and exponential trends, seasonality, and user specific external features. Our model is an\\nadaption of Facebook Prophet but instead of a Bayesian Model, we used a linear model for forecasting. Instead of a\\nclassical time series, we model the sales figures as a function of time, f(t), with trend T(t), seasonality S(t), external\\nfeatures X(t)at time point tand polynomial spline functions with œÄ(t). With that, the model can be formalized as:\\nf(t) =Œ≤0+Œ≤1¬∑T(t) +Œ≤2¬∑S(t) +Œ≤3¬∑X(t) +Œ≤4¬∑œÄ(t) +œµ,\\nwhere œµis the error term following a normal distribution.\\nTo determine trends, we used linear and exponential functions of time. For modeling seasonality, we used Fourier\\ntransformation on monthly and weekly frequency. The Œ≤coefficients were estimated through coordinate descent with\\nLasso penalization. [21]\\nEnsemble\\nThe methods under consideration often exhibit tendencies toward overestimation or underestimation in their forecasts.\\nTo mitigate this and enhance robustness, we combined the predictions from an ensemble of statistical models, including\\nHWES ,GAM ,ARIMA , and XGBoost . To integrate the forecasts from these diverse models, an appropriate aggregation\\nfunction is required. In the first analysis, we employed mean as well as median functions to aggregate the predictions,\\naiming to balance the individual model biases and achieve more accurate overall forecasts. We called the models\\nEnsemble Mean and Ensemble Median respectivly. We did not use a weighted mean as estimating robust weights in\\nproduction can be complex.\\n3.3 Experimental Set Up\\nFor analyzing the performance of the models, we splitted the data into train and test data. Data before January 1, 2020\\nwas considered for training and data after January 1, 2020 was used for evaluation. That led to a short training history\\nfor 4 products with less than a year. We trained all models described in the previous section. If not further specified,\\ndefault parameters were used. Next, we forecasted sales for the next 18 months starting from January 1, 2020, which\\nincreased the complexity of the forecasting task due to the long prediction horizon, which is essential in industry. As\\nwe had short historical data available, we used one-step ahead forecasting to use as much data as possible, and iterated\\nover the prediction as the input for the next month.\\nWe assessed the model performances using normalized Root-Mean-Squared Error ( nRMSE ) as evaluation metric to\\ncompare the model performance across the products:\\nRMSE (y,ÀÜy) =sX\\nt(yt‚àíÀÜyt)2,\\nnRMSE (y,ÀÜy) =RMSE (y,ÀÜy)\\nymax‚àíymin,(1)\\n5Automated Demand Forecasting in small to medium-sized enterprises\\nwhere yare the actual values and ÀÜyare the predicted values for each time point t.\\nAdditionally, we used the Mean Absolute Percentage Error (MAPE) defined as following for interpretation:\\nMAPE =Pn\\nt\\x0c\\x0c\\x0cyt‚àíÀÜyt\\nyt\\x0c\\x0c\\x0c\\nn, (2)\\nwhere nis the number of time points.\\nWe analyzed the overall performance across all products by calculating the mean nRMSE for each model, aggregating\\nthe results across all product time series.\\n3.4 Results\\nOur findings revealed that the top-performing models predominantly belong to the deep learning category, including\\ndilated CNN , aCNN without specifications, and a CNN pre-trained on weather data as shown in Table 1. A full list\\nof model performances can be found in the Appendix Table 6. Tree-based boosting methods, GRU ,LSTM , and MLP\\nall showed good performance, too, and were also ranked among the top 10 models, highlighting their robustness in\\ntime series forecasting. Nevertheless, the average nRMSE of the top-10 models are close to each other. The results of\\ntheMAPE confirmed that the CNN (Dilated) has the best model performance, deviating on average 33.21% from the\\nactual sales. Otherwise, the ranking of the models based on the MAPE would lead to slightly different results though\\nstill quite similar.\\nMethod nRMSE MAPE\\nCNN (Dilated) 0.3118 33.21 %\\nCNN 0.3119 37.0 %\\nCNN (pretrained) 0.3137 35.05 %\\nCNN (FIXED) 0.3245 36.81 %\\nTree-Based (XGBoost, log) 0.3451 34.97 %\\nTree-Based (XGBoost) 0.3460 36.63 %\\nGRU 0.3502 47.3 %\\nCNN (DAY) 0.3567 36.95 %\\nLSTM 0.3571 45.0 %\\nMLP 0.3641 42.46 %\\nTable 1: Top-10 models sorted by the normalized Root-Mean-Squared Error ( nRMSE ) and the corresponding Mean\\nAbsolute Percentage Error (MAPE) averaged over all products.\\nFigure 3 illustrates four example products with different behaviour and the corresponding predictions by 4 different\\nmodels such as CNN (Dilated) ,Tree-Based (XGBoost) ,LSTM , and Facebook-Prophet (Day) . Inspecting the figure\\nvisually, it can be seen that all models, except for the LSTM , deliver reasonable predictions for data generated with\\nseasonality, and generated with seasonality and trend, while Facebook-Prophet tends to slightly underestimate the\\ntrend in this product. In this example product with high variance, XGBoost andFacebook-Prophet were able to deliver\\nreasonable predictions. In this example product with a short history, all models beside the LSTM expected a peak of\\nsales at the end of 2020 larger then the actual sales.\\nProduct CNN (Dilated) Prophet LSTM XGBoost\\nHigh Variance 0.377 0.163 0.452 0.304\\nSeasonality 0.124 0.128 0.223 0.133\\nSeasonality and Trend 0.183 0.189 0.303 0.167\\nShort History 0.492 0.879 0.412 0.503\\nTable 2: nRMSE for example products predicted with CNN ,Facebook Prophet (Day) ,LSTM andXGBoost .\\nThe results of the visual inspection were confirmed by the nRMSE shown in Table 2. Additionally, the table showed\\nthat the prediction of products with a short history led to higher errors, which makes sense as less data points could be\\nused.\\n6Automated Demand Forecasting in small to medium-sized enterprises\\nFigure 3: Example Sales figures for 4 different products with predictions by CNN (Dilated) ,Tree-Based (XGBoost) , a\\nversion of LSTM , and Facebook-Prophet (Day) .\\nBased on these results, six models were selected to be included in the prediction pipeline: GAM ,ARIMA andSARIMAX ,\\nXGBoost ,CNN (Dilated) , and HWES . Furthermore, we included Ensemble Mean . We decided to include CNN (Dilated)\\nas the only deep learning approach in the pipeline, as it had the best prediction performance and computing multiple\\ndeep learning approaches would lead to higher computational costs. We excluded Facebook Prophet for multiple\\nreasons: The model model delivered promising results, however, it could not outperform the other deep learning\\napproaches. Further, it requires rstan , which was challenging to provide in the productive environment. We included\\nSARIMAX as well as ARIMA in the analysis pipeline as some potential future users of the pipeline might already use\\nthese models. The same holds for HWES , which additionally had on average a bad performance due to strong bias in\\nsome predictions. We included XGBoost andGAM in the pipeline as the results were promising and the computational\\ncosts were comparably low.\\nMethod nRMSE MAPE\\nCNN (Dilated) 0.3119 33.19 %\\nTree-Based (XGBoost, log) 0.3456 34.99 %\\nEnsemble (median) 0.3693 42.91 %\\nARIMA (Seasonal) 0.3832 44.67 %\\nARIMA 0.4105 53.0 %\\nGAM 0.4202 52.81 %\\nSmoothing (HWES) 4.8962 236.79 %\\nTable 3: Selected models for analysis pipeline with normalized Root-Mean-Squared Error ( nRMSE ) and Mean Absolute\\nPercentage Error (MAPE) averaged over all products.\\nBased on the results, we preselected seven models for the forecasting pipeline. Table 3 shows the nRMSE and MAPE\\nof the preselected models. Notably, HWES has by far the worst performance averaged over all products. This is due to\\noutliers in the predictions, where HWES highly overestimate the the sales. However, the average nRMSE as well as the\\nMAPE were biased through the outlier and HWES could deliver reasonable results for multiple products.\\n4 Automated Forecasting Pipeline\\nIn this section, we will start with an overview over the all the steps in the analysis pipeline including the automated\\nmodel recommendation step. Afterwards, we will describe the methods used in the analysis. The analysis pipeline\\n7Automated Demand Forecasting in small to medium-sized enterprises\\nincludes different sub tasks. After the data was loaded, it was checked for validity and processed in a specific format for\\nweekly and monthly sales data as visualized in Figure 4.\\nFigure 4: Illustration of the different steps of the analysis pipeline.\\nIn the pipeline, 6 models were trained and used for forecasting namely GAM ,ARIMA ,RF, Deep Learning, and HWES .\\n4.1 Step 1 - Preprocessing: Data Loading, Data Preprocessing and Data Checking\\nFirst, the data was loaded from the ERP system. The data contained previous sales aggregated by weeks and months for\\neach product as separate time series.\\nBefore starting with data processing, each time series was checked if enough data points were available. If a given time\\nseries contained less data then one year, e.g. for newly launched products, they were excluded in the automatic analysis.\\nAfter valid time series were identified, data for the weekly and monthly predictions was processed. Instead of using\\nhierarchical methods and predicting on a daily basis and aggregating the data afterwards, weeks and months were\\npredicted independently to improve the performance. As the data is highly standardized through the ERP system, data\\ncleaning steps are not necessary except for format checks. Additionally, no strategies were needed for missing values as\\nthe data was automatically collected.\\n4.2 Step 2 - Validation: Model Training Model evaluation and Model Selection\\nAfter preprocessing historical data, we trained various models using these data. We differentiated between models with\\nshared weights and models individualized for each product. For the analysis, we employed deep dearning and random\\nforest with shared weights for all products in our analysis. In contrast, auto regressive methods, such as ARIMA and\\nHWES , were trained on a per-product basis. At the first step, all models were trained on the historical data up to 1\\nyear in the past and predicted the the remaining, untouched year. This validation delivered insights about the model\\nperformances. Based on that, the model with the lowest Root-Mean-Squared Error ( RMSE ) will be recommended,\\nwhich potentially will have the best prediction power for forecasting the future. As a error metrics, we used the root\\nmean squared error defined as defined in Equation 1.\\n4.3 Step 3 - Generate Forecasts: Model Finetuning, Prediction and Export\\nAfter the model validation, all models were retrained including the test data. These final models were used to predict\\nthe next 18 months or 78 weeks respectively. All models used in validation were used for prediction to allow the user\\nselecting a different model manually. The export included besides the forecast a summary over the predictions and the\\nvalidation step to allow the user to get further information of the analysis results. Furthermore, plots were generated for\\neach product visualising the decomposed time series in seasonality and trend to enable understanding of the data by the\\ncostumer.\\n5 User Testing of the Final Forecasting Pipeline\\n5.1 Overview of the User group\\nTo evaluate the forecasting pipeline under real-world condition, we selected 5 different companies for testing based on\\nthe availability of their data and their expressed interest in utilizing the forecasting pipeline. The companies A, B and C\\nwere specialized in the food and nutrition industry while company A had strong sales in winter compared to B and C.\\nThe companies D and E were focusing on the electronic industry. Company D had the characteristic, that the sales were\\nbased on projects.\\nEach company was granted access to the pipeline and the users were getting a training on how to use the pipeline in\\nproduction. Within a test phase of half a year, we assumed that these companies would implement the pipeline as\\nintended, providing meaningful insights into its effectiveness. For that, we have collected data of the predictions and the\\nactual data to evaluate the automated pipeline. The baseline model for comparison was a naive model, which predicted\\n8Automated Demand Forecasting in small to medium-sized enterprises\\nfuture values based on the mean of previous years. This model was commonly employed by the companies for planning\\npurposes, making it a relevant and practical benchmark for our analysis.\\nCompany Total Items Active Items Active Items (%) First Date Last Date\\nCompany A 2445 482 19.71 % 2014-01-01 2023-12-01\\nCompany B 4356 1234 28.33 % 2000-01-01 2024-01-01\\nCompany C 112 112 100.0 % 2014-04-01 2023-12-01\\nCompany D 783 345 44.06 % 2018-12-01 2023-12-01\\nCompany E 6317 3774 59.74 % 2006-01-01 2024-02-01\\nTable 4: Summary of the number of total items in the data set, the number of active items sold at least once in 2023, and\\nthe first and last data point.\\nTable 4 provides a summary of the data obtained from the user group, highlighting variations among companies in\\nterms of the number of items and the onset of data collection. For instance, Company D began collecting data in 2018\\nwhile Company E had the first data entry in 2006. Furthermore, it is important to note that not all items sold at that time\\nwere projected to have sales figures in the subsequent year. Consequently, the dataset included items that have recorded\\nzero sales in 2023. The number of Active Items in Table 4 gives the count of items with sales in the year 2023, which\\nwere all used for validation.\\n5.2 Experimental Set Up\\nTo assess the forecasting pipeline, we simulated its use starting from January 1st2023. In this setup, the internal\\nvalidation was performed using data from 2022, allowing us to compare the pipeline‚Äôs predictions with the actual sales\\ndata from 2023. To quantify the model performance, we used the nRMSE defined in Equation 1. This normalized error\\nmetric can be used for comparing predictions at different scales.\\nIn our evaluation, we explored several key questions. Initially, we assessed the performance of the model recommended\\nby our internal validation process by comparing the forecasts of the recommended model to the actual sales of the year\\n2023. Subsequently, we compared all models to determine which achieve highest predictive accuracy.\\nWe investigated how often a model could achieve the lowest nRMSE in 2023. By comparing the selected models to\\nthe internal validation step with the model recommendation, we could evaluate how often we have selected the best\\nmodel based on the internal validation data. To evaluate the effectiveness of the model selection process critically, the\\nresults were compared to the naive model. For visualizing the results, a box plot of the ratio was used. Afterwards a\\nformal test was used to compare the performances. We used the Wilcoxon signed rank test by [ 22] to investigate if the\\nrecommended and best model performances are significantly better than the naive estimator.\\n5.3 Results\\nFigure 5 shows the average nRMSE across all products for a company, to compare the model performances within and\\nacross the different companies. The results showed that the model performance highly fluctuates among the companies,\\ne.g.HWES leads to a nRMSE of 1.31 in Company C while in Company E the value of 10.54 is the highest. On average,\\nDeep Learning had the lowest nRMSE in Companies B, D, and E, while the lowest average error in company A and C\\nwas achieved with ARIMA.\\nThe results in Figure 6 show that based on the validation step, HWES was recommended dominantly followed by\\ntheGAM . Only in Company E, GAM andHWES were recommended equally often in 42% of the products for the\\nforecast. Surprisingly, the evaluation of the year 2023 showed that the actual best model with the lowest MAPE differs\\nfrom the recommended models. HWES still seems to be the dominant model, but Deep Learning gained importance\\nand performed in 35% of the products for Company D (Company A 27%, Company B 24%) better. Furthermore, we\\nobserved that the naive estimator was selected in 21% of the cases for Company C.\\n5.4 Relevance in Practice\\nTo elaborate the benefits of the proposed model pipeline, we compared our results of the analysis pipeline to a naive\\nestimator using the mean of the previous data points as a prediction. For evaluation, we calculated the error ratio ras\\nthe ratio of the nRMSE of our model to the one of the Naive estimator:\\n9Automated Demand Forecasting in small to medium-sized enterprises\\nFigure 5: normalized Root-Mean-Squared Error averaged over all products per model and company.\\nFigure 6: The plot shows a heatmap of how often the model was recommended based on the data from 2022 (left panel)\\nand how often the model was actually the best in 2023 (right panel).\\nr=nRMSE Model\\nnRMSE Naive\\nHere, r <1if the selected model outperforms the naive estimator. Figure 7 presents boxplots visualizing the distribution\\nof the error ratio between the recommended models and the naive estimator in different companies. Examining the\\nresults for the recommended model based on the validation of the previous year, we see the following results.\\nThe boxplot for the best-performing model indicated that the recommended as well as the best models outperformed the\\nnaive estimator across all companies. However, some recommended models performed worse compared to the naive\\n10Automated Demand Forecasting in small to medium-sized enterprises\\nFigure 7: Error Ratio to naive estimator of the recommended model in the year 2023 and the actual best model. Note:\\nThe box plot was cut ad 3.5 while some outliers are greater then 3.5.\\nestimator across all companies. The quantiles on the boxplots provide an understanding of the variability and reliability\\nof these results.\\nThe statistical tests indicated that the differences in performance between the recommended and the naive estimator for\\ncompanies A, C, D, and E were significant, while for Company B, the results were not statistically significant as shown\\nin Table 5. This confirms the visual interpretation of the box plot.\\nRecommended Model Best Model\\np-Value p-Value\\nCompany A 1.41 √ó10‚àí61.05√ó10‚àí95\\nCompany B 0.42 7.37 √ó10‚àí211\\nCompany C 9.64 √ó10‚àí81.63√ó10‚àí9\\nCompany D 2.03 √ó10‚àí131.74√ó10‚àí75\\nCompany E 9.16 √ó10‚àí270.00√ó100\\nTable 5: Exact p-values for the Wilcoxon signed rank test indicating the difference of the recommended or best model\\ncompared to the naive estimator.\\nOverall, while the best models generally showed improved performance, they did not universally exceed the naive\\nestimator‚Äôs performance across all products, particularly in companies A, B, and C. These findings highlight the need for\\nfurther refinement of models for these companies. For companies D and E, the best models demonstrated the potential\\nfor consistent and significant improvement over the naive estimator, suggesting effective model application in these\\ncontexts.\\n6 Discussion\\nThis work presents an automated data analysis pipeline used in practice. After identifying and presenting relevant\\nliterature, most promising models were preselected and implemented in an automated forecasting pipeline. Next, the\\nautomated pipeline was tested in real world by five companies. The results from the user test showed that the suggested\\ndata pipeline can outperform naive estimators by automatically recommending the right model in a validation step in 4\\nout of the 5 companies.\\nIn our model pipeline, HWES outperforms other models, which was not reflected in other research, where deep learning\\napproaches or autoregressive models delivered the best performances. [15, 3]\\nHowever, since the validation step did not identify the best model in all products, further research has to be done.\\n11Automated Demand Forecasting in small to medium-sized enterprises\\nAs the data was collected within an ERP system and was exported with all available features, the data was highly\\nstandardized and did not need individual preprocessing or feature extraction. With that, this approach might be easily\\nscaleable and adoptable. Nevertheless, the history itself might be corrupt. For instance, we saw that data maintenance\\nwithin the company could issue sales which cannot be allocated to the product identifiers, leading to lower sales in the\\nhistory. That happens, if items changed their identifier without a data entry or note in the ERP system. Furthermore, the\\ndata collected in the ERP system were sales figures. Data about rare out-of-stock events or delayed deliveries are not\\ncollected, but might influence the practical relevance. With that, the data may not match the actual demand.\\nWithin this study, we used a prediction horizon of 1.5 years. As biases increase with longer-term predictions, the\\nforecasts must be handled with care. Unforeseen events or trend change points can highly bias the predictions. This\\neffects the validation step as well, as the environmental setting can change rapidly between the validation period and the\\nprediction period. With that, the models recommended based on the validation period can be totally different from the\\nactual best models.\\nWe saw, that in the first model selection approach, Deep Learning demonstrated strong performance. However, we\\nwere unable to reproduce these results in the pilot study. Several factors could contribute to this discrepancy. Firstly,\\nthe model selection dataset included 51 manually pre-selected, well-established products, ensuring high data quality.\\nIn contrast, the pilot study utilized real-world data from the ERP system, which is inherently lower in quality due to\\nshorter histories, outliers, and unmatched identifiers. Despite efforts to reduce bias and exclude extreme item histories\\nin the data pipeline, achieving the same quality level as the pre-selected dataset proved challenging. Additionally,\\nstatistical analysis indicates significant variability in model performance due to the aforementioned data quality issues.\\nThe robustness and generalizability of the models are also impacted, highlighting the limitations of selecting models on\\nthe previous year in real-world datasets.\\nIn this paper, we were focusing on the historical data. Further research can include additional features like marketing\\nexpanses or general information like market growth, which might have an huge influence on the model performances.\\nFurthermore, future work could focus on improving data preprocessing and validation techniques and exploring a robust\\nmodel selection approach that are less sensitive to data quality variations.\\nIn conclusion, this research provides a thorough assessment of an automated demand forecasting pipeline, demonstrating\\nits potential to address key real-world challenges. Additionally, it offers valuable insights into model performance and\\nintroduces a viable approach for automated model evaluation, paving the way for more efficient and scalable forecasting\\nsolutions.\\nAcknowledgements\\nWe thank all the cooperation partners that supported the publication with their data insights. Furthermore, we thank\\nThomas Kluth, Antje Heine, Hans-Peter Hantsch, Katharina Kluth from abacus edv-l√∂sungen for their experience from\\nindustry, technical support, and Juliana Schneider from Hasso-Plattner-Institute for scientific discussions.\\nFunding sources\\nThis work received funding from abacus edv-l√∂sungen GmbH & Co. KG .\\nCompeting Interests\\nThe authors declare that this work received funding from abacus edv-l√∂sungen GmbH & Co. KG , which also uses\\nthe described pipeline commercially. However, the research and findings were conducted independently and were not\\ninfluenced by financial or commercial interests.\\nReferences\\n[1]Navneet Vairagade, Doina Logofatu, Florin Leon, and Fitore Muharemi. Demand Forecasting Using Random\\nForest and Artificial Neural Network for Supply Chain Management. In Ngoc Thanh Nguyen, Richard Chbeir,\\nErnesto Exposito, Philippe Aniort√©, and Bogdan Trawi ¬¥nski, editors, Computational Collective Intelligence , pages\\n328‚Äì339, Cham, 2019. Springer International Publishing.\\n[2]Sean J. Taylor and Benjamin Letham. Forecasting at scale. Technical Report e3190v2, PeerJ Inc., September\\n2017. ISSN: 2167-9843.\\n12Automated Demand Forecasting in small to medium-sized enterprises\\n[3]Marco Alfonse Hassan El Madany. Procurement Forecasting in Enterprise Resource and Planning (ERP) System\\nusing Hybrid Time Series Model. Journal of Southwest Jiaotong University , 57(3), 2022. Number: 3.\\n[4] R.G. Brown. Exponential Smoothing for Predicting Demand . Little, 1956.\\n[5]Charles C. Holt. Forecasting seasonals and trends by exponentially weighted moving averages. International\\nJournal of Forecasting , 20(1):5‚Äì10, January 2004.\\n[6]Peter R. Winters. Forecasting Sales by Exponentially Weighted Moving Averages. Management Science ,\\n6(3):324‚Äì342, April 1960. Publisher: INFORMS.\\n[7]G.E.P. Box and G.M. Jenkins. Time Series Analysis: Forecasting and Control . Holden-Day series in time series\\nanalysis and digital processing. Holden-Day, 1970.\\n[8]Andrew C. Harvey. Forecasting, Structural Time Series Models and the Kalman Filter . Cambridge University\\nPress, Cambridge, 1990.\\n[9]Houssainy El, Amal Mohamed, and Haitham Fawzy. Time Series Forecasting Using Tree Based Methods. Journal\\nof Statistics Applications & Probability , 10:229, March 2021.\\n[10] Muhammad Syafrudin, Ganjar Alfian, Norma Latif Fitriyani, Imam Fahrurrozi, Muhammad Anshari, and Jongtae\\nRhee. A Personalized Blood Glucose Prediction Model Using Random Forest Regression. In 2022 ASU\\nInternational Conference in Emerging Technologies for Sustainability and Intelligent Systems (ICETSIS) , pages\\n295‚Äì299, June 2022.\\n[11] Sepp Hochreiter and J√ºrgen Schmidhuber. Long Short-term Memory. Neural computation , 9:1735‚Äì80, December\\n1997.\\n[12] Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the Properties of Neural\\nMachine Translation: Encoder-Decoder Approaches, October 2014. arXiv:1409.1259 [cs, stat].\\n[13] Irena Koprinska, Dengsong Wu, and Zheng Wang. Convolutional Neural Networks for Energy Time Series\\nForecasting. In 2018 International Joint Conference on Neural Networks (IJCNN) , pages 1‚Äì8, July 2018. ISSN:\\n2161-4407.\\n[14] Anastasia Borovykh, Sander Bohte, and Cornelis W. Oosterlee. Dilated convolutional neural networks for time\\nseries forecasting. Journal of Computational Finance , October 2018.\\n[15] Co¬∏ skun Hamza√ßebi, Diyar Akay, and Fevzi Kutay. Comparison of direct and iterative artificial neural network\\nforecast approaches in multi-periodic time series forecasting. Expert Systems with Applications , 36(2, Part\\n2):3839‚Äì3844, March 2009.\\n[16] Vicky Chrystian Sugiarto, Riyanarto Sarno, and Dwi Sunaryono. Sales forecasting using Holt-Winters in\\nEnterprise Resource Planning at sales and distribution module. In Proceedings of 2016 International Conference\\non Information and Communication Technology and Systems, ICTS 2016 , pages 8‚Äì13. Institute of Electrical and\\nElectronics Engineers Inc., April 2017.\\n[17] Katarzyna Grobler-Debska, Bart≈Çomiej ÀôZak, Jerzy Baranowski, Edyta Kucharska, and Adam Domagala. Research\\non effective analysis and forecasting of demand in ERP systems - case studies. In 2021 25th International\\nConference on Methods and Models in Automation and Robotics (MMAR) , pages 291‚Äì296, August 2021.\\n[18] Taylor G. Smith and others. pmdarima: ARIMA estimators for Python, 2017.\\n[19] R.J. Hyndman and G. Athanasopoulos. Forecasting: principles and practice . OTexts, 2014.\\n[20] Tianqi Chen and Carlos Guestrin. XGBoost: A Scalable Tree Boosting System. In Proceedings of the 22nd ACM\\nSIGKDD International Conference on Knowledge Discovery and Data Mining , pages 785‚Äì794, August 2016.\\narXiv:1603.02754 [cs].\\n[21] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,\\nV . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn:\\nMachine Learning in Python. Journal of Machine Learning Research , 12:2825‚Äì2830, 2011.\\n[22] Denise Rey and Markus Neuh√§user. Wilcoxon-Signed-Rank Test. In Miodrag Lovric, editor, International\\nEncyclopedia of Statistical Science , pages 1658‚Äì1659. Springer Berlin Heidelberg, Berlin, Heidelberg, 2011.\\n13Automated Demand Forecasting in small to medium-sized enterprises\\nAppendix\\nnRMSE MAPE\\nMethod Mean STD Mean STD\\nCNN (Dilated) 0.3119 0.1132 33.19 % 0.1623\\nCNN 0.3121 0.1403 37.01 % 0.2399\\nCNN (pretrained) 0.3138 0.1468 35.07 % 0.2123\\nCNN (FIXED) 0.3247 0.1621 36.83 % 0.2515\\nTree-Based (XGBoost, log) 0.3456 0.1571 34.99 % 0.1481\\nTree-Based (XGBoost) 0.3463 0.1876 36.64 % 0.1858\\nGRU 0.3498 0.1897 47.33 % 0.4430\\nLSTM (3) 0.3573 0.1082 45.01 % 0.1759\\nCNN (DAY) 0.3574 0.1312 36.96 % 0.1674\\nMLP 0.3642 0.2292 42.46 % 0.3151\\nLSTM (1) 0.3649 0.0622 40.05 % 0.1371\\nEnsemble (median) 0.3693 0.2038 42.91 % 0.2964\\nEnsemble (median, dependent) 0.3757 0.2055 44.27 % 0.3071\\nARIMA (Seasonal) 0.3832 0.2191 44.67 % 0.3232\\nFacebook-Prophet (Day) 0.3873 0.3495 50.2 % 0.5405\\nARIMA (Seasonal, median, dependent) 0.3906 0.2158 46.18 % 0.3207\\nARIMA (Seasonal, mean, dependent) 0.3924 0.2151 45.89 % 0.3229\\nARIMA (median, dependent) 0.3982 0.1947 48.78 % 0.2897\\nARIMA (mean, dependent) 0.3987 0.1947 49.28 % 0.2872\\nCNN (FIXED, DAY) 0.4026 0.4156 41.35 % 0.3529\\nTree-Based (Random-Forest, median, dependent) 0.4028 0.1952 47.92 % 0.2973\\nTree-Based (Random-Forest) 0.4029 0.1910 48.91 % 0.2959\\nTree-Based (Random-Forest, mean, dependent) 0.4042 0.1955 47.74 % 0.2891\\nARIMA 0.4105 0.1860 53.0 % 0.2990\\nGAM 0.4202 0.2577 52.81 % 0.3089\\nLSTM (2) 0.5097 0.2448 60.09 % 0.3311\\nFacebook-Prophet 0.5164 0.6405 76.06 % 1.1999\\nSmoothing (HWES, median, dependent) 0.5582 0.7761 64.59 % 0.8684\\nEnsemble (mean, dependent) 0.6119 1.5512 53.27 % 0.5645\\nSmoothing (ES) 0.6498 0.5617 93.4 % 0.5976\\nPytorchTS 0.7556 0.3324 86.08 % 0.0644\\nSmoothing (HWES, mean, dependent) 1.3867 6.2870 85.22 % 1.9953\\nEnsemble (mean) 1.4689 6.4076 89.94 % 1.9253\\nSmoothing (HWES) 4.8962 25.8723 236.79 % 7.7315\\nSupplementary Table 6: All Model Performances sorted by the mean of the normalized Root-Mean-Squared Error\\n(nRMSE ). Furthermore, the Standard Deviation ( STD) and Mean Absolute Percentage Error ( MAPE ) are presented.\\nAfter the top-10, there is a horizontal line and models used for the prototype are highlighted italic.\\n14']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7147398c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3cf592",
   "metadata": {},
   "source": [
    "Now we create a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ccd5264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(headers, columns = [\"Title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c54dd812",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Text\"] = text_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e786a3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Doc 2407.13549v1.pdf</td>\n",
       "      <td>Evaluating the effect of viral news on social ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Doc 2407.16014v1.pdf</td>\n",
       "      <td>Political Elites in the Attention Economy: Vis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Doc 2407.18471v1.pdf</td>\n",
       "      <td>Constructing the CORD-19 Vaccine Dataset\\nMani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Doc 2408.07322v1.pdf</td>\n",
       "      <td>arXiv:2408.07322v1  [cs.IT]  14 Aug 2024Encodi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Doc 2408.08126v1.pdf</td>\n",
       "      <td>Decoding Memes: A Comparative Study of Machine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Doc 2408.08437v1.pdf</td>\n",
       "      <td>PQV-Mobile: A Combined Pruning and Quantizatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Doc 2408.08964v3.pdf</td>\n",
       "      <td>BNSENTMIX: A Diverse Bengali-English Code-Mixe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Doc 2408.09435v1.pdf</td>\n",
       "      <td>A modified Ricci flow on arbitrary weighted gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Doc 2408.09683v1.pdf</td>\n",
       "      <td>SMART-TBI: Design and Evaluation of the Social...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Doc 2408.09725v1.pdf</td>\n",
       "      <td>State surveillance in the digital age: Factors...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Doc 2408.12449v2.pdf</td>\n",
       "      <td>Looking AT the Blue Skies of Bluesky\\nLeonhard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Doc 2408.12743v2.pdf</td>\n",
       "      <td>arXiv:2408.12743v2  [cs.CR]  7 Dec 2024The Mat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Doc 2408.12753v1.pdf</td>\n",
       "      <td>Contrastive Representation Learning for Dynami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Doc 2409.01470v1.pdf</td>\n",
       "      <td>Phantom: Untargeted Poisoning Attacks on Semi-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Doc 2409.02358v1.pdf</td>\n",
       "      <td>422Teen Talk: The Good, the Bad, and the Neutr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Doc 2409.08405v1.pdf</td>\n",
       "      <td>Consistent Strong Triadic Closure in\\nMultilay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Doc 2409.13461v1.pdf</td>\n",
       "      <td>Engagement, Content Quality and Ideology over ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Doc 2409.15652v3.pdf</td>\n",
       "      <td>2024 2nd International Conference on Informati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Doc 2409.18393v1.pdf</td>\n",
       "      <td>Social media algorithms can curb misinformatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Doc 2409.18931v1.pdf</td>\n",
       "      <td>Social Media Bot Policies:\\nEvaluating Passive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Doc 2410.01708v1.pdf</td>\n",
       "      <td>Examining the Role of Relationship Alignment i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Doc 2410.05401v1.pdf</td>\n",
       "      <td>Post-hoc Study of Climate Microtargeting on So...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Doc 2410.06443v1.pdf</td>\n",
       "      <td>Categorizing Social Media Screenshots for Iden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Doc 2410.14617v1.pdf</td>\n",
       "      <td>On the Use of Proxies in Political Ad Targetin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Doc 2410.16977v1.pdf</td>\n",
       "      <td>IPL: Leveraging Multimodal Large Language Mode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Doc 2410.17496v1.pdf</td>\n",
       "      <td>Measuring Network Dynamics of Opioid Overdose ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Doc 2410.20293v2.pdf</td>\n",
       "      <td>1 \\n \\n \\n \\nA Systematic Review of Machine L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Doc 2410.22716v1.pdf</td>\n",
       "      <td>The 2024 Election Integrity Initiative\\nExposi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Doc 2411.04542v1.pdf</td>\n",
       "      <td>Automatic Identification of Political Hate Art...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Doc 2411.04752v1.pdf</td>\n",
       "      <td>RetrieveGPT: Merging Prompts and Mathematical ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Doc 2411.05043v1.pdf</td>\n",
       "      <td>\\n 1 \\nARTICLE INFORMATION  \\nArticle title  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Doc 2411.05788v1.pdf</td>\n",
       "      <td>News-Driven Stock Price Forecasting in Indian\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Doc 2411.06122v1.pdf</td>\n",
       "      <td>Characteristics of Political Misinformation Ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Doc 2411.09214v1.pdf</td>\n",
       "      <td>HateGPT: Unleashing GPT-3.5 Turbo to Combat Ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Doc 2411.11426v1.pdf</td>\n",
       "      <td>SpiderDAN: Matching Augmentation in Demand-Awa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Doc 2411.12508v1.pdf</td>\n",
       "      <td>Probe-Me-Not: Protecting Pre-trained Encoders\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Doc 2411.14613v1.pdf</td>\n",
       "      <td>\\n \\n  1 \\n Optimal Transcoding Preset Sele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Doc 2411.16285v1.pdf</td>\n",
       "      <td>1\\nA Graph Neural Architecture Search Approach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Doc 2411.16826v1.pdf</td>\n",
       "      <td>Characterizing the Fragmentation of the Social...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Doc 2412.02349v1.pdf</td>\n",
       "      <td>CTRAPS: CTAP Client Impersonation and API Conf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Doc 2412.04484v1.pdf</td>\n",
       "      <td>Epinet for Content Cold Start\\nHong Jun Jeon\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Doc 2412.05861v1.pdf</td>\n",
       "      <td>Depression detection from Social Media Bangla\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Doc 2412.07550v1.pdf</td>\n",
       "      <td>Use of diverse data sources to control which\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Doc 2412.08484v1.pdf</td>\n",
       "      <td>ConvMesh: Reimagining Mesh Quality\\nThrough Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Doc 2412.08648v1.pdf</td>\n",
       "      <td>Detecting Visual Triggers in Cannabis Imagery:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Doc 2412.14985v1.pdf</td>\n",
       "      <td>Exploration of the Dynamics of Buy and Sale of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Doc 2412.15072v1.pdf</td>\n",
       "      <td>ScamChatBot: An End-to-End Analysis of Fake Ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Doc 2412.15621v1.pdf</td>\n",
       "      <td>Pirates of Charity : Exploring Donation-based ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Doc 2412.18779v1.pdf</td>\n",
       "      <td>Integrating Zero -Shot Classification to Advan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Doc 2412.20420v1.pdf</td>\n",
       "      <td>AUTOMATED DEMAND FORECASTING IN SMALL TO\\nMEDI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Title                                               Text\n",
       "0   Doc 2407.13549v1.pdf  Evaluating the effect of viral news on social ...\n",
       "1   Doc 2407.16014v1.pdf  Political Elites in the Attention Economy: Vis...\n",
       "2   Doc 2407.18471v1.pdf  Constructing the CORD-19 Vaccine Dataset\\nMani...\n",
       "3   Doc 2408.07322v1.pdf  arXiv:2408.07322v1  [cs.IT]  14 Aug 2024Encodi...\n",
       "4   Doc 2408.08126v1.pdf  Decoding Memes: A Comparative Study of Machine...\n",
       "5   Doc 2408.08437v1.pdf  PQV-Mobile: A Combined Pruning and Quantizatio...\n",
       "6   Doc 2408.08964v3.pdf  BNSENTMIX: A Diverse Bengali-English Code-Mixe...\n",
       "7   Doc 2408.09435v1.pdf  A modified Ricci flow on arbitrary weighted gr...\n",
       "8   Doc 2408.09683v1.pdf  SMART-TBI: Design and Evaluation of the Social...\n",
       "9   Doc 2408.09725v1.pdf  State surveillance in the digital age: Factors...\n",
       "10  Doc 2408.12449v2.pdf  Looking AT the Blue Skies of Bluesky\\nLeonhard...\n",
       "11  Doc 2408.12743v2.pdf  arXiv:2408.12743v2  [cs.CR]  7 Dec 2024The Mat...\n",
       "12  Doc 2408.12753v1.pdf  Contrastive Representation Learning for Dynami...\n",
       "13  Doc 2409.01470v1.pdf  Phantom: Untargeted Poisoning Attacks on Semi-...\n",
       "14  Doc 2409.02358v1.pdf  422Teen Talk: The Good, the Bad, and the Neutr...\n",
       "15  Doc 2409.08405v1.pdf  Consistent Strong Triadic Closure in\\nMultilay...\n",
       "16  Doc 2409.13461v1.pdf  Engagement, Content Quality and Ideology over ...\n",
       "17  Doc 2409.15652v3.pdf  2024 2nd International Conference on Informati...\n",
       "18  Doc 2409.18393v1.pdf  Social media algorithms can curb misinformatio...\n",
       "19  Doc 2409.18931v1.pdf  Social Media Bot Policies:\\nEvaluating Passive...\n",
       "20  Doc 2410.01708v1.pdf  Examining the Role of Relationship Alignment i...\n",
       "21  Doc 2410.05401v1.pdf  Post-hoc Study of Climate Microtargeting on So...\n",
       "22  Doc 2410.06443v1.pdf  Categorizing Social Media Screenshots for Iden...\n",
       "23  Doc 2410.14617v1.pdf  On the Use of Proxies in Political Ad Targetin...\n",
       "24  Doc 2410.16977v1.pdf  IPL: Leveraging Multimodal Large Language Mode...\n",
       "25  Doc 2410.17496v1.pdf  Measuring Network Dynamics of Opioid Overdose ...\n",
       "26  Doc 2410.20293v2.pdf   1 \\n \\n \\n \\nA Systematic Review of Machine L...\n",
       "27  Doc 2410.22716v1.pdf  The 2024 Election Integrity Initiative\\nExposi...\n",
       "28  Doc 2411.04542v1.pdf  Automatic Identification of Political Hate Art...\n",
       "29  Doc 2411.04752v1.pdf  RetrieveGPT: Merging Prompts and Mathematical ...\n",
       "30  Doc 2411.05043v1.pdf   \\n 1 \\nARTICLE INFORMATION  \\nArticle title  ...\n",
       "31  Doc 2411.05788v1.pdf  News-Driven Stock Price Forecasting in Indian\\...\n",
       "32  Doc 2411.06122v1.pdf  Characteristics of Political Misinformation Ov...\n",
       "33  Doc 2411.09214v1.pdf  HateGPT: Unleashing GPT-3.5 Turbo to Combat Ha...\n",
       "34  Doc 2411.11426v1.pdf  SpiderDAN: Matching Augmentation in Demand-Awa...\n",
       "35  Doc 2411.12508v1.pdf  Probe-Me-Not: Protecting Pre-trained Encoders\\...\n",
       "36  Doc 2411.14613v1.pdf     \\n \\n  1 \\n Optimal Transcoding Preset Sele...\n",
       "37  Doc 2411.16285v1.pdf  1\\nA Graph Neural Architecture Search Approach...\n",
       "38  Doc 2411.16826v1.pdf  Characterizing the Fragmentation of the Social...\n",
       "39  Doc 2412.02349v1.pdf  CTRAPS: CTAP Client Impersonation and API Conf...\n",
       "40  Doc 2412.04484v1.pdf  Epinet for Content Cold Start\\nHong Jun Jeon\\n...\n",
       "41  Doc 2412.05861v1.pdf  Depression detection from Social Media Bangla\\...\n",
       "42  Doc 2412.07550v1.pdf  Use of diverse data sources to control which\\n...\n",
       "43  Doc 2412.08484v1.pdf  ConvMesh: Reimagining Mesh Quality\\nThrough Co...\n",
       "44  Doc 2412.08648v1.pdf  Detecting Visual Triggers in Cannabis Imagery:...\n",
       "45  Doc 2412.14985v1.pdf  Exploration of the Dynamics of Buy and Sale of...\n",
       "46  Doc 2412.15072v1.pdf  ScamChatBot: An End-to-End Analysis of Fake Ac...\n",
       "47  Doc 2412.15621v1.pdf  Pirates of Charity : Exploring Donation-based ...\n",
       "48  Doc 2412.18779v1.pdf  Integrating Zero -Shot Classification to Advan...\n",
       "49  Doc 2412.20420v1.pdf  AUTOMATED DEMAND FORECASTING IN SMALL TO\\nMEDI..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b48763a",
   "metadata": {},
   "source": [
    "And finally we save it into our laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f5e3c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"pdf_facebook.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5f1901",
   "metadata": {},
   "source": [
    "# EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cba26d0",
   "metadata": {},
   "source": [
    "Now let's create a new corpus of PDFs from ArXiv to use in our future data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a418c42",
   "metadata": {},
   "source": [
    "* In this same folder you will find a notebook containing code to do an ArXiv query. So far we have used the key terms \"twitter\" and \"facebook\". Try doing a new one using a key term that is interesting to you. If you would like to use two words (Mark Zuckerberg, Climate Change, Donald Trump...) use this syntax: '\"climate change\"' (quotations inside quotations).\n",
    "\n",
    "* Now that you have your data, **duplicate this notebook** to have an extra copy of your code. Call the new version \"PDF text extraction EXERCISE\".\n",
    "\n",
    "* Once you have dubplicated your notebook and acquired your data, then first extract the text of one PDF (just like we do in here). Remember to change the name to the new file!\n",
    "\n",
    "* Then repeat the process with the whole folder. Remember to: \n",
    "                * Change the name of the folder in path_dir\n",
    "                * Change the name of the csv data folder containing your data to not overwrite your previous data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
